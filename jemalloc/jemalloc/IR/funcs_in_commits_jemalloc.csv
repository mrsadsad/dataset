pa_shard_nactive,"size_t
pa_shard_nactive(pa_shard_t *shard) {
	return atomic_load_zu(&shard->nactive, ATOMIC_RELAXED);
}",pa_shard_ndirty,"size_t
pa_shard_ndirty(pa_shard_t *shard) {
	size_t ndirty = ecache_npages_get(&shard->pac.ecache_dirty);
	if (shard->ever_used_hpa) {
		ndirty += psset_ndirty(&shard->hpa_shard.psset);
	}
	return ndirty;
}",pa_shard_nmuzzy,"size_t
pa_shard_nmuzzy(pa_shard_t *shard) {
	return ecache_npages_get(&shard->pac.ecache_muzzy);
}"
do_fill_test,"void
do_fill_test(cache_bin_t *bin, void **ptrs, cache_bin_sz_t ncached_max,
    cache_bin_sz_t nfill_attempt, cache_bin_sz_t nfill_succeed) {
	bool success;
	void *ptr;
	assert_true(cache_bin_ncached_get_local(bin) == 0, """");
	CACHE_BIN_PTR_ARRAY_DECLARE(arr, nfill_attempt);
	cache_bin_init_ptr_array_for_fill(bin, &arr, nfill_attempt);
	for (cache_bin_sz_t i = 0; i < nfill_succeed; i++) {
		arr.ptr[i] = &ptrs[i];
	}
	cache_bin_finish_fill(bin, &arr, nfill_succeed);
	expect_true(cache_bin_ncached_get_local(bin) == nfill_succeed,
	    """");
	cache_bin_low_water_set(bin);

	for (cache_bin_sz_t i = 0; i < nfill_succeed; i++) {
		ptr = cache_bin_alloc(bin, &success);
		expect_true(success, """");
		expect_ptr_eq(ptr, (void *)&ptrs[i],
		    ""Should pop in order filled"");
		expect_true(cache_bin_low_water_get(bin)
		    == nfill_succeed - i - 1, """");
	}
	expect_true(cache_bin_ncached_get_local(bin) == 0, """");
	expect_true(cache_bin_low_water_get(bin) == 0, """");
}",do_flush_test,"void
do_flush_test(cache_bin_t *bin, void **ptrs, cache_bin_sz_t nfill,
    cache_bin_sz_t nflush) {
	bool success;
	assert_true(cache_bin_ncached_get_local(bin) == 0, """");

	for (cache_bin_sz_t i = 0; i < nfill; i++) {
		success = cache_bin_dalloc_easy(bin, &ptrs[i]);
		expect_true(success, """");
	}

	CACHE_BIN_PTR_ARRAY_DECLARE(arr, nflush);
	cache_bin_init_ptr_array_for_flush(bin, &arr, nflush);
	for (cache_bin_sz_t i = 0; i < nflush; i++) {
		expect_ptr_eq(arr.ptr[i], &ptrs[nflush - i - 1], """");
	}
	cache_bin_finish_flush(bin, &arr, nflush);

	expect_true(cache_bin_ncached_get_local(bin) == nfill - nflush,
	    """");
	while (cache_bin_ncached_get_local(bin) > 0) {
		cache_bin_alloc(bin, &success);
	}
}",do_batch_alloc_test,"void
do_batch_alloc_test(cache_bin_t *bin, void **ptrs, cache_bin_sz_t nfill,
    size_t batch) {
	assert_true(cache_bin_ncached_get_local(bin) == 0, """");
	CACHE_BIN_PTR_ARRAY_DECLARE(arr, nfill);
	cache_bin_init_ptr_array_for_fill(bin, &arr, nfill);
	for (cache_bin_sz_t i = 0; i < nfill; i++) {
		arr.ptr[i] = &ptrs[i];
	}
	cache_bin_finish_fill(bin, &arr, nfill);
	assert_true(cache_bin_ncached_get_local(bin) == nfill, """");
	cache_bin_low_water_set(bin);

	void **out = malloc((batch + 1) * sizeof(void *));
	size_t n = cache_bin_alloc_batch(bin, batch, out);
	assert_true(n == ((size_t)nfill < batch ? (size_t)nfill : batch), """");
	for (cache_bin_sz_t i = 0; i < (cache_bin_sz_t)n; i++) {
		expect_ptr_eq(out[i], &ptrs[i], """");
	}
	expect_true(cache_bin_low_water_get(bin) == nfill -
	    (cache_bin_sz_t)n, """");
	while (cache_bin_ncached_get_local(bin) > 0) {
		bool success;
		cache_bin_alloc(bin, &success);
	}
	free(out);
}",do_flush_stashed_test,"void
do_flush_stashed_test(cache_bin_t *bin, void **ptrs, cache_bin_sz_t nfill,
    cache_bin_sz_t nstash) {
	expect_true(cache_bin_ncached_get_local(bin) == 0,
	    ""Bin not empty"");
	expect_true(cache_bin_nstashed_get_local(bin) == 0,
	    ""Bin not empty"");
	expect_true(nfill + nstash <= bin->bin_info.ncached_max, ""Exceeded max"");

	bool ret;
	/* Fill */
	for (cache_bin_sz_t i = 0; i < nfill; i++) {
		ret = cache_bin_dalloc_easy(bin, &ptrs[i]);
		expect_true(ret, ""Unexpected fill failure"");
	}
	expect_true(cache_bin_ncached_get_local(bin) == nfill,
	    ""Wrong cached count"");

	/* Stash */
	for (cache_bin_sz_t i = 0; i < nstash; i++) {
		ret = cache_bin_stash(bin, &ptrs[i + nfill]);
		expect_true(ret, ""Unexpected stash failure"");
	}
	expect_true(cache_bin_nstashed_get_local(bin) == nstash,
	    ""Wrong stashed count"");

	if (nfill + nstash == bin->bin_info.ncached_max) {
		ret = cache_bin_dalloc_easy(bin, &ptrs[0]);
		expect_false(ret, ""Should not dalloc into a full bin"");
		ret = cache_bin_stash(bin, &ptrs[0]);
		expect_false(ret, ""Should not stash into a full bin"");
	}

	/* Alloc filled ones */
	for (cache_bin_sz_t i = 0; i < nfill; i++) {
		void *ptr = cache_bin_alloc(bin, &ret);
		expect_true(ret, ""Unexpected alloc failure"");
		/* Verify it's not from the stashed range. */
		expect_true((uintptr_t)ptr < (uintptr_t)&ptrs[nfill],
		    ""Should not alloc stashed ptrs"");
	}
	expect_true(cache_bin_ncached_get_local(bin) == 0,
	    ""Wrong cached count"");
	expect_true(cache_bin_nstashed_get_local(bin) == nstash,
	    ""Wrong stashed count"");

	cache_bin_alloc(bin, &ret);
	expect_false(ret, ""Should not alloc stashed"");

	/* Clear stashed ones */
	cache_bin_finish_flush_stashed(bin);
	expect_true(cache_bin_ncached_get_local(bin) == 0,
	    ""Wrong cached count"");
	expect_true(cache_bin_nstashed_get_local(bin) == 0,
	    ""Wrong stashed count"");

	cache_bin_alloc(bin, &ret);
	expect_false(ret, ""Should not alloc from empty bin"");
}"
tsd_tcache_data_init,"bool
tsd_tcache_data_init(tsd_t *tsd, arena_t *arena,
    const cache_bin_info_t tcache_bin_info[TCACHE_NBINS_MAX]) {
	assert(tcache_bin_info != NULL);
	return tsd_tcache_data_init_impl(tsd, arena, tcache_bin_info);
}",tcache_bin_info_compute,"void
tcache_bin_info_compute(cache_bin_info_t tcache_bin_info[TCACHE_NBINS_MAX]) {
	/*
	 * Compute the values for each bin, but for bins with indices larger
	 * than tcache_nbins, no items will be cached.
	 */
	for (szind_t i = 0; i < TCACHE_NBINS_MAX; i++) {
		unsigned ncached_max = tcache_get_default_ncached_max_set(i) ?
		    (unsigned)tcache_get_default_ncached_max()[i].ncached_max:
		    tcache_ncached_max_compute(i);
		assert(ncached_max <= CACHE_BIN_NCACHED_MAX);
		cache_bin_info_init(&tcache_bin_info[i],
		    (cache_bin_sz_t)ncached_max);
	}
}"
tcache_bin_flush_stashed,"void
tcache_bin_flush_stashed(tsd_t *tsd, tcache_t *tcache, cache_bin_t *cache_bin,
    szind_t binind, bool is_small) {
	assert(!tcache_bin_disabled(binind, cache_bin, tcache->tcache_slow));
	/*
	 * The two below are for assertion only.  The content of original cached
	 * items remain unchanged -- the stashed items reside on the other end
	 * of the stack.  Checking the stack head and ncached to verify.
	 */
	void *head_content = *cache_bin->stack_head;
	cache_bin_sz_t orig_cached = cache_bin_ncached_get_local(cache_bin);

	cache_bin_sz_t nstashed = cache_bin_nstashed_get_local(cache_bin);
	assert(orig_cached + nstashed <= cache_bin_ncached_max_get(cache_bin));
	if (nstashed == 0) {
		return;
	}

	CACHE_BIN_PTR_ARRAY_DECLARE(ptrs, nstashed);
	cache_bin_init_ptr_array_for_stashed(cache_bin, binind, &ptrs,
	    nstashed);
	san_check_stashed_ptrs(ptrs.ptr, nstashed, sz_index2size(binind));
	tcache_bin_flush_impl(tsd, tcache, cache_bin, binind, &ptrs, nstashed,
	    is_small);
	cache_bin_finish_flush_stashed(cache_bin);

	assert(cache_bin_nstashed_get_local(cache_bin) == 0);
	assert(cache_bin_ncached_get_local(cache_bin) == orig_cached);
	assert(head_content == *cache_bin->stack_head);
}",tcache_cleanup,"void
tcache_cleanup(tsd_t *tsd) {
	tcache_t *tcache = tsd_tcachep_get(tsd);
	if (!tcache_available(tsd)) {
		assert(tsd_tcache_enabled_get(tsd) == false);
		assert(cache_bin_still_zero_initialized(&tcache->bins[0]));
		return;
	}
	assert(tsd_tcache_enabled_get(tsd));
	assert(!cache_bin_still_zero_initialized(&tcache->bins[0]));

	tcache_destroy(tsd, tcache, true);
	/* Make sure all bins used are reinitialized to the clean state. */
	memset(tcache->bins, 0, sizeof(cache_bin_t) * TCACHE_NBINS_MAX);
}"
cache_bin_info_init,"void
cache_bin_info_init(cache_bin_info_t *info,
    cache_bin_sz_t ncached_max) {
	assert(ncached_max <= CACHE_BIN_NCACHED_MAX);
	size_t stack_size = (size_t)ncached_max * sizeof(void *);
	assert(stack_size < ((size_t)1 << (sizeof(cache_bin_sz_t) * 8)));
	info->ncached_max = (cache_bin_sz_t)ncached_max;
}",tcache_ncached_max_compute,"unsigned
tcache_ncached_max_compute(szind_t szind) {
	if (szind >= SC_NBINS) {
		return opt_tcache_nslots_large;
	}
	unsigned slab_nregs = bin_infos[szind].nregs;

	/* We may modify these values; start with the opt versions. */
	unsigned nslots_small_min = opt_tcache_nslots_small_min;
	unsigned nslots_small_max = opt_tcache_nslots_small_max;

	/*
	 * Clamp values to meet our constraints -- even, nonzero, min < max, and
	 * suitable for a cache bin size.
	 */
	if (opt_tcache_nslots_small_max > CACHE_BIN_NCACHED_MAX) {
		nslots_small_max = CACHE_BIN_NCACHED_MAX;
	}
	if (nslots_small_min % 2 != 0) {
		nslots_small_min++;
	}
	if (nslots_small_max % 2 != 0) {
		nslots_small_max--;
	}
	if (nslots_small_min < 2) {
		nslots_small_min = 2;
	}
	if (nslots_small_max < 2) {
		nslots_small_max = 2;
	}
	if (nslots_small_min > nslots_small_max) {
		nslots_small_min = nslots_small_max;
	}

	unsigned candidate;
	if (opt_lg_tcache_nslots_mul < 0) {
		candidate = slab_nregs >> (-opt_lg_tcache_nslots_mul);
	} else {
		candidate = slab_nregs << opt_lg_tcache_nslots_mul;
	}
	if (candidate % 2 != 0) {
		/*
		 * We need the candidate size to be even -- we assume that we
		 * can divide by two and get a positive number (e.g. when
		 * flushing).
		 */
		++candidate;
	}
	if (candidate <= nslots_small_min) {
		return nslots_small_min;
	} else if (candidate <= nslots_small_max) {
		return candidate;
	} else {
		return nslots_small_max;
	}
}",tcache_bin_info_compute,"void
tcache_bin_info_compute(cache_bin_info_t tcache_bin_info[TCACHE_NBINS_MAX]) {
	/*
	 * Compute the values for each bin, but for bins with indices larger
	 * than tcache_nbins, no items will be cached.
	 */
	for (szind_t i = 0; i < TCACHE_NBINS_MAX; i++) {
		unsigned ncached_max = tcache_get_default_ncached_max_set(i) ?
		    (unsigned)tcache_get_default_ncached_max()[i].ncached_max:
		    tcache_ncached_max_compute(i);
		assert(ncached_max <= CACHE_BIN_NCACHED_MAX);
		cache_bin_info_init(&tcache_bin_info[i],
		    (cache_bin_sz_t)ncached_max);
	}
}",tcache_default_settings_init,"void
tcache_default_settings_init(tcache_slow_t *tcache_slow) {
	assert(tcache_slow != NULL);
	assert(global_do_not_change_tcache_maxclass != 0);
	assert(global_do_not_change_tcache_nbins != 0);
	tcache_slow->tcache_nbins = global_do_not_change_tcache_nbins;
}",tsd_tcache_data_init,"bool
tsd_tcache_data_init(tsd_t *tsd, arena_t *arena,
    const cache_bin_info_t tcache_bin_info[TCACHE_NBINS_MAX]) {
	assert(tcache_bin_info != NULL);
	return tsd_tcache_data_init_impl(tsd, arena, tcache_bin_info);
}",tcache_cleanup,"void
tcache_cleanup(tsd_t *tsd) {
	tcache_t *tcache = tsd_tcachep_get(tsd);
	if (!tcache_available(tsd)) {
		assert(tsd_tcache_enabled_get(tsd) == false);
		assert(cache_bin_still_zero_initialized(&tcache->bins[0]));
		return;
	}
	assert(tsd_tcache_enabled_get(tsd));
	assert(!cache_bin_still_zero_initialized(&tcache->bins[0]));

	tcache_destroy(tsd, tcache, true);
	/* Make sure all bins used are reinitialized to the clean state. */
	memset(tcache->bins, 0, sizeof(cache_bin_t) * TCACHE_NBINS_MAX);
}",do_fill_test,"void
do_fill_test(cache_bin_t *bin, void **ptrs, cache_bin_sz_t ncached_max,
    cache_bin_sz_t nfill_attempt, cache_bin_sz_t nfill_succeed) {
	bool success;
	void *ptr;
	assert_true(cache_bin_ncached_get_local(bin) == 0, """");
	CACHE_BIN_PTR_ARRAY_DECLARE(arr, nfill_attempt);
	cache_bin_init_ptr_array_for_fill(bin, &arr, nfill_attempt);
	for (cache_bin_sz_t i = 0; i < nfill_succeed; i++) {
		arr.ptr[i] = &ptrs[i];
	}
	cache_bin_finish_fill(bin, &arr, nfill_succeed);
	expect_true(cache_bin_ncached_get_local(bin) == nfill_succeed,
	    """");
	cache_bin_low_water_set(bin);

	for (cache_bin_sz_t i = 0; i < nfill_succeed; i++) {
		ptr = cache_bin_alloc(bin, &success);
		expect_true(success, """");
		expect_ptr_eq(ptr, (void *)&ptrs[i],
		    ""Should pop in order filled"");
		expect_true(cache_bin_low_water_get(bin)
		    == nfill_succeed - i - 1, """");
	}
	expect_true(cache_bin_ncached_get_local(bin) == 0, """");
	expect_true(cache_bin_low_water_get(bin) == 0, """");
}",do_flush_test,"void
do_flush_test(cache_bin_t *bin, void **ptrs, cache_bin_sz_t nfill,
    cache_bin_sz_t nflush) {
	bool success;
	assert_true(cache_bin_ncached_get_local(bin) == 0, """");

	for (cache_bin_sz_t i = 0; i < nfill; i++) {
		success = cache_bin_dalloc_easy(bin, &ptrs[i]);
		expect_true(success, """");
	}

	CACHE_BIN_PTR_ARRAY_DECLARE(arr, nflush);
	cache_bin_init_ptr_array_for_flush(bin, &arr, nflush);
	for (cache_bin_sz_t i = 0; i < nflush; i++) {
		expect_ptr_eq(arr.ptr[i], &ptrs[nflush - i - 1], """");
	}
	cache_bin_finish_flush(bin, &arr, nflush);

	expect_true(cache_bin_ncached_get_local(bin) == nfill - nflush,
	    """");
	while (cache_bin_ncached_get_local(bin) > 0) {
		cache_bin_alloc(bin, &success);
	}
}",do_batch_alloc_test,"void
do_batch_alloc_test(cache_bin_t *bin, void **ptrs, cache_bin_sz_t nfill,
    size_t batch) {
	assert_true(cache_bin_ncached_get_local(bin) == 0, """");
	CACHE_BIN_PTR_ARRAY_DECLARE(arr, nfill);
	cache_bin_init_ptr_array_for_fill(bin, &arr, nfill);
	for (cache_bin_sz_t i = 0; i < nfill; i++) {
		arr.ptr[i] = &ptrs[i];
	}
	cache_bin_finish_fill(bin, &arr, nfill);
	assert_true(cache_bin_ncached_get_local(bin) == nfill, """");
	cache_bin_low_water_set(bin);

	void **out = malloc((batch + 1) * sizeof(void *));
	size_t n = cache_bin_alloc_batch(bin, batch, out);
	assert_true(n == ((size_t)nfill < batch ? (size_t)nfill : batch), """");
	for (cache_bin_sz_t i = 0; i < (cache_bin_sz_t)n; i++) {
		expect_ptr_eq(out[i], &ptrs[i], """");
	}
	expect_true(cache_bin_low_water_get(bin) == nfill -
	    (cache_bin_sz_t)n, """");
	while (cache_bin_ncached_get_local(bin) > 0) {
		bool success;
		cache_bin_alloc(bin, &success);
	}
	free(out);
}",do_flush_stashed_test,"void
do_flush_stashed_test(cache_bin_t *bin, void **ptrs, cache_bin_sz_t nfill,
    cache_bin_sz_t nstash) {
	expect_true(cache_bin_ncached_get_local(bin) == 0,
	    ""Bin not empty"");
	expect_true(cache_bin_nstashed_get_local(bin) == 0,
	    ""Bin not empty"");
	expect_true(nfill + nstash <= bin->bin_info.ncached_max, ""Exceeded max"");

	bool ret;
	/* Fill */
	for (cache_bin_sz_t i = 0; i < nfill; i++) {
		ret = cache_bin_dalloc_easy(bin, &ptrs[i]);
		expect_true(ret, ""Unexpected fill failure"");
	}
	expect_true(cache_bin_ncached_get_local(bin) == nfill,
	    ""Wrong cached count"");

	/* Stash */
	for (cache_bin_sz_t i = 0; i < nstash; i++) {
		ret = cache_bin_stash(bin, &ptrs[i + nfill]);
		expect_true(ret, ""Unexpected stash failure"");
	}
	expect_true(cache_bin_nstashed_get_local(bin) == nstash,
	    ""Wrong stashed count"");

	if (nfill + nstash == bin->bin_info.ncached_max) {
		ret = cache_bin_dalloc_easy(bin, &ptrs[0]);
		expect_false(ret, ""Should not dalloc into a full bin"");
		ret = cache_bin_stash(bin, &ptrs[0]);
		expect_false(ret, ""Should not stash into a full bin"");
	}

	/* Alloc filled ones */
	for (cache_bin_sz_t i = 0; i < nfill; i++) {
		void *ptr = cache_bin_alloc(bin, &ret);
		expect_true(ret, ""Unexpected alloc failure"");
		/* Verify it's not from the stashed range. */
		expect_true((uintptr_t)ptr < (uintptr_t)&ptrs[nfill],
		    ""Should not alloc stashed ptrs"");
	}
	expect_true(cache_bin_ncached_get_local(bin) == 0,
	    ""Wrong cached count"");
	expect_true(cache_bin_nstashed_get_local(bin) == nstash,
	    ""Wrong stashed count"");

	cache_bin_alloc(bin, &ret);
	expect_false(ret, ""Should not alloc stashed"");

	/* Clear stashed ones */
	cache_bin_finish_flush_stashed(bin);
	expect_true(cache_bin_ncached_get_local(bin) == 0,
	    ""Wrong cached count"");
	expect_true(cache_bin_nstashed_get_local(bin) == 0,
	    ""Wrong stashed count"");

	cache_bin_alloc(bin, &ret);
	expect_false(ret, ""Should not alloc from empty bin"");
}"
base_edata_heap_insert,"void
base_edata_heap_insert(tsdn_t *tsdn, base_t *base, edata_t *edata) {
	malloc_mutex_assert_owner(tsdn, &base->mtx);

	size_t bsize = edata_bsize_get(edata);
	assert(bsize > 0);
	/*
	 * Compute the index for the largest size class that does not exceed
	 * extent's size.
	 */
	szind_t index_floor = sz_size2index(bsize + 1) - 1;
	edata_heap_insert(&base->avail[index_floor], edata);
}",b0_alloc_header_size,"void
b0_alloc_header_size(size_t *header_size, size_t *alignment) {
	*alignment = QUANTUM;
	*header_size = QUANTUM > sizeof(edata_t *) ? QUANTUM :
	    sizeof(edata_t *);
}"
cache_bin_info_init,"void
cache_bin_info_init(cache_bin_info_t *info,
    cache_bin_sz_t ncached_max) {
	assert(ncached_max <= CACHE_BIN_NCACHED_MAX);
	size_t stack_size = (size_t)ncached_max * sizeof(void *);
	assert(stack_size < ((size_t)1 << (sizeof(cache_bin_sz_t) * 8)));
	info->ncached_max = (cache_bin_sz_t)ncached_max;
}",mallocx_tcache_get,"unsigned
mallocx_tcache_get(int flags) {
	if (likely((flags & MALLOCX_TCACHE_MASK) == 0)) {
		return TCACHE_IND_AUTOMATIC;
	} else if ((flags & MALLOCX_TCACHE_MASK) == MALLOCX_TCACHE_NONE) {
		return TCACHE_IND_NONE;
	} else {
		return MALLOCX_TCACHE_GET(flags);
	}
}",tsd_tcache_data_init,"bool
tsd_tcache_data_init(tsd_t *tsd, arena_t *arena,
    const cache_bin_info_t tcache_bin_info[TCACHE_NBINS_MAX]) {
	assert(tcache_bin_info != NULL);
	return tsd_tcache_data_init_impl(tsd, arena, tcache_bin_info);
}",tcache_bin_info_compute,"void
tcache_bin_info_compute(cache_bin_info_t tcache_bin_info[TCACHE_NBINS_MAX]) {
	/*
	 * Compute the values for each bin, but for bins with indices larger
	 * than tcache_nbins, no items will be cached.
	 */
	for (szind_t i = 0; i < TCACHE_NBINS_MAX; i++) {
		unsigned ncached_max = tcache_get_default_ncached_max_set(i) ?
		    (unsigned)tcache_get_default_ncached_max()[i].ncached_max:
		    tcache_ncached_max_compute(i);
		assert(ncached_max <= CACHE_BIN_NCACHED_MAX);
		cache_bin_info_init(&tcache_bin_info[i],
		    (cache_bin_sz_t)ncached_max);
	}
}",dalloc_func,"void
dalloc_func(void *ptr, size_t sz, unsigned dalloc_option) {
	switch (dalloc_option) {
	case use_free:
		free(ptr);
		break;
	case use_dallocx:
		dallocx(ptr, 0);
		break;
	case use_sdallocx:
		sdallocx(ptr, sz, 0);
		break;
	default:
		unreachable();
	}
}",test_tcache_bytes_alloc,"void
test_tcache_bytes_alloc(size_t alloc_size, size_t tcache_max,
    unsigned alloc_option, unsigned dalloc_option) {
	expect_d_eq(mallctl(""thread.tcache.flush"", NULL, NULL, NULL, 0), 0,
	    ""Unexpected tcache flush failure"");

	size_t usize = sz_s2u(alloc_size);
	/* No change is expected if usize is outside of tcache_max range. */
	bool cached = (usize <= tcache_max);
	ssize_t diff = cached ? usize : 0;

	void *ptr1 = alloc_func(alloc_size, alloc_option);
	void *ptr2 = alloc_func(alloc_size, alloc_option);

	size_t bytes = global_test ? tcache_bytes_read_global() :
	    tcache_bytes_read_local();
	dalloc_func(ptr2, alloc_size, dalloc_option);
	/* Expect tcache_bytes increase after dalloc */
	tcache_bytes_check_update(&bytes, diff);

	dalloc_func(ptr1, alloc_size, alloc_option);
	/* Expect tcache_bytes increase again */
	tcache_bytes_check_update(&bytes, diff);

	void *ptr3 = alloc_func(alloc_size, alloc_option);
	if (cached) {
		expect_ptr_eq(ptr1, ptr3, ""Unexpected cached ptr"");
	}
	/* Expect tcache_bytes decrease after alloc */
	tcache_bytes_check_update(&bytes, -diff);

	void *ptr4 = alloc_func(alloc_size, alloc_option);
	if (cached) {
		expect_ptr_eq(ptr2, ptr4, ""Unexpected cached ptr"");
	}
	/* Expect tcache_bytes decrease again */
	tcache_bytes_check_update(&bytes, -diff);

	dalloc_func(ptr3, alloc_size, dalloc_option);
	tcache_bytes_check_update(&bytes, diff);
	dalloc_func(ptr4, alloc_size, dalloc_option);
	tcache_bytes_check_update(&bytes, diff);
}"
arena_prof_demote,"size_t
arena_prof_demote(tsdn_t *tsdn, edata_t *edata, const void *ptr) {
	cassert(config_prof);
	assert(ptr != NULL);
	size_t usize = isalloc(tsdn, ptr);
	size_t bumped_usize = sz_sa2u(usize, PROF_SAMPLE_ALIGNMENT);
	assert(bumped_usize <= SC_LARGE_MINCLASS &&
	    PAGE_CEILING(bumped_usize) == bumped_usize);
	assert(edata_size_get(edata) - bumped_usize <= sz_large_pad);
	szind_t szind = sz_size2index(bumped_usize);

	edata_szind_set(edata, szind);
	emap_remap(tsdn, &arena_emap_global, edata, szind, /* slab */ false);

	assert(isalloc(tsdn, ptr) == bumped_usize);

	return bumped_usize;
}",arena_dalloc_promoted_impl,"void
arena_dalloc_promoted_impl(tsdn_t *tsdn, void *ptr, tcache_t *tcache,
    bool slow_path, edata_t *edata) {
	cassert(config_prof);
	assert(opt_prof);

	size_t usize = edata_usize_get(edata);
	size_t bumped_usize = arena_prof_demote(tsdn, edata, ptr);
	if (config_opt_safety_checks && usize < SC_LARGE_MINCLASS) {
		/*
		 * Currently, we only do redzoning for small sampled
		 * allocations.
		 */
		safety_check_verify_redzone(ptr, usize, bumped_usize);
	}
	szind_t bumped_ind = sz_size2index(bumped_usize);
	if (bumped_usize >= SC_LARGE_MINCLASS &&
	    tcache != NULL && bumped_ind < TCACHE_NBINS_MAX &&
	    !tcache_bin_disabled(bumped_ind, &tcache->bins[bumped_ind],
	    tcache->tcache_slow)) {
		tcache_dalloc_large(tsdn_tsd(tsdn), tcache, ptr, bumped_ind,
		    slow_path);
	} else {
		large_dalloc(tsdn, edata);
	}
}"
psset_hpdata_heap_index,"pszind_t
psset_hpdata_heap_index(const hpdata_t *ps) {
	assert(!hpdata_full(ps));
	assert(!hpdata_empty(ps));
	size_t longest_free_range = hpdata_longest_free_range_get(ps);
	pszind_t pind = sz_psz2ind(sz_psz_quantize_floor(
	    longest_free_range << LG_PAGE));
	assert(pind < PSSET_NPSIZES);
	return pind;
}",psset_hpdata_heap_insert,"void
psset_hpdata_heap_insert(psset_t *psset, hpdata_t *ps) {
	pszind_t pind = psset_hpdata_heap_index(ps);
	if (hpdata_age_heap_empty(&psset->pageslabs[pind])) {
		fb_set(psset->pageslab_bitmap, PSSET_NPSIZES, (size_t)pind);
	}
	hpdata_age_heap_insert(&psset->pageslabs[pind], ps);
}",psset_hpdata_heap_remove,"void
psset_hpdata_heap_remove(psset_t *psset, hpdata_t *ps) {
	pszind_t pind = psset_hpdata_heap_index(ps);
	hpdata_age_heap_remove(&psset->pageslabs[pind], ps);
	if (hpdata_age_heap_empty(&psset->pageslabs[pind])) {
		fb_unset(psset->pageslab_bitmap, PSSET_NPSIZES, (size_t)pind);
	}
}"
prof_sample_hook_get,"prof_sample_hook_t
prof_sample_hook_get(void) {
	return (prof_sample_hook_t)atomic_load_p(&prof_sample_hook,
	    ATOMIC_ACQUIRE);
}",prof_sample_free_hook_get,"prof_sample_free_hook_t
prof_sample_free_hook_get(void) {
	return (prof_sample_free_hook_t)atomic_load_p(&prof_sample_free_hook,
	    ATOMIC_ACQUIRE);
}",read_write_prof_sample_hook,"void
read_write_prof_sample_hook(prof_sample_hook_t *to_read, bool do_write,
    prof_sample_hook_t to_write) {
	size_t hook_sz = sizeof(prof_sample_hook_t);
	expect_d_eq(mallctl(""experimental.hooks.prof_sample"",
	    (void *)to_read, &hook_sz, do_write ? &to_write : NULL, hook_sz), 0,
	    ""Unexpected prof_sample_hook mallctl failure"");
}",read_write_prof_sample_free_hook,"void
read_write_prof_sample_free_hook(prof_sample_free_hook_t *to_read,
    bool do_write, prof_sample_free_hook_t to_write) {
	size_t hook_sz = sizeof(prof_sample_free_hook_t);
	expect_d_eq(mallctl(""experimental.hooks.prof_sample_free"",
	    (void *)to_read, &hook_sz, do_write ? &to_write : NULL, hook_sz), 0,
	    ""Unexpected prof_sample_free_hook mallctl failure"");
}",read_prof_sample_hook,"prof_sample_hook_t
read_prof_sample_hook(void) {
	prof_sample_hook_t curr_hook;
	read_write_prof_sample_hook(&curr_hook, false, NULL);

	return curr_hook;
}",read_prof_sample_free_hook,"prof_sample_free_hook_t
read_prof_sample_free_hook(void) {
	prof_sample_free_hook_t curr_hook;
	read_write_prof_sample_free_hook(&curr_hook, false, NULL);

	return curr_hook;
}",check_prof_sample_hooks,"void
check_prof_sample_hooks(bool sample_hook_set, bool sample_free_hook_set) {
	expect_false(mock_prof_sample_hook_called,
	    ""Should not have called prof_sample hook"");
	expect_false(mock_prof_sample_free_hook_called,
	    ""Should not have called prof_sample_free hook"");
	expect_ptr_null(sampled_ptr, ""Unexpected sampled ptr"");
	expect_zu_eq(sampled_ptr_sz, 0, ""Unexpected sampled ptr size"");
	expect_ptr_null(free_sampled_ptr, ""Unexpected free sampled ptr"");
	expect_zu_eq(free_sampled_ptr_sz, 0,
	    ""Unexpected free sampled ptr size"");

	prof_sample_hook_t curr_hook = read_prof_sample_hook();
	expect_ptr_eq(curr_hook, sample_hook_set ? mock_prof_sample_hook : NULL,
	    ""Unexpected non NULL default hook"");

	prof_sample_free_hook_t curr_free_hook = read_prof_sample_free_hook();
	expect_ptr_eq(curr_free_hook, sample_free_hook_set ?
	    mock_prof_sample_free_hook : NULL,
	    ""Unexpected non NULL default hook"");

	size_t alloc_sz = 10;
	void *p = mallocx(alloc_sz, 0);
	expect_ptr_not_null(p, ""Failed to allocate"");
	expect_true(mock_prof_sample_hook_called == sample_hook_set,
	   ""Incorrect prof_sample hook usage"");
	if (sample_hook_set) {
		expect_ptr_eq(p, sampled_ptr, ""Unexpected sampled ptr"");
		expect_zu_eq(alloc_sz, sampled_ptr_sz,
		    ""Unexpected sampled usize"");
	}

	dallocx(p, 0);
	expect_true(mock_prof_sample_free_hook_called == sample_free_hook_set,
	   ""Incorrect prof_sample_free hook usage"");
	if (sample_free_hook_set) {
		size_t usz = sz_s2u(alloc_sz);
		expect_ptr_eq(p, free_sampled_ptr, ""Unexpected sampled ptr"");
		expect_zu_eq(usz, free_sampled_ptr_sz, ""Unexpected sampled usize"");
	}

	sampled_ptr = free_sampled_ptr = NULL;
	sampled_ptr_sz = free_sampled_ptr_sz = 0;
	mock_prof_sample_hook_called = false;
	mock_prof_sample_free_hook_called = false;
}",write_prof_sample_hook,"void
write_prof_sample_hook(prof_sample_hook_t new_hook) {
	read_write_prof_sample_hook(NULL, true, new_hook);
}",write_prof_sample_free_hook,"void
write_prof_sample_free_hook(prof_sample_free_hook_t new_hook) {
	read_write_prof_sample_free_hook(NULL, true, new_hook);
}"
test_double_free_pre,"void
test_double_free_pre(void) {
	safety_check_set_abort(&fake_abort);
	fake_abort_called = false;
}",test_double_free_post,"void
test_double_free_post(void) {
	expect_b_eq(fake_abort_called, true, ""Double-free check didn't fire."");
	safety_check_set_abort(NULL);
}"
test_double_free_pre,"void
test_double_free_pre(void) {
	safety_check_set_abort(&fake_abort);
	fake_abort_called = false;
}",test_double_free_post,"void
test_double_free_post(void) {
	expect_b_eq(fake_abort_called, true, ""Double-free check didn't fire."");
	safety_check_set_abort(NULL);
}"
static_opts_init,"void
static_opts_init(static_opts_t *static_opts) {
	static_opts->may_overflow = false;
	static_opts->bump_empty_aligned_alloc = false;
	static_opts->assert_nonempty_alloc = false;
	static_opts->null_out_result_on_error = false;
	static_opts->set_errno_on_error = false;
	static_opts->min_alignment = 0;
	static_opts->oom_string = """";
	static_opts->invalid_alignment_string = """";
	static_opts->slow = false;
	static_opts->usize = false;
}",dynamic_opts_init,"void
dynamic_opts_init(dynamic_opts_t *dynamic_opts) {
	dynamic_opts->result = NULL;
	dynamic_opts->usize = 0;
	dynamic_opts->num_items = 0;
	dynamic_opts->item_size = 0;
	dynamic_opts->alignment = 0;
	dynamic_opts->zero = false;
	dynamic_opts->tcache_ind = TCACHE_IND_AUTOMATIC;
	dynamic_opts->arena_ind = ARENA_IND_AUTOMATIC;
}",reset,"void
reset(void) {
	call_count = 0;
	reset_args();
}"
tcache_bin_flush_stashed,"void
tcache_bin_flush_stashed(tsd_t *tsd, tcache_t *tcache, cache_bin_t *cache_bin,
    szind_t binind, bool is_small) {
	assert(!tcache_bin_disabled(binind, cache_bin, tcache->tcache_slow));
	/*
	 * The two below are for assertion only.  The content of original cached
	 * items remain unchanged -- the stashed items reside on the other end
	 * of the stack.  Checking the stack head and ncached to verify.
	 */
	void *head_content = *cache_bin->stack_head;
	cache_bin_sz_t orig_cached = cache_bin_ncached_get_local(cache_bin);

	cache_bin_sz_t nstashed = cache_bin_nstashed_get_local(cache_bin);
	assert(orig_cached + nstashed <= cache_bin_ncached_max_get(cache_bin));
	if (nstashed == 0) {
		return;
	}

	CACHE_BIN_PTR_ARRAY_DECLARE(ptrs, nstashed);
	cache_bin_init_ptr_array_for_stashed(cache_bin, binind, &ptrs,
	    nstashed);
	san_check_stashed_ptrs(ptrs.ptr, nstashed, sz_index2size(binind));
	tcache_bin_flush_impl(tsd, tcache, cache_bin, binind, &ptrs, nstashed,
	    is_small);
	cache_bin_finish_flush_stashed(cache_bin);

	assert(cache_bin_nstashed_get_local(cache_bin) == 0);
	assert(cache_bin_ncached_get_local(cache_bin) == orig_cached);
	assert(head_content == *cache_bin->stack_head);
}",test_bin_init,"void
test_bin_init(cache_bin_t *bin, cache_bin_info_t *info) {
	size_t size;
	size_t alignment;
	cache_bin_info_compute_alloc(info, 1, &size, &alignment);
	void *mem = mallocx(size, MALLOCX_ALIGN(alignment));
	assert_ptr_not_null(mem, ""Unexpected mallocx failure"");

	size_t cur_offset = 0;
	cache_bin_preincrement(info, 1, mem, &cur_offset);
	cache_bin_init(bin, info, mem, &cur_offset);
	cache_bin_postincrement(mem, &cur_offset);
	assert_zu_eq(cur_offset, size, ""Should use all requested memory"");
}",do_flush_stashed_test,"void
do_flush_stashed_test(cache_bin_t *bin, void **ptrs, cache_bin_sz_t nfill,
    cache_bin_sz_t nstash) {
	expect_true(cache_bin_ncached_get_local(bin) == 0,
	    ""Bin not empty"");
	expect_true(cache_bin_nstashed_get_local(bin) == 0,
	    ""Bin not empty"");
	expect_true(nfill + nstash <= bin->bin_info.ncached_max, ""Exceeded max"");

	bool ret;
	/* Fill */
	for (cache_bin_sz_t i = 0; i < nfill; i++) {
		ret = cache_bin_dalloc_easy(bin, &ptrs[i]);
		expect_true(ret, ""Unexpected fill failure"");
	}
	expect_true(cache_bin_ncached_get_local(bin) == nfill,
	    ""Wrong cached count"");

	/* Stash */
	for (cache_bin_sz_t i = 0; i < nstash; i++) {
		ret = cache_bin_stash(bin, &ptrs[i + nfill]);
		expect_true(ret, ""Unexpected stash failure"");
	}
	expect_true(cache_bin_nstashed_get_local(bin) == nstash,
	    ""Wrong stashed count"");

	if (nfill + nstash == bin->bin_info.ncached_max) {
		ret = cache_bin_dalloc_easy(bin, &ptrs[0]);
		expect_false(ret, ""Should not dalloc into a full bin"");
		ret = cache_bin_stash(bin, &ptrs[0]);
		expect_false(ret, ""Should not stash into a full bin"");
	}

	/* Alloc filled ones */
	for (cache_bin_sz_t i = 0; i < nfill; i++) {
		void *ptr = cache_bin_alloc(bin, &ret);
		expect_true(ret, ""Unexpected alloc failure"");
		/* Verify it's not from the stashed range. */
		expect_true((uintptr_t)ptr < (uintptr_t)&ptrs[nfill],
		    ""Should not alloc stashed ptrs"");
	}
	expect_true(cache_bin_ncached_get_local(bin) == 0,
	    ""Wrong cached count"");
	expect_true(cache_bin_nstashed_get_local(bin) == nstash,
	    ""Wrong stashed count"");

	cache_bin_alloc(bin, &ret);
	expect_false(ret, ""Should not alloc stashed"");

	/* Clear stashed ones */
	cache_bin_finish_flush_stashed(bin);
	expect_true(cache_bin_ncached_get_local(bin) == 0,
	    ""Wrong cached count"");
	expect_true(cache_bin_nstashed_get_local(bin) == 0,
	    ""Wrong stashed count"");

	cache_bin_alloc(bin, &ret);
	expect_false(ret, ""Should not alloc from empty bin"");
}"
tcache_bin_flush_edatas_lookup,"void
tcache_bin_flush_edatas_lookup(tsd_t *tsd, cache_bin_ptr_array_t *arr,
    szind_t binind, size_t nflush, emap_batch_lookup_result_t *edatas) {

	/*
	 * This gets compiled away when config_opt_safety_checks is false.
	 * Checks for sized deallocation bugs, failing early rather than
	 * corrupting metadata.
	 */
	size_t szind_sum = binind * nflush;
	emap_edata_lookup_batch(tsd, &arena_emap_global, nflush,
	    &tcache_bin_flush_ptr_getter, (void *)arr,
	    &tcache_bin_flush_metadata_visitor, (void *)&szind_sum,
	    edatas);
	if (config_opt_safety_checks && unlikely(szind_sum != 0)) {
		tcache_bin_flush_size_check_fail(arr, binind, nflush, edatas);
	}
}",tcache_bin_flush_bottom,"void
tcache_bin_flush_bottom(tsd_t *tsd, tcache_t *tcache, cache_bin_t *cache_bin,
    szind_t binind, unsigned rem, bool small) {
	assert(rem <= cache_bin_ncached_max_get(cache_bin));
	assert(!tcache_bin_disabled(binind, cache_bin, tcache->tcache_slow));
	cache_bin_sz_t orig_nstashed = cache_bin_nstashed_get_local(cache_bin);
	tcache_bin_flush_stashed(tsd, tcache, cache_bin, binind, small);

	cache_bin_sz_t ncached = cache_bin_ncached_get_local(cache_bin);
	assert((cache_bin_sz_t)rem <= ncached + orig_nstashed);
	if ((cache_bin_sz_t)rem > ncached) {
		/*
		 * The flush_stashed above could have done enough flushing, if
		 * there were many items stashed.  Validate that: 1) non zero
		 * stashed, and 2) bin stack has available space now.
		 */
		assert(orig_nstashed > 0);
		assert(ncached + cache_bin_nstashed_get_local(cache_bin)
		    < cache_bin_ncached_max_get(cache_bin));
		/* Still go through the flush logic for stats purpose only. */
		rem = ncached;
	}
	cache_bin_sz_t nflush = ncached - (cache_bin_sz_t)rem;

	CACHE_BIN_PTR_ARRAY_DECLARE(ptrs, nflush);
	cache_bin_init_ptr_array_for_flush(cache_bin, &ptrs, nflush);

	tcache_bin_flush_impl(tsd, tcache, cache_bin, binind, &ptrs, nflush,
	    small);

	cache_bin_finish_flush(cache_bin, &ptrs, nflush);
}"
release_batch,"void
release_batch(void **ptrs, size_t batch, size_t size) {
	for (size_t i = 0; i < batch; ++i) {
		sdallocx(ptrs[i], size, 0);
	}
}",get_lg_prof_sample,"size_t
get_lg_prof_sample(void) {
	size_t ret;
	size_t sz = sizeof(size_t);

	expect_d_eq(mallctl(""prof.lg_sample"", (void *)&ret, &sz, NULL, 0), 0,
	    ""Unexpected mallctl failure while reading profiling sample rate"");
	return ret;
}",do_get_size_impl,"size_t
do_get_size_impl(const char *cmd, unsigned ind) {
	size_t mib[4];
	size_t miblen = sizeof(mib) / sizeof(size_t);
	size_t z = sizeof(size_t);

	expect_d_eq(mallctlnametomib(cmd, mib, &miblen),
	    0, ""Unexpected mallctlnametomib(\""%s\"", ...) failure"", cmd);
	mib[2] = ind;
	size_t size;
	expect_d_eq(mallctlbymib(mib, miblen, (void *)&size, &z, NULL, 0),
	    0, ""Unexpected mallctlbymib([\""%s\""], ...) failure"", cmd);

	return size;
}"
malloc_cpu_count_is_deterministic,"bool
malloc_cpu_count_is_deterministic(void)
{
#ifdef _WIN32
	return true;
#else
	long cpu_onln = sysconf(_SC_NPROCESSORS_ONLN);
	long cpu_conf = sysconf(_SC_NPROCESSORS_CONF);
	if (cpu_onln != cpu_conf) {
		return false;
	}
#  if defined(CPU_COUNT)
#    if defined(__FreeBSD__) || defined(__DragonFly__)
	cpuset_t set;
#    else
	cpu_set_t set;
#    endif /* __FreeBSD__ */
#    if defined(JEMALLOC_HAVE_SCHED_SETAFFINITY)
	sched_getaffinity(0, sizeof(set), &set);
#    else /* !JEMALLOC_HAVE_SCHED_SETAFFINITY */
	pthread_getaffinity_np(pthread_self(), sizeof(set), &set);
#    endif /* JEMALLOC_HAVE_SCHED_SETAFFINITY */
	long cpu_affinity = CPU_COUNT(&set);
	if (cpu_affinity != cpu_conf) {
		return false;
	}
#  endif /* CPU_COUNT */
	return true;
#endif
}",malloc_abort_invalid_conf,"void
malloc_abort_invalid_conf(void) {
	assert(opt_abort_conf);
	malloc_printf(""<jemalloc>: Abort (abort_conf:true) on invalid conf ""
	    ""value (see above).\n"");
	invalid_conf_abort();
}"
nstime_assert_initialized,"void
nstime_assert_initialized(const nstime_t *time) {
#ifdef JEMALLOC_DEBUG
	/*
	 * Some parts (e.g. stats) rely on memset to zero initialize.  Treat
	 * these as valid initialization.
	 */
	assert(time->magic == NSTIME_MAGIC ||
	    (time->magic == 0 && time->ns == 0));
#endif
}",nstime_copy,"void
nstime_copy(nstime_t *time, const nstime_t *source) {
	/* Source is required to be initialized. */
	nstime_assert_initialized(source);
	*time = *source;
	nstime_assert_initialized(time);
}",test_nstime_since_once,"void
test_nstime_since_once(nstime_t *t) {
	nstime_t old_t;
	nstime_copy(&old_t, t);

	uint64_t ns_since = nstime_ns_since(t);
	nstime_update(t);

	nstime_t new_t;
	nstime_copy(&new_t, t);
	nstime_subtract(&new_t, &old_t);

	expect_u64_ge(nstime_ns(&new_t), ns_since,
	    ""Incorrect time since result"");
}"
malloc_cpu_count_is_deterministic,"bool
malloc_cpu_count_is_deterministic(void)
{
#ifdef _WIN32
	return true;
#else
	long cpu_onln = sysconf(_SC_NPROCESSORS_ONLN);
	long cpu_conf = sysconf(_SC_NPROCESSORS_CONF);
	if (cpu_onln != cpu_conf) {
		return false;
	}
#  if defined(CPU_COUNT)
#    if defined(__FreeBSD__) || defined(__DragonFly__)
	cpuset_t set;
#    else
	cpu_set_t set;
#    endif /* __FreeBSD__ */
#    if defined(JEMALLOC_HAVE_SCHED_SETAFFINITY)
	sched_getaffinity(0, sizeof(set), &set);
#    else /* !JEMALLOC_HAVE_SCHED_SETAFFINITY */
	pthread_getaffinity_np(pthread_self(), sizeof(set), &set);
#    endif /* JEMALLOC_HAVE_SCHED_SETAFFINITY */
	long cpu_affinity = CPU_COUNT(&set);
	if (cpu_affinity != cpu_conf) {
		return false;
	}
#  endif /* CPU_COUNT */
	return true;
#endif
}",malloc_abort_invalid_conf,"void
malloc_abort_invalid_conf(void) {
	assert(opt_abort_conf);
	malloc_printf(""<jemalloc>: Abort (abort_conf:true) on invalid conf ""
	    ""value (see above).\n"");
	invalid_conf_abort();
}"
nstime_assert_initialized,"void
nstime_assert_initialized(const nstime_t *time) {
#ifdef JEMALLOC_DEBUG
	/*
	 * Some parts (e.g. stats) rely on memset to zero initialize.  Treat
	 * these as valid initialization.
	 */
	assert(time->magic == NSTIME_MAGIC ||
	    (time->magic == 0 && time->ns == 0));
#endif
}",nstime_set_initialized,"void
nstime_set_initialized(nstime_t *time) {
#ifdef JEMALLOC_DEBUG
	time->magic = NSTIME_MAGIC;
#endif
}",nstime_pair_assert_initialized,"void
nstime_pair_assert_initialized(const nstime_t *t1, const nstime_t *t2) {
	nstime_assert_initialized(t1);
	nstime_assert_initialized(t2);
}",nstime_initialize_operand,"void
nstime_initialize_operand(nstime_t *time) {
	/*
	 * Operations like nstime_add may have the initial operand being zero
	 * initialized (covered by the assert below).  Full-initialize needed
	 * before changing it to non-zero.
	 */
	nstime_assert_initialized(time);
	nstime_set_initialized(time);
}",nstime_get,"void
nstime_get(nstime_t *time) {
	FILETIME ft;
	uint64_t ticks_100ns;

	GetSystemTimeAsFileTime(&ft);
	ticks_100ns = (((uint64_t)ft.dwHighDateTime) << 32) | ft.dwLowDateTime;

	nstime_init(time, ticks_100ns * 100);
}"
arena_background_thread_inactivity_check,"void
arena_background_thread_inactivity_check(tsdn_t *tsdn, arena_t *arena,
    bool is_background_thread) {
	if (!background_thread_enabled() || is_background_thread) {
		return;
	}
	background_thread_info_t *info =
	    arena_background_thread_info_get(arena);
	if (background_thread_indefinite_sleep(info)) {
		arena_maybe_do_deferred_work(tsdn, arena,
		    &arena->pa_shard.pac.decay_dirty, 0);
	}
}",arena_handle_deferred_work,"void arena_handle_deferred_work(tsdn_t *tsdn, arena_t *arena) {
	witness_assert_depth_to_rank(tsdn_witness_tsdp_get(tsdn),
	    WITNESS_RANK_CORE, 0);

	if (decay_immediately(&arena->pa_shard.pac.decay_dirty)) {
		arena_decay_dirty(tsdn, arena, false, true);
	}
	arena_background_thread_inactivity_check(tsdn, arena, false);
}",hpa_should_purge,"bool
hpa_should_purge(tsdn_t *tsdn, hpa_shard_t *shard) {
	malloc_mutex_assert_owner(tsdn, &shard->mtx);
	if (hpa_adjusted_ndirty(tsdn, shard) > hpa_ndirty_max(tsdn, shard)) {
		return true;
	}
	if (hpa_hugify_blocked_by_ndirty(tsdn, shard)) {
		return true;
	}
	return false;
}",hpa_shard_has_deferred_work,"bool
hpa_shard_has_deferred_work(tsdn_t *tsdn, hpa_shard_t *shard) {
	malloc_mutex_assert_owner(tsdn, &shard->mtx);
	hpdata_t *to_hugify = psset_pick_hugify(&shard->psset);
	return to_hugify != NULL || hpa_should_purge(tsdn, shard);
}",hpa_dalloc_batch,"void
hpa_dalloc_batch(tsdn_t *tsdn, pai_t *self, edata_list_active_t *list,
    bool *deferred_work_generated) {
	hpa_shard_t *shard = hpa_from_pai(self);

	edata_t *edata;
	ql_foreach(edata, &list->head, ql_link_active) {
		hpa_dalloc_prepare_unlocked(tsdn, shard, edata);
	}

	malloc_mutex_lock(tsdn, &shard->mtx);
	/* Now, remove from the list. */
	while ((edata = edata_list_active_first(list)) != NULL) {
		edata_list_active_remove(list, edata);
		hpa_dalloc_locked(tsdn, shard, edata);
	}
	hpa_shard_maybe_do_deferred_work(tsdn, shard, /* forced */ false);
	*deferred_work_generated =
	    hpa_shard_has_deferred_work(tsdn, shard);

	malloc_mutex_unlock(tsdn, &shard->mtx);
}",wait_until_thread_is_enabled,"void
wait_until_thread_is_enabled(unsigned arena_id) {
	tsd_t* tsd = tsd_fetch();

	bool sleeping = false;
	int iterations = 0;
	do {
		background_thread_info_t *info =
		    background_thread_info_get(arena_id);
		malloc_mutex_lock(tsd_tsdn(tsd), &info->mtx);
		malloc_mutex_unlock(tsd_tsdn(tsd), &info->mtx);
		sleeping = background_thread_indefinite_sleep(info);
		assert_d_lt(iterations, UINT64_C(1000000),
		    ""Waiting for a thread to start for too long"");
	} while (!sleeping);
}"
arena_maybe_do_deferred_work,"void
arena_maybe_do_deferred_work(tsdn_t *tsdn, arena_t *arena, decay_t *decay,
    size_t npages_new) {
	background_thread_info_t *info = arena_background_thread_info_get(
	    arena);
	if (malloc_mutex_trylock(tsdn, &info->mtx)) {
		/*
		 * Background thread may hold the mutex for a long period of
		 * time.  We'd like to avoid the variance on application
		 * threads.  So keep this non-blocking, and leave the work to a
		 * future epoch.
		 */
		return;
	}
	if (!background_thread_is_started(info)) {
		goto label_done;
	}

	nstime_t remaining_sleep;
	if (background_thread_indefinite_sleep(info)) {
		background_thread_wakeup_early(info, NULL);
	} else if (arena_should_decay_early(tsdn, arena, decay, info,
	    &remaining_sleep, npages_new)) {
		info->npages_to_purge_new = 0;
		background_thread_wakeup_early(info, &remaining_sleep);
	}
label_done:
	malloc_mutex_unlock(tsdn, &info->mtx);
}",pac_time_until_deferred_work,"uint64_t
pac_time_until_deferred_work(tsdn_t *tsdn, pai_t *self) {
	uint64_t time;
	pac_t *pac = (pac_t *)self;

	time = pac_ns_until_purge(tsdn,
	    &pac->decay_dirty,
	    ecache_npages_get(&pac->ecache_dirty));
	if (time == BACKGROUND_THREAD_DEFERRED_MIN) {
		return time;
	}

	uint64_t muzzy = pac_ns_until_purge(tsdn,
	    &pac->decay_muzzy,
	    ecache_npages_get(&pac->ecache_muzzy));
	if (muzzy < time) {
		time = muzzy;
	}
	return time;
}"
eset_bin_init,"void
eset_bin_init(eset_bin_t *bin) {
	edata_heap_new(&bin->heap);
	/*
	 * heap_min doesn't need initialization; it gets filled in when the bin
	 * goes from non-empty to empty.
	 */
}",eset_bin_stats_init,"void
eset_bin_stats_init(eset_bin_stats_t *bin_stats) {
	atomic_store_zu(&bin_stats->nextents, 0, ATOMIC_RELAXED);
	atomic_store_zu(&bin_stats->nbytes, 0, ATOMIC_RELAXED);
}"
node_print,"void
node_print(const node_t *node, unsigned depth) {
	unsigned i;
	node_t *leftmost_child, *sibling;

	for (i = 0; i < depth; i++) {
		malloc_printf(""\t"");
	}
	malloc_printf(""%2""FMTu64""\n"", node->key);

	leftmost_child = node_lchild_get(node);
	if (leftmost_child == NULL) {
		return;
	}
	node_print(leftmost_child, depth + 1);

	for (sibling = node_next_get(leftmost_child); sibling !=
	    NULL; sibling = node_next_get(sibling)) {
		node_print(sibling, depth + 1);
	}
}",node_validate,"unsigned
node_validate(const node_t *node, const node_t *parent) {
	unsigned nnodes = 1;
	node_t *leftmost_child, *sibling;

	if (parent != NULL) {
		expect_d_ge(node_cmp_magic(node, parent), 0,
		    ""Child is less than parent"");
	}

	leftmost_child = node_lchild_get(node);
	if (leftmost_child == NULL) {
		return nnodes;
	}
	expect_ptr_eq(node_prev_get(leftmost_child),
	    (void *)node, ""Leftmost child does not link to node"");
	nnodes += node_validate(leftmost_child, node);

	for (sibling = node_next_get(leftmost_child); sibling !=
	    NULL; sibling = node_next_get(sibling)) {
		expect_ptr_eq(node_next_get(node_prev_get(sibling)), sibling,
		    ""sibling's prev doesn't link to sibling"");
		nnodes += node_validate(sibling, node);
	}
	return nnodes;
}"
hpa_try_hugify,"bool
hpa_try_hugify(tsdn_t *tsdn, hpa_shard_t *shard) {
	malloc_mutex_assert_owner(tsdn, &shard->mtx);

	if (hpa_hugify_blocked_by_ndirty(tsdn, shard)) {
		return false;
	}

	hpdata_t *to_hugify = psset_pick_hugify(&shard->psset);
	if (to_hugify == NULL) {
		return false;
	}
	assert(hpdata_hugify_allowed_get(to_hugify));
	assert(!hpdata_changing_state_get(to_hugify));

	/* Make sure that it's been hugifiable for long enough. */
	nstime_t time_hugify_allowed = hpdata_time_hugify_allowed(to_hugify);
	uint64_t millis = shard->central->hooks.ms_since(&time_hugify_allowed);
	if (millis < shard->opts.hugify_delay_ms) {
		return false;
	}

	/*
	 * Don't let anyone else purge or hugify this page while
	 * we're hugifying it (allocations and deallocations are
	 * OK).
	 */
	psset_update_begin(&shard->psset, to_hugify);
	hpdata_mid_hugify_set(to_hugify, true);
	hpdata_purge_allowed_set(to_hugify, false);
	hpdata_disallow_hugify(to_hugify);
	assert(hpdata_alloc_allowed_get(to_hugify));
	psset_update_end(&shard->psset, to_hugify);

	malloc_mutex_unlock(tsdn, &shard->mtx);

	shard->central->hooks.hugify(hpdata_addr_get(to_hugify), HUGEPAGE);

	malloc_mutex_lock(tsdn, &shard->mtx);
	shard->stats.nhugifies++;

	psset_update_begin(&shard->psset, to_hugify);
	hpdata_hugify(to_hugify);
	hpdata_mid_hugify_set(to_hugify, false);
	hpa_update_purge_hugify_eligibility(tsdn, shard, to_hugify);
	psset_update_end(&shard->psset, to_hugify);

	return true;
}",hpa_hooks_curtime,"void
hpa_hooks_curtime(nstime_t *r_nstime, bool first_reading) {
	if (first_reading) {
		nstime_init_zero(r_nstime);
	}
	nstime_update(r_nstime);
}",destroy_test_data,"void
destroy_test_data(hpa_shard_t *shard) {
	test_data_t *test_data = (test_data_t *)shard;
	base_delete(TSDN_NULL, test_data->base);
	free(test_data);
}"
extent_deactivate_locked,"void
extent_deactivate_locked(tsdn_t *tsdn, pac_t *pac, ecache_t *ecache,
    edata_t *edata) {
	assert(edata_state_get(edata) == extent_state_active);
	extent_deactivate_locked_impl(tsdn, pac, ecache, edata);
}",extent_activate_locked,"void
extent_activate_locked(tsdn_t *tsdn, pac_t *pac, ecache_t *ecache, eset_t *eset,
    edata_t *edata) {
	assert(edata_arena_ind_get(edata) == ecache_ind_get(ecache));
	assert(edata_state_get(edata) == ecache->state ||
	    edata_state_get(edata) == extent_state_merging);

	eset_remove(eset, edata);
	emap_update_edata_state(tsdn, pac->emap, edata, extent_state_active);
}"
hpa_ndirty_max,"size_t
hpa_ndirty_max(tsdn_t *tsdn, hpa_shard_t *shard) {
	malloc_mutex_assert_owner(tsdn, &shard->mtx);
	if (shard->opts.dirty_mult == (fxp_t)-1) {
		return (size_t)-1;
	}
	return fxp_mul_frac(psset_nactive(&shard->psset),
	    shard->opts.dirty_mult);
}",hpa_update_purge_hugify_eligibility,"void
hpa_update_purge_hugify_eligibility(tsdn_t *tsdn, hpa_shard_t *shard,
    hpdata_t *ps) {
	malloc_mutex_assert_owner(tsdn, &shard->mtx);
	if (hpdata_changing_state_get(ps)) {
		hpdata_purge_allowed_set(ps, false);
		hpdata_disallow_hugify(ps);
		return;
	}
	/*
	 * Hugepages are distinctly costly to purge, so try to avoid it unless
	 * they're *particularly* full of dirty pages.  Eventually, we should
	 * use a smarter / more dynamic heuristic for situations where we have
	 * to manually hugify.
	 *
	 * In situations where we don't manually hugify, this problem is
	 * reduced.  The ""bad"" situation we're trying to avoid is one's that's
	 * common in some Linux configurations (where both enabled and defrag
	 * are set to madvise) that can lead to long latency spikes on the first
	 * access after a hugification.  The ideal policy in such configurations
	 * is probably time-based for both purging and hugifying; only hugify a
	 * hugepage if it's met the criteria for some extended period of time,
	 * and only dehugify it if it's failed to meet the criteria for an
	 * extended period of time.  When background threads are on, we should
	 * try to take this hit on one of them, as well.
	 *
	 * I think the ideal setting is THP always enabled, and defrag set to
	 * deferred; in that case we don't need any explicit calls on the
	 * allocator's end at all; we just try to pack allocations in a
	 * hugepage-friendly manner and let the OS hugify in the background.
	 */
	hpdata_purge_allowed_set(ps, hpdata_ndirty_get(ps) > 0);
	if (hpa_good_hugification_candidate(shard, ps)
	    && !hpdata_huge_get(ps)) {
		nstime_t now;
		shard->central->hooks.curtime(&now, /* first_reading */ true);
		hpdata_allow_hugify(ps, now);
	}
	/*
	 * Once a hugepage has become eligible for hugification, we don't mark
	 * it as ineligible just because it stops meeting the criteria (this
	 * could lead to situations where a hugepage that spends most of its
	 * time meeting the criteria never quite getting hugified if there are
	 * intervening deallocations).  The idea is that the hugification delay
	 * will allow them to get purged, reseting their ""hugify-allowed"" bit.
	 * If they don't get purged, then the hugification isn't hurting and
	 * might help.  As an exception, we don't hugify hugepages that are now
	 * empty; it definitely doesn't help there until the hugepage gets
	 * reused, which is likely not for a while.
	 */
	if (hpdata_nactive_get(ps) == 0) {
		hpdata_disallow_hugify(ps);
	}
}"
expect_contiguous,"void
expect_contiguous(edata_t **edatas, size_t nedatas) {
	for (size_t i = 0; i < nedatas; i++) {
		size_t expected = (size_t)edata_base_get(edatas[0])
		    + i * PAGE;
		expect_zu_eq(expected, (size_t)edata_base_get(edatas[i]),
		    ""Mismatch at index %zu"", i);
	}
}",destroy_test_data,"void
destroy_test_data(hpa_shard_t *shard) {
	test_data_t *test_data = (test_data_t *)shard;
	base_delete(TSDN_NULL, test_data->base);
	free(test_data);
}"
sec_bin_init,"void
sec_bin_init(sec_bin_t *bin) {
	bin->being_batch_filled = false;
	bin->bytes_cur = 0;
	edata_list_active_init(&bin->freelist);
}",sec_shard_dalloc_and_unlock,"void
sec_shard_dalloc_and_unlock(tsdn_t *tsdn, sec_t *sec, sec_shard_t *shard,
    edata_t *edata) {
	malloc_mutex_assert_owner(tsdn, &shard->mtx);
	assert(shard->bytes_cur <= sec->opts.max_bytes);
	size_t size = edata_size_get(edata);
	pszind_t pszind = sz_psz2ind(size);
	assert(pszind < sec->npsizes);
	/*
	 * Prepending here results in LIFO allocation per bin, which seems
	 * reasonable.
	 */
	sec_bin_t *bin = &shard->bins[pszind];
	edata_list_active_prepend(&bin->freelist, edata);
	bin->bytes_cur += size;
	shard->bytes_cur += size;
	if (shard->bytes_cur > sec->opts.max_bytes) {
		/*
		 * We've exceeded the shard limit.  We make two nods in the
		 * direction of fragmentation avoidance: we flush everything in
		 * the shard, rather than one particular bin, and we hold the
		 * lock while flushing (in case one of the extents we flush is
		 * highly preferred from a fragmentation-avoidance perspective
		 * in the backing allocator).  This has the extra advantage of
		 * not requiring advanced cache balancing strategies.
		 */
		sec_flush_some_and_unlock(tsdn, sec, shard);
		malloc_mutex_assert_not_owner(tsdn, &shard->mtx);
	} else {
		malloc_mutex_unlock(tsdn, &shard->mtx);
	}
}",sec_flush_all_locked,"void
sec_flush_all_locked(tsdn_t *tsdn, sec_t *sec, sec_shard_t *shard) {
	malloc_mutex_assert_owner(tsdn, &shard->mtx);
	shard->bytes_cur = 0;
	edata_list_active_t to_flush;
	edata_list_active_init(&to_flush);
	for (pszind_t i = 0; i < sec->npsizes; i++) {
		sec_bin_t *bin = &shard->bins[i];
		bin->bytes_cur = 0;
		edata_list_active_concat(&to_flush, &bin->freelist);
	}

	/*
	 * Ordinarily we would try to avoid doing the batch deallocation while
	 * holding the shard mutex, but the flush_all pathways only happen when
	 * we're disabling the HPA or resetting the arena, both of which are
	 * rare pathways.
	 */
	bool deferred_work_generated = false;
	pai_dalloc_batch(tsdn, sec->fallback, &to_flush,
	    &deferred_work_generated);
}"
hpa_dalloc_prepare_unlocked,"void
hpa_dalloc_prepare_unlocked(tsdn_t *tsdn, hpa_shard_t *shard, edata_t *edata) {
	malloc_mutex_assert_not_owner(tsdn, &shard->mtx);

	assert(edata_pai_get(edata) == EXTENT_PAI_HPA);
	assert(edata_state_get(edata) == extent_state_active);
	assert(edata_arena_ind_get(edata) == shard->ind);
	assert(edata_szind_get_maybe_invalid(edata) == SC_NSIZES);
	assert(edata_committed_get(edata));
	assert(edata_base_get(edata) != NULL);

	/*
	 * Another thread shouldn't be trying to touch the metadata of an
	 * allocation being freed.  The one exception is a merge attempt from a
	 * lower-addressed PAC extent; in this case we have a nominal race on
	 * the edata metadata bits, but in practice the fact that the PAI bits
	 * are different will prevent any further access.  The race is bad, but
	 * benign in practice, and the long term plan is to track enough state
	 * in the rtree to prevent these merge attempts in the first place.
	 */
	edata_addr_set(edata, edata_base_get(edata));
	edata_zeroed_set(edata, false);
	emap_deregister_boundary(tsdn, shard->emap, edata);
}",hpa_dalloc_locked,"void
hpa_dalloc_locked(tsdn_t *tsdn, hpa_shard_t *shard, edata_t *edata) {
	malloc_mutex_assert_owner(tsdn, &shard->mtx);

	/*
	 * Release the metadata early, to avoid having to remember to do it
	 * while we're also doing tricky purging logic.  First, we need to grab
	 * a few bits of metadata from it.
	 *
	 * Note that the shard mutex protects ps's metadata too; it wouldn't be
	 * correct to try to read most information out of it without the lock.
	 */
	hpdata_t *ps = edata_ps_get(edata);
	/* Currently, all edatas come from pageslabs. */
	assert(ps != NULL);
	void *unreserve_addr = edata_addr_get(edata);
	size_t unreserve_size = edata_size_get(edata);
	edata_cache_fast_put(tsdn, &shard->ecf, edata);

	psset_update_begin(&shard->psset, ps);
	hpdata_unreserve(ps, unreserve_addr, unreserve_size);
	hpa_update_purge_hugify_eligibility(tsdn, shard, ps);
	psset_update_end(&shard->psset, ps);
}"
xparse_fxp,"fxp_t
xparse_fxp(const char *str) {
	fxp_t result;
	bool err = fxp_parse(&result, str, NULL);
	assert_false(err, ""Invalid fxp string: %s"", str);
	return result;
}",expect_init_percent,"void
expect_init_percent(unsigned percent, const char *str) {
	fxp_t result_init = FXP_INIT_PERCENT(percent);
	fxp_t result_parse = xparse_fxp(str);
	expect_u32_eq(result_init, result_parse,
	    ""Expect representations of FXP_INIT_PERCENT(%u) and ""
	    ""fxp_parse(\""%s\"") to be equal; got %x and %x"",
	    percent, str, result_init, result_parse);

}"
xparse_fxp,"fxp_t
xparse_fxp(const char *str) {
	fxp_t result;
	bool err = fxp_parse(&result, str, NULL);
	assert_false(err, ""Invalid fxp string: %s"", str);
	return result;
}",expect_mul_frac,"void
expect_mul_frac(size_t a, const char *fracstr, size_t expected) {
	fxp_t frac = xparse_fxp(fracstr);
	size_t result = fxp_mul_frac(a, frac);
	expect_true(double_close(expected, result),
	    ""Expected %zu * %s == %zu (fracmul); got %zu"", a, fracstr,
	    expected, result);
}"
hpa_update_purge_hugify_eligibility,"void
hpa_update_purge_hugify_eligibility(tsdn_t *tsdn, hpa_shard_t *shard,
    hpdata_t *ps) {
	malloc_mutex_assert_owner(tsdn, &shard->mtx);
	if (hpdata_changing_state_get(ps)) {
		hpdata_purge_allowed_set(ps, false);
		hpdata_disallow_hugify(ps);
		return;
	}
	/*
	 * Hugepages are distinctly costly to purge, so try to avoid it unless
	 * they're *particularly* full of dirty pages.  Eventually, we should
	 * use a smarter / more dynamic heuristic for situations where we have
	 * to manually hugify.
	 *
	 * In situations where we don't manually hugify, this problem is
	 * reduced.  The ""bad"" situation we're trying to avoid is one's that's
	 * common in some Linux configurations (where both enabled and defrag
	 * are set to madvise) that can lead to long latency spikes on the first
	 * access after a hugification.  The ideal policy in such configurations
	 * is probably time-based for both purging and hugifying; only hugify a
	 * hugepage if it's met the criteria for some extended period of time,
	 * and only dehugify it if it's failed to meet the criteria for an
	 * extended period of time.  When background threads are on, we should
	 * try to take this hit on one of them, as well.
	 *
	 * I think the ideal setting is THP always enabled, and defrag set to
	 * deferred; in that case we don't need any explicit calls on the
	 * allocator's end at all; we just try to pack allocations in a
	 * hugepage-friendly manner and let the OS hugify in the background.
	 */
	hpdata_purge_allowed_set(ps, hpdata_ndirty_get(ps) > 0);
	if (hpa_good_hugification_candidate(shard, ps)
	    && !hpdata_huge_get(ps)) {
		nstime_t now;
		shard->central->hooks.curtime(&now, /* first_reading */ true);
		hpdata_allow_hugify(ps, now);
	}
	/*
	 * Once a hugepage has become eligible for hugification, we don't mark
	 * it as ineligible just because it stops meeting the criteria (this
	 * could lead to situations where a hugepage that spends most of its
	 * time meeting the criteria never quite getting hugified if there are
	 * intervening deallocations).  The idea is that the hugification delay
	 * will allow them to get purged, reseting their ""hugify-allowed"" bit.
	 * If they don't get purged, then the hugification isn't hurting and
	 * might help.  As an exception, we don't hugify hugepages that are now
	 * empty; it definitely doesn't help there until the hugepage gets
	 * reused, which is likely not for a while.
	 */
	if (hpdata_nactive_get(ps) == 0) {
		hpdata_disallow_hugify(ps);
	}
}",hpa_try_purge,"bool
hpa_try_purge(tsdn_t *tsdn, hpa_shard_t *shard) {
	malloc_mutex_assert_owner(tsdn, &shard->mtx);

	hpdata_t *to_purge = psset_pick_purge(&shard->psset);
	if (to_purge == NULL) {
		return false;
	}
	assert(hpdata_purge_allowed_get(to_purge));
	assert(!hpdata_changing_state_get(to_purge));

	/*
	 * Don't let anyone else purge or hugify this page while
	 * we're purging it (allocations and deallocations are
	 * OK).
	 */
	psset_update_begin(&shard->psset, to_purge);
	assert(hpdata_alloc_allowed_get(to_purge));
	hpdata_mid_purge_set(to_purge, true);
	hpdata_purge_allowed_set(to_purge, false);
	hpdata_disallow_hugify(to_purge);
	/*
	 * Unlike with hugification (where concurrent
	 * allocations are allowed), concurrent allocation out
	 * of a hugepage being purged is unsafe; we might hand
	 * out an extent for an allocation and then purge it
	 * (clearing out user data).
	 */
	hpdata_alloc_allowed_set(to_purge, false);
	psset_update_end(&shard->psset, to_purge);

	/* Gather all the metadata we'll need during the purge. */
	bool dehugify = hpdata_huge_get(to_purge);
	hpdata_purge_state_t purge_state;
	size_t num_to_purge = hpdata_purge_begin(to_purge, &purge_state);

	shard->npending_purge += num_to_purge;

	malloc_mutex_unlock(tsdn, &shard->mtx);

	/* Actually do the purging, now that the lock is dropped. */
	if (dehugify) {
		shard->central->hooks.dehugify(hpdata_addr_get(to_purge),
		    HUGEPAGE);
	}
	size_t total_purged = 0;
	uint64_t purges_this_pass = 0;
	void *purge_addr;
	size_t purge_size;
	while (hpdata_purge_next(to_purge, &purge_state, &purge_addr,
	    &purge_size)) {
		total_purged += purge_size;
		assert(total_purged <= HUGEPAGE);
		purges_this_pass++;
		shard->central->hooks.purge(purge_addr, purge_size);
	}

	malloc_mutex_lock(tsdn, &shard->mtx);
	/* The shard updates */
	shard->npending_purge -= num_to_purge;
	shard->stats.npurge_passes++;
	shard->stats.npurges += purges_this_pass;
	shard->central->hooks.curtime(&shard->last_purge,
	    /* first_reading */ false);
	if (dehugify) {
		shard->stats.ndehugifies++;
	}

	/* The hpdata updates. */
	psset_update_begin(&shard->psset, to_purge);
	if (dehugify) {
		hpdata_dehugify(to_purge);
	}
	hpdata_purge_end(to_purge, &purge_state);
	hpdata_mid_purge_set(to_purge, false);

	hpdata_alloc_allowed_set(to_purge, true);
	hpa_update_purge_hugify_eligibility(tsdn, shard, to_purge);

	psset_update_end(&shard->psset, to_purge);

	return true;
}"
psset_bin_stats_insert,"void
psset_bin_stats_insert(psset_t *psset, psset_bin_stats_t *binstats,
    hpdata_t *ps) {
	psset_bin_stats_insert_remove(psset, binstats, ps, true);
}",psset_bin_stats_remove,"void
psset_bin_stats_remove(psset_t *psset, psset_bin_stats_t *binstats,
    hpdata_t *ps) {
	psset_bin_stats_insert_remove(psset, binstats, ps, false);
}"
hpa_try_hugify,"bool
hpa_try_hugify(tsdn_t *tsdn, hpa_shard_t *shard) {
	malloc_mutex_assert_owner(tsdn, &shard->mtx);

	if (hpa_hugify_blocked_by_ndirty(tsdn, shard)) {
		return false;
	}

	hpdata_t *to_hugify = psset_pick_hugify(&shard->psset);
	if (to_hugify == NULL) {
		return false;
	}
	assert(hpdata_hugify_allowed_get(to_hugify));
	assert(!hpdata_changing_state_get(to_hugify));

	/* Make sure that it's been hugifiable for long enough. */
	nstime_t time_hugify_allowed = hpdata_time_hugify_allowed(to_hugify);
	uint64_t millis = shard->central->hooks.ms_since(&time_hugify_allowed);
	if (millis < shard->opts.hugify_delay_ms) {
		return false;
	}

	/*
	 * Don't let anyone else purge or hugify this page while
	 * we're hugifying it (allocations and deallocations are
	 * OK).
	 */
	psset_update_begin(&shard->psset, to_hugify);
	hpdata_mid_hugify_set(to_hugify, true);
	hpdata_purge_allowed_set(to_hugify, false);
	hpdata_disallow_hugify(to_hugify);
	assert(hpdata_alloc_allowed_get(to_hugify));
	psset_update_end(&shard->psset, to_hugify);

	malloc_mutex_unlock(tsdn, &shard->mtx);

	shard->central->hooks.hugify(hpdata_addr_get(to_hugify), HUGEPAGE);

	malloc_mutex_lock(tsdn, &shard->mtx);
	shard->stats.nhugifies++;

	psset_update_begin(&shard->psset, to_hugify);
	hpdata_hugify(to_hugify);
	hpdata_mid_hugify_set(to_hugify, false);
	hpa_update_purge_hugify_eligibility(tsdn, shard, to_hugify);
	psset_update_end(&shard->psset, to_hugify);

	return true;
}",hpa_try_purge,"bool
hpa_try_purge(tsdn_t *tsdn, hpa_shard_t *shard) {
	malloc_mutex_assert_owner(tsdn, &shard->mtx);

	hpdata_t *to_purge = psset_pick_purge(&shard->psset);
	if (to_purge == NULL) {
		return false;
	}
	assert(hpdata_purge_allowed_get(to_purge));
	assert(!hpdata_changing_state_get(to_purge));

	/*
	 * Don't let anyone else purge or hugify this page while
	 * we're purging it (allocations and deallocations are
	 * OK).
	 */
	psset_update_begin(&shard->psset, to_purge);
	assert(hpdata_alloc_allowed_get(to_purge));
	hpdata_mid_purge_set(to_purge, true);
	hpdata_purge_allowed_set(to_purge, false);
	hpdata_disallow_hugify(to_purge);
	/*
	 * Unlike with hugification (where concurrent
	 * allocations are allowed), concurrent allocation out
	 * of a hugepage being purged is unsafe; we might hand
	 * out an extent for an allocation and then purge it
	 * (clearing out user data).
	 */
	hpdata_alloc_allowed_set(to_purge, false);
	psset_update_end(&shard->psset, to_purge);

	/* Gather all the metadata we'll need during the purge. */
	bool dehugify = hpdata_huge_get(to_purge);
	hpdata_purge_state_t purge_state;
	size_t num_to_purge = hpdata_purge_begin(to_purge, &purge_state);

	shard->npending_purge += num_to_purge;

	malloc_mutex_unlock(tsdn, &shard->mtx);

	/* Actually do the purging, now that the lock is dropped. */
	if (dehugify) {
		shard->central->hooks.dehugify(hpdata_addr_get(to_purge),
		    HUGEPAGE);
	}
	size_t total_purged = 0;
	uint64_t purges_this_pass = 0;
	void *purge_addr;
	size_t purge_size;
	while (hpdata_purge_next(to_purge, &purge_state, &purge_addr,
	    &purge_size)) {
		total_purged += purge_size;
		assert(total_purged <= HUGEPAGE);
		purges_this_pass++;
		shard->central->hooks.purge(purge_addr, purge_size);
	}

	malloc_mutex_lock(tsdn, &shard->mtx);
	/* The shard updates */
	shard->npending_purge -= num_to_purge;
	shard->stats.npurge_passes++;
	shard->stats.npurges += purges_this_pass;
	shard->central->hooks.curtime(&shard->last_purge,
	    /* first_reading */ false);
	if (dehugify) {
		shard->stats.ndehugifies++;
	}

	/* The hpdata updates. */
	psset_update_begin(&shard->psset, to_purge);
	if (dehugify) {
		hpdata_dehugify(to_purge);
	}
	hpdata_purge_end(to_purge, &purge_state);
	hpdata_mid_purge_set(to_purge, false);

	hpdata_alloc_allowed_set(to_purge, true);
	hpa_update_purge_hugify_eligibility(tsdn, shard, to_purge);

	psset_update_end(&shard->psset, to_purge);

	return true;
}",psset_bin_stats_insert,"void
psset_bin_stats_insert(psset_t *psset, psset_bin_stats_t *binstats,
    hpdata_t *ps) {
	psset_bin_stats_insert_remove(psset, binstats, ps, true);
}",psset_bin_stats_remove,"void
psset_bin_stats_remove(psset_t *psset, psset_bin_stats_t *binstats,
    hpdata_t *ps) {
	psset_bin_stats_insert_remove(psset, binstats, ps, false);
}",psset_stats_remove,"void
psset_stats_remove(psset_t *psset, hpdata_t *ps) {
	if (hpdata_empty(ps)) {
		psset_bin_stats_remove(psset, psset->stats.empty_slabs, ps);
	} else if (hpdata_full(ps)) {
		psset_bin_stats_remove(psset, psset->stats.full_slabs, ps);
	} else {
		pszind_t pind = psset_hpdata_heap_index(ps);
		psset_bin_stats_remove(psset, psset->stats.nonfull_slabs[pind],
		    ps);
	}
}",psset_alloc_container_remove,"void
psset_alloc_container_remove(psset_t *psset, hpdata_t *ps) {
	assert(hpdata_in_psset_alloc_container_get(ps));
	hpdata_in_psset_alloc_container_set(ps, false);

	if (hpdata_empty(ps)) {
		hpdata_empty_list_remove(&psset->empty, ps);
	} else if (hpdata_full(ps)) {
		/* Same as above -- do nothing in this case. */
	} else {
		psset_hpdata_heap_remove(psset, ps);
	}
}",psset_stats_insert,"void
psset_stats_insert(psset_t* psset, hpdata_t *ps) {
	if (hpdata_empty(ps)) {
		psset_bin_stats_insert(psset, psset->stats.empty_slabs, ps);
	} else if (hpdata_full(ps)) {
		psset_bin_stats_insert(psset, psset->stats.full_slabs, ps);
	} else {
		pszind_t pind = psset_hpdata_heap_index(ps);
		psset_bin_stats_insert(psset, psset->stats.nonfull_slabs[pind],
		    ps);
	}
}",psset_alloc_container_insert,"void
psset_alloc_container_insert(psset_t *psset, hpdata_t *ps) {
	assert(!hpdata_in_psset_alloc_container_get(ps));
	hpdata_in_psset_alloc_container_set(ps, true);
	if (hpdata_empty(ps)) {
		/*
		 * This prepend, paired with popping the head in psset_fit,
		 * means we implement LIFO ordering for the empty slabs set,
		 * which seems reasonable.
		 */
		hpdata_empty_list_prepend(&psset->empty, ps);
	} else if (hpdata_full(ps)) {
		/*
		 * We don't need to keep track of the full slabs; we're never
		 * going to return them from a psset_pick_alloc call.
		 */
	} else {
		psset_hpdata_heap_insert(psset, ps);
	}
}"
psset_bin_stats_accum,"void
psset_bin_stats_accum(psset_bin_stats_t *dst, psset_bin_stats_t *src) {
	dst->npageslabs += src->npageslabs;
	dst->nactive += src->nactive;
	dst->ndirty += src->ndirty;
}",psset_bin_stats_insert,"void
psset_bin_stats_insert(psset_t *psset, psset_bin_stats_t *binstats,
    hpdata_t *ps) {
	psset_bin_stats_insert_remove(psset, binstats, ps, true);
}",psset_hpdata_heap_insert,"void
psset_hpdata_heap_insert(psset_t *psset, hpdata_t *ps) {
	pszind_t pind = psset_hpdata_heap_index(ps);
	if (hpdata_age_heap_empty(&psset->pageslabs[pind])) {
		fb_set(psset->pageslab_bitmap, PSSET_NPSIZES, (size_t)pind);
	}
	hpdata_age_heap_insert(&psset->pageslabs[pind], ps);
}",psset_bin_stats_remove,"void
psset_bin_stats_remove(psset_t *psset, psset_bin_stats_t *binstats,
    hpdata_t *ps) {
	psset_bin_stats_insert_remove(psset, binstats, ps, false);
}",psset_hpdata_heap_remove,"void
psset_hpdata_heap_remove(psset_t *psset, hpdata_t *ps) {
	pszind_t pind = psset_hpdata_heap_index(ps);
	hpdata_age_heap_remove(&psset->pageslabs[pind], ps);
	if (hpdata_age_heap_empty(&psset->pageslabs[pind])) {
		fb_unset(psset->pageslab_bitmap, PSSET_NPSIZES, (size_t)pind);
	}
}",test_psset_fake_purge,"void
test_psset_fake_purge(hpdata_t *ps) {
	hpdata_purge_state_t purge_state;
	hpdata_alloc_allowed_set(ps, false);
	hpdata_purge_begin(ps, &purge_state);
	void *addr;
	size_t size;
	while (hpdata_purge_next(ps, &purge_state, &addr, &size)) {
	}
	hpdata_purge_end(ps, &purge_state);
	hpdata_alloc_allowed_set(ps, true);
}"
psset_bin_stats_remove,"void
psset_bin_stats_remove(psset_t *psset, psset_bin_stats_t *binstats,
    hpdata_t *ps) {
	psset_bin_stats_insert_remove(psset, binstats, ps, false);
}",psset_hpdata_heap_remove,"void
psset_hpdata_heap_remove(psset_t *psset, hpdata_t *ps) {
	pszind_t pind = psset_hpdata_heap_index(ps);
	hpdata_age_heap_remove(&psset->pageslabs[pind], ps);
	if (hpdata_age_heap_empty(&psset->pageslabs[pind])) {
		fb_unset(psset->pageslab_bitmap, PSSET_NPSIZES, (size_t)pind);
	}
}",psset_bin_stats_insert,"void
psset_bin_stats_insert(psset_t *psset, psset_bin_stats_t *binstats,
    hpdata_t *ps) {
	psset_bin_stats_insert_remove(psset, binstats, ps, true);
}",psset_hpdata_heap_insert,"void
psset_hpdata_heap_insert(psset_t *psset, hpdata_t *ps) {
	pszind_t pind = psset_hpdata_heap_index(ps);
	if (hpdata_age_heap_empty(&psset->pageslabs[pind])) {
		fb_set(psset->pageslab_bitmap, PSSET_NPSIZES, (size_t)pind);
	}
	hpdata_age_heap_insert(&psset->pageslabs[pind], ps);
}"
hpa_shard_assert_stats_empty,"void
hpa_shard_assert_stats_empty(psset_bin_stats_t *bin_stats) {
	assert(bin_stats->npageslabs == 0);
	assert(bin_stats->nactive == 0);
}",psset_bin_stats_accum,"void
psset_bin_stats_accum(psset_bin_stats_t *dst, psset_bin_stats_t *src) {
	dst->npageslabs += src->npageslabs;
	dst->nactive += src->nactive;
	dst->ndirty += src->ndirty;
}",psset_bin_stats_remove,"void
psset_bin_stats_remove(psset_t *psset, psset_bin_stats_t *binstats,
    hpdata_t *ps) {
	psset_bin_stats_insert_remove(psset, binstats, ps, false);
}",psset_bin_stats_insert,"void
psset_bin_stats_insert(psset_t *psset, psset_bin_stats_t *binstats,
    hpdata_t *ps) {
	psset_bin_stats_insert_remove(psset, binstats, ps, true);
}",stats_expect_empty,"void
stats_expect_empty(psset_bin_stats_t *stats) {
	assert_zu_eq(0, stats->npageslabs,
	    ""Supposedly empty bin had positive npageslabs"");
	expect_zu_eq(0, stats->nactive, ""Unexpected nonempty bin""
	    ""Supposedly empty bin had positive nactive"");
}"
arena_dalloc_bin_slab_prepare,"void
arena_dalloc_bin_slab_prepare(tsdn_t *tsdn, edata_t *slab, bin_t *bin) {
	malloc_mutex_assert_owner(tsdn, &bin->lock);

	assert(slab != bin->slabcur);
	if (config_stats) {
		bin->stats.curslabs--;
	}
}",arena_bin_slabs_full_remove,"void
arena_bin_slabs_full_remove(arena_t *arena, bin_t *bin, edata_t *slab) {
	if (arena_is_auto(arena)) {
		return;
	}
	edata_list_active_remove(&bin->slabs_full, slab);
}"
test_combinations,"void
test_combinations(szind_t ind, size_t sizes_array[N_PTRS],
    int flags_array[N_PTRS]) {
#define MALLCTL_STR_LEN 64
	assert(opt_prof && opt_prof_stats);

	char mallctl_live_str[MALLCTL_STR_LEN];
	char mallctl_accum_str[MALLCTL_STR_LEN];
	if (ind < SC_NBINS) {
		malloc_snprintf(mallctl_live_str, MALLCTL_STR_LEN,
		    ""prof.stats.bins.%u.live"", (unsigned)ind);
		malloc_snprintf(mallctl_accum_str, MALLCTL_STR_LEN,
		    ""prof.stats.bins.%u.accum"", (unsigned)ind);
	} else {
		malloc_snprintf(mallctl_live_str, MALLCTL_STR_LEN,
		    ""prof.stats.lextents.%u.live"", (unsigned)(ind - SC_NBINS));
		malloc_snprintf(mallctl_accum_str, MALLCTL_STR_LEN,
		    ""prof.stats.lextents.%u.accum"", (unsigned)(ind - SC_NBINS));
	}

	size_t stats_len = 2 * sizeof(uint64_t);

	uint64_t live_stats_orig[2];
	assert_d_eq(mallctl(mallctl_live_str, &live_stats_orig, &stats_len,
	    NULL, 0), 0, """");
	uint64_t accum_stats_orig[2];
	assert_d_eq(mallctl(mallctl_accum_str, &accum_stats_orig, &stats_len,
	    NULL, 0), 0, """");

	void *ptrs[N_PTRS];

	uint64_t live_req_sum = 0;
	uint64_t live_count = 0;
	uint64_t accum_req_sum = 0;
	uint64_t accum_count = 0;

	for (size_t i = 0; i < N_PTRS; ++i) {
		size_t sz = sizes_array[i];
		int flags = flags_array[i];
		void *p = mallocx(sz, flags);
		assert_ptr_not_null(p, ""malloc() failed"");
		assert(TEST_MALLOC_SIZE(p) == sz_index2size(ind));
		ptrs[i] = p;
		live_req_sum += sz;
		live_count++;
		accum_req_sum += sz;
		accum_count++;
		uint64_t live_stats[2];
		assert_d_eq(mallctl(mallctl_live_str, &live_stats, &stats_len,
		    NULL, 0), 0, """");
		expect_u64_eq(live_stats[0] - live_stats_orig[0],
		    live_req_sum, """");
		expect_u64_eq(live_stats[1] - live_stats_orig[1],
		    live_count, """");
		uint64_t accum_stats[2];
		assert_d_eq(mallctl(mallctl_accum_str, &accum_stats, &stats_len,
		    NULL, 0), 0, """");
		expect_u64_eq(accum_stats[0] - accum_stats_orig[0],
		    accum_req_sum, """");
		expect_u64_eq(accum_stats[1] - accum_stats_orig[1],
		    accum_count, """");
	}

	for (size_t i = 0; i < N_PTRS; ++i) {
		size_t sz = sizes_array[i];
		int flags = flags_array[i];
		sdallocx(ptrs[i], sz, flags);
		live_req_sum -= sz;
		live_count--;
		uint64_t live_stats[2];
		assert_d_eq(mallctl(mallctl_live_str, &live_stats, &stats_len,
		    NULL, 0), 0, """");
		expect_u64_eq(live_stats[0] - live_stats_orig[0],
		    live_req_sum, """");
		expect_u64_eq(live_stats[1] - live_stats_orig[1],
		    live_count, """");
		uint64_t accum_stats[2];
		assert_d_eq(mallctl(mallctl_accum_str, &accum_stats, &stats_len,
		    NULL, 0), 0, """");
		expect_u64_eq(accum_stats[0] - accum_stats_orig[0],
		    accum_req_sum, """");
		expect_u64_eq(accum_stats[1] - accum_stats_orig[1],
		    accum_count, """");
	}
#undef MALLCTL_STR_LEN
}",test_szind_wrapper,"void
test_szind_wrapper(szind_t ind) {
	size_t sizes_array[N_PTRS];
	int flags_array[N_PTRS];
	for (size_t i = 0, sz = sz_index2size(ind) - N_PTRS; i < N_PTRS;
	    ++i, ++sz) {
		sizes_array[i] = sz;
		flags_array[i] = 0;
	}
	test_combinations(ind, sizes_array, flags_array);
}",test_szind_aligned_wrapper,"void
test_szind_aligned_wrapper(szind_t ind, unsigned lg_align) {
	size_t sizes_array[N_PTRS];
	int flags_array[N_PTRS];
	int flags = MALLOCX_LG_ALIGN(lg_align);
	for (size_t i = 0, sz = sz_index2size(ind) - N_PTRS; i < N_PTRS;
	    ++i, ++sz) {
		sizes_array[i] = sz;
		flags_array[i] = flags;
	}
	test_combinations(
	    sz_size2index(sz_sa2u(sz_index2size(ind), 1 << lg_align)),
	    sizes_array, flags_array);
}"
batch_alloc_wrapper,"void
batch_alloc_wrapper(size_t batch) {
	batch_alloc_packet_t batch_alloc_packet =
	    {batch_ptrs + batch_ptrs_next, batch, SIZE, 0};
	size_t filled;
	size_t len = sizeof(size_t);
	assert_d_eq(mallctlbymib(mib, miblen, &filled, &len,
	    &batch_alloc_packet, sizeof(batch_alloc_packet)), 0, """");
	assert_zu_eq(filled, batch, """");
}",item_alloc_wrapper,"void
item_alloc_wrapper(size_t batch) {
	for (size_t i = item_ptrs_next, end = i + batch; i < end; ++i) {
		item_ptrs[i] = malloc(SIZE);
	}
}",release_and_clear,"void
release_and_clear(void **ptrs, size_t len) {
	for (size_t i = 0; i < len; ++i) {
		void *p = ptrs[i];
		assert_ptr_not_null(p, ""allocation failed"");
		sdallocx(p, SIZE, 0);
		ptrs[i] = NULL;
	}
}",batch_alloc_without_free,"void
batch_alloc_without_free(size_t batch) {
	batch_alloc_wrapper(batch);
	batch_ptrs_next += batch;
}",item_alloc_without_free,"void
item_alloc_without_free(size_t batch) {
	item_alloc_wrapper(batch);
	item_ptrs_next += batch;
}",batch_alloc_with_free,"void
batch_alloc_with_free(size_t batch) {
	batch_alloc_wrapper(batch);
	release_and_clear(batch_ptrs + batch_ptrs_next, batch);
	batch_ptrs_next += batch;
}",item_alloc_with_free,"void
item_alloc_with_free(size_t batch) {
	item_alloc_wrapper(batch);
	release_and_clear(item_ptrs + item_ptrs_next, batch);
	item_ptrs_next += batch;
}"
test_psset_alloc_reuse,"bool
test_psset_alloc_reuse(psset_t *psset, edata_t *r_edata, size_t size) {
	hpdata_t *ps = psset_pick_alloc(psset, size);
	if (ps == NULL) {
		return true;
	}
	psset_update_begin(psset, ps);
	void *addr = hpdata_reserve_alloc(ps, size);
	edata_init(r_edata, edata_arena_ind_get(r_edata), addr, size,
	    /* slab */ false, SC_NSIZES, /* sn */ 0, extent_state_active,
	    /* zeroed */ false, /* committed */ true, EXTENT_PAI_HPA,
	    EXTENT_NOT_HEAD);
	edata_ps_set(r_edata, ps);
	psset_update_end(psset, ps);
	return false;
}",test_psset_alloc_new,"void
test_psset_alloc_new(psset_t *psset, hpdata_t *ps, edata_t *r_edata,
    size_t size) {
	hpdata_assert_empty(ps);

	test_psset_fake_purge(ps);

	psset_insert(psset, ps);
	psset_update_begin(psset, ps);

        void *addr = hpdata_reserve_alloc(ps, size);
        edata_init(r_edata, edata_arena_ind_get(r_edata), addr, size,
	    /* slab */ false, SC_NSIZES, /* sn */ 0, extent_state_active,
            /* zeroed */ false, /* committed */ true, EXTENT_PAI_HPA,
            EXTENT_NOT_HEAD);
        edata_ps_set(r_edata, ps);
	psset_update_end(psset, ps);
}"
psset_insert,"void
psset_insert(psset_t *psset, hpdata_t *ps) {
	hpdata_in_psset_set(ps, true);

	psset_stats_insert(psset, ps);
	if (hpdata_alloc_allowed_get(ps)) {
		psset_alloc_container_insert(psset, ps);
	}
	psset_maybe_insert_purge_list(psset, ps);

	if (hpdata_hugify_allowed_get(ps)) {
		hpdata_in_psset_hugify_container_set(ps, true);
		hpdata_hugify_list_append(&psset->to_hugify, ps);
	}
}",psset_remove,"void
psset_remove(psset_t *psset, hpdata_t *ps) {
	hpdata_in_psset_set(ps, false);

	psset_stats_remove(psset, ps);
	if (hpdata_in_psset_alloc_container_get(ps)) {
		psset_alloc_container_remove(psset, ps);
	}
	psset_maybe_remove_purge_list(psset, ps);
	if (hpdata_in_psset_hugify_container_get(ps)) {
		hpdata_in_psset_hugify_container_set(ps, false);
		hpdata_hugify_list_remove(&psset->to_hugify, ps);
	}
}"
psset_hpdata_heap_insert,"void
psset_hpdata_heap_insert(psset_t *psset, hpdata_t *ps) {
	pszind_t pind = psset_hpdata_heap_index(ps);
	if (hpdata_age_heap_empty(&psset->pageslabs[pind])) {
		fb_set(psset->pageslab_bitmap, PSSET_NPSIZES, (size_t)pind);
	}
	hpdata_age_heap_insert(&psset->pageslabs[pind], ps);
}",psset_hpdata_heap_remove,"void
psset_hpdata_heap_remove(psset_t *psset, hpdata_t *ps) {
	pszind_t pind = psset_hpdata_heap_index(ps);
	hpdata_age_heap_remove(&psset->pageslabs[pind], ps);
	if (hpdata_age_heap_empty(&psset->pageslabs[pind])) {
		fb_unset(psset->pageslab_bitmap, PSSET_NPSIZES, (size_t)pind);
	}
}"
hpa_shard_assert_stats_empty,"void
hpa_shard_assert_stats_empty(psset_bin_stats_t *bin_stats) {
	assert(bin_stats->npageslabs == 0);
	assert(bin_stats->nactive == 0);
}",malloc_abort_invalid_conf,"void
malloc_abort_invalid_conf(void) {
	assert(opt_abort_conf);
	malloc_printf(""<jemalloc>: Abort (abort_conf:true) on invalid conf ""
	    ""value (see above).\n"");
	invalid_conf_abort();
}",psset_bin_stats_remove,"void
psset_bin_stats_remove(psset_t *psset, psset_bin_stats_t *binstats,
    hpdata_t *ps) {
	psset_bin_stats_insert_remove(psset, binstats, ps, false);
}",psset_bin_stats_insert,"void
psset_bin_stats_insert(psset_t *psset, psset_bin_stats_t *binstats,
    hpdata_t *ps) {
	psset_bin_stats_insert_remove(psset, binstats, ps, true);
}"
hpa_shard_assert_stats_empty,"void
hpa_shard_assert_stats_empty(psset_bin_stats_t *bin_stats) {
	assert(bin_stats->npageslabs == 0);
	assert(bin_stats->nactive == 0);
}",psset_bin_stats_accum,"void
psset_bin_stats_accum(psset_bin_stats_t *dst, psset_bin_stats_t *src) {
	dst->npageslabs += src->npageslabs;
	dst->nactive += src->nactive;
	dst->ndirty += src->ndirty;
}",stats_expect_empty,"void
stats_expect_empty(psset_bin_stats_t *stats) {
	assert_zu_eq(0, stats->npageslabs,
	    ""Supposedly empty bin had positive npageslabs"");
	expect_zu_eq(0, stats->nactive, ""Unexpected nonempty bin""
	    ""Supposedly empty bin had positive nactive"");
}"
mallocx_arena_get,"unsigned
mallocx_arena_get(int flags) {
	if (unlikely((flags & MALLOCX_ARENA_MASK) != 0)) {
		return MALLOCX_ARENA_GET(flags);
	} else {
		return ARENA_IND_AUTOMATIC;
	}
}",mallocx_tcache_get,"unsigned
mallocx_tcache_get(int flags) {
	if (likely((flags & MALLOCX_TCACHE_MASK) == 0)) {
		return TCACHE_IND_AUTOMATIC;
	} else if ((flags & MALLOCX_TCACHE_MASK) == MALLOCX_TCACHE_NONE) {
		return TCACHE_IND_NONE;
	} else {
		return MALLOCX_TCACHE_GET(flags);
	}
}",release_batch,"void
release_batch(void **ptrs, size_t batch, size_t size) {
	for (size_t i = 0; i < batch; ++i) {
		sdallocx(ptrs[i], size, 0);
	}
}"
test_edata_cache_destroy,"void
test_edata_cache_destroy(edata_cache_t *edata_cache) {
	base_delete(TSDN_NULL, edata_cache->base);
}",test_edata_cache_init,"void
test_edata_cache_init(edata_cache_t *edata_cache) {
	base_t *base = base_new(TSDN_NULL, /* ind */ 1,
	    &ehooks_default_extent_hooks, /* metadata_use_hooks */ true);
	assert_ptr_not_null(base, """");
	bool err = edata_cache_init(edata_cache, base);
	assert_false(err, """");
}"
stats_expect_empty,"void
stats_expect_empty(psset_bin_stats_t *stats) {
	assert_zu_eq(0, stats->npageslabs,
	    ""Supposedly empty bin had positive npageslabs"");
	expect_zu_eq(0, stats->nactive, ""Unexpected nonempty bin""
	    ""Supposedly empty bin had positive nactive"");
}",edata_init_test,"void
edata_init_test(edata_t *edata) {
	memset(edata, 0, sizeof(*edata));
	edata_arena_ind_set(edata, ALLOC_ARENA_IND);
	edata_esn_set(edata, ALLOC_ESN);
}"
prof_double_uint64_cast,"uint64_t
prof_double_uint64_cast(double d) {
	/*
	 * Note: UINT64_MAX + 1 is exactly representable as a double on all
	 * reasonable platforms (certainly those we'll support).  Writing this
	 * as !(a < b) instead of (a >= b) means that we're NaN-safe.
	 */
	double rounded = round(d);
	if (!(rounded < (double)UINT64_MAX)) {
		return UINT64_MAX;
	}
	return (uint64_t)rounded;
}",prof_unbias_map_init,"void prof_unbias_map_init(void) {
	/* See the comment in prof_sample_new_event_wait */
#ifdef JEMALLOC_PROF
	for (szind_t i = 0; i < SC_NSIZES; i++) {
		double sz = (double)sz_index2size(i);
		double rate = (double)(ZU(1) << lg_prof_sample);
		double div_val = 1.0 - exp(-sz / rate);
		double unbiased_sz = sz / div_val;
		/*
		 * The ""true"" right value for the unbiased count is
		 * 1.0/(1 - exp(-sz/rate)).  The problem is, we keep the counts
		 * as integers (for a variety of reasons -- rounding errors
		 * could trigger asserts, and not all libcs can properly handle
		 * floating point arithmetic during malloc calls inside libc).
		 * Rounding to an integer, though, can lead to rounding errors
		 * of over 30% for sizes close to the sampling rate.  So
		 * instead, we multiply by a constant, dividing the maximum
		 * possible roundoff error by that constant.  To avoid overflow
		 * in summing up size_t values, the largest safe constant we can
		 * pick is the size of the smallest allocation.
		 */
		double cnt_shift = (double)(ZU(1) << SC_LG_TINY_MIN);
		double shifted_unbiased_cnt = cnt_shift / div_val;
		prof_unbiased_sz[i] = (size_t)round(unbiased_sz);
		prof_shifted_unbiased_cnt[i] = (size_t)round(
		    shifted_unbiased_cnt);
	}
#else
	unreachable();
#endif
}"
mallocx_arena_get,"unsigned
mallocx_arena_get(int flags) {
	if (unlikely((flags & MALLOCX_ARENA_MASK) != 0)) {
		return MALLOCX_ARENA_GET(flags);
	} else {
		return ARENA_IND_AUTOMATIC;
	}
}",batch_alloc_prof_sample_assert,"void
batch_alloc_prof_sample_assert(tsd_t *tsd, size_t batch, size_t usize) {
	assert(config_prof && opt_prof);
	bool prof_sample_event = te_prof_sample_event_lookahead(tsd,
	    batch * usize);
	assert(!prof_sample_event);
	size_t surplus;
	prof_sample_event = te_prof_sample_event_lookahead_surplus(tsd,
	    (batch + 1) * usize, &surplus);
	assert(prof_sample_event);
	assert(surplus < usize);
}"
arena_decay_ms_get,"ssize_t
arena_decay_ms_get(arena_t *arena, extent_state_t state) {
	return pa_decay_ms_get(&arena->pa_shard, state);
}",arena_decide_unforced_purge_eagerness,"pac_purge_eagerness_t
arena_decide_unforced_purge_eagerness(bool is_background_thread) {
	if (is_background_thread) {
		return PAC_PURGE_ALWAYS;
	} else if (!is_background_thread && background_thread_enabled()) {
		return PAC_PURGE_NEVER;
	} else {
		return PAC_PURGE_ON_EPOCH_ADVANCE;
	}
}",pac_decay_data_get,"void
pac_decay_data_get(pac_t *pac, extent_state_t state,
    decay_t **r_decay, pac_decay_stats_t **r_decay_stats, ecache_t **r_ecache) {
	switch(state) {
	case extent_state_dirty:
		*r_decay = &pac->decay_dirty;
		*r_decay_stats = &pac->stats->decay_dirty;
		*r_ecache = &pac->ecache_dirty;
		return;
	case extent_state_muzzy:
		*r_decay = &pac->decay_muzzy;
		*r_decay_stats = &pac->stats->decay_muzzy;
		*r_ecache = &pac->ecache_muzzy;
		return;
	case extent_state_active:
	case extent_state_retained:
	case extent_state_transition:
	case extent_state_merging:
	default:
		unreachable();
	}
}"
extent_deregister,"void
extent_deregister(tsdn_t *tsdn, pac_t *pac, edata_t *edata) {
	extent_deregister_impl(tsdn, pac, edata, true);
}",extent_register_impl,"bool
extent_register_impl(tsdn_t *tsdn, pac_t *pac, edata_t *edata, bool gdump_add) {
	assert(edata_state_get(edata) == extent_state_active);
	/*
	 * No locking needed, as the edata must be in active state, which
	 * prevents other threads from accessing the edata.
	 */
	if (emap_register_boundary(tsdn, pac->emap, edata, SC_NSIZES,
	    /* slab */ false)) {
		return true;
	}

	if (config_prof && gdump_add) {
		extent_gdump_add(tsdn, edata);
	}

	return false;
}",extent_register,"bool
extent_register(tsdn_t *tsdn, pac_t *pac, edata_t *edata) {
	return extent_register_impl(tsdn, pac, edata, true);
}",extent_deregister_impl,"void
extent_deregister_impl(tsdn_t *tsdn, pac_t *pac, edata_t *edata,
    bool gdump) {
	emap_deregister_boundary(tsdn, pac->emap, edata);

	if (config_prof && gdump) {
		extent_gdump_sub(tsdn, edata);
	}
}",extent_deregister_no_gdump_sub,"void
extent_deregister_no_gdump_sub(tsdn_t *tsdn, pac_t *pac,
    edata_t *edata) {
	extent_deregister_impl(tsdn, pac, edata, false);
}",extent_maximally_purge,"void
extent_maximally_purge(tsdn_t *tsdn, pac_t *pac, ehooks_t *ehooks,
    edata_t *edata) {
	size_t extent_size = edata_size_get(edata);
	extent_dalloc_wrapper(tsdn, pac, ehooks, edata);
	if (config_stats) {
		/* Update stats accordingly. */
		LOCKEDINT_MTX_LOCK(tsdn, *pac->stats_mtx);
		locked_inc_u64(tsdn,
		    LOCKEDINT_MTX(*pac->stats_mtx),
		    &pac->stats->decay_dirty.nmadvise, 1);
		locked_inc_u64(tsdn,
		    LOCKEDINT_MTX(*pac->stats_mtx),
		    &pac->stats->decay_dirty.purged,
		    extent_size >> LG_PAGE);
		LOCKEDINT_MTX_UNLOCK(tsdn, *pac->stats_mtx);
		atomic_fetch_sub_zu(&pac->stats->pac_mapped, extent_size,
		    ATOMIC_RELAXED);
	}
}",extent_reregister,"void
extent_reregister(tsdn_t *tsdn, pac_t *pac, edata_t *edata) {
	bool err = extent_register(tsdn, pac, edata);
	assert(!err);
}",extent_merge_impl,"bool
extent_merge_impl(tsdn_t *tsdn, pac_t *pac, ehooks_t *ehooks, edata_t *a,
    edata_t *b, bool holding_core_locks) {
	/* Only the expanding path may merge w/o holding ecache locks. */
	if (holding_core_locks) {
		witness_assert_positive_depth_to_rank(
		    tsdn_witness_tsdp_get(tsdn), WITNESS_RANK_CORE);
	} else {
		witness_assert_depth_to_rank(tsdn_witness_tsdp_get(tsdn),
		    WITNESS_RANK_CORE, 0);
	}

	assert(edata_base_get(a) < edata_base_get(b));
	assert(edata_arena_ind_get(a) == edata_arena_ind_get(b));
	assert(edata_arena_ind_get(a) == ehooks_ind_get(ehooks));
	emap_assert_mapped(tsdn, pac->emap, a);
	emap_assert_mapped(tsdn, pac->emap, b);

	bool err = ehooks_merge(tsdn, ehooks, edata_base_get(a),
	    edata_size_get(a), edata_base_get(b), edata_size_get(b),
	    edata_committed_get(a));

	if (err) {
		return true;
	}

	/*
	 * The rtree writes must happen while all the relevant elements are
	 * owned, so the following code uses decomposed helper functions rather
	 * than extent_{,de}register() to do things in the right order.
	 */
	emap_prepare_t prepare;
	emap_merge_prepare(tsdn, pac->emap, &prepare, a, b);

	assert(edata_state_get(a) == extent_state_active ||
	    edata_state_get(a) == extent_state_merging);
	edata_state_set(a, extent_state_active);
	edata_size_set(a, edata_size_get(a) + edata_size_get(b));
	edata_sn_set(a, (edata_sn_get(a) < edata_sn_get(b)) ?
	    edata_sn_get(a) : edata_sn_get(b));
	edata_zeroed_set(a, edata_zeroed_get(a) && edata_zeroed_get(b));

	emap_merge_commit(tsdn, pac->emap, &prepare, a, b);

	edata_cache_put(tsdn, pac->edata_cache, b);

	return false;
}"
pa_nactive_add,"void
pa_nactive_add(pa_shard_t *shard, size_t add_pages) {
	atomic_fetch_add_zu(&shard->nactive, add_pages, ATOMIC_RELAXED);
}",pa_nactive_sub,"void
pa_nactive_sub(pa_shard_t *shard, size_t sub_pages) {
	assert(pa_shard_nactive(shard) >= sub_pages);
	atomic_fetch_sub_zu(&shard->nactive, sub_pages, ATOMIC_RELAXED);
}"
prof_dump_open,"void
prof_dump_open(prof_dump_arg_t *arg, const char *filename) {
	arg->prof_dump_fd = prof_dump_open_file(filename, 0644);
	prof_dump_check_possible_error(arg, arg->prof_dump_fd == -1,
	    ""<jemalloc>: failed to open \""%s\""\n"", filename);
}",prof_dump_maps,"void
prof_dump_maps(buf_writer_t *buf_writer) {
	buf_writer_cb(buf_writer, ""\nMAPPED_LIBRARIES:\n"");
	/* No proc map file to read on MacOS, dump dyld maps for backtrace. */
	prof_dump_dyld_maps(buf_writer);
}",prof_dump_close,"void
prof_dump_close(prof_dump_arg_t *arg) {
	if (arg->prof_dump_fd != -1) {
		close(arg->prof_dump_fd);
	}
}"
prof_dump_print_cnts,"void
prof_dump_print_cnts(write_cb_t *prof_dump_write, void *cbopaque,
    const prof_cnt_t *cnts) {
	uint64_t curobjs;
	uint64_t curbytes;
	uint64_t accumobjs;
	uint64_t accumbytes;
	if (opt_prof_unbias) {
		prof_do_unbias(cnts->curobjs_shifted_unbiased,
		    cnts->curbytes_unbiased, &curobjs, &curbytes);
		prof_do_unbias(cnts->accumobjs_shifted_unbiased,
		    cnts->accumbytes_unbiased, &accumobjs, &accumbytes);
	} else {
		curobjs = cnts->curobjs;
		curbytes = cnts->curbytes;
		accumobjs = cnts->accumobjs;
		accumbytes = cnts->accumbytes;
	}
	prof_dump_printf(prof_dump_write, cbopaque,
	    ""%""FMTu64"": %""FMTu64"" [%""FMTu64"": %""FMTu64""]"",
	    curobjs, curbytes, accumobjs, accumbytes);
}",prof_dump_gctx,"void
prof_dump_gctx(prof_dump_iter_arg_t *arg, prof_gctx_t *gctx,
    const prof_bt_t *bt, prof_gctx_tree_t *gctxs) {
	cassert(config_prof);
	malloc_mutex_assert_owner(arg->tsdn, gctx->lock);

	/* Avoid dumping such gctx's that have no useful data. */
	if ((!opt_prof_accum && gctx->cnt_summed.curobjs == 0) ||
	    (opt_prof_accum && gctx->cnt_summed.accumobjs == 0)) {
		assert(gctx->cnt_summed.curobjs == 0);
		assert(gctx->cnt_summed.curbytes == 0);
		/*
		 * These asserts would not be correct -- see the comment on races
		 * in prof.c
		 * assert(gctx->cnt_summed.curobjs_unbiased == 0);
		 * assert(gctx->cnt_summed.curbytes_unbiased == 0);
		*/
		assert(gctx->cnt_summed.accumobjs == 0);
		assert(gctx->cnt_summed.accumobjs_shifted_unbiased == 0);
		assert(gctx->cnt_summed.accumbytes == 0);
		assert(gctx->cnt_summed.accumbytes_unbiased == 0);
		return;
	}

	arg->prof_dump_write(arg->cbopaque, ""@"");
	for (unsigned i = 0; i < bt->len; i++) {
		prof_dump_printf(arg->prof_dump_write, arg->cbopaque,
		    "" %#""FMTxPTR, (uintptr_t)bt->vec[i]);
	}

	arg->prof_dump_write(arg->cbopaque, ""\n  t*: "");
	prof_dump_print_cnts(arg->prof_dump_write, arg->cbopaque,
	    &gctx->cnt_summed);
	arg->prof_dump_write(arg->cbopaque, ""\n"");

	tctx_tree_iter(&gctx->tctxs, NULL, prof_tctx_dump_iter, arg);
}",prof_dump_prep,"void
prof_dump_prep(tsd_t *tsd, prof_tdata_t *tdata, prof_cnt_t *cnt_all,
    size_t *leak_ngctx, prof_gctx_tree_t *gctxs) {
	size_t tabind;
	union {
		prof_gctx_t	*p;
		void		*v;
	} gctx;

	prof_enter(tsd, tdata);

	/*
	 * Put gctx's in limbo and clear their counters in preparation for
	 * summing.
	 */
	gctx_tree_new(gctxs);
	for (tabind = 0; !ckh_iter(&bt2gctx, &tabind, NULL, &gctx.v);) {
		prof_dump_gctx_prep(tsd_tsdn(tsd), gctx.p, gctxs);
	}

	/*
	 * Iterate over tdatas, and for the non-expired ones snapshot their tctx
	 * stats and merge them into the associated gctx's.
	 */
	memset(cnt_all, 0, sizeof(prof_cnt_t));
	prof_tdata_merge_iter_arg_t prof_tdata_merge_iter_arg = {tsd_tsdn(tsd),
	    cnt_all};
	malloc_mutex_lock(tsd_tsdn(tsd), &tdatas_mtx);
	tdata_tree_iter(&tdatas, NULL, prof_tdata_merge_iter,
	    &prof_tdata_merge_iter_arg);
	malloc_mutex_unlock(tsd_tsdn(tsd), &tdatas_mtx);

	/* Merge tctx stats into gctx's. */
	*leak_ngctx = 0;
	prof_gctx_merge_iter_arg_t prof_gctx_merge_iter_arg = {tsd_tsdn(tsd),
	    leak_ngctx};
	gctx_tree_iter(gctxs, NULL, prof_gctx_merge_iter,
	    &prof_gctx_merge_iter_arg);

	prof_leave(tsd, tdata);
}",prof_dump_header,"void
prof_dump_header(prof_dump_iter_arg_t *arg, const prof_cnt_t *cnt_all) {
	prof_dump_printf(arg->prof_dump_write, arg->cbopaque,
	    ""heap_v2/%""FMTu64""\n  t*: "", ((uint64_t)1U << lg_prof_sample));
	prof_dump_print_cnts(arg->prof_dump_write, arg->cbopaque, cnt_all);
	arg->prof_dump_write(arg->cbopaque, ""\n"");

	malloc_mutex_lock(arg->tsdn, &tdatas_mtx);
	tdata_tree_iter(&tdatas, NULL, prof_tdata_dump_iter, arg);
	malloc_mutex_unlock(arg->tsdn, &tdatas_mtx);
}"
prof_dump_flush,"void
prof_dump_flush(void *opaque, const char *s) {
	cassert(config_prof);
	prof_dump_arg_t *arg = (prof_dump_arg_t *)opaque;
	if (!arg->error) {
		ssize_t err = prof_dump_write_file(arg->prof_dump_fd, s,
		    strlen(s));
		prof_dump_check_possible_error(arg, err == -1,
		    ""<jemalloc>: failed to write during heap profile flush\n"");
	}
}",prof_open_maps_internal,"int
prof_open_maps_internal(const char *format, ...) {
	int mfd;
	va_list ap;
	char filename[PATH_MAX + 1];

	va_start(ap, format);
	malloc_vsnprintf(filename, sizeof(filename), format, ap);
	va_end(ap);

#if defined(O_CLOEXEC)
	mfd = open(filename, O_RDONLY | O_CLOEXEC);
#else
	mfd = open(filename, O_RDONLY);
	if (mfd != -1) {
		fcntl(mfd, F_SETFD, fcntl(mfd, F_GETFD) | FD_CLOEXEC);
	}
#endif

	return mfd;
}",prof_getpid,"int
prof_getpid(void) {
#ifdef _WIN32
	return GetCurrentProcessId();
#else
	return getpid();
#endif
}",prof_dump_open,"void
prof_dump_open(prof_dump_arg_t *arg, const char *filename) {
	arg->prof_dump_fd = prof_dump_open_file(filename, 0644);
	prof_dump_check_possible_error(arg, arg->prof_dump_fd == -1,
	    ""<jemalloc>: failed to open \""%s\""\n"", filename);
}",prof_dump_maps,"void
prof_dump_maps(buf_writer_t *buf_writer) {
	buf_writer_cb(buf_writer, ""\nMAPPED_LIBRARIES:\n"");
	/* No proc map file to read on MacOS, dump dyld maps for backtrace. */
	prof_dump_dyld_maps(buf_writer);
}",prof_dump_close,"void
prof_dump_close(prof_dump_arg_t *arg) {
	if (arg->prof_dump_fd != -1) {
		close(arg->prof_dump_fd);
	}
}"
prof_dump,"bool
prof_dump(tsd_t *tsd, bool propagate_err, const char *filename,
    bool leakcheck) {
	cassert(config_prof);
	assert(tsd_reentrancy_level_get(tsd) == 0);

	prof_tdata_t * tdata = prof_tdata_get(tsd, true);
	if (tdata == NULL) {
		return true;
	}

	prof_dump_arg_t arg = {/* handle_error_locally */ !propagate_err,
	    /* error */ false, /* prof_dump_fd */ -1};

	pre_reentrancy(tsd, NULL);
	malloc_mutex_lock(tsd_tsdn(tsd), &prof_dump_mtx);

	prof_dump_open(&arg, filename);
	buf_writer_t buf_writer;
	bool err = buf_writer_init(tsd_tsdn(tsd), &buf_writer, prof_dump_flush,
	    &arg, prof_dump_buf, PROF_DUMP_BUFSIZE);
	assert(!err);
	prof_dump_impl(tsd, buf_writer_cb, &buf_writer, tdata, leakcheck);
	prof_dump_maps(&buf_writer);
	buf_writer_terminate(tsd_tsdn(tsd), &buf_writer);
	prof_dump_close(&arg);

	prof_dump_hook_t dump_hook = prof_dump_hook_get();
	if (dump_hook != NULL) {
		dump_hook(filename);
	}
	malloc_mutex_unlock(tsd_tsdn(tsd), &prof_dump_mtx);
	post_reentrancy(tsd);

	return arg.error;
}",prof_strncpy,"void
prof_strncpy(char *UNUSED dest, const char *UNUSED src, size_t UNUSED size) {
	cassert(config_prof);
#ifdef JEMALLOC_PROF
	strncpy(dest, src, size);
#endif
}"
prof_dump_printf,"void
prof_dump_printf(write_cb_t *prof_dump_write, void *cbopaque,
    const char *format, ...) {
	va_list ap;
	char buf[PROF_PRINTF_BUFSIZE];

	va_start(ap, format);
	malloc_vsnprintf(buf, sizeof(buf), format, ap);
	va_end(ap);
	prof_dump_write(cbopaque, buf);
}",prof_dump_print_cnts,"void
prof_dump_print_cnts(write_cb_t *prof_dump_write, void *cbopaque,
    const prof_cnt_t *cnts) {
	uint64_t curobjs;
	uint64_t curbytes;
	uint64_t accumobjs;
	uint64_t accumbytes;
	if (opt_prof_unbias) {
		prof_do_unbias(cnts->curobjs_shifted_unbiased,
		    cnts->curbytes_unbiased, &curobjs, &curbytes);
		prof_do_unbias(cnts->accumobjs_shifted_unbiased,
		    cnts->accumbytes_unbiased, &accumobjs, &accumbytes);
	} else {
		curobjs = cnts->curobjs;
		curbytes = cnts->curbytes;
		accumobjs = cnts->accumobjs;
		accumbytes = cnts->accumbytes;
	}
	prof_dump_printf(prof_dump_write, cbopaque,
	    ""%""FMTu64"": %""FMTu64"" [%""FMTu64"": %""FMTu64""]"",
	    curobjs, curbytes, accumobjs, accumbytes);
}"
nstime_init2,"void
nstime_init2(nstime_t *time, uint64_t sec, uint64_t nsec) {
	nstime_set_initialized(time);
	time->ns = sec * BILLION + nsec;
}",nstime_copy,"void
nstime_copy(nstime_t *time, const nstime_t *source) {
	/* Source is required to be initialized. */
	nstime_assert_initialized(source);
	*time = *source;
	nstime_assert_initialized(time);
}",nstime_get_realtime,"void
nstime_get_realtime(nstime_t *time) {
#if defined(JEMALLOC_HAVE_CLOCK_REALTIME) && !defined(_WIN32)
	struct timespec ts;

	clock_gettime(CLOCK_REALTIME, &ts);
	nstime_init2(time, ts.tv_sec, ts.tv_nsec);
#else
	unreachable();
#endif
}",nstime_get,"void
nstime_get(nstime_t *time) {
	FILETIME ft;
	uint64_t ticks_100ns;

	GetSystemTimeAsFileTime(&ft);
	ticks_100ns = (((uint64_t)ft.dwHighDateTime) << 32) | ft.dwLowDateTime;

	nstime_init(time, ticks_100ns * 100);
}"
prof_recent_alloc_max_get,"ssize_t
prof_recent_alloc_max_get(tsd_t *tsd) {
	malloc_mutex_assert_owner(tsd_tsdn(tsd), &prof_recent_alloc_mtx);
	return prof_recent_alloc_max_get_no_lock();
}",prof_recent_alloc_assert_count,"void
prof_recent_alloc_assert_count(tsd_t *tsd) {
	malloc_mutex_assert_owner(tsd_tsdn(tsd), &prof_recent_alloc_mtx);
	if (!config_debug) {
		return;
	}
	ssize_t count = 0;
	prof_recent_t *n;
	ql_foreach(n, &prof_recent_alloc_list, link) {
		++count;
	}
	assert(count == prof_recent_alloc_count);
	assert(prof_recent_alloc_max_get(tsd) == -1 ||
	    count <= prof_recent_alloc_max_get(tsd));
}"
prof_recent_alloc_max_get,"ssize_t
prof_recent_alloc_max_get(tsd_t *tsd) {
	malloc_mutex_assert_owner(tsd_tsdn(tsd), &prof_recent_alloc_mtx);
	return prof_recent_alloc_max_get_no_lock();
}",prof_recent_alloc_assert_count,"void
prof_recent_alloc_assert_count(tsd_t *tsd) {
	malloc_mutex_assert_owner(tsd_tsdn(tsd), &prof_recent_alloc_mtx);
	if (!config_debug) {
		return;
	}
	ssize_t count = 0;
	prof_recent_t *n;
	ql_foreach(n, &prof_recent_alloc_list, link) {
		++count;
	}
	assert(count == prof_recent_alloc_count);
	assert(prof_recent_alloc_max_get(tsd) == -1 ||
	    count <= prof_recent_alloc_max_get(tsd));
}",prof_recent_alloc_max_update,"ssize_t
prof_recent_alloc_max_update(tsd_t *tsd, ssize_t max) {
	malloc_mutex_assert_owner(tsd_tsdn(tsd), &prof_recent_alloc_mtx);
	ssize_t old_max = prof_recent_alloc_max_get(tsd);
	atomic_store_zd(&prof_recent_alloc_max, max, ATOMIC_RELAXED);
	return old_max;
}"
prof_recent_alloc_edata_set,"void
prof_recent_alloc_edata_set(tsd_t *tsd, prof_recent_t *n, edata_t *edata) {
	malloc_mutex_assert_owner(tsd_tsdn(tsd), &prof_recent_alloc_mtx);
	atomic_store_p(&n->alloc_edata, edata, ATOMIC_RELEASE);
}",edata_prof_recent_alloc_reset,"void
edata_prof_recent_alloc_reset(tsd_t *tsd, edata_t *edata,
    prof_recent_t *recent_alloc) {
	malloc_mutex_assert_owner(tsd_tsdn(tsd), &prof_recent_alloc_mtx);
	assert(recent_alloc != NULL);
	prof_recent_t *old_recent_alloc =
	    edata_prof_recent_alloc_update_internal(tsd, edata, NULL);
	assert(old_recent_alloc == recent_alloc);
	assert(edata == prof_recent_alloc_edata_get(tsd, recent_alloc));
	prof_recent_alloc_edata_set(tsd, recent_alloc, NULL);
}",edata_prof_recent_alloc_set,"void
edata_prof_recent_alloc_set(tsd_t *tsd, edata_t *edata,
    prof_recent_t *recent_alloc) {
	malloc_mutex_assert_owner(tsd_tsdn(tsd), &prof_recent_alloc_mtx);
	assert(recent_alloc != NULL);
	prof_recent_t *old_recent_alloc =
	    edata_prof_recent_alloc_update_internal(tsd, edata, recent_alloc);
	assert(old_recent_alloc == NULL);
	prof_recent_alloc_edata_set(tsd, recent_alloc, edata);
}"
tcache_gc_item_delay_compute,"uint8_t
tcache_gc_item_delay_compute(szind_t szind) {
	assert(szind < SC_NBINS);
	size_t sz = sz_index2size(szind);
	size_t item_delay = opt_tcache_gc_delay_bytes / sz;
	size_t delay_max = ZU(1)
	    << (sizeof(((tcache_slow_t *)NULL)->bin_flush_delay_items[0]) * 8);
	if (item_delay >= delay_max) {
		item_delay = delay_max - 1;
	}
	return (uint8_t)item_delay;
}",tcache_bin_flush_small,"void
tcache_bin_flush_small(tsd_t *tsd, tcache_t *tcache, cache_bin_t *cache_bin,
    szind_t binind, unsigned rem) {
	tcache_bin_flush_bottom(tsd, tcache, cache_bin, binind, rem, true);
}",tcache_gc_small,"void
tcache_gc_small(tsd_t *tsd, tcache_slow_t *tcache_slow, tcache_t *tcache,
    szind_t szind) {
	/* Aim to flush 3/4 of items below low-water. */
	assert(szind < SC_NBINS);

	cache_bin_t *cache_bin = &tcache->bins[szind];
	assert(!tcache_bin_disabled(szind, cache_bin, tcache->tcache_slow));
	cache_bin_sz_t ncached = cache_bin_ncached_get_local(cache_bin);
	cache_bin_sz_t low_water = cache_bin_low_water_get(cache_bin);
	assert(!tcache_slow->bin_refilled[szind]);

	size_t nflush = low_water - (low_water >> 2);
	if (nflush < tcache_slow->bin_flush_delay_items[szind]) {
		/* Workaround for a conversion warning. */
		uint8_t nflush_uint8 = (uint8_t)nflush;
		assert(sizeof(tcache_slow->bin_flush_delay_items[0]) ==
		    sizeof(nflush_uint8));
		tcache_slow->bin_flush_delay_items[szind] -= nflush_uint8;
		return;
	} else {
		tcache_slow->bin_flush_delay_items[szind]
		    = tcache_gc_item_delay_compute(szind);
	}

	tcache_bin_flush_small(tsd, tcache, cache_bin, szind,
	    (unsigned)(ncached - nflush));

	/*
	 * Reduce fill count by 2X.  Limit lg_fill_div such that
	 * the fill count is always at least 1.
	 */
	if ((cache_bin_ncached_max_get(cache_bin) >>
	    (tcache_slow->lg_fill_div[szind] + 1)) >= 1) {
		tcache_slow->lg_fill_div[szind]++;
	}
}",tcache_gc_large,"void
tcache_gc_large(tsd_t *tsd, tcache_slow_t *tcache_slow, tcache_t *tcache,
    szind_t szind) {
	/* Like the small GC; flush 3/4 of untouched items. */
	assert(szind >= SC_NBINS);
	cache_bin_t *cache_bin = &tcache->bins[szind];
	assert(!tcache_bin_disabled(szind, cache_bin, tcache->tcache_slow));
	cache_bin_sz_t ncached = cache_bin_ncached_get_local(cache_bin);
	cache_bin_sz_t low_water = cache_bin_low_water_get(cache_bin);
	tcache_bin_flush_large(tsd, tcache, cache_bin, szind,
	    (unsigned)(ncached - low_water + (low_water >> 2)));
}"
tcache_bin_flush_small,"void
tcache_bin_flush_small(tsd_t *tsd, tcache_t *tcache, cache_bin_t *cache_bin,
    szind_t binind, unsigned rem) {
	tcache_bin_flush_bottom(tsd, tcache, cache_bin, binind, rem, true);
}",tcache_bin_flush_large,"void
tcache_bin_flush_large(tsd_t *tsd, tcache_t *tcache, cache_bin_t *cache_bin,
    szind_t binind, unsigned rem) {
	tcache_bin_flush_bottom(tsd, tcache, cache_bin, binind, rem, false);
}"
extent_deregister,"void
extent_deregister(tsdn_t *tsdn, pac_t *pac, edata_t *edata) {
	extent_deregister_impl(tsdn, pac, edata, true);
}",extent_register_impl,"bool
extent_register_impl(tsdn_t *tsdn, pac_t *pac, edata_t *edata, bool gdump_add) {
	assert(edata_state_get(edata) == extent_state_active);
	/*
	 * No locking needed, as the edata must be in active state, which
	 * prevents other threads from accessing the edata.
	 */
	if (emap_register_boundary(tsdn, pac->emap, edata, SC_NSIZES,
	    /* slab */ false)) {
		return true;
	}

	if (config_prof && gdump_add) {
		extent_gdump_add(tsdn, edata);
	}

	return false;
}",extent_register,"bool
extent_register(tsdn_t *tsdn, pac_t *pac, edata_t *edata) {
	return extent_register_impl(tsdn, pac, edata, true);
}",extent_deregister_impl,"void
extent_deregister_impl(tsdn_t *tsdn, pac_t *pac, edata_t *edata,
    bool gdump) {
	emap_deregister_boundary(tsdn, pac->emap, edata);

	if (config_prof && gdump) {
		extent_gdump_sub(tsdn, edata);
	}
}",extent_deregister_no_gdump_sub,"void
extent_deregister_no_gdump_sub(tsdn_t *tsdn, pac_t *pac,
    edata_t *edata) {
	extent_deregister_impl(tsdn, pac, edata, false);
}",extent_reregister,"void
extent_reregister(tsdn_t *tsdn, pac_t *pac, edata_t *edata) {
	bool err = extent_register(tsdn, pac, edata);
	assert(!err);
}",extent_merge_impl,"bool
extent_merge_impl(tsdn_t *tsdn, pac_t *pac, ehooks_t *ehooks, edata_t *a,
    edata_t *b, bool holding_core_locks) {
	/* Only the expanding path may merge w/o holding ecache locks. */
	if (holding_core_locks) {
		witness_assert_positive_depth_to_rank(
		    tsdn_witness_tsdp_get(tsdn), WITNESS_RANK_CORE);
	} else {
		witness_assert_depth_to_rank(tsdn_witness_tsdp_get(tsdn),
		    WITNESS_RANK_CORE, 0);
	}

	assert(edata_base_get(a) < edata_base_get(b));
	assert(edata_arena_ind_get(a) == edata_arena_ind_get(b));
	assert(edata_arena_ind_get(a) == ehooks_ind_get(ehooks));
	emap_assert_mapped(tsdn, pac->emap, a);
	emap_assert_mapped(tsdn, pac->emap, b);

	bool err = ehooks_merge(tsdn, ehooks, edata_base_get(a),
	    edata_size_get(a), edata_base_get(b), edata_size_get(b),
	    edata_committed_get(a));

	if (err) {
		return true;
	}

	/*
	 * The rtree writes must happen while all the relevant elements are
	 * owned, so the following code uses decomposed helper functions rather
	 * than extent_{,de}register() to do things in the right order.
	 */
	emap_prepare_t prepare;
	emap_merge_prepare(tsdn, pac->emap, &prepare, a, b);

	assert(edata_state_get(a) == extent_state_active ||
	    edata_state_get(a) == extent_state_merging);
	edata_state_set(a, extent_state_active);
	edata_size_set(a, edata_size_get(a) + edata_size_get(b));
	edata_sn_set(a, (edata_sn_get(a) < edata_sn_get(b)) ?
	    edata_sn_get(a) : edata_sn_get(b));
	edata_zeroed_set(a, edata_zeroed_get(a) && edata_zeroed_get(b));

	emap_merge_commit(tsdn, pac->emap, &prepare, a, b);

	edata_cache_put(tsdn, pac->edata_cache, b);

	return false;
}"
pa_nactive_add,"void
pa_nactive_add(pa_shard_t *shard, size_t add_pages) {
	atomic_fetch_add_zu(&shard->nactive, add_pages, ATOMIC_RELAXED);
}",pa_nactive_sub,"void
pa_nactive_sub(pa_shard_t *shard, size_t sub_pages) {
	assert(pa_shard_nactive(shard) >= sub_pages);
	atomic_fetch_sub_zu(&shard->nactive, sub_pages, ATOMIC_RELAXED);
}"
decay_deadline_init,"void
decay_deadline_init(decay_t *decay) {
	nstime_copy(&decay->deadline, &decay->epoch);
	nstime_add(&decay->deadline, &decay->interval);
	if (decay_ms_read(decay) > 0) {
		nstime_t jitter;

		nstime_init(&jitter, prng_range_u64(&decay->jitter_state,
		    nstime_ns(&decay->interval)));
		nstime_add(&decay->deadline, &jitter);
	}
}",decay_reinit,"void
decay_reinit(decay_t *decay, nstime_t *cur_time, ssize_t decay_ms) {
	atomic_store_zd(&decay->time_ms, decay_ms, ATOMIC_RELAXED);
	if (decay_ms > 0) {
		nstime_init(&decay->interval, (uint64_t)decay_ms *
		    KQU(1000000));
		nstime_idivide(&decay->interval, SMOOTHSTEP_NSTEPS);
	}

	nstime_copy(&decay->epoch, cur_time);
	decay->jitter_state = (uint64_t)(uintptr_t)decay;
	decay_deadline_init(decay);
	decay->nunpurged = 0;
	memset(decay->backlog, 0, SMOOTHSTEP_NSTEPS * sizeof(size_t));
}",decay_backlog_npages_limit,"size_t
decay_backlog_npages_limit(const decay_t *decay) {
	/*
	 * For each element of decay_backlog, multiply by the corresponding
	 * fixed-point smoothstep decay factor.  Sum the products, then divide
	 * to round down to the nearest whole number of pages.
	 */
	uint64_t sum = 0;
	for (unsigned i = 0; i < SMOOTHSTEP_NSTEPS; i++) {
		sum += decay->backlog[i] * h_steps[i];
	}
	size_t npages_limit_backlog = (size_t)(sum >> SMOOTHSTEP_BFP);

	return npages_limit_backlog;
}",decay_maybe_update_time,"void
decay_maybe_update_time(decay_t *decay, nstime_t *new_time) {
	if (unlikely(!nstime_monotonic() && nstime_compare(&decay->epoch,
	    new_time) > 0)) {
		/*
		 * Time went backwards.  Move the epoch back in time and
		 * generate a new deadline, with the expectation that time
		 * typically flows forward for long enough periods of time that
		 * epochs complete.  Unfortunately, this strategy is susceptible
		 * to clock jitter triggering premature epoch advances, but
		 * clock jitter estimation and compensation isn't feasible here
		 * because calls into this code are event-driven.
		 */
		nstime_copy(&decay->epoch, new_time);
		decay_deadline_init(decay);
	} else {
		/* Verify that time does not go backwards. */
		assert(nstime_compare(&decay->epoch, new_time) <= 0);
	}
}"
confirm_malloc,"void
confirm_malloc(void *p) {
	assert_ptr_not_null(p, ""malloc failed unexpectedly"");
	edata_t *e = emap_edata_lookup(TSDN_NULL, &arena_emap_global, p);
	assert_ptr_not_null(e, ""NULL edata for living pointer"");
	prof_recent_t *n = edata_prof_recent_alloc_get_no_lock_test(e);
	assert_ptr_not_null(n, ""Record in edata should not be NULL"");
	expect_ptr_not_null(n->alloc_tctx,
	    ""alloc_tctx in record should not be NULL"");
	expect_ptr_eq(e, prof_recent_alloc_edata_get_no_lock_test(n),
	    ""edata pointer in record is not correct"");
	expect_ptr_null(n->dalloc_tctx, ""dalloc_tctx in record should be NULL"");
}",confirm_record_size,"void
confirm_record_size(prof_recent_t *n, unsigned kth) {
	expect_zu_eq(n->size, NTH_REQ_SIZE(kth),
	    ""Recorded allocation size is wrong"");
}",confirm_record_living,"void
confirm_record_living(prof_recent_t *n) {
	expect_ptr_not_null(n->alloc_tctx,
	    ""alloc_tctx in record should not be NULL"");
	edata_t *edata = prof_recent_alloc_edata_get_no_lock_test(n);
	assert_ptr_not_null(edata,
	    ""Recorded edata should not be NULL for living pointer"");
	expect_ptr_eq(n, edata_prof_recent_alloc_get_no_lock_test(edata),
	    ""Record in edata is not correct"");
	expect_ptr_null(n->dalloc_tctx, ""dalloc_tctx in record should be NULL"");
}",confirm_record_released,"void
confirm_record_released(prof_recent_t *n) {
	expect_ptr_not_null(n->alloc_tctx,
	    ""alloc_tctx in record should not be NULL"");
	expect_ptr_null(prof_recent_alloc_edata_get_no_lock_test(n),
	    ""Recorded edata should be NULL for released pointer"");
	expect_ptr_not_null(n->dalloc_tctx,
	    ""dalloc_tctx in record should not be NULL for released pointer"");
}"
prof_recent_alloc_evict_edata,"void
prof_recent_alloc_evict_edata(tsd_t *tsd, prof_recent_t *recent_alloc) {
	malloc_mutex_assert_owner(tsd_tsdn(tsd), &prof_recent_alloc_mtx);
	edata_t *edata = prof_recent_alloc_edata_get(tsd, recent_alloc);
	if (edata != NULL) {
		edata_prof_recent_alloc_reset(tsd, edata, recent_alloc);
	}
}",edata_prof_recent_alloc_set,"void
edata_prof_recent_alloc_set(tsd_t *tsd, edata_t *edata,
    prof_recent_t *recent_alloc) {
	malloc_mutex_assert_owner(tsd_tsdn(tsd), &prof_recent_alloc_mtx);
	assert(recent_alloc != NULL);
	prof_recent_t *old_recent_alloc =
	    edata_prof_recent_alloc_update_internal(tsd, edata, recent_alloc);
	assert(old_recent_alloc == NULL);
	prof_recent_alloc_edata_set(tsd, recent_alloc, edata);
}"
test_empty_list,"void
test_empty_list(list_head_t *head) {
	list_t *t;
	unsigned i;

	expect_true(ql_empty(head), ""Unexpected element for empty list"");
	expect_ptr_null(ql_first(head), ""Unexpected element for empty list"");
	expect_ptr_null(ql_last(head, link),
	    ""Unexpected element for empty list"");

	i = 0;
	ql_foreach(t, head, link) {
		i++;
	}
	expect_u_eq(i, 0, ""Unexpected element for empty list"");

	i = 0;
	ql_reverse_foreach(t, head, link) {
		i++;
	}
	expect_u_eq(i, 0, ""Unexpected element for empty list"");
}",test_entries_list,"void
test_entries_list(list_head_t *head, list_t *entries, unsigned nentries) {
	list_t *t;
	unsigned i;

	expect_false(ql_empty(head), ""List should not be empty"");
	expect_c_eq(ql_first(head)->id, entries[0].id, ""Element id mismatch"");
	expect_c_eq(ql_last(head, link)->id, entries[nentries-1].id,
	    ""Element id mismatch"");

	i = 0;
	ql_foreach(t, head, link) {
		expect_c_eq(t->id, entries[i].id, ""Element id mismatch"");
		i++;
	}

	i = 0;
	ql_reverse_foreach(t, head, link) {
		expect_c_eq(t->id, entries[nentries-i-1].id,
		    ""Element id mismatch"");
		i++;
	}

	for (i = 0; i < nentries-1; i++) {
		t = ql_next(head, &entries[i], link);
		expect_c_eq(t->id, entries[i+1].id, ""Element id mismatch"");
	}
	expect_ptr_null(ql_next(head, &entries[nentries-1], link),
	    ""Unexpected element"");

	expect_ptr_null(ql_prev(head, &entries[0], link), ""Unexpected element"");
	for (i = 1; i < nentries; i++) {
		t = ql_prev(head, &entries[i], link);
		expect_c_eq(t->id, entries[i-1].id, ""Element id mismatch"");
	}
}"
init_entries,"void
init_entries(list_t *entries, unsigned nentries) {
	unsigned i;

	for (i = 0; i < nentries; i++) {
		entries[i].id = 'a' + i;
		ql_elm_new(&entries[i], link);
	}
}",test_empty_list,"void
test_empty_list(list_head_t *head) {
	list_t *t;
	unsigned i;

	expect_true(ql_empty(head), ""Unexpected element for empty list"");
	expect_ptr_null(ql_first(head), ""Unexpected element for empty list"");
	expect_ptr_null(ql_last(head, link),
	    ""Unexpected element for empty list"");

	i = 0;
	ql_foreach(t, head, link) {
		i++;
	}
	expect_u_eq(i, 0, ""Unexpected element for empty list"");

	i = 0;
	ql_reverse_foreach(t, head, link) {
		i++;
	}
	expect_u_eq(i, 0, ""Unexpected element for empty list"");
}",test_entries_list,"void
test_entries_list(list_head_t *head, list_t *entries, unsigned nentries) {
	list_t *t;
	unsigned i;

	expect_false(ql_empty(head), ""List should not be empty"");
	expect_c_eq(ql_first(head)->id, entries[0].id, ""Element id mismatch"");
	expect_c_eq(ql_last(head, link)->id, entries[nentries-1].id,
	    ""Element id mismatch"");

	i = 0;
	ql_foreach(t, head, link) {
		expect_c_eq(t->id, entries[i].id, ""Element id mismatch"");
		i++;
	}

	i = 0;
	ql_reverse_foreach(t, head, link) {
		expect_c_eq(t->id, entries[nentries-i-1].id,
		    ""Element id mismatch"");
		i++;
	}

	for (i = 0; i < nentries-1; i++) {
		t = ql_next(head, &entries[i], link);
		expect_c_eq(t->id, entries[i+1].id, ""Element id mismatch"");
	}
	expect_ptr_null(ql_next(head, &entries[nentries-1], link),
	    ""Unexpected element"");

	expect_ptr_null(ql_prev(head, &entries[0], link), ""Unexpected element"");
	for (i = 1; i < nentries; i++) {
		t = ql_prev(head, &entries[i], link);
		expect_c_eq(t->id, entries[i-1].id, ""Element id mismatch"");
	}
}",test_concat_split_entries,"void
test_concat_split_entries(list_t *entries, unsigned nentries_a,
    unsigned nentries_b) {
	init_entries(entries, nentries_a + nentries_b);

	list_head_t head_a;
	ql_new(&head_a);
	for (unsigned i = 0; i < nentries_a; i++) {
		ql_tail_insert(&head_a, &entries[i], link);
	}
	if (nentries_a == 0) {
		test_empty_list(&head_a);
	} else {
		test_entries_list(&head_a, entries, nentries_a);
	}

	list_head_t head_b;
	ql_new(&head_b);
	for (unsigned i = 0; i < nentries_b; i++) {
		ql_tail_insert(&head_b, &entries[nentries_a + i], link);
	}
	if (nentries_b == 0) {
		test_empty_list(&head_b);
	} else {
		test_entries_list(&head_b, entries + nentries_a, nentries_b);
	}

	ql_concat(&head_a, &head_b, link);
	if (nentries_a + nentries_b == 0) {
		test_empty_list(&head_a);
	} else {
		test_entries_list(&head_a, entries, nentries_a + nentries_b);
	}
	test_empty_list(&head_b);

	if (nentries_b == 0) {
		return;
	}

	list_head_t head_c;
	ql_split(&head_a, &entries[nentries_a], &head_c, link);
	if (nentries_a == 0) {
		test_empty_list(&head_a);
	} else {
		test_entries_list(&head_a, entries, nentries_a);
	}
	test_entries_list(&head_c, entries + nentries_a, nentries_b);
}"
buf_writer_assert,"void
buf_writer_assert(buf_writer_t *buf_writer) {
	assert(buf_writer != NULL);
	assert(buf_writer->write_cb != NULL);
	if (buf_writer->buf != NULL) {
		assert(buf_writer->buf_size > 0);
	} else {
		assert(buf_writer->buf_size == 0);
		assert(buf_writer->internal_buf);
	}
	assert(buf_writer->buf_end <= buf_writer->buf_size);
}",buf_writer_flush,"void
buf_writer_flush(buf_writer_t *buf_writer) {
	buf_writer_assert(buf_writer);
	if (buf_writer->buf == NULL) {
		return;
	}
	buf_writer->buf[buf_writer->buf_end] = '\0';
	buf_writer->write_cb(buf_writer->cbopaque, buf_writer->buf);
	buf_writer->buf_end = 0;
	buf_writer_assert(buf_writer);
}",test_buf_writer_pipe_body,"void
test_buf_writer_pipe_body(tsdn_t *tsdn, buf_writer_t *buf_writer) {
	arg = 4; /* Starting value of random argument. */
	for (int count = 5; count > 0; --count) {
		arg = prng_lg_range_u64(&arg, 64);
		arg_sum = 0;
		test_read_count = count;
		test_read_len = 0;
		test_write_len = 0;
		buf_writer_pipe(buf_writer, test_read_cb, &arg);
		assert(test_read_count == 0);
		expect_u64_eq(arg_sum, arg * count, """");
		expect_zu_eq(test_write_len, test_read_len,
		    ""Write length should be equal to read length"");
	}
	buf_writer_terminate(tsdn, buf_writer);
}"
edata_cache_put,"void
edata_cache_put(tsdn_t *tsdn, edata_cache_t *edata_cache, edata_t *edata) {
	malloc_mutex_lock(tsdn, &edata_cache->mtx);
	edata_avail_insert(&edata_cache->avail, edata);
	atomic_load_add_store_zu(&edata_cache->count, 1);
	malloc_mutex_unlock(tsdn, &edata_cache->mtx);
}",test_edata_cache_init,"void
test_edata_cache_init(edata_cache_t *edata_cache) {
	base_t *base = base_new(TSDN_NULL, /* ind */ 1,
	    &ehooks_default_extent_hooks, /* metadata_use_hooks */ true);
	assert_ptr_not_null(base, """");
	bool err = edata_cache_init(edata_cache, base);
	assert_false(err, """");
}",test_edata_cache_destroy,"void
test_edata_cache_destroy(edata_cache_t *edata_cache) {
	base_delete(TSDN_NULL, edata_cache->base);
}"
do_fill_test,"void
do_fill_test(cache_bin_t *bin, void **ptrs, cache_bin_sz_t ncached_max,
    cache_bin_sz_t nfill_attempt, cache_bin_sz_t nfill_succeed) {
	bool success;
	void *ptr;
	assert_true(cache_bin_ncached_get_local(bin) == 0, """");
	CACHE_BIN_PTR_ARRAY_DECLARE(arr, nfill_attempt);
	cache_bin_init_ptr_array_for_fill(bin, &arr, nfill_attempt);
	for (cache_bin_sz_t i = 0; i < nfill_succeed; i++) {
		arr.ptr[i] = &ptrs[i];
	}
	cache_bin_finish_fill(bin, &arr, nfill_succeed);
	expect_true(cache_bin_ncached_get_local(bin) == nfill_succeed,
	    """");
	cache_bin_low_water_set(bin);

	for (cache_bin_sz_t i = 0; i < nfill_succeed; i++) {
		ptr = cache_bin_alloc(bin, &success);
		expect_true(success, """");
		expect_ptr_eq(ptr, (void *)&ptrs[i],
		    ""Should pop in order filled"");
		expect_true(cache_bin_low_water_get(bin)
		    == nfill_succeed - i - 1, """");
	}
	expect_true(cache_bin_ncached_get_local(bin) == 0, """");
	expect_true(cache_bin_low_water_get(bin) == 0, """");
}",do_flush_test,"void
do_flush_test(cache_bin_t *bin, void **ptrs, cache_bin_sz_t nfill,
    cache_bin_sz_t nflush) {
	bool success;
	assert_true(cache_bin_ncached_get_local(bin) == 0, """");

	for (cache_bin_sz_t i = 0; i < nfill; i++) {
		success = cache_bin_dalloc_easy(bin, &ptrs[i]);
		expect_true(success, """");
	}

	CACHE_BIN_PTR_ARRAY_DECLARE(arr, nflush);
	cache_bin_init_ptr_array_for_flush(bin, &arr, nflush);
	for (cache_bin_sz_t i = 0; i < nflush; i++) {
		expect_ptr_eq(arr.ptr[i], &ptrs[nflush - i - 1], """");
	}
	cache_bin_finish_flush(bin, &arr, nflush);

	expect_true(cache_bin_ncached_get_local(bin) == nfill - nflush,
	    """");
	while (cache_bin_ncached_get_local(bin) > 0) {
		cache_bin_alloc(bin, &success);
	}
}"
expect_counter_value,"void
expect_counter_value(counter_accum_t *c, uint64_t v) {
	uint64_t accum = locked_read_u64_unsynchronized(&c->accumbytes);
	expect_u64_eq(accum, v, ""Counter value mismatch"");
}",expect_args_raw,"void
expect_args_raw(uintptr_t *args_raw_expected, int nargs) {
	int cmp = memcmp(args_raw_expected, arg_args_raw,
	    sizeof(uintptr_t) * nargs);
	expect_d_eq(cmp, 0, ""Raw args mismatch"");
}",expect_log_empty,"void expect_log_empty(void) {
	expect_zu_eq(prof_log_bt_count(), 0,
	    ""The log has backtraces; it isn't empty"");
	expect_zu_eq(prof_log_thr_count(), 0,
	    ""The log has threads; it isn't empty"");
	expect_zu_eq(prof_log_alloc_count(), 0,
	    ""The log has allocations; it isn't empty"");
}",expect_rep,"void expect_rep(void) {
	expect_b_eq(prof_log_rep_check(), false, ""Rep check failed"");
}",expect_data,"void
expect_data(data_t *data) {
	int num = data->arr[0];
	for (int i = 0; i < 10; i++) {
		expect_d_eq(num, data->arr[i], ""Data consistency error"");
	}
}"
confirm_prof_setup,"void
confirm_prof_setup(void) {
	/* Options */
	assert_true(opt_prof, ""opt_prof not on"");
	assert_true(opt_prof_active, ""opt_prof_active not on"");
	assert_zd_eq(opt_prof_recent_alloc_max, OPT_ALLOC_MAX,
	    ""opt_prof_recent_alloc_max not set correctly"");

	/* Dynamics */
	assert_true(prof_active_state, ""prof_active not on"");
	assert_zd_eq(prof_recent_alloc_max_ctl_read(), OPT_ALLOC_MAX,
	    ""prof_recent_alloc_max not set correctly"");
}",call_dump,"void
call_dump(void) {
	static void *in[2] = {test_dump_write_cb, NULL};
	dump_out_len = 0;
	assert_d_eq(mallctl(""experimental.prof_recent.alloc_dump"",
	    NULL, NULL, in, sizeof(in)), 0, ""Dump mallctl raised error"");
}",confirm_record,"void
confirm_record(const char *template, const confirm_record_t *records,
    const size_t n_records) {
	static const char *types[2] = {""alloc"", ""dalloc""};
	static char buf[64];

	/*
	 * The template string would be in the form of:
	 * ""{...,\""recent_alloc\"":[]}"",
	 * and dump_out would be in the form of:
	 * ""{...,\""recent_alloc\"":[...]}"".
	 * Using ""- 2"" serves to cut right before the ending ""]}"
arena_slab_dalloc,"void
arena_slab_dalloc(tsdn_t *tsdn, arena_t *arena, edata_t *slab) {
	bool deferred_work_generated = false;
	pa_dalloc(tsdn, &arena->pa_shard, slab, &deferred_work_generated);
	if (deferred_work_generated) {
		arena_handle_deferred_work(tsdn, arena);
	}
}",arena_dalloc_bin_slab_prepare,"void
arena_dalloc_bin_slab_prepare(tsdn_t *tsdn, edata_t *slab, bin_t *bin) {
	malloc_mutex_assert_owner(tsdn, &bin->lock);

	assert(slab != bin->slabcur);
	if (config_stats) {
		bin->stats.curslabs--;
	}
}"
buf_writer_assert,"void
buf_writer_assert(buf_writer_t *buf_writer) {
	assert(buf_writer != NULL);
	assert(buf_writer->write_cb != NULL);
	if (buf_writer->buf != NULL) {
		assert(buf_writer->buf_size > 0);
	} else {
		assert(buf_writer->buf_size == 0);
		assert(buf_writer->internal_buf);
	}
	assert(buf_writer->buf_end <= buf_writer->buf_size);
}",buf_writer_flush,"void
buf_writer_flush(buf_writer_t *buf_writer) {
	buf_writer_assert(buf_writer);
	if (buf_writer->buf == NULL) {
		return;
	}
	buf_writer->buf[buf_writer->buf_end] = '\0';
	buf_writer->write_cb(buf_writer->cbopaque, buf_writer->buf);
	buf_writer->buf_end = 0;
	buf_writer_assert(buf_writer);
}",buf_writer_free_internal_buf,"void
buf_writer_free_internal_buf(tsdn_t *tsdn, void *buf) {
	if (buf != NULL) {
		idalloctm(tsdn, buf, NULL, NULL, true, true);
	}
}"
te_assert_invariants_impl,"void
te_assert_invariants_impl(tsd_t *tsd, te_ctx_t *ctx) {
	uint64_t current_bytes = te_ctx_current_bytes_get(ctx);
	uint64_t last_event = te_ctx_last_event_get(ctx);
	uint64_t next_event = te_ctx_next_event_get(ctx);
	uint64_t next_event_fast = te_ctx_next_event_fast_get(ctx);

	assert(last_event != next_event);
	if (next_event > TE_NEXT_EVENT_FAST_MAX || !tsd_fast(tsd)) {
		assert(next_event_fast == 0U);
	} else {
		assert(next_event_fast == next_event);
	}

	/* The subtraction is intentionally susceptible to underflow. */
	uint64_t interval = next_event - last_event;

	/* The subtraction is intentionally susceptible to underflow. */
	assert(current_bytes - last_event < interval);
	uint64_t min_wait = te_next_event_compute(tsd, te_ctx_is_alloc(ctx));
	/*
	 * next_event should have been pushed up only except when no event is
	 * on and the TSD is just initialized.  The last_event == 0U guard
	 * below is stronger than needed, but having an exactly accurate guard
	 * is more complicated to implement.
	 */
	assert((!te_ctx_has_active_events(ctx) && last_event == 0U) ||
	    interval == min_wait ||
	    (interval < min_wait && interval == TE_MAX_INTERVAL));
}",te_ctx_next_event_fast_update,"void
te_ctx_next_event_fast_update(te_ctx_t *ctx) {
	uint64_t next_event = te_ctx_next_event_get(ctx);
	uint64_t next_event_fast = (next_event <= TE_NEXT_EVENT_FAST_MAX) ?
	    next_event : 0U;
	te_ctx_next_event_fast_set(ctx, next_event_fast);
}",te_adjust_thresholds_helper,"void
te_adjust_thresholds_helper(tsd_t *tsd, te_ctx_t *ctx,
    uint64_t wait) {
	/*
	 * The next threshold based on future events can only be adjusted after
	 * progressing the last_event counter (which is set to current).
	 */
	assert(te_ctx_current_bytes_get(ctx) == te_ctx_last_event_get(ctx));
	assert(wait <= TE_MAX_START_WAIT);

	uint64_t next_event = te_ctx_last_event_get(ctx) + (wait <=
	    TE_MAX_INTERVAL ? wait : TE_MAX_INTERVAL);
	te_ctx_next_event_set(tsd, ctx, next_event);
}"
arena_prof_demote,"size_t
arena_prof_demote(tsdn_t *tsdn, edata_t *edata, const void *ptr) {
	cassert(config_prof);
	assert(ptr != NULL);
	size_t usize = isalloc(tsdn, ptr);
	size_t bumped_usize = sz_sa2u(usize, PROF_SAMPLE_ALIGNMENT);
	assert(bumped_usize <= SC_LARGE_MINCLASS &&
	    PAGE_CEILING(bumped_usize) == bumped_usize);
	assert(edata_size_get(edata) - bumped_usize <= sz_large_pad);
	szind_t szind = sz_size2index(bumped_usize);

	edata_szind_set(edata, szind);
	emap_remap(tsdn, &arena_emap_global, edata, szind, /* slab */ false);

	assert(isalloc(tsdn, ptr) == bumped_usize);

	return bumped_usize;
}",arena_dalloc_bin,"void
arena_dalloc_bin(tsdn_t *tsdn, arena_t *arena, edata_t *edata, void *ptr) {
	szind_t binind = edata_szind_get(edata);
	unsigned binshard = edata_binshard_get(edata);
	bin_t *bin = arena_get_bin(arena, binind, binshard);

	malloc_mutex_lock(tsdn, &bin->lock);
	arena_dalloc_bin_locked_info_t info;
	arena_dalloc_bin_locked_begin(&info, binind);
	bool ret = arena_dalloc_bin_locked_step(tsdn, arena, bin,
	    &info, binind, edata, ptr);
	arena_dalloc_bin_locked_finish(tsdn, arena, bin, &info);
	malloc_mutex_unlock(tsdn, &bin->lock);

	if (ret) {
		arena_slab_dalloc(tsdn, arena, edata);
	}
}",large_dalloc_finish_impl,"void
large_dalloc_finish_impl(tsdn_t *tsdn, arena_t *arena, edata_t *edata) {
	bool deferred_work_generated = false;
	pa_dalloc(tsdn, &arena->pa_shard, edata, &deferred_work_generated);
	if (deferred_work_generated) {
		arena_handle_deferred_work(tsdn, arena);
	}
}",large_prof_tctx_set,"void
large_prof_tctx_set(edata_t *edata, prof_tctx_t *tctx) {
	edata_prof_tctx_set(edata, tctx);
}"
ehooks_default_commit_impl,"bool
ehooks_default_commit_impl(void *addr, size_t offset, size_t length) {
	return pages_commit((void *)((byte_t *)addr + (uintptr_t)offset),
	    length);
}",ehooks_default_decommit_impl,"bool
ehooks_default_decommit_impl(void *addr, size_t offset, size_t length) {
	return pages_decommit((void *)((byte_t *)addr + (uintptr_t)offset),
	    length);
}"
static_opts_init,"void
static_opts_init(static_opts_t *static_opts) {
	static_opts->may_overflow = false;
	static_opts->bump_empty_aligned_alloc = false;
	static_opts->assert_nonempty_alloc = false;
	static_opts->null_out_result_on_error = false;
	static_opts->set_errno_on_error = false;
	static_opts->min_alignment = 0;
	static_opts->oom_string = """";
	static_opts->invalid_alignment_string = """";
	static_opts->slow = false;
	static_opts->usize = false;
}",dynamic_opts_init,"void
dynamic_opts_init(dynamic_opts_t *dynamic_opts) {
	dynamic_opts->result = NULL;
	dynamic_opts->usize = 0;
	dynamic_opts->num_items = 0;
	dynamic_opts->item_size = 0;
	dynamic_opts->alignment = 0;
	dynamic_opts->zero = false;
	dynamic_opts->tcache_ind = TCACHE_IND_AUTOMATIC;
	dynamic_opts->arena_ind = ARENA_IND_AUTOMATIC;
}"
eset_stats_add,"void
eset_stats_add(eset_t *eset, pszind_t pind, size_t sz) {
	size_t cur = atomic_load_zu(&eset->bin_stats[pind].nextents,
	    ATOMIC_RELAXED);
	atomic_store_zu(&eset->bin_stats[pind].nextents, cur + 1,
	    ATOMIC_RELAXED);
	cur = atomic_load_zu(&eset->bin_stats[pind].nbytes, ATOMIC_RELAXED);
	atomic_store_zu(&eset->bin_stats[pind].nbytes, cur + sz,
	    ATOMIC_RELAXED);
}",eset_stats_sub,"void
eset_stats_sub(eset_t *eset, pszind_t pind, size_t sz) {
	size_t cur = atomic_load_zu(&eset->bin_stats[pind].nextents,
	    ATOMIC_RELAXED);
	atomic_store_zu(&eset->bin_stats[pind].nextents, cur - 1,
	    ATOMIC_RELAXED);
	cur = atomic_load_zu(&eset->bin_stats[pind].nbytes, ATOMIC_RELAXED);
	atomic_store_zu(&eset->bin_stats[pind].nbytes, cur - sz,
	    ATOMIC_RELAXED);
}"
extent_deactivate_locked,"void
extent_deactivate_locked(tsdn_t *tsdn, pac_t *pac, ecache_t *ecache,
    edata_t *edata) {
	assert(edata_state_get(edata) == extent_state_active);
	extent_deactivate_locked_impl(tsdn, pac, ecache, edata);
}",extent_activate_locked,"void
extent_activate_locked(tsdn_t *tsdn, pac_t *pac, ecache_t *ecache, eset_t *eset,
    edata_t *edata) {
	assert(edata_arena_ind_get(edata) == ecache_ind_get(ecache));
	assert(edata_state_get(edata) == ecache->state ||
	    edata_state_get(edata) == extent_state_merging);

	eset_remove(eset, edata);
	emap_update_edata_state(tsdn, pac->emap, edata, extent_state_active);
}"
rate_per_second,"uint64_t
rate_per_second(uint64_t value, uint64_t uptime_ns) {
	uint64_t billion = 1000000000;
	if (uptime_ns == 0 || value == 0) {
		return 0;
	}
	if (uptime_ns < billion) {
		return value;
	} else {
		uint64_t uptime_s = uptime_ns / billion;
		return value / uptime_s;
	}
}",stats_arena_mutexes_print,"void
stats_arena_mutexes_print(emitter_t *emitter, unsigned arena_ind, uint64_t uptime) {
	emitter_row_t row;
	emitter_col_t col_name;
	emitter_col_t col64[mutex_prof_num_uint64_t_counters];
	emitter_col_t col32[mutex_prof_num_uint32_t_counters];

	emitter_row_init(&row);
	mutex_stats_init_cols(&row, """", &col_name, col64, col32);

	emitter_json_object_kv_begin(emitter, ""mutexes"");
	emitter_table_row(emitter, &row);

	size_t stats_arenas_mib[CTL_MAX_DEPTH];
	CTL_LEAF_PREPARE(stats_arenas_mib, 0, ""stats.arenas"");
	stats_arenas_mib[2] = arena_ind;
	CTL_LEAF_PREPARE(stats_arenas_mib, 3, ""mutexes"");

	for (mutex_prof_arena_ind_t i = 0; i < mutex_prof_num_arena_mutexes;
	    i++) {
		const char *name = arena_mutex_names[i];
		emitter_json_object_kv_begin(emitter, name);
		mutex_stats_read_arena(stats_arenas_mib, 4, name, &col_name,
		    col64, col32, uptime);
		mutex_stats_emit(emitter, &row, col64, col32);
		emitter_json_object_end(emitter); /* Close the mutex dict. */
	}
	emitter_json_object_end(emitter); /* End ""mutexes"". */
}"
extent_deregister,"void
extent_deregister(tsdn_t *tsdn, pac_t *pac, edata_t *edata) {
	extent_deregister_impl(tsdn, pac, edata, true);
}",extent_reregister,"void
extent_reregister(tsdn_t *tsdn, pac_t *pac, edata_t *edata) {
	bool err = extent_register(tsdn, pac, edata);
	assert(!err);
}"
static_opts_init,"void
static_opts_init(static_opts_t *static_opts) {
	static_opts->may_overflow = false;
	static_opts->bump_empty_aligned_alloc = false;
	static_opts->assert_nonempty_alloc = false;
	static_opts->null_out_result_on_error = false;
	static_opts->set_errno_on_error = false;
	static_opts->min_alignment = 0;
	static_opts->oom_string = """";
	static_opts->invalid_alignment_string = """";
	static_opts->slow = false;
	static_opts->usize = false;
}",dynamic_opts_init,"void
dynamic_opts_init(dynamic_opts_t *dynamic_opts) {
	dynamic_opts->result = NULL;
	dynamic_opts->usize = 0;
	dynamic_opts->num_items = 0;
	dynamic_opts->item_size = 0;
	dynamic_opts->alignment = 0;
	dynamic_opts->zero = false;
	dynamic_opts->tcache_ind = TCACHE_IND_AUTOMATIC;
	dynamic_opts->arena_ind = ARENA_IND_AUTOMATIC;
}"
tsd_slow_update,"void
tsd_slow_update(tsd_t *tsd) {
	uint8_t old_state;
	do {
		uint8_t new_state = tsd_state_compute(tsd);
		old_state = tsd_atomic_exchange(&tsd->state, new_state,
		    ATOMIC_ACQUIRE);
	} while (old_state == tsd_state_nominal_recompute);

	te_recompute_fast_threshold(tsd);
}",tsd_data_init,"bool
tsd_data_init(tsd_t *tsd) {
	/*
	 * We initialize the rtree context first (before the tcache), since the
	 * tcache initialization depends on it.
	 */
	rtree_ctx_data_init(tsd_rtree_ctxp_get_unsafe(tsd));
	tsd_prng_state_init(tsd);
	tsd_te_init(tsd); /* event_init may use the prng state above. */
	tsd_san_init(tsd);
	return tsd_tcache_enabled_data_init(tsd);
}"
reg_size_compute,"size_t
reg_size_compute(int lg_base, int lg_delta, int ndelta) {
	return (ZU(1) << lg_base) + (ZU(ndelta) << lg_delta);
}",sc_data_init,"void
sc_data_init(sc_data_t *sc_data) {
	size_classes(sc_data, LG_SIZEOF_PTR, LG_QUANTUM, SC_LG_TINY_MIN,
	    SC_LG_MAX_LOOKUP, LG_PAGE, SC_LG_NGROUP);

	sc_data->initialized = true;
}"
sz_boot_index2size_tab,"void
sz_boot_index2size_tab(const sc_data_t *sc_data) {
	for (unsigned i = 0; i < SC_NSIZES; i++) {
		const sc_t *sc = &sc_data->sc[i];
		sz_index2size_tab[i] = (ZU(1) << sc->lg_base)
		    + (ZU(sc->ndelta) << (sc->lg_delta));
	}
}",test_junk,"void
test_junk(void *ptr, size_t usize) {
	last_junked_ptr = ptr;
	last_junked_usize = usize;
}"
alloc_free_size,"void
alloc_free_size(size_t sz) {
	void *ptr = mallocx(1, 0);
	free(ptr);
	ptr = mallocx(1, 0);
	free(ptr);
	ptr = mallocx(1, MALLOCX_TCACHE_NONE);
	dallocx(ptr, MALLOCX_TCACHE_NONE);
}",be_reentrant,"void
be_reentrant(void) {
	/* Let's make sure the tcache is non-empty if enabled. */
	alloc_free_size(1);
	alloc_free_size(1024);
	alloc_free_size(64 * 1024);
	alloc_free_size(256 * 1024);
	alloc_free_size(1024 * 1024);

	/* Some reallocation. */
	void *ptr = mallocx(129, 0);
	ptr = rallocx(ptr, 130, 0);
	free(ptr);

	ptr = mallocx(2 * 1024 * 1024, 0);
	free(ptr);
	ptr = mallocx(1 * 1024 * 1024, 0);
	ptr = rallocx(ptr, 2 * 1024 * 1024, 0);
	free(ptr);

	ptr = mallocx(1, 0);
	ptr = rallocx(ptr, 1000, 0);
	free(ptr);
}"
static_opts_init,"void
static_opts_init(static_opts_t *static_opts) {
	static_opts->may_overflow = false;
	static_opts->bump_empty_aligned_alloc = false;
	static_opts->assert_nonempty_alloc = false;
	static_opts->null_out_result_on_error = false;
	static_opts->set_errno_on_error = false;
	static_opts->min_alignment = 0;
	static_opts->oom_string = """";
	static_opts->invalid_alignment_string = """";
	static_opts->slow = false;
	static_opts->usize = false;
}",dynamic_opts_init,"void
dynamic_opts_init(dynamic_opts_t *dynamic_opts) {
	dynamic_opts->result = NULL;
	dynamic_opts->usize = 0;
	dynamic_opts->num_items = 0;
	dynamic_opts->item_size = 0;
	dynamic_opts->alignment = 0;
	dynamic_opts->zero = false;
	dynamic_opts->tcache_ind = TCACHE_IND_AUTOMATIC;
	dynamic_opts->arena_ind = ARENA_IND_AUTOMATIC;
}",reset_args,"void
reset_args(void) {
	arg_extra = NULL;
	arg_type = 12345;
	arg_result = NULL;
	arg_address = NULL;
	arg_old_usize = 0;
	arg_new_usize = 0;
	arg_result_raw = 0;
	memset(arg_args_raw, 77, sizeof(arg_args_raw));
}",reset,"void
reset(void) {
	call_count = 0;
	reset_args();
}"
tsd_force_recompute,"void
tsd_force_recompute(tsdn_t *tsdn) {
	/*
	 * The stores to tsd->state here need to synchronize with the exchange
	 * in tsd_slow_update.
	 */
	atomic_fence(ATOMIC_RELEASE);
	malloc_mutex_lock(tsdn, &tsd_nominal_tsds_lock);
	tsd_t *remote_tsd;
	ql_foreach(remote_tsd, &tsd_nominal_tsds, TSD_MANGLE(tsd_link)) {
		assert(tsd_atomic_load(&remote_tsd->state, ATOMIC_RELAXED)
		    <= tsd_state_nominal_max);
		tsd_atomic_store(&remote_tsd->state,
		    tsd_state_nominal_recompute, ATOMIC_RELAXED);
		/* See comments in te_recompute_fast_threshold(). */
		atomic_fence(ATOMIC_SEQ_CST);
		te_next_event_fast_set_non_nominal(remote_tsd);
	}
	malloc_mutex_unlock(tsdn, &tsd_nominal_tsds_lock);
}",tsd_state_compute,"uint8_t
tsd_state_compute(tsd_t *tsd) {
	if (!tsd_nominal(tsd)) {
		return tsd_state_get(tsd);
	}
	/* We're in *a* nominal state; but which one? */
	if (malloc_slow || tsd_local_slow(tsd) || tsd_global_slow()) {
		return tsd_state_nominal_slow;
	} else {
		return tsd_state_nominal;
	}
}",tsd_add_nominal,"void
tsd_add_nominal(tsd_t *tsd) {
	assert(!tsd_in_nominal_list(tsd));
	assert(tsd_state_get(tsd) <= tsd_state_nominal_max);
	ql_elm_new(tsd, TSD_MANGLE(tsd_link));
	malloc_mutex_lock(tsd_tsdn(tsd), &tsd_nominal_tsds_lock);
	ql_tail_insert(&tsd_nominal_tsds, tsd, TSD_MANGLE(tsd_link));
	malloc_mutex_unlock(tsd_tsdn(tsd), &tsd_nominal_tsds_lock);
}",tsd_remove_nominal,"void
tsd_remove_nominal(tsd_t *tsd) {
	assert(tsd_in_nominal_list(tsd));
	assert(tsd_state_get(tsd) <= tsd_state_nominal_max);
	malloc_mutex_lock(tsd_tsdn(tsd), &tsd_nominal_tsds_lock);
	ql_remove(&tsd_nominal_tsds, tsd, TSD_MANGLE(tsd_link));
	malloc_mutex_unlock(tsd_tsdn(tsd), &tsd_nominal_tsds_lock);
}",tsd_slow_update,"void
tsd_slow_update(tsd_t *tsd) {
	uint8_t old_state;
	do {
		uint8_t new_state = tsd_state_compute(tsd);
		old_state = tsd_atomic_exchange(&tsd->state, new_state,
		    ATOMIC_ACQUIRE);
	} while (old_state == tsd_state_nominal_recompute);

	te_recompute_fast_threshold(tsd);
}"
extent_deactivate_locked,"void
extent_deactivate_locked(tsdn_t *tsdn, pac_t *pac, ecache_t *ecache,
    edata_t *edata) {
	assert(edata_state_get(edata) == extent_state_active);
	extent_deactivate_locked_impl(tsdn, pac, ecache, edata);
}",extent_activate_locked,"void
extent_activate_locked(tsdn_t *tsdn, pac_t *pac, ecache_t *ecache, eset_t *eset,
    edata_t *edata) {
	assert(edata_arena_ind_get(edata) == ecache_ind_get(ecache));
	assert(edata_state_get(edata) == ecache->state ||
	    edata_state_get(edata) == extent_state_merging);

	eset_remove(eset, edata);
	emap_update_edata_state(tsdn, pac->emap, edata, extent_state_active);
}"
mutex_stats_emit,"void
mutex_stats_emit(emitter_t *emitter, emitter_row_t *row,
    emitter_col_t col_uint64_t[mutex_prof_num_uint64_t_counters],
    emitter_col_t col_uint32_t[mutex_prof_num_uint32_t_counters]) {
	if (row != NULL) {
		emitter_table_row(emitter, row);
	}

	mutex_prof_uint64_t_counter_ind_t k_uint64_t = 0;
	mutex_prof_uint32_t_counter_ind_t k_uint32_t = 0;

	emitter_col_t *col;

#define EMITTER_TYPE_uint32_t emitter_type_uint32
#define EMITTER_TYPE_uint64_t emitter_type_uint64
#define OP(counter, type, human, derived, base_counter)		\
	if (!derived) {                    \
		col = &col_##type[k_##type];                        \
		++k_##type;                            \
		emitter_json_kv(emitter, #counter, EMITTER_TYPE_##type,        \
		    (const void *)&col->bool_val); \
	}
	MUTEX_PROF_COUNTERS;
#undef OP
#undef EMITTER_TYPE_uint32_t
#undef EMITTER_TYPE_uint64_t
}",stats_arena_mutexes_print,"void
stats_arena_mutexes_print(emitter_t *emitter, unsigned arena_ind, uint64_t uptime) {
	emitter_row_t row;
	emitter_col_t col_name;
	emitter_col_t col64[mutex_prof_num_uint64_t_counters];
	emitter_col_t col32[mutex_prof_num_uint32_t_counters];

	emitter_row_init(&row);
	mutex_stats_init_cols(&row, """", &col_name, col64, col32);

	emitter_json_object_kv_begin(emitter, ""mutexes"");
	emitter_table_row(emitter, &row);

	size_t stats_arenas_mib[CTL_MAX_DEPTH];
	CTL_LEAF_PREPARE(stats_arenas_mib, 0, ""stats.arenas"");
	stats_arenas_mib[2] = arena_ind;
	CTL_LEAF_PREPARE(stats_arenas_mib, 3, ""mutexes"");

	for (mutex_prof_arena_ind_t i = 0; i < mutex_prof_num_arena_mutexes;
	    i++) {
		const char *name = arena_mutex_names[i];
		emitter_json_object_kv_begin(emitter, name);
		mutex_stats_read_arena(stats_arenas_mib, 4, name, &col_name,
		    col64, col32, uptime);
		mutex_stats_emit(emitter, &row, col64, col32);
		emitter_json_object_end(emitter); /* Close the mutex dict. */
	}
	emitter_json_object_end(emitter); /* End ""mutexes"". */
}"
pages_huge_impl,"bool
pages_huge_impl(void *addr, size_t size, bool aligned) {
	if (aligned) {
		assert(HUGEPAGE_ADDR2BASE(addr) == addr);
		assert(HUGEPAGE_CEILING(size) == size);
	}
#if defined(JEMALLOC_HAVE_MADVISE_HUGE)
	return (madvise(addr, size, MADV_HUGEPAGE) != 0);
#elif defined(JEMALLOC_HAVE_MEMCNTL)
	struct memcntl_mha m = {0};
	m.mha_cmd = MHA_MAPSIZE_VA;
	m.mha_pagesize = HUGEPAGE;
	return (memcntl(addr, size, MC_HAT_ADVISE, (caddr_t)&m, 0, 0) == 0);
#else
	return true;
#endif
}",pages_nohuge_impl,"bool
pages_nohuge_impl(void *addr, size_t size, bool aligned) {
	if (aligned) {
		assert(HUGEPAGE_ADDR2BASE(addr) == addr);
		assert(HUGEPAGE_CEILING(size) == size);
	}

#ifdef JEMALLOC_HAVE_MADVISE_HUGE
	return (madvise(addr, size, MADV_NOHUGEPAGE) != 0);
#else
	return false;
#endif
}",pages_huge_unaligned,"bool
pages_huge_unaligned(void *addr, size_t size) {
	return pages_huge_impl(addr, size, false);
}",pages_nohuge_unaligned,"bool
pages_nohuge_unaligned(void *addr, size_t size) {
	return pages_nohuge_impl(addr, size, false);
}"
extent_deregister_impl,"void
extent_deregister_impl(tsdn_t *tsdn, pac_t *pac, edata_t *edata,
    bool gdump) {
	emap_deregister_boundary(tsdn, pac->emap, edata);

	if (config_prof && gdump) {
		extent_gdump_sub(tsdn, edata);
	}
}",extent_deregister_no_gdump_sub,"void
extent_deregister_no_gdump_sub(tsdn_t *tsdn, pac_t *pac,
    edata_t *edata) {
	extent_deregister_impl(tsdn, pac, edata, false);
}"
test_extent_body,"void
test_extent_body(unsigned arena_ind) {
	void *p;
	size_t large0, large1, large2, sz;
	size_t purge_mib[3];
	size_t purge_miblen;
	int flags;
	bool xallocx_success_a, xallocx_success_b, xallocx_success_c;

	flags = MALLOCX_ARENA(arena_ind) | MALLOCX_TCACHE_NONE;

	/* Get large size classes. */
	sz = sizeof(size_t);
	expect_d_eq(mallctl(""arenas.lextent.0.size"", (void *)&large0, &sz, NULL,
	    0), 0, ""Unexpected arenas.lextent.0.size failure"");
	expect_d_eq(mallctl(""arenas.lextent.1.size"", (void *)&large1, &sz, NULL,
	    0), 0, ""Unexpected arenas.lextent.1.size failure"");
	expect_d_eq(mallctl(""arenas.lextent.2.size"", (void *)&large2, &sz, NULL,
	    0), 0, ""Unexpected arenas.lextent.2.size failure"");

	/* Test dalloc/decommit/purge cascade. */
	purge_miblen = sizeof(purge_mib)/sizeof(size_t);
	expect_d_eq(mallctlnametomib(""arena.0.purge"", purge_mib, &purge_miblen),
	    0, ""Unexpected mallctlnametomib() failure"");
	purge_mib[1] = (size_t)arena_ind;
	called_alloc = false;
	try_alloc = true;
	try_dalloc = false;
	try_decommit = false;
	p = mallocx(large0 * 2, flags);
	expect_ptr_not_null(p, ""Unexpected mallocx() error"");
	expect_true(called_alloc, ""Expected alloc call"");
	called_dalloc = false;
	called_decommit = false;
	did_purge_lazy = false;
	did_purge_forced = false;
	called_split = false;
	xallocx_success_a = (xallocx(p, large0, 0, flags) == large0);
	expect_d_eq(mallctlbymib(purge_mib, purge_miblen, NULL, NULL, NULL, 0),
	    0, ""Unexpected arena.%u.purge error"", arena_ind);
	if (xallocx_success_a) {
		expect_true(called_dalloc, ""Expected dalloc call"");
		expect_true(called_decommit, ""Expected decommit call"");
		expect_true(did_purge_lazy || did_purge_forced,
		    ""Expected purge"");
		expect_true(called_split, ""Expected split call"");
	}
	dallocx(p, flags);
	try_dalloc = true;

	/* Test decommit/commit and observe split/merge. */
	try_dalloc = false;
	try_decommit = true;
	p = mallocx(large0 * 2, flags);
	expect_ptr_not_null(p, ""Unexpected mallocx() error"");
	did_decommit = false;
	did_commit = false;
	called_split = false;
	did_split = false;
	did_merge = false;
	xallocx_success_b = (xallocx(p, large0, 0, flags) == large0);
	expect_d_eq(mallctlbymib(purge_mib, purge_miblen, NULL, NULL, NULL, 0),
	    0, ""Unexpected arena.%u.purge error"", arena_ind);
	if (xallocx_success_b) {
		expect_true(did_split, ""Expected split"");
	}
	xallocx_success_c = (xallocx(p, large0 * 2, 0, flags) == large0 * 2);
	if (did_split) {
		expect_b_eq(did_decommit, did_commit,
		    ""Expected decommit/commit match"");
	}
	if (xallocx_success_b && xallocx_success_c) {
		expect_true(did_merge, ""Expected merge"");
	}
	dallocx(p, flags);
	try_dalloc = true;
	try_decommit = false;

	/* Make sure non-large allocation succeeds. */
	p = mallocx(42, flags);
	expect_ptr_not_null(p, ""Unexpected mallocx() error"");
	dallocx(p, flags);
}",test_manual_hook_body,"void
test_manual_hook_body(void) {
	unsigned arena_ind;
	size_t old_size, new_size, sz;
	size_t hooks_mib[3];
	size_t hooks_miblen;
	extent_hooks_t *new_hooks, *old_hooks;

	extent_hooks_prep();

	sz = sizeof(unsigned);
	expect_d_eq(mallctl(""arenas.create"", (void *)&arena_ind, &sz, NULL, 0),
	    0, ""Unexpected mallctl() failure"");

	/* Install custom extent hooks. */
	hooks_miblen = sizeof(hooks_mib)/sizeof(size_t);
	expect_d_eq(mallctlnametomib(""arena.0.extent_hooks"", hooks_mib,
	    &hooks_miblen), 0, ""Unexpected mallctlnametomib() failure"");
	hooks_mib[1] = (size_t)arena_ind;
	old_size = sizeof(extent_hooks_t *);
	new_hooks = &hooks;
	new_size = sizeof(extent_hooks_t *);
	expect_d_eq(mallctlbymib(hooks_mib, hooks_miblen, (void *)&old_hooks,
	    &old_size, (void *)&new_hooks, new_size), 0,
	    ""Unexpected extent_hooks error"");
	expect_ptr_ne(old_hooks->alloc, extent_alloc_hook,
	    ""Unexpected extent_hooks error"");
	expect_ptr_ne(old_hooks->dalloc, extent_dalloc_hook,
	    ""Unexpected extent_hooks error"");
	expect_ptr_ne(old_hooks->commit, extent_commit_hook,
	    ""Unexpected extent_hooks error"");
	expect_ptr_ne(old_hooks->decommit, extent_decommit_hook,
	    ""Unexpected extent_hooks error"");
	expect_ptr_ne(old_hooks->purge_lazy, extent_purge_lazy_hook,
	    ""Unexpected extent_hooks error"");
	expect_ptr_ne(old_hooks->purge_forced, extent_purge_forced_hook,
	    ""Unexpected extent_hooks error"");
	expect_ptr_ne(old_hooks->split, extent_split_hook,
	    ""Unexpected extent_hooks error"");
	expect_ptr_ne(old_hooks->merge, extent_merge_hook,
	    ""Unexpected extent_hooks error"");

	if (!is_background_thread_enabled()) {
		test_extent_body(arena_ind);
	}

	/* Restore extent hooks. */
	expect_d_eq(mallctlbymib(hooks_mib, hooks_miblen, NULL, NULL,
	    (void *)&old_hooks, new_size), 0, ""Unexpected extent_hooks error"");
	expect_d_eq(mallctlbymib(hooks_mib, hooks_miblen, (void *)&old_hooks,
	    &old_size, NULL, 0), 0, ""Unexpected extent_hooks error"");
	expect_ptr_eq(old_hooks, default_hooks, ""Unexpected extent_hooks error"");
	expect_ptr_eq(old_hooks->alloc, default_hooks->alloc,
	    ""Unexpected extent_hooks error"");
	expect_ptr_eq(old_hooks->dalloc, default_hooks->dalloc,
	    ""Unexpected extent_hooks error"");
	expect_ptr_eq(old_hooks->commit, default_hooks->commit,
	    ""Unexpected extent_hooks error"");
	expect_ptr_eq(old_hooks->decommit, default_hooks->decommit,
	    ""Unexpected extent_hooks error"");
	expect_ptr_eq(old_hooks->purge_lazy, default_hooks->purge_lazy,
	    ""Unexpected extent_hooks error"");
	expect_ptr_eq(old_hooks->purge_forced, default_hooks->purge_forced,
	    ""Unexpected extent_hooks error"");
	expect_ptr_eq(old_hooks->split, default_hooks->split,
	    ""Unexpected extent_hooks error"");
	expect_ptr_eq(old_hooks->merge, default_hooks->merge,
	    ""Unexpected extent_hooks error"");
}"
tsd_slow_update,"void
tsd_slow_update(tsd_t *tsd) {
	uint8_t old_state;
	do {
		uint8_t new_state = tsd_state_compute(tsd);
		old_state = tsd_atomic_exchange(&tsd->state, new_state,
		    ATOMIC_ACQUIRE);
	} while (old_state == tsd_state_nominal_recompute);

	te_recompute_fast_threshold(tsd);
}",tsd_data_init,"bool
tsd_data_init(tsd_t *tsd) {
	/*
	 * We initialize the rtree context first (before the tcache), since the
	 * tcache initialization depends on it.
	 */
	rtree_ctx_data_init(tsd_rtree_ctxp_get_unsafe(tsd));
	tsd_prng_state_init(tsd);
	tsd_te_init(tsd); /* event_init may use the prng state above. */
	tsd_san_init(tsd);
	return tsd_tcache_enabled_data_init(tsd);
}",tsd_data_init_nocleanup,"bool
tsd_data_init_nocleanup(tsd_t *tsd) {
	assert(tsd_state_get(tsd) == tsd_state_reincarnated ||
	    tsd_state_get(tsd) == tsd_state_minimal_initialized);
	/*
	 * During reincarnation, there is no guarantee that the cleanup function
	 * will be called (deallocation may happen after all tsd destructors).
	 * We set up tsd in a way that no cleanup is needed.
	 */
	rtree_ctx_data_init(tsd_rtree_ctxp_get_unsafe(tsd));
	*tsd_tcache_enabledp_get_unsafe(tsd) = false;
	*tsd_reentrancy_levelp_get(tsd) = 1;
	tsd_prng_state_init(tsd);
	tsd_te_init(tsd); /* event_init may use the prng state above. */
	tsd_san_init(tsd);
	assert_tsd_data_cleanup_done(tsd);

	return false;
}",assert_tsd_data_cleanup_done,"void
assert_tsd_data_cleanup_done(tsd_t *tsd) {
	assert(!tsd_nominal(tsd));
	assert(!tsd_in_nominal_list(tsd));
	assert(*tsd_arenap_get_unsafe(tsd) == NULL);
	assert(*tsd_iarenap_get_unsafe(tsd) == NULL);
	assert(*tsd_tcache_enabledp_get_unsafe(tsd) == false);
	assert(*tsd_prof_tdatap_get_unsafe(tsd) == NULL);
}"
background_threads_disable_single,"bool
background_threads_disable_single(tsd_t *tsd, background_thread_info_t *info) {
	if (info == &background_thread_info[0]) {
		malloc_mutex_assert_owner(tsd_tsdn(tsd),
		    &background_thread_lock);
	} else {
		malloc_mutex_assert_not_owner(tsd_tsdn(tsd),
		    &background_thread_lock);
	}

	pre_reentrancy(tsd, NULL);
	malloc_mutex_lock(tsd_tsdn(tsd), &info->mtx);
	bool has_thread;
	assert(info->state != background_thread_paused);
	if (info->state == background_thread_started) {
		has_thread = true;
		info->state = background_thread_stopped;
		pthread_cond_signal(&info->cond);
	} else {
		has_thread = false;
	}
	malloc_mutex_unlock(tsd_tsdn(tsd), &info->mtx);

	if (!has_thread) {
		post_reentrancy(tsd);
		return false;
	}
	void *ret;
	if (pthread_join(info->thread, &ret)) {
		post_reentrancy(tsd);
		return true;
	}
	assert(ret == NULL);
	n_background_threads--;
	post_reentrancy(tsd);

	return false;
}",background_thread_init,"void
background_thread_init(tsd_t *tsd, background_thread_info_t *info) {
	malloc_mutex_assert_owner(tsd_tsdn(tsd), &background_thread_lock);
	info->state = background_thread_started;
	background_thread_info_init(tsd_tsdn(tsd), info);
	n_background_threads++;
}",background_thread_create,"bool
background_thread_create(tsd_t *tsd, unsigned arena_ind) {
	assert(have_background_thread);

	bool ret;
	malloc_mutex_lock(tsd_tsdn(tsd), &background_thread_lock);
	ret = background_thread_create_locked(tsd, arena_ind);
	malloc_mutex_unlock(tsd_tsdn(tsd), &background_thread_lock);

	return ret;
}",background_thread_info_init,"void
background_thread_info_init(tsdn_t *tsdn, background_thread_info_t *info) {
	background_thread_wakeup_time_set(tsdn, info, 0);
	info->npages_to_purge_new = 0;
	if (config_stats) {
		info->tot_n_runs = 0;
		nstime_init_zero(&info->tot_sleep_time);
	}
}"
assert_tsd_data_cleanup_done,"void
assert_tsd_data_cleanup_done(tsd_t *tsd) {
	assert(!tsd_nominal(tsd));
	assert(!tsd_in_nominal_list(tsd));
	assert(*tsd_arenap_get_unsafe(tsd) == NULL);
	assert(*tsd_iarenap_get_unsafe(tsd) == NULL);
	assert(*tsd_tcache_enabledp_get_unsafe(tsd) == false);
	assert(*tsd_prof_tdatap_get_unsafe(tsd) == NULL);
}",tsd_data_init_nocleanup,"bool
tsd_data_init_nocleanup(tsd_t *tsd) {
	assert(tsd_state_get(tsd) == tsd_state_reincarnated ||
	    tsd_state_get(tsd) == tsd_state_minimal_initialized);
	/*
	 * During reincarnation, there is no guarantee that the cleanup function
	 * will be called (deallocation may happen after all tsd destructors).
	 * We set up tsd in a way that no cleanup is needed.
	 */
	rtree_ctx_data_init(tsd_rtree_ctxp_get_unsafe(tsd));
	*tsd_tcache_enabledp_get_unsafe(tsd) = false;
	*tsd_reentrancy_levelp_get(tsd) = 1;
	tsd_prng_state_init(tsd);
	tsd_te_init(tsd); /* event_init may use the prng state above. */
	tsd_san_init(tsd);
	assert_tsd_data_cleanup_done(tsd);

	return false;
}"
percpu_arena_as_initialized,"percpu_arena_mode_t
percpu_arena_as_initialized(percpu_arena_mode_t mode) {
	assert(!malloc_initialized());
	assert(mode <= percpu_arena_disabled);

	if (mode != percpu_arena_disabled) {
		mode += percpu_arena_mode_enabled_base;
	}

	return mode;
}",malloc_init_percpu,"void
malloc_init_percpu(void) {
	opt_percpu_arena = percpu_arena_as_initialized(opt_percpu_arena);
}"
arena_decay_dirty,"bool
arena_decay_dirty(tsdn_t *tsdn, arena_t *arena, bool is_background_thread,
    bool all) {
	return arena_decay_impl(tsdn, arena, &arena->pa_shard.pac.decay_dirty,
	    &arena->pa_shard.pac.stats->decay_dirty,
	    &arena->pa_shard.pac.ecache_dirty, is_background_thread, all);
}",arena_decay_muzzy,"bool
arena_decay_muzzy(tsdn_t *tsdn, arena_t *arena, bool is_background_thread,
    bool all) {
	if (pa_shard_dont_decay_muzzy(&arena->pa_shard)) {
		return false;
	}
	return arena_decay_impl(tsdn, arena, &arena->pa_shard.pac.decay_muzzy,
	    &arena->pa_shard.pac.stats->decay_muzzy,
	    &arena->pa_shard.pac.ecache_muzzy, is_background_thread, all);
}"
arena_decay_ms_get,"ssize_t
arena_decay_ms_get(arena_t *arena, extent_state_t state) {
	return pa_decay_ms_get(&arena->pa_shard, state);
}",arena_dirty_decay_ms_default_set,"bool
arena_dirty_decay_ms_default_set(ssize_t decay_ms) {
	if (!decay_ms_valid(decay_ms)) {
		return true;
	}
	atomic_store_zd(&dirty_decay_ms_default, decay_ms, ATOMIC_RELAXED);
	return false;
}",arena_muzzy_decay_ms_default_set,"bool
arena_muzzy_decay_ms_default_set(ssize_t decay_ms) {
	if (!decay_ms_valid(decay_ms)) {
		return true;
	}
	atomic_store_zd(&muzzy_decay_ms_default, decay_ms, ATOMIC_RELAXED);
	return false;
}"
idxof,"int idxof(int i) {
    return i ^ 1;
}",rshift128,"void rshift128(w128_t *out, w128_t const *in, int shift) {
    uint64_t th, tl, oh, ol;

    th = ((uint64_t)in->u[2] << 32) | ((uint64_t)in->u[3]);
    tl = ((uint64_t)in->u[0] << 32) | ((uint64_t)in->u[1]);

    oh = th >> (shift * 8);
    ol = tl >> (shift * 8);
    ol |= th << (64 - shift * 8);
    out->u[0] = (uint32_t)(ol >> 32);
    out->u[1] = (uint32_t)ol;
    out->u[2] = (uint32_t)(oh >> 32);
    out->u[3] = (uint32_t)oh;
}",lshift128,"void lshift128(w128_t *out, w128_t const *in, int shift) {
    uint64_t th, tl, oh, ol;

    th = ((uint64_t)in->u[2] << 32) | ((uint64_t)in->u[3]);
    tl = ((uint64_t)in->u[0] << 32) | ((uint64_t)in->u[1]);

    oh = th << (shift * 8);
    ol = tl << (shift * 8);
    oh |= tl >> (64 - shift * 8);
    out->u[0] = (uint32_t)(ol >> 32);
    out->u[1] = (uint32_t)ol;
    out->u[2] = (uint32_t)(oh >> 32);
    out->u[3] = (uint32_t)oh;
}",gen_rand_all,"void gen_rand_all(sfmt_t *ctx) {
    int i;
    w128_t *r1, *r2;

    r1 = &ctx->sfmt[N - 2];
    r2 = &ctx->sfmt[N - 1];
    for (i = 0; i < N - POS1; i++) {
	do_recursion(&ctx->sfmt[i], &ctx->sfmt[i], &ctx->sfmt[i + POS1], r1,
	  r2);
	r1 = r2;
	r2 = &ctx->sfmt[i];
    }
    for (; i < N; i++) {
	do_recursion(&ctx->sfmt[i], &ctx->sfmt[i], &ctx->sfmt[i + POS1 - N], r1,
	  r2);
	r1 = r2;
	r2 = &ctx->sfmt[i];
    }
}",gen_rand_array,"void gen_rand_array(sfmt_t *ctx, w128_t *array, int size) {
    int i, j;
    w128_t *r1, *r2;

    r1 = &ctx->sfmt[N - 2];
    r2 = &ctx->sfmt[N - 1];
    for (i = 0; i < N - POS1; i++) {
	do_recursion(&array[i], &ctx->sfmt[i], &ctx->sfmt[i + POS1], r1, r2);
	r1 = r2;
	r2 = &array[i];
    }
    for (; i < N; i++) {
	do_recursion(&array[i], &ctx->sfmt[i], &array[i + POS1 - N], r1, r2);
	r1 = r2;
	r2 = &array[i];
    }
    for (; i < size - N; i++) {
	do_recursion(&array[i], &array[i - N], &array[i + POS1 - N], r1, r2);
	r1 = r2;
	r2 = &array[i];
    }
    for (j = 0; j < 2 * N - size; j++) {
	ctx->sfmt[j] = array[j + size - N];
    }
    for (; i < size; i++, j++) {
	do_recursion(&array[i], &array[i - N], &array[i + POS1 - N], r1, r2);
	r1 = r2;
	r2 = &array[i];
	ctx->sfmt[j] = array[i];
    }
}",func1,"uint32_t func1(uint32_t x) {
    return (x ^ (x >> 27)) * (uint32_t)1664525UL;
}",func2,"uint32_t func2(uint32_t x) {
    return (x ^ (x >> 27)) * (uint32_t)1566083941UL;
}",swap,"void swap(w128_t *array, int size) {
    int i;
    uint32_t x, y;

    for (i = 0; i < size; i++) {
	x = array[i].u[0];
	y = array[i].u[2];
	array[i].u[0] = array[i].u[1];
	array[i].u[2] = array[i].u[3];
	array[i].u[1] = x;
	array[i].u[3] = y;
    }
}"
arena_bin_slabs_full_remove,"void
arena_bin_slabs_full_remove(arena_t *arena, bin_t *bin, edata_t *slab) {
	if (arena_is_auto(arena)) {
		return;
	}
	edata_list_active_remove(&bin->slabs_full, slab);
}",arena_bin_slabs_full_insert,"void
arena_bin_slabs_full_insert(arena_t *arena, bin_t *bin, edata_t *slab) {
	assert(edata_nfree_get(slab) == 0);
	/*
	 *  Tracking extents is required by arena_reset, which is not allowed
	 *  for auto arenas.  Bypass this step to avoid touching the edata
	 *  linkage (often results in cache misses) for auto arenas.
	 */
	if (arena_is_auto(arena)) {
		return;
	}
	edata_list_active_append(&bin->slabs_full, slab);
}"
tsd_slow_update,"void
tsd_slow_update(tsd_t *tsd) {
	uint8_t old_state;
	do {
		uint8_t new_state = tsd_state_compute(tsd);
		old_state = tsd_atomic_exchange(&tsd->state, new_state,
		    ATOMIC_ACQUIRE);
	} while (old_state == tsd_state_nominal_recompute);

	te_recompute_fast_threshold(tsd);
}",tsd_data_init,"bool
tsd_data_init(tsd_t *tsd) {
	/*
	 * We initialize the rtree context first (before the tcache), since the
	 * tcache initialization depends on it.
	 */
	rtree_ctx_data_init(tsd_rtree_ctxp_get_unsafe(tsd));
	tsd_prng_state_init(tsd);
	tsd_te_init(tsd); /* event_init may use the prng state above. */
	tsd_san_init(tsd);
	return tsd_tcache_enabled_data_init(tsd);
}"
tsd_tcache_data_init,"bool
tsd_tcache_data_init(tsd_t *tsd, arena_t *arena,
    const cache_bin_info_t tcache_bin_info[TCACHE_NBINS_MAX]) {
	assert(tcache_bin_info != NULL);
	return tsd_tcache_data_init_impl(tsd, arena, tcache_bin_info);
}",tcache_destroy,"void
tcache_destroy(tsd_t *tsd, tcache_t *tcache, bool tsd_tcache) {
	tcache_slow_t *tcache_slow = tcache->tcache_slow;
	tcache_flush_cache(tsd, tcache);
	arena_t *arena = tcache_slow->arena;
	tcache_arena_dissociate(tsd_tsdn(tsd), tcache_slow, tcache);

	if (tsd_tcache) {
		cache_bin_t *cache_bin = &tcache->bins[0];
		cache_bin_assert_empty(cache_bin);
	}
	if (tsd_tcache && cache_bin_stack_use_thp()) {
		b0_dalloc_tcache_stack(tsd_tsdn(tsd), tcache_slow->dyn_alloc);
	} else {
		idalloctm(tsd_tsdn(tsd), tcache_slow->dyn_alloc, NULL, NULL,
		    true, true);
	}

	/*
	 * The deallocation and tcache flush above may not trigger decay since
	 * we are on the tcache shutdown path (potentially with non-nominal
	 * tsd).  Manually trigger decay to avoid pathological cases.  Also
	 * include arena 0 because the tcache array is allocated from it.
	 */
	arena_decay(tsd_tsdn(tsd), arena_get(tsd_tsdn(tsd), 0, false),
	    false, false);

	if (arena_nthreads_get(arena, false) == 0 &&
	    !background_thread_enabled()) {
		/* Force purging when no threads assigned to the arena anymore. */
		arena_decay(tsd_tsdn(tsd), arena,
		    /* is_background_thread */ false, /* all */ true);
	} else {
		arena_decay(tsd_tsdn(tsd), arena,
		    /* is_background_thread */ false, /* all */ false);
	}
}"
arena_decay_dirty,"bool
arena_decay_dirty(tsdn_t *tsdn, arena_t *arena, bool is_background_thread,
    bool all) {
	return arena_decay_impl(tsdn, arena, &arena->pa_shard.pac.decay_dirty,
	    &arena->pa_shard.pac.stats->decay_dirty,
	    &arena->pa_shard.pac.ecache_dirty, is_background_thread, all);
}",arena_decay_muzzy,"bool
arena_decay_muzzy(tsdn_t *tsdn, arena_t *arena, bool is_background_thread,
    bool all) {
	if (pa_shard_dont_decay_muzzy(&arena->pa_shard)) {
		return false;
	}
	return arena_decay_impl(tsdn, arena, &arena->pa_shard.pac.decay_muzzy,
	    &arena->pa_shard.pac.stats->decay_muzzy,
	    &arena->pa_shard.pac.ecache_muzzy, is_background_thread, all);
}"
mallctl_failure,"void
mallctl_failure(int err) {
	char buf[BUFERROR_BUF];

	buferror(err, buf, sizeof(buf));
	test_fail(""Error in mallctl(): %s"", buf);
}",gen_mallctl_str,"void
gen_mallctl_str(char *cmd, char *name, unsigned arena_ind) {
	sprintf(cmd, ""stats.arenas.%u.bins.0.%s"", arena_ind, name);
}"
extent_deactivate_locked,"void
extent_deactivate_locked(tsdn_t *tsdn, pac_t *pac, ecache_t *ecache,
    edata_t *edata) {
	assert(edata_state_get(edata) == extent_state_active);
	extent_deactivate_locked_impl(tsdn, pac, ecache, edata);
}",extent_activate_locked,"void
extent_activate_locked(tsdn_t *tsdn, pac_t *pac, ecache_t *ecache, eset_t *eset,
    edata_t *edata) {
	assert(edata_arena_ind_get(edata) == ecache_ind_get(ecache));
	assert(edata_state_get(edata) == ecache->state ||
	    edata_state_get(edata) == extent_state_merging);

	eset_remove(eset, edata);
	emap_update_edata_state(tsdn, pac->emap, edata, extent_state_active);
}"
extent_activate_locked,"void
extent_activate_locked(tsdn_t *tsdn, pac_t *pac, ecache_t *ecache, eset_t *eset,
    edata_t *edata) {
	assert(edata_arena_ind_get(edata) == ecache_ind_get(ecache));
	assert(edata_state_get(edata) == ecache->state ||
	    edata_state_get(edata) == extent_state_merging);

	eset_remove(eset, edata);
	emap_update_edata_state(tsdn, pac->emap, edata, extent_state_active);
}",extent_deactivate_locked,"void
extent_deactivate_locked(tsdn_t *tsdn, pac_t *pac, ecache_t *ecache,
    edata_t *edata) {
	assert(edata_state_get(edata) == extent_state_active);
	extent_deactivate_locked_impl(tsdn, pac, ecache, edata);
}"
arena_nthreads_get,"unsigned
arena_nthreads_get(arena_t *arena, bool internal) {
	return atomic_load_u(&arena->nthreads[internal], ATOMIC_RELAXED);
}",arena_large_dalloc_stats_update,"void
arena_large_dalloc_stats_update(tsdn_t *tsdn, arena_t *arena, size_t usize) {
	cassert(config_stats);

	szind_t index = sz_size2index(usize);
	/* This only occurs when we have a sampled small allocation */
	if (usize < SC_LARGE_MINCLASS) {
		assert(index < SC_NBINS);
		assert(usize >= PAGE && usize % PAGE == 0);
		bin_t *bin = arena_get_bin(arena, index, /* binshard */ 0);
		malloc_mutex_lock(tsdn, &bin->lock);
		bin->stats.ndalloc++;
		malloc_mutex_unlock(tsdn, &bin->lock);
	} else {
		assert(index >= SC_NBINS);
		szind_t hindex = index - SC_NBINS;
		LOCKEDINT_MTX_LOCK(tsdn, arena->stats.mtx);
		locked_inc_u64(tsdn, LOCKEDINT_MTX(arena->stats.mtx),
			&arena->stats.lstats[hindex].ndalloc, 1);
		LOCKEDINT_MTX_UNLOCK(tsdn, arena->stats.mtx);
	}
}",arena_large_malloc_stats_update,"void
arena_large_malloc_stats_update(tsdn_t *tsdn, arena_t *arena, size_t usize) {
	cassert(config_stats);

	szind_t index = sz_size2index(usize);
	/* This only occurs when we have a sampled small allocation */
	if (usize < SC_LARGE_MINCLASS) {
		assert(index < SC_NBINS);
		assert(usize >= PAGE && usize % PAGE == 0);
		bin_t *bin = arena_get_bin(arena, index, /* binshard */ 0);
		malloc_mutex_lock(tsdn, &bin->lock);
		bin->stats.nmalloc++;
		malloc_mutex_unlock(tsdn, &bin->lock);
	} else {
		assert(index >= SC_NBINS);
		szind_t hindex = index - SC_NBINS;
		LOCKEDINT_MTX_LOCK(tsdn, arena->stats.mtx);
		locked_inc_u64(tsdn, LOCKEDINT_MTX(arena->stats.mtx),
			&arena->stats.lstats[hindex].nmalloc, 1);
		LOCKEDINT_MTX_UNLOCK(tsdn, arena->stats.mtx);
	}
}",arena_large_ralloc_stats_update,"void
arena_large_ralloc_stats_update(tsdn_t *tsdn, arena_t *arena, size_t oldusize,
    size_t usize) {
	arena_large_malloc_stats_update(tsdn, arena, usize);
	arena_large_dalloc_stats_update(tsdn, arena, oldusize);
}"
arena_bin_slabs_full_remove,"void
arena_bin_slabs_full_remove(arena_t *arena, bin_t *bin, edata_t *slab) {
	if (arena_is_auto(arena)) {
		return;
	}
	edata_list_active_remove(&bin->slabs_full, slab);
}",extent_deregister,"void
extent_deregister(tsdn_t *tsdn, pac_t *pac, edata_t *edata) {
	extent_deregister_impl(tsdn, pac, edata, true);
}",extent_deactivate_locked,"void
extent_deactivate_locked(tsdn_t *tsdn, pac_t *pac, ecache_t *ecache,
    edata_t *edata) {
	assert(edata_state_get(edata) == extent_state_active);
	extent_deactivate_locked_impl(tsdn, pac, ecache, edata);
}",extent_gdump_sub,"void
extent_gdump_sub(tsdn_t *tsdn, const edata_t *edata) {
	cassert(config_prof);

	if (opt_prof && edata_state_get(edata) == extent_state_active) {
		size_t nsub = edata_size_get(edata) >> LG_PAGE;
		assert(atomic_load_zu(&curpages, ATOMIC_RELAXED) >= nsub);
		atomic_fetch_sub_zu(&curpages, nsub, ATOMIC_RELAXED);
	}
}",extent_activate_locked,"void
extent_activate_locked(tsdn_t *tsdn, pac_t *pac, ecache_t *ecache, eset_t *eset,
    edata_t *edata) {
	assert(edata_arena_ind_get(edata) == ecache_ind_get(ecache));
	assert(edata_state_get(edata) == ecache->state ||
	    edata_state_get(edata) == extent_state_merging);

	eset_remove(eset, edata);
	emap_update_edata_state(tsdn, pac->emap, edata, extent_state_active);
}",extent_merge_wrapper,"bool
extent_merge_wrapper(tsdn_t *tsdn, pac_t *pac, ehooks_t *ehooks,
    edata_t *a, edata_t *b) {
	return extent_merge_impl(tsdn, pac, ehooks, a, b,
	    /* holding_core_locks */ false);
}",large_dalloc_finish_impl,"void
large_dalloc_finish_impl(tsdn_t *tsdn, arena_t *arena, edata_t *edata) {
	bool deferred_work_generated = false;
	pa_dalloc(tsdn, &arena->pa_shard, edata, &deferred_work_generated);
	if (deferred_work_generated) {
		arena_handle_deferred_work(tsdn, arena);
	}
}"
malloc_init_hard_a0,"bool
malloc_init_hard_a0(void) {
	bool ret;

	malloc_mutex_lock(TSDN_NULL, &init_lock);
	ret = malloc_init_hard_a0_locked();
	malloc_mutex_unlock(TSDN_NULL, &init_lock);
	return ret;
}",pages_commit_impl,"bool
pages_commit_impl(void *addr, size_t size, bool commit) {
	if (os_overcommits) {
		return true;
	}

	return os_pages_commit(addr, size, commit);
}",get_nsizes_impl,"unsigned
get_nsizes_impl(const char *cmd) {
	unsigned ret;
	size_t z;

	z = sizeof(unsigned);
	expect_d_eq(mallctl(cmd, (void *)&ret, &z, NULL, 0), 0,
	    ""Unexpected mallctl(\""%s\"", ...) failure"", cmd);

	return ret;
}",get_size_impl,"size_t
get_size_impl(const char *cmd, size_t ind) {
	size_t ret;
	size_t z;
	size_t mib[4];
	size_t miblen = 4;

	z = sizeof(size_t);
	expect_d_eq(mallctlnametomib(cmd, mib, &miblen),
	    0, ""Unexpected mallctlnametomib(\""%s\"", ...) failure"", cmd);
	mib[2] = ind;
	z = sizeof(size_t);
	expect_d_eq(mallctlbymib(mib, miblen, (void *)&ret, &z, NULL, 0),
	    0, ""Unexpected mallctlbymib([\""%s\"", %zu], ...) failure"", cmd, ind);

	return ret;
}",node_cmp,"int
node_cmp(const node_t *a, const node_t *b) {
	int ret;

	ret = (a->key > b->key) - (a->key < b->key);
	if (ret == 0) {
		/*
		 * Duplicates are not allowed in the heap, so force an
		 * arbitrary ordering for non-identical items with equal keys.
		 */
		ret = (((uintptr_t)a) > ((uintptr_t)b))
		    - (((uintptr_t)a) < ((uintptr_t)b));
	}
	return ret;
}"
compute_size_with_overflow,"bool
compute_size_with_overflow(bool may_overflow, dynamic_opts_t *dopts,
    size_t *size) {
	/*
	 * This function is just num_items * item_size, except that we may have
	 * to check for overflow.
	 */

	if (!may_overflow) {
		assert(dopts->num_items == 1);
		*size = dopts->item_size;
		return false;
	}

	/* A size_t with its high-half bits all set to 1. */
	static const size_t high_bits = SIZE_T_MAX << (sizeof(size_t) * 8 / 2);

	*size = dopts->item_size * dopts->num_items;

	if (unlikely(*size == 0)) {
		return (dopts->num_items != 0 && dopts->item_size != 0);
	}

	/*
	 * We got a non-zero size, but we don't know if we overflowed to get
	 * there.  To avoid having to do a divide, we'll be clever and note that
	 * if both A and B can be represented in N/2 bits, then their product
	 * can be represented in N bits (without the possibility of overflow).
	 */
	if (likely((high_bits & (dopts->num_items | dopts->item_size)) == 0)) {
		return false;
	}
	if (likely(*size / dopts->item_size == dopts->num_items)) {
		return false;
	}
	return true;
}",static_opts_init,"void
static_opts_init(static_opts_t *static_opts) {
	static_opts->may_overflow = false;
	static_opts->bump_empty_aligned_alloc = false;
	static_opts->assert_nonempty_alloc = false;
	static_opts->null_out_result_on_error = false;
	static_opts->set_errno_on_error = false;
	static_opts->min_alignment = 0;
	static_opts->oom_string = """";
	static_opts->invalid_alignment_string = """";
	static_opts->slow = false;
	static_opts->usize = false;
}",dynamic_opts_init,"void
dynamic_opts_init(dynamic_opts_t *dynamic_opts) {
	dynamic_opts->result = NULL;
	dynamic_opts->usize = 0;
	dynamic_opts->num_items = 0;
	dynamic_opts->item_size = 0;
	dynamic_opts->alignment = 0;
	dynamic_opts->zero = false;
	dynamic_opts->tcache_ind = TCACHE_IND_AUTOMATIC;
	dynamic_opts->arena_ind = ARENA_IND_AUTOMATIC;
}"
zone_pressure_relief,"size_t
zone_pressure_relief(struct _malloc_zone_t *zone, size_t goal) {
	return 0;
}",zone_check,"boolean_t
zone_check(malloc_zone_t *zone) {
	return true;
}",zone_print,"void
zone_print(malloc_zone_t *zone, boolean_t verbose) {
}",zone_log,"void
zone_log(malloc_zone_t *zone, void *address) {
}",zone_locked,"boolean_t
zone_locked(malloc_zone_t *zone) {
	/* Pretend no lock is being held */
	return false;
}",zone_reinit_lock,"void
zone_reinit_lock(malloc_zone_t *zone) {
	/* As of OSX 10.12, this function is only used when force_unlock would
	 * be used if the zone version were < 9. So just use force_unlock. */
	zone_force_unlock(zone);
}",zone_free,"void
zone_free(malloc_zone_t *zone, void *ptr) {
	if (ivsalloc(tsdn_fetch(), ptr) != 0) {
		je_free(ptr);
		return;
	}

	free(ptr);
}"
zone_size,"size_t
zone_size(malloc_zone_t *zone, const void *ptr) {
	/*
	 * There appear to be places within Darwin (such as setenv(3)) that
	 * cause calls to this function with pointers that *no* zone owns.  If
	 * we knew that all pointers were owned by *some* zone, we could split
	 * our zone into two parts, and use one as the default allocator and
	 * the other as the default deallocator/reallocator.  Since that will
	 * not work in practice, we must check all pointers to assure that they
	 * reside within a mapped extent before determining size.
	 */
	return ivsalloc(tsdn_fetch(), ptr);
}",zone_destroy,"void
zone_destroy(malloc_zone_t *zone) {
	/* This function should never be called. */
	not_reached();
}"
zone_pressure_relief,"size_t
zone_pressure_relief(struct _malloc_zone_t *zone, size_t goal) {
	return 0;
}",zone_check,"boolean_t
zone_check(malloc_zone_t *zone) {
	return true;
}",zone_print,"void
zone_print(malloc_zone_t *zone, boolean_t verbose) {
}",zone_log,"void
zone_log(malloc_zone_t *zone, void *address) {
}",zone_locked,"boolean_t
zone_locked(malloc_zone_t *zone) {
	/* Pretend no lock is being held */
	return false;
}",zone_reinit_lock,"void
zone_reinit_lock(malloc_zone_t *zone) {
	/* As of OSX 10.12, this function is only used when force_unlock would
	 * be used if the zone version were < 9. So just use force_unlock. */
	zone_force_unlock(zone);
}",zone_free,"void
zone_free(malloc_zone_t *zone, void *ptr) {
	if (ivsalloc(tsdn_fetch(), ptr) != 0) {
		je_free(ptr);
		return;
	}

	free(ptr);
}"
zone_size,"size_t
zone_size(malloc_zone_t *zone, const void *ptr) {
	/*
	 * There appear to be places within Darwin (such as setenv(3)) that
	 * cause calls to this function with pointers that *no* zone owns.  If
	 * we knew that all pointers were owned by *some* zone, we could split
	 * our zone into two parts, and use one as the default allocator and
	 * the other as the default deallocator/reallocator.  Since that will
	 * not work in practice, we must check all pointers to assure that they
	 * reside within a mapped extent before determining size.
	 */
	return ivsalloc(tsdn_fetch(), ptr);
}",zone_destroy,"void
zone_destroy(malloc_zone_t *zone) {
	/* This function should never be called. */
	not_reached();
}"
ctl_arena_refresh,"void
ctl_arena_refresh(tsdn_t *tsdn, arena_t *arena, ctl_arena_t *ctl_sdarena,
    unsigned i, bool destroyed) {
	ctl_arena_t *ctl_arena = arenas_i(i);

	ctl_arena_clear(ctl_arena);
	ctl_arena_stats_amerge(tsdn, ctl_arena, arena);
	/* Merge into sum stats as well. */
	ctl_arena_stats_sdmerge(ctl_sdarena, ctl_arena, destroyed);
}",ctl_arena_clear,"void
ctl_arena_clear(ctl_arena_t *ctl_arena) {
	ctl_arena->nthreads = 0;
	ctl_arena->dss = dss_prec_names[dss_prec_limit];
	ctl_arena->dirty_decay_ms = -1;
	ctl_arena->muzzy_decay_ms = -1;
	ctl_arena->pactive = 0;
	ctl_arena->pdirty = 0;
	ctl_arena->pmuzzy = 0;
	if (config_stats) {
		memset(ctl_arena->astats, 0, sizeof(*(ctl_arena->astats)));
	}
}",do_arena_reset_destroy,"void
do_arena_reset_destroy(const char *name, unsigned arena_ind) {
	size_t mib[3];
	size_t miblen;

	miblen = sizeof(mib)/sizeof(size_t);
	expect_d_eq(mallctlnametomib(name, mib, &miblen), 0,
	    ""Unexpected mallctlnametomib() failure"");
	mib[1] = (size_t)arena_ind;
	expect_d_eq(mallctlbymib(mib, miblen, NULL, NULL, NULL, 0), 0,
	    ""Unexpected mallctlbymib() failure"");
}",do_arena_create,"unsigned
do_arena_create(extent_hooks_t *h) {
	unsigned arena_ind;
	size_t sz = sizeof(unsigned);
	expect_d_eq(mallctl(""arenas.create"", (void *)&arena_ind, &sz,
	    (void *)(h != NULL ? &h : NULL), (h != NULL ? sizeof(h) : 0)), 0,
	    ""Unexpected mallctl() failure"");
	return arena_ind;
}",do_arena_reset_pre,"void
do_arena_reset_pre(unsigned arena_ind, void ***ptrs, unsigned *nptrs) {
#define NLARGE	32
	unsigned nsmall, nlarge, i;
	size_t sz;
	int flags;
	tsdn_t *tsdn;

	flags = MALLOCX_ARENA(arena_ind) | MALLOCX_TCACHE_NONE;

	nsmall = get_nsmall();
	nlarge = get_nlarge() > NLARGE ? NLARGE : get_nlarge();
	*nptrs = nsmall + nlarge;
	*ptrs = (void **)malloc(*nptrs * sizeof(void *));
	expect_ptr_not_null(*ptrs, ""Unexpected malloc() failure"");

	/* Allocate objects with a wide range of sizes. */
	for (i = 0; i < nsmall; i++) {
		sz = get_small_size(i);
		(*ptrs)[i] = mallocx(sz, flags);
		expect_ptr_not_null((*ptrs)[i],
		    ""Unexpected mallocx(%zu, %#x) failure"", sz, flags);
	}
	for (i = 0; i < nlarge; i++) {
		sz = get_large_size(i);
		(*ptrs)[nsmall + i] = mallocx(sz, flags);
		expect_ptr_not_null((*ptrs)[i],
		    ""Unexpected mallocx(%zu, %#x) failure"", sz, flags);
	}

	tsdn = tsdn_fetch();

	/* Verify allocations. */
	for (i = 0; i < *nptrs; i++) {
		expect_zu_gt(ivsalloc(tsdn, (*ptrs)[i]), 0,
		    ""Allocation should have queryable size"");
	}
}",do_arena_reset,"void
do_arena_reset(unsigned arena_ind) {
	do_arena_reset_destroy(""arena.0.reset"", arena_ind);
}",do_arena_reset_post,"void
do_arena_reset_post(void **ptrs, unsigned nptrs, unsigned arena_ind) {
	tsdn_t *tsdn;
	unsigned i;

	tsdn = tsdn_fetch();

	if (have_background_thread) {
		malloc_mutex_lock(tsdn,
		    &background_thread_info_get(arena_ind)->mtx);
	}
	/* Verify allocations no longer exist. */
	for (i = 0; i < nptrs; i++) {
		expect_zu_eq(vsalloc(tsdn, ptrs[i]), 0,
		    ""Allocation should no longer exist"");
	}
	if (have_background_thread) {
		malloc_mutex_unlock(tsdn,
		    &background_thread_info_get(arena_ind)->mtx);
	}

	free(ptrs);
}",do_arena_destroy,"void
do_arena_destroy(unsigned arena_ind) {
	do_arena_reset_destroy(""arena.0.destroy"", arena_ind);
}"
nstime_init,"void
nstime_init(nstime_t *time, uint64_t ns) {
	nstime_set_initialized(time);
	time->ns = ns;
}",nstime_init2,"void
nstime_init2(nstime_t *time, uint64_t sec, uint64_t nsec) {
	nstime_set_initialized(time);
	time->ns = sec * BILLION + nsec;
}",nstime_get,"void
nstime_get(nstime_t *time) {
	FILETIME ft;
	uint64_t ticks_100ns;

	GetSystemTimeAsFileTime(&ft);
	ticks_100ns = (((uint64_t)ft.dwHighDateTime) << 32) | ft.dwLowDateTime;

	nstime_init(time, ticks_100ns * 100);
}"
nstime_init,"void
nstime_init(nstime_t *time, uint64_t ns) {
	nstime_set_initialized(time);
	time->ns = ns;
}",nstime_init2,"void
nstime_init2(nstime_t *time, uint64_t sec, uint64_t nsec) {
	nstime_set_initialized(time);
	time->ns = sec * BILLION + nsec;
}",nstime_get,"void
nstime_get(nstime_t *time) {
	FILETIME ft;
	uint64_t ticks_100ns;

	GetSystemTimeAsFileTime(&ft);
	ticks_100ns = (((uint64_t)ft.dwHighDateTime) << 32) | ft.dwLowDateTime;

	nstime_init(time, ticks_100ns * 100);
}"
extent_register,"bool
extent_register(tsdn_t *tsdn, pac_t *pac, edata_t *edata) {
	return extent_register_impl(tsdn, pac, edata, true);
}",extent_deregister,"void
extent_deregister(tsdn_t *tsdn, pac_t *pac, edata_t *edata) {
	extent_deregister_impl(tsdn, pac, edata, true);
}",extent_reregister,"void
extent_reregister(tsdn_t *tsdn, pac_t *pac, edata_t *edata) {
	bool err = extent_register(tsdn, pac, edata);
	assert(!err);
}"
arena_large_dalloc_stats_update,"void
arena_large_dalloc_stats_update(tsdn_t *tsdn, arena_t *arena, size_t usize) {
	cassert(config_stats);

	szind_t index = sz_size2index(usize);
	/* This only occurs when we have a sampled small allocation */
	if (usize < SC_LARGE_MINCLASS) {
		assert(index < SC_NBINS);
		assert(usize >= PAGE && usize % PAGE == 0);
		bin_t *bin = arena_get_bin(arena, index, /* binshard */ 0);
		malloc_mutex_lock(tsdn, &bin->lock);
		bin->stats.ndalloc++;
		malloc_mutex_unlock(tsdn, &bin->lock);
	} else {
		assert(index >= SC_NBINS);
		szind_t hindex = index - SC_NBINS;
		LOCKEDINT_MTX_LOCK(tsdn, arena->stats.mtx);
		locked_inc_u64(tsdn, LOCKEDINT_MTX(arena->stats.mtx),
			&arena->stats.lstats[hindex].ndalloc, 1);
		LOCKEDINT_MTX_UNLOCK(tsdn, arena->stats.mtx);
	}
}",arena_large_malloc_stats_update,"void
arena_large_malloc_stats_update(tsdn_t *tsdn, arena_t *arena, size_t usize) {
	cassert(config_stats);

	szind_t index = sz_size2index(usize);
	/* This only occurs when we have a sampled small allocation */
	if (usize < SC_LARGE_MINCLASS) {
		assert(index < SC_NBINS);
		assert(usize >= PAGE && usize % PAGE == 0);
		bin_t *bin = arena_get_bin(arena, index, /* binshard */ 0);
		malloc_mutex_lock(tsdn, &bin->lock);
		bin->stats.nmalloc++;
		malloc_mutex_unlock(tsdn, &bin->lock);
	} else {
		assert(index >= SC_NBINS);
		szind_t hindex = index - SC_NBINS;
		LOCKEDINT_MTX_LOCK(tsdn, arena->stats.mtx);
		locked_inc_u64(tsdn, LOCKEDINT_MTX(arena->stats.mtx),
			&arena->stats.lstats[hindex].nmalloc, 1);
		LOCKEDINT_MTX_UNLOCK(tsdn, arena->stats.mtx);
	}
}",arena_large_ralloc_stats_update,"void
arena_large_ralloc_stats_update(tsdn_t *tsdn, arena_t *arena, size_t oldusize,
    size_t usize) {
	arena_large_malloc_stats_update(tsdn, arena, usize);
	arena_large_dalloc_stats_update(tsdn, arena, oldusize);
}",tcache_bin_flush_large,"void
tcache_bin_flush_large(tsd_t *tsd, tcache_t *tcache, cache_bin_t *cache_bin,
    szind_t binind, unsigned rem) {
	tcache_bin_flush_bottom(tsd, tcache, cache_bin, binind, rem, false);
}",get_large_size,"size_t
get_large_size(size_t ind) {
	return get_size_impl(""arenas.lextent.0.size"", ind);
}",get_nlarge,"unsigned
get_nlarge(void) {
	return get_nsizes_impl(""arenas.nlextents"");
}"
arena_bin_slabs_full_insert,"void
arena_bin_slabs_full_insert(arena_t *arena, bin_t *bin, edata_t *slab) {
	assert(edata_nfree_get(slab) == 0);
	/*
	 *  Tracking extents is required by arena_reset, which is not allowed
	 *  for auto arenas.  Bypass this step to avoid touching the edata
	 *  linkage (often results in cache misses) for auto arenas.
	 */
	if (arena_is_auto(arena)) {
		return;
	}
	edata_list_active_append(&bin->slabs_full, slab);
}",arena_bin_slabs_full_remove,"void
arena_bin_slabs_full_remove(arena_t *arena, bin_t *bin, edata_t *slab) {
	if (arena_is_auto(arena)) {
		return;
	}
	edata_list_active_remove(&bin->slabs_full, slab);
}",arena_bin_slabs_nonfull_remove,"void
arena_bin_slabs_nonfull_remove(bin_t *bin, edata_t *slab) {
	edata_heap_remove(&bin->slabs_nonfull, slab);
	if (config_stats) {
		bin->stats.nonfull_slabs--;
	}
}",arena_slab_dalloc,"void
arena_slab_dalloc(tsdn_t *tsdn, arena_t *arena, edata_t *slab) {
	bool deferred_work_generated = false;
	pa_dalloc(tsdn, &arena->pa_shard, slab, &deferred_work_generated);
	if (deferred_work_generated) {
		arena_handle_deferred_work(tsdn, arena);
	}
}",arena_bin_slabs_nonfull_insert,"void
arena_bin_slabs_nonfull_insert(bin_t *bin, edata_t *slab) {
	assert(edata_nfree_get(slab) > 0);
	edata_heap_insert(&bin->slabs_nonfull, slab);
	if (config_stats) {
		bin->stats.nonfull_slabs++;
	}
}",arena_dalloc_bin,"void
arena_dalloc_bin(tsdn_t *tsdn, arena_t *arena, edata_t *edata, void *ptr) {
	szind_t binind = edata_szind_get(edata);
	unsigned binshard = edata_binshard_get(edata);
	bin_t *bin = arena_get_bin(arena, binind, binshard);

	malloc_mutex_lock(tsdn, &bin->lock);
	arena_dalloc_bin_locked_info_t info;
	arena_dalloc_bin_locked_begin(&info, binind);
	bool ret = arena_dalloc_bin_locked_step(tsdn, arena, bin,
	    &info, binind, edata, ptr);
	arena_dalloc_bin_locked_finish(tsdn, arena, bin, &info);
	malloc_mutex_unlock(tsdn, &bin->lock);

	if (ret) {
		arena_slab_dalloc(tsdn, arena, edata);
	}
}"
arena_prof_demote,"size_t
arena_prof_demote(tsdn_t *tsdn, edata_t *edata, const void *ptr) {
	cassert(config_prof);
	assert(ptr != NULL);
	size_t usize = isalloc(tsdn, ptr);
	size_t bumped_usize = sz_sa2u(usize, PROF_SAMPLE_ALIGNMENT);
	assert(bumped_usize <= SC_LARGE_MINCLASS &&
	    PAGE_CEILING(bumped_usize) == bumped_usize);
	assert(edata_size_get(edata) - bumped_usize <= sz_large_pad);
	szind_t szind = sz_size2index(bumped_usize);

	edata_szind_set(edata, szind);
	emap_remap(tsdn, &arena_emap_global, edata, szind, /* slab */ false);

	assert(isalloc(tsdn, ptr) == bumped_usize);

	return bumped_usize;
}",test_junk,"void
test_junk(void *ptr, size_t usize) {
	last_junked_ptr = ptr;
	last_junked_usize = usize;
}"
test_bitmap_size_body,"size_t
test_bitmap_size_body(const bitmap_info_t *binfo, size_t nbits,
    size_t prev_size) {
	size_t size = bitmap_size(binfo);
	expect_zu_ge(size, (nbits >> 3),
	    ""Bitmap size is smaller than expected"");
	expect_zu_ge(size, prev_size, ""Bitmap size is smaller than expected"");
	return size;
}",test_bitmap_init_body,"void
test_bitmap_init_body(const bitmap_info_t *binfo, size_t nbits) {
	size_t i;
	bitmap_t *bitmap = (bitmap_t *)malloc(bitmap_size(binfo));
	expect_ptr_not_null(bitmap, ""Unexpected malloc() failure"");

	bitmap_init(bitmap, binfo, false);
	for (i = 0; i < nbits; i++) {
		expect_false(bitmap_get(bitmap, binfo, i),
		    ""Bit should be unset"");
	}

	bitmap_init(bitmap, binfo, true);
	for (i = 0; i < nbits; i++) {
		expect_true(bitmap_get(bitmap, binfo, i), ""Bit should be set"");
	}

	free(bitmap);
}",test_bitmap_set_body,"void
test_bitmap_set_body(const bitmap_info_t *binfo, size_t nbits) {
	size_t i;
	bitmap_t *bitmap = (bitmap_t *)malloc(bitmap_size(binfo));
	expect_ptr_not_null(bitmap, ""Unexpected malloc() failure"");
	bitmap_init(bitmap, binfo, false);

	for (i = 0; i < nbits; i++) {
		bitmap_set(bitmap, binfo, i);
	}
	expect_true(bitmap_full(bitmap, binfo), ""All bits should be set"");
	free(bitmap);
}",test_bitmap_unset_body,"void
test_bitmap_unset_body(const bitmap_info_t *binfo, size_t nbits) {
	size_t i;
	bitmap_t *bitmap = (bitmap_t *)malloc(bitmap_size(binfo));
	expect_ptr_not_null(bitmap, ""Unexpected malloc() failure"");
	bitmap_init(bitmap, binfo, false);

	for (i = 0; i < nbits; i++) {
		bitmap_set(bitmap, binfo, i);
	}
	expect_true(bitmap_full(bitmap, binfo), ""All bits should be set"");
	for (i = 0; i < nbits; i++) {
		bitmap_unset(bitmap, binfo, i);
	}
	for (i = 0; i < nbits; i++) {
		bitmap_set(bitmap, binfo, i);
	}
	expect_true(bitmap_full(bitmap, binfo), ""All bits should be set"");
	free(bitmap);
}"
arena_dalloc_bin,"void
arena_dalloc_bin(tsdn_t *tsdn, arena_t *arena, edata_t *edata, void *ptr) {
	szind_t binind = edata_szind_get(edata);
	unsigned binshard = edata_binshard_get(edata);
	bin_t *bin = arena_get_bin(arena, binind, binshard);

	malloc_mutex_lock(tsdn, &bin->lock);
	arena_dalloc_bin_locked_info_t info;
	arena_dalloc_bin_locked_begin(&info, binind);
	bool ret = arena_dalloc_bin_locked_step(tsdn, arena, bin,
	    &info, binind, edata, ptr);
	arena_dalloc_bin_locked_finish(tsdn, arena, bin, &info);
	malloc_mutex_unlock(tsdn, &bin->lock);

	if (ret) {
		arena_slab_dalloc(tsdn, arena, edata);
	}
}",ckh_shrink,"void
ckh_shrink(tsd_t *tsd, ckh_t *ckh) {
	ckhc_t *tab, *ttab;
	size_t usize;
	unsigned lg_prevbuckets, lg_curcells;

	/*
	 * It is possible (though unlikely, given well behaved hashes) that the
	 * table rebuild will fail.
	 */
	lg_prevbuckets = ckh->lg_curbuckets;
	lg_curcells = ckh->lg_curbuckets + LG_CKH_BUCKET_CELLS - 1;
	usize = sz_sa2u(sizeof(ckhc_t) << lg_curcells, CACHELINE);
	if (unlikely(usize == 0 || usize > SC_LARGE_MAXCLASS)) {
		return;
	}
	tab = (ckhc_t *)ipallocztm(tsd_tsdn(tsd), usize, CACHELINE, true, NULL,
	    true, arena_ichoose(tsd, NULL));
	if (tab == NULL) {
		/*
		 * An OOM error isn't worth propagating, since it doesn't
		 * prevent this or future operations from proceeding.
		 */
		return;
	}
	/* Swap in new table. */
	ttab = ckh->tab;
	ckh->tab = tab;
	tab = ttab;
	ckh->lg_curbuckets = lg_curcells - LG_CKH_BUCKET_CELLS;

	if (!ckh_rebuild(ckh, tab)) {
		idalloctm(tsd_tsdn(tsd), tab, NULL, NULL, true, true);
#ifdef CKH_COUNT
		ckh->nshrinks++;
#endif
		return;
	}

	/* Rebuilding failed, so back out partially rebuilt table. */
	idalloctm(tsd_tsdn(tsd), ckh->tab, NULL, NULL, true, true);
	ckh->tab = tab;
	ckh->lg_curbuckets = lg_prevbuckets;
#ifdef CKH_COUNT
	ckh->nshrinkfails++;
#endif
}",ctl_arena_refresh,"void
ctl_arena_refresh(tsdn_t *tsdn, arena_t *arena, ctl_arena_t *ctl_sdarena,
    unsigned i, bool destroyed) {
	ctl_arena_t *ctl_arena = arenas_i(i);

	ctl_arena_clear(ctl_arena);
	ctl_arena_stats_amerge(tsdn, ctl_arena, arena);
	/* Merge into sum stats as well. */
	ctl_arena_stats_sdmerge(ctl_sdarena, ctl_arena, destroyed);
}",inallocx,"size_t
inallocx(tsdn_t *tsdn, size_t size, int flags) {
	check_entry_exit_locking(tsdn);
	size_t usize;
	/* In case of out of range, let the user see it rather than fail. */
	aligned_usize_get(size, MALLOCX_ALIGN_GET(flags), &usize, NULL, false);
	check_entry_exit_locking(tsdn);
	return usize;
}",tcache_arena_associate,"void
tcache_arena_associate(tsdn_t *tsdn, tcache_slow_t *tcache_slow,
    tcache_t *tcache, arena_t *arena) {
	assert(tcache_slow->arena == NULL);
	tcache_slow->arena = arena;

	if (config_stats) {
		/* Link into list of extant tcaches. */
		malloc_mutex_lock(tsdn, &arena->tcache_ql_mtx);

		ql_elm_new(tcache_slow, link);
		ql_tail_insert(&arena->tcache_ql, tcache_slow, link);
		cache_bin_array_descriptor_init(
		    &tcache_slow->cache_bin_array_descriptor, tcache->bins);
		ql_tail_insert(&arena->cache_bin_array_descriptor_ql,
		    &tcache_slow->cache_bin_array_descriptor, link);

		malloc_mutex_unlock(tsdn, &arena->tcache_ql_mtx);
	}
}"
os_overcommits_sysctl,"bool
os_overcommits_sysctl(void) {
	int vm_overcommit;
	size_t sz;

	sz = sizeof(vm_overcommit);
#if defined(__FreeBSD__) && defined(VM_OVERCOMMIT)
	int mib[2];

	mib[0] = CTL_VM;
	mib[1] = VM_OVERCOMMIT;
	if (sysctl(mib, 2, &vm_overcommit, &sz, NULL, 0) != 0) {
		return false; /* Error. */
	}
#else
	if (sysctlbyname(""vm.overcommit"", &vm_overcommit, &sz, NULL, 0) != 0) {
		return false; /* Error. */
	}
#endif

	return ((vm_overcommit & 0x3) == 0);
}",os_overcommits_proc,"bool
os_overcommits_proc(void) {
	int fd;
	char buf[1];

#if defined(JEMALLOC_USE_SYSCALL) && defined(SYS_open)
	#if defined(O_CLOEXEC)
		fd = (int)syscall(SYS_open, ""/proc/sys/vm/overcommit_memory"", O_RDONLY |
			O_CLOEXEC);
	#else
		fd = (int)syscall(SYS_open, ""/proc/sys/vm/overcommit_memory"", O_RDONLY);
		if (fd != -1) {
			fcntl(fd, F_SETFD, fcntl(fd, F_GETFD) | FD_CLOEXEC);
		}
	#endif
#elif defined(JEMALLOC_USE_SYSCALL) && defined(SYS_openat)
	#if defined(O_CLOEXEC)
		fd = (int)syscall(SYS_openat,
			AT_FDCWD, ""/proc/sys/vm/overcommit_memory"", O_RDONLY | O_CLOEXEC);
	#else
		fd = (int)syscall(SYS_openat,
			AT_FDCWD, ""/proc/sys/vm/overcommit_memory"", O_RDONLY);
		if (fd != -1) {
			fcntl(fd, F_SETFD, fcntl(fd, F_GETFD) | FD_CLOEXEC);
		}
	#endif
#else
	#if defined(O_CLOEXEC)
		fd = open(""/proc/sys/vm/overcommit_memory"", O_RDONLY | O_CLOEXEC);
	#else
		fd = open(""/proc/sys/vm/overcommit_memory"", O_RDONLY);
		if (fd != -1) {
			fcntl(fd, F_SETFD, fcntl(fd, F_GETFD) | FD_CLOEXEC);
		}
	#endif
#endif

	if (fd == -1) {
		return false; /* Error. */
	}

	ssize_t nread = malloc_read_fd(fd, &buf, sizeof(buf));
#if defined(JEMALLOC_USE_SYSCALL) && defined(SYS_close)
	syscall(SYS_close, fd);
#else
	close(fd);
#endif

	if (nread < 1) {
		return false; /* Error. */
	}
	/*
	 * /proc/sys/vm/overcommit_memory meanings:
	 * 0: Heuristic overcommit.
	 * 1: Always overcommit.
	 * 2: Never overcommit.
	 */
	return (buf[0] == '0' || buf[0] == '1');
}"
arena_nthreads_get,"unsigned
arena_nthreads_get(arena_t *arena, bool internal) {
	return atomic_load_u(&arena->nthreads[internal], ATOMIC_RELAXED);
}",arena_unbind,"void
arena_unbind(tsd_t *tsd, unsigned ind, bool internal) {
	arena_t *arena;

	arena = arena_get(tsd_tsdn(tsd), ind, false);
	arena_nthreads_dec(arena, internal);

	if (internal) {
		tsd_iarena_set(tsd, NULL);
	} else {
		tsd_arena_set(tsd, NULL);
	}
}"
arena_dalloc_bin,"void
arena_dalloc_bin(tsdn_t *tsdn, arena_t *arena, edata_t *edata, void *ptr) {
	szind_t binind = edata_szind_get(edata);
	unsigned binshard = edata_binshard_get(edata);
	bin_t *bin = arena_get_bin(arena, binind, binshard);

	malloc_mutex_lock(tsdn, &bin->lock);
	arena_dalloc_bin_locked_info_t info;
	arena_dalloc_bin_locked_begin(&info, binind);
	bool ret = arena_dalloc_bin_locked_step(tsdn, arena, bin,
	    &info, binind, edata, ptr);
	arena_dalloc_bin_locked_finish(tsdn, arena, bin, &info);
	malloc_mutex_unlock(tsdn, &bin->lock);

	if (ret) {
		arena_slab_dalloc(tsdn, arena, edata);
	}
}",ctl_arena_refresh,"void
ctl_arena_refresh(tsdn_t *tsdn, arena_t *arena, ctl_arena_t *ctl_sdarena,
    unsigned i, bool destroyed) {
	ctl_arena_t *ctl_arena = arenas_i(i);

	ctl_arena_clear(ctl_arena);
	ctl_arena_stats_amerge(tsdn, ctl_arena, arena);
	/* Merge into sum stats as well. */
	ctl_arena_stats_sdmerge(ctl_sdarena, ctl_arena, destroyed);
}",malloc_init_hard_a0,"bool
malloc_init_hard_a0(void) {
	bool ret;

	malloc_mutex_lock(TSDN_NULL, &init_lock);
	ret = malloc_init_hard_a0_locked();
	malloc_mutex_unlock(TSDN_NULL, &init_lock);
	return ret;
}",inallocx,"size_t
inallocx(tsdn_t *tsdn, size_t size, int flags) {
	check_entry_exit_locking(tsdn);
	size_t usize;
	/* In case of out of range, let the user see it rather than fail. */
	aligned_usize_get(size, MALLOCX_ALIGN_GET(flags), &usize, NULL, false);
	check_entry_exit_locking(tsdn);
	return usize;
}",prof_idump,"void
prof_idump(tsdn_t *tsdn) {
	tsd_t *tsd;
	prof_tdata_t *tdata;

	cassert(config_prof);

	if (!prof_booted || tsdn_null(tsdn) || !prof_active_get_unlocked()) {
		return;
	}
	tsd = tsdn_tsd(tsdn);
	if (tsd_reentrancy_level_get(tsd) > 0) {
		return;
	}

	tdata = prof_tdata_get(tsd, true);
	if (tdata == NULL) {
		return;
	}
	if (tdata->enq) {
		tdata->enq_idump = true;
		return;
	}

	prof_idump_impl(tsd);
}",prof_gdump,"void
prof_gdump(tsdn_t *tsdn) {
	tsd_t *tsd;
	prof_tdata_t *tdata;

	cassert(config_prof);

	if (!prof_booted || tsdn_null(tsdn) || !prof_active_get_unlocked()) {
		return;
	}
	tsd = tsdn_tsd(tsdn);
	if (tsd_reentrancy_level_get(tsd) > 0) {
		return;
	}

	tdata = prof_tdata_get(tsd, false);
	if (tdata == NULL) {
		return;
	}
	if (tdata->enq) {
		tdata->enq_gdump = true;
		return;
	}

	prof_gdump_impl(tsd);
}",tcache_arena_associate,"void
tcache_arena_associate(tsdn_t *tsdn, tcache_slow_t *tcache_slow,
    tcache_t *tcache, arena_t *arena) {
	assert(tcache_slow->arena == NULL);
	tcache_slow->arena = arena;

	if (config_stats) {
		/* Link into list of extant tcaches. */
		malloc_mutex_lock(tsdn, &arena->tcache_ql_mtx);

		ql_elm_new(tcache_slow, link);
		ql_tail_insert(&arena->tcache_ql, tcache_slow, link);
		cache_bin_array_descriptor_init(
		    &tcache_slow->cache_bin_array_descriptor, tcache->bins);
		ql_tail_insert(&arena->cache_bin_array_descriptor_ql,
		    &tcache_slow->cache_bin_array_descriptor, link);

		malloc_mutex_unlock(tsdn, &arena->tcache_ql_mtx);
	}
}"
node_print,"void
node_print(const node_t *node, unsigned depth) {
	unsigned i;
	node_t *leftmost_child, *sibling;

	for (i = 0; i < depth; i++) {
		malloc_printf(""\t"");
	}
	malloc_printf(""%2""FMTu64""\n"", node->key);

	leftmost_child = node_lchild_get(node);
	if (leftmost_child == NULL) {
		return;
	}
	node_print(leftmost_child, depth + 1);

	for (sibling = node_next_get(leftmost_child); sibling !=
	    NULL; sibling = node_next_get(sibling)) {
		node_print(sibling, depth + 1);
	}
}",node_validate,"unsigned
node_validate(const node_t *node, const node_t *parent) {
	unsigned nnodes = 1;
	node_t *leftmost_child, *sibling;

	if (parent != NULL) {
		expect_d_ge(node_cmp_magic(node, parent), 0,
		    ""Child is less than parent"");
	}

	leftmost_child = node_lchild_get(node);
	if (leftmost_child == NULL) {
		return nnodes;
	}
	expect_ptr_eq(node_prev_get(leftmost_child),
	    (void *)node, ""Leftmost child does not link to node"");
	nnodes += node_validate(leftmost_child, node);

	for (sibling = node_next_get(leftmost_child); sibling !=
	    NULL; sibling = node_next_get(sibling)) {
		expect_ptr_eq(node_next_get(node_prev_get(sibling)), sibling,
		    ""sibling's prev doesn't link to sibling"");
		nnodes += node_validate(sibling, node);
	}
	return nnodes;
}",heap_print,"void
heap_print(const heap_t *heap) {
	node_t *auxelm;

	malloc_printf(""vvv heap %p vvv\n"", heap);
	if (heap->ph.root == NULL) {
		goto label_return;
	}

	node_print(heap->ph.root, 0);

	for (auxelm = node_next_get(heap->ph.root); auxelm != NULL;
	    auxelm = node_next_get(auxelm)) {
		expect_ptr_eq(node_next_get(node_prev_get(auxelm)), auxelm,
		    ""auxelm's prev doesn't link to auxelm"");
		node_print(auxelm, 0);
	}

label_return:
	malloc_printf(""^^^ heap %p ^^^\n"", heap);
}",node_remove,"void
node_remove(heap_t *heap, node_t *node) {
	heap_remove(heap, node);

	node->magic = 0;
}"
inallocx,"size_t
inallocx(tsdn_t *tsdn, size_t size, int flags) {
	check_entry_exit_locking(tsdn);
	size_t usize;
	/* In case of out of range, let the user see it rather than fail. */
	aligned_usize_get(size, MALLOCX_ALIGN_GET(flags), &usize, NULL, false);
	check_entry_exit_locking(tsdn);
	return usize;
}",get_max_size_class,"size_t
get_max_size_class(void) {
	unsigned nlextents;
	size_t mib[4];
	size_t sz, miblen, max_size_class;

	sz = sizeof(unsigned);
	expect_d_eq(mallctl(""arenas.nlextents"", (void *)&nlextents, &sz, NULL,
	    0), 0, ""Unexpected mallctl() error"");

	miblen = sizeof(mib) / sizeof(size_t);
	expect_d_eq(mallctlnametomib(""arenas.lextent.0.size"", mib, &miblen), 0,
	    ""Unexpected mallctlnametomib() error"");
	mib[2] = nlextents - 1;

	sz = sizeof(size_t);
	expect_d_eq(mallctlbymib(mib, miblen, (void *)&max_size_class, &sz,
	    NULL, 0), 0, ""Unexpected mallctlbymib() error"");

	return max_size_class;
}"
narenas_total_inc,"void
narenas_total_inc(void) {
	atomic_fetch_add_u(&narenas_total, 1, ATOMIC_RELEASE);
}",arena_set,"void
arena_set(unsigned ind, arena_t *arena) {
	atomic_store_p(&arenas[ind], arena, ATOMIC_RELEASE);
}",narenas_total_set,"void
narenas_total_set(unsigned narenas) {
	atomic_store_u(&narenas_total, narenas, ATOMIC_RELEASE);
}",narenas_total_get,"unsigned
narenas_total_get(void) {
	return atomic_load_u(&narenas_total, ATOMIC_ACQUIRE);
}"
a0dalloc,"void
a0dalloc(void *ptr) {
	a0idalloc(ptr, true);
}",narenas_total_get,"unsigned
narenas_total_get(void) {
	return atomic_load_u(&narenas_total, ATOMIC_ACQUIRE);
}"
get_small_size,"size_t
get_small_size(size_t ind) {
	return get_size_impl(""arenas.bin.0.size"", ind);
}",get_large_size,"size_t
get_large_size(size_t ind) {
	return get_size_impl(""arenas.lextent.0.size"", ind);
}"
set_prof_active,"void
set_prof_active(bool active) {
	expect_d_eq(mallctl(""prof.active"", NULL, NULL, (void *)&active,
	    sizeof(active)), 0, ""Unexpected mallctl failure"");
}",get_lg_prof_sample,"size_t
get_lg_prof_sample(void) {
	size_t ret;
	size_t sz = sizeof(size_t);

	expect_d_eq(mallctl(""prof.lg_sample"", (void *)&ret, &sz, NULL, 0), 0,
	    ""Unexpected mallctl failure while reading profiling sample rate"");
	return ret;
}",do_prof_reset,"void
do_prof_reset(size_t lg_prof_sample_input) {
	expect_d_eq(mallctl(""prof.reset"", NULL, NULL,
	    (void *)&lg_prof_sample_input, sizeof(size_t)), 0,
	    ""Unexpected mallctl failure while resetting profile data"");
	expect_zu_eq(lg_prof_sample_input, get_lg_prof_sample(),
	    ""Expected profile sample rate change"");
}"
get_lg_prof_sample,"size_t
get_lg_prof_sample(void) {
	size_t ret;
	size_t sz = sizeof(size_t);

	expect_d_eq(mallctl(""prof.lg_sample"", (void *)&ret, &sz, NULL, 0), 0,
	    ""Unexpected mallctl failure while reading profiling sample rate"");
	return ret;
}",do_prof_reset,"void
do_prof_reset(size_t lg_prof_sample_input) {
	expect_d_eq(mallctl(""prof.reset"", NULL, NULL,
	    (void *)&lg_prof_sample_input, sizeof(size_t)), 0,
	    ""Unexpected mallctl failure while resetting profile data"");
	expect_zu_eq(lg_prof_sample_input, get_lg_prof_sample(),
	    ""Expected profile sample rate change"");
}"
tcache_arena_associate,"void
tcache_arena_associate(tsdn_t *tsdn, tcache_slow_t *tcache_slow,
    tcache_t *tcache, arena_t *arena) {
	assert(tcache_slow->arena == NULL);
	tcache_slow->arena = arena;

	if (config_stats) {
		/* Link into list of extant tcaches. */
		malloc_mutex_lock(tsdn, &arena->tcache_ql_mtx);

		ql_elm_new(tcache_slow, link);
		ql_tail_insert(&arena->tcache_ql, tcache_slow, link);
		cache_bin_array_descriptor_init(
		    &tcache_slow->cache_bin_array_descriptor, tcache->bins);
		ql_tail_insert(&arena->cache_bin_array_descriptor_ql,
		    &tcache_slow->cache_bin_array_descriptor, link);

		malloc_mutex_unlock(tsdn, &arena->tcache_ql_mtx);
	}
}",tcache_bin_flush_small,"void
tcache_bin_flush_small(tsd_t *tsd, tcache_t *tcache, cache_bin_t *cache_bin,
    szind_t binind, unsigned rem) {
	tcache_bin_flush_bottom(tsd, tcache, cache_bin, binind, rem, true);
}",tcache_bin_flush_large,"void
tcache_bin_flush_large(tsd_t *tsd, tcache_t *tcache, cache_bin_t *cache_bin,
    szind_t binind, unsigned rem) {
	tcache_bin_flush_bottom(tsd, tcache, cache_bin, binind, rem, false);
}",tcache_destroy,"void
tcache_destroy(tsd_t *tsd, tcache_t *tcache, bool tsd_tcache) {
	tcache_slow_t *tcache_slow = tcache->tcache_slow;
	tcache_flush_cache(tsd, tcache);
	arena_t *arena = tcache_slow->arena;
	tcache_arena_dissociate(tsd_tsdn(tsd), tcache_slow, tcache);

	if (tsd_tcache) {
		cache_bin_t *cache_bin = &tcache->bins[0];
		cache_bin_assert_empty(cache_bin);
	}
	if (tsd_tcache && cache_bin_stack_use_thp()) {
		b0_dalloc_tcache_stack(tsd_tsdn(tsd), tcache_slow->dyn_alloc);
	} else {
		idalloctm(tsd_tsdn(tsd), tcache_slow->dyn_alloc, NULL, NULL,
		    true, true);
	}

	/*
	 * The deallocation and tcache flush above may not trigger decay since
	 * we are on the tcache shutdown path (potentially with non-nominal
	 * tsd).  Manually trigger decay to avoid pathological cases.  Also
	 * include arena 0 because the tcache array is allocated from it.
	 */
	arena_decay(tsd_tsdn(tsd), arena_get(tsd_tsdn(tsd), 0, false),
	    false, false);

	if (arena_nthreads_get(arena, false) == 0 &&
	    !background_thread_enabled()) {
		/* Force purging when no threads assigned to the arena anymore. */
		arena_decay(tsd_tsdn(tsd), arena,
		    /* is_background_thread */ false, /* all */ true);
	} else {
		arena_decay(tsd_tsdn(tsd), arena,
		    /* is_background_thread */ false, /* all */ false);
	}
}"
malloc_init_hard_a0,"bool
malloc_init_hard_a0(void) {
	bool ret;

	malloc_mutex_lock(TSDN_NULL, &init_lock);
	ret = malloc_init_hard_a0_locked();
	malloc_mutex_unlock(TSDN_NULL, &init_lock);
	return ret;
}",a0idalloc,"void
a0idalloc(void *ptr, bool is_internal) {
	idalloctm(TSDN_NULL, ptr, NULL, NULL, is_internal, true);
}",a0dalloc,"void
a0dalloc(void *ptr) {
	a0idalloc(ptr, true);
}"
prof_idump,"void
prof_idump(tsdn_t *tsdn) {
	tsd_t *tsd;
	prof_tdata_t *tdata;

	cassert(config_prof);

	if (!prof_booted || tsdn_null(tsdn) || !prof_active_get_unlocked()) {
		return;
	}
	tsd = tsdn_tsd(tsdn);
	if (tsd_reentrancy_level_get(tsd) > 0) {
		return;
	}

	tdata = prof_tdata_get(tsd, true);
	if (tdata == NULL) {
		return;
	}
	if (tdata->enq) {
		tdata->enq_idump = true;
		return;
	}

	prof_idump_impl(tsd);
}",prof_gdump,"void
prof_gdump(tsdn_t *tsdn) {
	tsd_t *tsd;
	prof_tdata_t *tdata;

	cassert(config_prof);

	if (!prof_booted || tsdn_null(tsdn) || !prof_active_get_unlocked()) {
		return;
	}
	tsd = tsdn_tsd(tsdn);
	if (tsd_reentrancy_level_get(tsd) > 0) {
		return;
	}

	tdata = prof_tdata_get(tsd, false);
	if (tdata == NULL) {
		return;
	}
	if (tdata->enq) {
		tdata->enq_gdump = true;
		return;
	}

	prof_gdump_impl(tsd);
}"
narenas_total_get,"unsigned
narenas_total_get(void) {
	return atomic_load_u(&narenas_total, ATOMIC_ACQUIRE);
}",arena_unbind,"void
arena_unbind(tsd_t *tsd, unsigned ind, bool internal) {
	arena_t *arena;

	arena = arena_get(tsd_tsdn(tsd), ind, false);
	arena_nthreads_dec(arena, internal);

	if (internal) {
		tsd_iarena_set(tsd, NULL);
	} else {
		tsd_arena_set(tsd, NULL);
	}
}",tcache_arena_associate,"void
tcache_arena_associate(tsdn_t *tsdn, tcache_slow_t *tcache_slow,
    tcache_t *tcache, arena_t *arena) {
	assert(tcache_slow->arena == NULL);
	tcache_slow->arena = arena;

	if (config_stats) {
		/* Link into list of extant tcaches. */
		malloc_mutex_lock(tsdn, &arena->tcache_ql_mtx);

		ql_elm_new(tcache_slow, link);
		ql_tail_insert(&arena->tcache_ql, tcache_slow, link);
		cache_bin_array_descriptor_init(
		    &tcache_slow->cache_bin_array_descriptor, tcache->bins);
		ql_tail_insert(&arena->cache_bin_array_descriptor_ql,
		    &tcache_slow->cache_bin_array_descriptor, link);

		malloc_mutex_unlock(tsdn, &arena->tcache_ql_mtx);
	}
}"
ckh_shrink,"void
ckh_shrink(tsd_t *tsd, ckh_t *ckh) {
	ckhc_t *tab, *ttab;
	size_t usize;
	unsigned lg_prevbuckets, lg_curcells;

	/*
	 * It is possible (though unlikely, given well behaved hashes) that the
	 * table rebuild will fail.
	 */
	lg_prevbuckets = ckh->lg_curbuckets;
	lg_curcells = ckh->lg_curbuckets + LG_CKH_BUCKET_CELLS - 1;
	usize = sz_sa2u(sizeof(ckhc_t) << lg_curcells, CACHELINE);
	if (unlikely(usize == 0 || usize > SC_LARGE_MAXCLASS)) {
		return;
	}
	tab = (ckhc_t *)ipallocztm(tsd_tsdn(tsd), usize, CACHELINE, true, NULL,
	    true, arena_ichoose(tsd, NULL));
	if (tab == NULL) {
		/*
		 * An OOM error isn't worth propagating, since it doesn't
		 * prevent this or future operations from proceeding.
		 */
		return;
	}
	/* Swap in new table. */
	ttab = ckh->tab;
	ckh->tab = tab;
	tab = ttab;
	ckh->lg_curbuckets = lg_curcells - LG_CKH_BUCKET_CELLS;

	if (!ckh_rebuild(ckh, tab)) {
		idalloctm(tsd_tsdn(tsd), tab, NULL, NULL, true, true);
#ifdef CKH_COUNT
		ckh->nshrinks++;
#endif
		return;
	}

	/* Rebuilding failed, so back out partially rebuilt table. */
	idalloctm(tsd_tsdn(tsd), ckh->tab, NULL, NULL, true, true);
	ckh->tab = tab;
	ckh->lg_curbuckets = lg_prevbuckets;
#ifdef CKH_COUNT
	ckh->nshrinkfails++;
#endif
}",tcache_enabled_set,"void
tcache_enabled_set(tsd_t *tsd, bool enabled) {
	bool was_enabled = tsd_tcache_enabled_get(tsd);

	if (!was_enabled && enabled) {
		tsd_tcache_data_init(tsd, NULL,
		    tcache_get_default_ncached_max());
	} else if (was_enabled && !enabled) {
		tcache_cleanup(tsd);
	}
	/* Commit the state last.  Above calls check current state. */
	tsd_tcache_enabled_set(tsd, enabled);
	tsd_slow_update(tsd);
}",tcache_destroy,"void
tcache_destroy(tsd_t *tsd, tcache_t *tcache, bool tsd_tcache) {
	tcache_slow_t *tcache_slow = tcache->tcache_slow;
	tcache_flush_cache(tsd, tcache);
	arena_t *arena = tcache_slow->arena;
	tcache_arena_dissociate(tsd_tsdn(tsd), tcache_slow, tcache);

	if (tsd_tcache) {
		cache_bin_t *cache_bin = &tcache->bins[0];
		cache_bin_assert_empty(cache_bin);
	}
	if (tsd_tcache && cache_bin_stack_use_thp()) {
		b0_dalloc_tcache_stack(tsd_tsdn(tsd), tcache_slow->dyn_alloc);
	} else {
		idalloctm(tsd_tsdn(tsd), tcache_slow->dyn_alloc, NULL, NULL,
		    true, true);
	}

	/*
	 * The deallocation and tcache flush above may not trigger decay since
	 * we are on the tcache shutdown path (potentially with non-nominal
	 * tsd).  Manually trigger decay to avoid pathological cases.  Also
	 * include arena 0 because the tcache array is allocated from it.
	 */
	arena_decay(tsd_tsdn(tsd), arena_get(tsd_tsdn(tsd), 0, false),
	    false, false);

	if (arena_nthreads_get(arena, false) == 0 &&
	    !background_thread_enabled()) {
		/* Force purging when no threads assigned to the arena anymore. */
		arena_decay(tsd_tsdn(tsd), arena,
		    /* is_background_thread */ false, /* all */ true);
	} else {
		arena_decay(tsd_tsdn(tsd), arena,
		    /* is_background_thread */ false, /* all */ false);
	}
}"
gen_rand32,"uint32_t gen_rand32(sfmt_t *ctx) {
    uint32_t r;
    uint32_t *psfmt32 = &ctx->sfmt[0].u[0];

    assert(ctx->initialized);
    if (ctx->idx >= N32) {
	gen_rand_all(ctx);
	ctx->idx = 0;
    }
    r = psfmt32[ctx->idx++];
    return r;
}",gen_rand64,"uint64_t gen_rand64(sfmt_t *ctx) {
#if defined(BIG_ENDIAN64) && !defined(ONLY64)
    uint32_t r1, r2;
    uint32_t *psfmt32 = &ctx->sfmt[0].u[0];
#else
    uint64_t r;
    uint64_t *psfmt64 = (uint64_t *)&ctx->sfmt[0].u[0];
#endif

    assert(ctx->initialized);
    assert(ctx->idx % 2 == 0);

    if (ctx->idx >= N32) {
	gen_rand_all(ctx);
	ctx->idx = 0;
    }
#if defined(BIG_ENDIAN64) && !defined(ONLY64)
    r1 = psfmt32[ctx->idx];
    r2 = psfmt32[ctx->idx + 1];
    ctx->idx += 2;
    return ((uint64_t)r2 << 32) | r1;
#else
    r = psfmt64[ctx->idx / 2];
    ctx->idx += 2;
    return r;
#endif
}"
idxof,"int idxof(int i) {
    return i ^ 1;
}",rshift128,"void rshift128(w128_t *out, w128_t const *in, int shift) {
    uint64_t th, tl, oh, ol;

    th = ((uint64_t)in->u[2] << 32) | ((uint64_t)in->u[3]);
    tl = ((uint64_t)in->u[0] << 32) | ((uint64_t)in->u[1]);

    oh = th >> (shift * 8);
    ol = tl >> (shift * 8);
    ol |= th << (64 - shift * 8);
    out->u[0] = (uint32_t)(ol >> 32);
    out->u[1] = (uint32_t)ol;
    out->u[2] = (uint32_t)(oh >> 32);
    out->u[3] = (uint32_t)oh;
}",lshift128,"void lshift128(w128_t *out, w128_t const *in, int shift) {
    uint64_t th, tl, oh, ol;

    th = ((uint64_t)in->u[2] << 32) | ((uint64_t)in->u[3]);
    tl = ((uint64_t)in->u[0] << 32) | ((uint64_t)in->u[1]);

    oh = th << (shift * 8);
    ol = tl << (shift * 8);
    oh |= tl >> (64 - shift * 8);
    out->u[0] = (uint32_t)(ol >> 32);
    out->u[1] = (uint32_t)ol;
    out->u[2] = (uint32_t)(oh >> 32);
    out->u[3] = (uint32_t)oh;
}",gen_rand_all,"void gen_rand_all(sfmt_t *ctx) {
    int i;
    w128_t *r1, *r2;

    r1 = &ctx->sfmt[N - 2];
    r2 = &ctx->sfmt[N - 1];
    for (i = 0; i < N - POS1; i++) {
	do_recursion(&ctx->sfmt[i], &ctx->sfmt[i], &ctx->sfmt[i + POS1], r1,
	  r2);
	r1 = r2;
	r2 = &ctx->sfmt[i];
    }
    for (; i < N; i++) {
	do_recursion(&ctx->sfmt[i], &ctx->sfmt[i], &ctx->sfmt[i + POS1 - N], r1,
	  r2);
	r1 = r2;
	r2 = &ctx->sfmt[i];
    }
}",gen_rand_array,"void gen_rand_array(sfmt_t *ctx, w128_t *array, int size) {
    int i, j;
    w128_t *r1, *r2;

    r1 = &ctx->sfmt[N - 2];
    r2 = &ctx->sfmt[N - 1];
    for (i = 0; i < N - POS1; i++) {
	do_recursion(&array[i], &ctx->sfmt[i], &ctx->sfmt[i + POS1], r1, r2);
	r1 = r2;
	r2 = &array[i];
    }
    for (; i < N; i++) {
	do_recursion(&array[i], &ctx->sfmt[i], &array[i + POS1 - N], r1, r2);
	r1 = r2;
	r2 = &array[i];
    }
    for (; i < size - N; i++) {
	do_recursion(&array[i], &array[i - N], &array[i + POS1 - N], r1, r2);
	r1 = r2;
	r2 = &array[i];
    }
    for (j = 0; j < 2 * N - size; j++) {
	ctx->sfmt[j] = array[j + size - N];
    }
    for (; i < size; i++, j++) {
	do_recursion(&array[i], &array[i - N], &array[i + POS1 - N], r1, r2);
	r1 = r2;
	r2 = &array[i];
	ctx->sfmt[j] = array[i];
    }
}",func1,"uint32_t func1(uint32_t x) {
    return (x ^ (x >> 27)) * (uint32_t)1664525UL;
}",func2,"uint32_t func2(uint32_t x) {
    return (x ^ (x >> 27)) * (uint32_t)1566083941UL;
}",swap,"void swap(w128_t *array, int size) {
    int i;
    uint32_t x, y;

    for (i = 0; i < size; i++) {
	x = array[i].u[0];
	y = array[i].u[2];
	array[i].u[0] = array[i].u[1];
	array[i].u[2] = array[i].u[3];
	array[i].u[1] = x;
	array[i].u[3] = y;
    }
}"
ctl_arena_clear,"void
ctl_arena_clear(ctl_arena_t *ctl_arena) {
	ctl_arena->nthreads = 0;
	ctl_arena->dss = dss_prec_names[dss_prec_limit];
	ctl_arena->dirty_decay_ms = -1;
	ctl_arena->muzzy_decay_ms = -1;
	ctl_arena->pactive = 0;
	ctl_arena->pdirty = 0;
	ctl_arena->pmuzzy = 0;
	if (config_stats) {
		memset(ctl_arena->astats, 0, sizeof(*(ctl_arena->astats)));
	}
}",narenas_total_get,"unsigned
narenas_total_get(void) {
	return atomic_load_u(&narenas_total, ATOMIC_ACQUIRE);
}"
tcache_bin_flush_small,"void
tcache_bin_flush_small(tsd_t *tsd, tcache_t *tcache, cache_bin_t *cache_bin,
    szind_t binind, unsigned rem) {
	tcache_bin_flush_bottom(tsd, tcache, cache_bin, binind, rem, true);
}",tcache_bin_flush_large,"void
tcache_bin_flush_large(tsd_t *tsd, tcache_t *tcache, cache_bin_t *cache_bin,
    szind_t binind, unsigned rem) {
	tcache_bin_flush_bottom(tsd, tcache, cache_bin, binind, rem, false);
}"
