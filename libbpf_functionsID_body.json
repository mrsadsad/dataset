{
    "resources/libbpf/fuzz/bpf-object-fuzzer.c@libbpf_print_fn": "static int libbpf_print_fn ( enum libbpf_print_level level , const char * format , va_list args ) { return 0 ; }",
    "resources/libbpf/fuzz/bpf-object-fuzzer.c@LLVMFuzzerTestOneInput": "int LLVMFuzzerTestOneInput ( const uint8_t * data , size_t size ) { struct bpf_object * obj = NULL ; DECLARE_LIBBPF_OPTS ( bpf_object_open_opts , opts ) ; int err ; libbpf_set_print ( libbpf_print_fn ) ; opts . object_name = \"fuzz-object\" ; obj = bpf_object__open_mem ( data , size , & opts ) ; err = libbpf_get_error ( obj ) ; if ( err ) return 0 ; bpf_object__close ( obj ) ; return 0 ; }",
    "resources/libbpf/src/strset.c@hash_bits": "static inline size_t hash_bits ( size_t h , int bits ) { /* shuffle bits and return requested number of upper bits */ if ( bits == 0 ) return 0 ; # if ( __SIZEOF_SIZE_T__ == __SIZEOF_LONG_LONG__ ) /* LP64 case */ return ( h * 11400714819323198485llu ) >> ( __SIZEOF_LONG_LONG__ * 8 - bits ) ; # elif ( __SIZEOF_SIZE_T__ <= __SIZEOF_LONG__ ) return ( h * 2654435769lu ) >> ( __SIZEOF_LONG__ * 8 - bits ) ; # else # error \"Unsupported size_t size\" # endif }",
    "resources/libbpf/src/strset.c@str_hash": "static inline size_t str_hash ( const char * s ) { size_t h = 0 ; while ( * s ) { h = h * 31 + * s ; s ++ ; } return h ; }",
    "resources/libbpf/src/strset.c@btf_kind": "static inline __u16 btf_kind ( const struct btf_type * t ) { return BTF_INFO_KIND ( t -> info ) ; }",
    "resources/libbpf/src/strset.c@btf_vlen": "static inline __u16 btf_vlen ( const struct btf_type * t ) { return BTF_INFO_VLEN ( t -> info ) ; }",
    "resources/libbpf/src/strset.c@btf_kflag": "static inline bool btf_kflag ( const struct btf_type * t ) { return BTF_INFO_KFLAG ( t -> info ) ; }",
    "resources/libbpf/src/strset.c@btf_is_void": "static inline bool btf_is_void ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_UNKN ; }",
    "resources/libbpf/src/strset.c@btf_is_int": "static inline bool btf_is_int ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_INT ; }",
    "resources/libbpf/src/strset.c@btf_is_ptr": "static inline bool btf_is_ptr ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_PTR ; }",
    "resources/libbpf/src/strset.c@btf_is_array": "static inline bool btf_is_array ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_ARRAY ; }",
    "resources/libbpf/src/strset.c@btf_is_struct": "static inline bool btf_is_struct ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_STRUCT ; }",
    "resources/libbpf/src/strset.c@btf_is_union": "static inline bool btf_is_union ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_UNION ; }",
    "resources/libbpf/src/strset.c@btf_is_composite": "static inline bool btf_is_composite ( const struct btf_type * t ) { __u16 kind = btf_kind ( t ) ; return kind == BTF_KIND_STRUCT || kind == BTF_KIND_UNION ; }",
    "resources/libbpf/src/strset.c@btf_is_enum": "static inline bool btf_is_enum ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_ENUM ; }",
    "resources/libbpf/src/strset.c@btf_is_enum64": "static inline bool btf_is_enum64 ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_ENUM64 ; }",
    "resources/libbpf/src/strset.c@btf_is_fwd": "static inline bool btf_is_fwd ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_FWD ; }",
    "resources/libbpf/src/strset.c@btf_is_typedef": "static inline bool btf_is_typedef ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_TYPEDEF ; }",
    "resources/libbpf/src/strset.c@btf_is_volatile": "static inline bool btf_is_volatile ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_VOLATILE ; }",
    "resources/libbpf/src/strset.c@btf_is_const": "static inline bool btf_is_const ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_CONST ; }",
    "resources/libbpf/src/strset.c@btf_is_restrict": "static inline bool btf_is_restrict ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_RESTRICT ; }",
    "resources/libbpf/src/strset.c@btf_is_mod": "static inline bool btf_is_mod ( const struct btf_type * t ) { __u16 kind = btf_kind ( t ) ; return kind == BTF_KIND_VOLATILE || kind == BTF_KIND_CONST || kind == BTF_KIND_RESTRICT || kind == BTF_KIND_TYPE_TAG ; }",
    "resources/libbpf/src/strset.c@btf_is_func": "static inline bool btf_is_func ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_FUNC ; }",
    "resources/libbpf/src/strset.c@btf_is_func_proto": "static inline bool btf_is_func_proto ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_FUNC_PROTO ; }",
    "resources/libbpf/src/strset.c@btf_is_var": "static inline bool btf_is_var ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_VAR ; }",
    "resources/libbpf/src/strset.c@btf_is_datasec": "static inline bool btf_is_datasec ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_DATASEC ; }",
    "resources/libbpf/src/strset.c@btf_is_float": "static inline bool btf_is_float ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_FLOAT ; }",
    "resources/libbpf/src/strset.c@btf_is_decl_tag": "static inline bool btf_is_decl_tag ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_DECL_TAG ; }",
    "resources/libbpf/src/strset.c@btf_is_type_tag": "static inline bool btf_is_type_tag ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_TYPE_TAG ; }",
    "resources/libbpf/src/strset.c@btf_is_any_enum": "static inline bool btf_is_any_enum ( const struct btf_type * t ) { return btf_is_enum ( t ) || btf_is_enum64 ( t ) ; }",
    "resources/libbpf/src/strset.c@btf_kind_core_compat": "static inline bool btf_kind_core_compat ( const struct btf_type * t1 , const struct btf_type * t2 ) { return btf_kind ( t1 ) == btf_kind ( t2 ) || ( btf_is_any_enum ( t1 ) && btf_is_any_enum ( t2 ) ) ; }",
    "resources/libbpf/src/strset.c@btf_int_encoding": "static inline __u8 btf_int_encoding ( const struct btf_type * t ) { return BTF_INT_ENCODING ( * ( __u32 * ) ( t + 1 ) ) ; }",
    "resources/libbpf/src/strset.c@btf_int_offset": "static inline __u8 btf_int_offset ( const struct btf_type * t ) { return BTF_INT_OFFSET ( * ( __u32 * ) ( t + 1 ) ) ; }",
    "resources/libbpf/src/strset.c@btf_int_bits": "static inline __u8 btf_int_bits ( const struct btf_type * t ) { return BTF_INT_BITS ( * ( __u32 * ) ( t + 1 ) ) ; }",
    "resources/libbpf/src/strset.c@btf_array": "static inline struct btf_array * btf_array ( const struct btf_type * t ) { return ( struct btf_array * ) ( t + 1 ) ; }",
    "resources/libbpf/src/strset.c@btf_enum": "static inline struct btf_enum * btf_enum ( const struct btf_type * t ) { return ( struct btf_enum * ) ( t + 1 ) ; }",
    "resources/libbpf/src/strset.c@btf_enum64": "static inline struct btf_enum64 * btf_enum64 ( const struct btf_type * t ) { return ( struct btf_enum64 * ) ( t + 1 ) ; }",
    "resources/libbpf/src/strset.c@btf_enum64_value": "static inline __u64 btf_enum64_value ( const struct btf_enum64 * e ) { /* struct btf_enum64 is introduced in Linux 6.0, which is very\n\t * bleeding-edge. Here we are avoiding relying on struct btf_enum64\n\t * definition coming from kernel UAPI headers to support wider range\n\t * of system-wide kernel headers.\n\t *\n\t * Given this header can be also included from C++ applications, that\n\t * further restricts C tricks we can use (like using compatible\n\t * anonymous struct). So just treat struct btf_enum64 as\n\t * a three-element array of u32 and access second (lo32) and third\n\t * (hi32) elements directly.\n\t *\n\t * For reference, here is a struct btf_enum64 definition:\n\t *\n\t * const struct btf_enum64 {\n\t *\t__u32\tname_off;\n\t *\t__u32\tval_lo32;\n\t *\t__u32\tval_hi32;\n\t * };\n\t */ const __u32 * e64 = ( const __u32 * ) e ; return ( ( __u64 ) e64 [ 2 ] << 32 ) | e64 [ 1 ] ; }",
    "resources/libbpf/src/strset.c@btf_members": "static inline struct btf_member * btf_members ( const struct btf_type * t ) { return ( struct btf_member * ) ( t + 1 ) ; }",
    "resources/libbpf/src/strset.c@btf_member_bit_offset": "static inline __u32 btf_member_bit_offset ( const struct btf_type * t , __u32 member_idx ) { const struct btf_member * m = btf_members ( t ) + member_idx ; bool kflag = btf_kflag ( t ) ; return kflag ? BTF_MEMBER_BIT_OFFSET ( m -> offset ) : m -> offset ; }",
    "resources/libbpf/src/strset.c@btf_member_bitfield_size": "static inline __u32 btf_member_bitfield_size ( const struct btf_type * t , __u32 member_idx ) { const struct btf_member * m = btf_members ( t ) + member_idx ; bool kflag = btf_kflag ( t ) ; return kflag ? BTF_MEMBER_BITFIELD_SIZE ( m -> offset ) : 0 ; }",
    "resources/libbpf/src/strset.c@btf_params": "static inline struct btf_param * btf_params ( const struct btf_type * t ) { return ( struct btf_param * ) ( t + 1 ) ; }",
    "resources/libbpf/src/strset.c@btf_var": "static inline struct btf_var * btf_var ( const struct btf_type * t ) { return ( struct btf_var * ) ( t + 1 ) ; }",
    "resources/libbpf/src/strset.c@btf_var_secinfos": "static inline struct btf_var_secinfo * btf_var_secinfos ( const struct btf_type * t ) { return ( struct btf_var_secinfo * ) ( t + 1 ) ; }",
    "resources/libbpf/src/strset.c@btf_decl_tag": "static inline struct btf_decl_tag * btf_decl_tag ( const struct btf_type * t ) { return ( struct btf_decl_tag * ) ( t + 1 ) ; }",
    "resources/libbpf/src/strset.c@str_has_sfx": "static inline bool str_has_sfx ( const char * str , const char * sfx ) { size_t str_len = strlen ( str ) ; size_t sfx_len = strlen ( sfx ) ; if ( sfx_len > str_len ) return false ; return strcmp ( str + str_len - sfx_len , sfx ) == 0 ; }",
    "resources/libbpf/src/strset.c@libbpf_reallocarray": "static inline void * libbpf_reallocarray ( void * ptr , size_t nmemb , size_t size ) { size_t total ; # if __has_builtin ( __builtin_mul_overflow ) if ( unlikely ( __builtin_mul_overflow ( nmemb , size , & total ) ) ) return NULL ; # else if ( size == 0 || nmemb > ULONG_MAX / size ) return NULL ; total = nmemb * size ; # endif return realloc ( ptr , total ) ; }",
    "resources/libbpf/src/strset.c@libbpf_strlcpy": "static inline void libbpf_strlcpy ( char * dst , const char * src , size_t sz ) { size_t i ; if ( sz == 0 ) return ; sz -- ; for ( i = 0 ; i < sz && src [ i ] ; i ++ ) dst [ i ] = src [ i ] ; dst [ i ] = '\\0' ; }",
    "resources/libbpf/src/strset.c@btf_func_linkage": "static inline enum btf_func_linkage btf_func_linkage ( const struct btf_type * t ) { return ( enum btf_func_linkage ) ( int ) btf_vlen ( t ) ; }",
    "resources/libbpf/src/strset.c@btf_type_info": "static inline __u32 btf_type_info ( int kind , int vlen , int kflag ) { return ( kflag << 31 ) | ( kind << 24 ) | vlen ; }",
    "resources/libbpf/src/strset.c@libbpf_is_mem_zeroed": "static inline bool libbpf_is_mem_zeroed ( const char * p , ssize_t len ) { while ( len > 0 ) { if ( * p ) return false ; p ++ ; len -- ; } return true ; }",
    "resources/libbpf/src/strset.c@libbpf_validate_opts": "static inline bool libbpf_validate_opts ( const char * opts , size_t opts_sz , size_t user_sz , const char * type_name ) { if ( user_sz < sizeof ( size_t ) ) { pr_warn ( \"%s size (%zu) is too small\\n\" , type_name , user_sz ) ; return false ; } if ( ! libbpf_is_mem_zeroed ( opts + opts_sz , ( ssize_t ) user_sz - opts_sz ) ) { pr_warn ( \"%s has non-zero extra bytes\\n\" , type_name ) ; return false ; } return true ; }",
    "resources/libbpf/src/strset.c@libbpf_err": "static inline int libbpf_err ( int ret ) { if ( ret < 0 ) errno = - ret ; return ret ; }",
    "resources/libbpf/src/strset.c@libbpf_err_errno": "static inline int libbpf_err_errno ( int ret ) { /* errno is already assumed to be set on error */ return ret < 0 ? - errno : ret ; }",
    "resources/libbpf/src/strset.c@libbpf_err_ptr": "static inline void * libbpf_err_ptr ( int err ) { /* set errno on error, this doesn't break anything */ errno = - err ; return NULL ; }",
    "resources/libbpf/src/strset.c@libbpf_ptr": "static inline void * libbpf_ptr ( void * ret ) { /* set errno on error, this doesn't break anything */ if ( IS_ERR ( ret ) ) errno = - PTR_ERR ( ret ) ; return IS_ERR ( ret ) ? NULL : ret ; }",
    "resources/libbpf/src/strset.c@str_is_empty": "static inline bool str_is_empty ( const char * s ) { return ! s || ! s [ 0 ] ; }",
    "resources/libbpf/src/strset.c@is_ldimm64_insn": "static inline bool is_ldimm64_insn ( struct bpf_insn * insn ) { return insn -> code == ( BPF_LD | BPF_IMM | BPF_DW ) ; }",
    "resources/libbpf/src/strset.c@dup_good_fd": "static inline int dup_good_fd ( int fd ) { if ( fd < 0 ) return fd ; return fcntl ( fd , F_DUPFD_CLOEXEC , 3 ) ; }",
    "resources/libbpf/src/strset.c@ensure_good_fd": "static inline int ensure_good_fd ( int fd ) { int old_fd = fd , saved_errno ; if ( fd < 0 ) return fd ; if ( fd < 3 ) { fd = dup_good_fd ( fd ) ; saved_errno = errno ; close ( old_fd ) ; errno = saved_errno ; if ( fd < 0 ) { pr_warn ( \"failed to dup FD %d to FD > 2: %d\\n\" , old_fd , - saved_errno ) ; errno = saved_errno ; } } return fd ; }",
    "resources/libbpf/src/strset.c@sys_dup2": "static inline int sys_dup2 ( int oldfd , int newfd ) { # ifdef __NR_dup2 return syscall ( __NR_dup2 , oldfd , newfd ) ; # else return syscall ( __NR_dup3 , oldfd , newfd , 0 ) ; # endif }",
    "resources/libbpf/src/strset.c@reuse_fd": "static inline int reuse_fd ( int fixed_fd , int tmp_fd ) { int err ; err = sys_dup2 ( tmp_fd , fixed_fd ) ; err = err < 0 ? - errno : 0 ; close ( tmp_fd ) ; /* clean up temporary FD */ return err ; }",
    "resources/libbpf/src/strset.c@is_pow_of_2": "static inline bool is_pow_of_2 ( size_t x ) { return x && ( x & ( x - 1 ) ) == 0 ; }",
    "resources/libbpf/src/strset.c@strset_hash_fn": "static size_t strset_hash_fn ( long key , void * ctx ) { const struct strset * s = ctx ; const char * str = s -> strs_data + key ; return str_hash ( str ) ; }",
    "resources/libbpf/src/strset.c@strset_equal_fn": "static bool strset_equal_fn ( long key1 , long key2 , void * ctx ) { const struct strset * s = ctx ; const char * str1 = s -> strs_data + key1 ; const char * str2 = s -> strs_data + key2 ; return strcmp ( str1 , str2 ) == 0 ; }",
    "resources/libbpf/src/strset.c@strset__new": "struct strset * strset__new ( size_t max_data_sz , const char * init_data , size_t init_data_sz ) { struct strset * set = calloc ( 1 , sizeof ( * set ) ) ; struct hashmap * hash ; int err = - ENOMEM ; if ( ! set ) return ERR_PTR ( - ENOMEM ) ; hash = hashmap__new ( strset_hash_fn , strset_equal_fn , set ) ; if ( IS_ERR ( hash ) ) goto err_out ; set -> strs_data_max_len = max_data_sz ; set -> strs_hash = hash ; if ( init_data ) { long off ; set -> strs_data = malloc ( init_data_sz ) ; if ( ! set -> strs_data ) goto err_out ; memcpy ( set -> strs_data , init_data , init_data_sz ) ; set -> strs_data_len = init_data_sz ; set -> strs_data_cap = init_data_sz ; for ( off = 0 ; off < set -> strs_data_len ; off += strlen ( set -> strs_data + off ) + 1 ) { /* hashmap__add() returns EEXIST if string with the same\n\t\t\t * content already is in the hash map\n\t\t\t */ err = hashmap__add ( hash , off , off ) ; if ( err == - EEXIST ) continue ; /* duplicate */ if ( err ) goto err_out ; } } return set ; err_out : strset__free ( set ) ; return ERR_PTR ( err ) ; }",
    "resources/libbpf/src/strset.c@strset__free": "void strset__free ( struct strset * set ) { if ( IS_ERR_OR_NULL ( set ) ) return ; hashmap__free ( set -> strs_hash ) ; free ( set -> strs_data ) ; free ( set ) ; }",
    "resources/libbpf/src/strset.c@strset__data_size": "size_t strset__data_size ( const struct strset * set ) { return set -> strs_data_len ; }",
    "resources/libbpf/src/strset.c@strset__data": "const char * strset__data ( const struct strset * set ) { return set -> strs_data ; }",
    "resources/libbpf/src/strset.c@strset_add_str_mem": "static void * strset_add_str_mem ( struct strset * set , size_t add_sz ) { return libbpf_add_mem ( & set -> strs_data , & set -> strs_data_cap , 1 , set -> strs_data_len , set -> strs_data_max_len , add_sz ) ; }",
    "resources/libbpf/src/strset.c@strset__find_str": "int strset__find_str ( struct strset * set , const char * s ) { long old_off , new_off , len ; void * p ; /* see strset__add_str() for why we do this */ len = strlen ( s ) + 1 ; p = strset_add_str_mem ( set , len ) ; if ( ! p ) return - ENOMEM ; new_off = set -> strs_data_len ; memcpy ( p , s , len ) ; if ( hashmap__find ( set -> strs_hash , new_off , & old_off ) ) return old_off ; return - ENOENT ; }",
    "resources/libbpf/src/strset.c@strset__add_str": "int strset__add_str ( struct strset * set , const char * s ) { long old_off , new_off , len ; void * p ; int err ; /* Hashmap keys are always offsets within set->strs_data, so to even\n\t * look up some string from the \"outside\", we need to first append it\n\t * at the end, so that it can be addressed with an offset. Luckily,\n\t * until set->strs_data_len is incremented, that string is just a piece\n\t * of garbage for the rest of the code, so no harm, no foul. On the\n\t * other hand, if the string is unique, it's already appended and\n\t * ready to be used, only a simple set->strs_data_len increment away.\n\t */ len = strlen ( s ) + 1 ; p = strset_add_str_mem ( set , len ) ; if ( ! p ) return - ENOMEM ; new_off = set -> strs_data_len ; memcpy ( p , s , len ) ; /* Now attempt to add the string, but only if the string with the same\n\t * contents doesn't exist already (HASHMAP_ADD strategy). If such\n\t * string exists, we'll get its offset in old_off (that's old_key).\n\t */ err = hashmap__insert ( set -> strs_hash , new_off , new_off , HASHMAP_ADD , & old_off , NULL ) ; if ( err == - EEXIST ) return old_off ; /* duplicated string, return existing offset */ if ( err ) return err ; set -> strs_data_len += len ; /* new unique string, adjust data length */ return new_off ; }",
    "resources/libbpf/src/str_error.c@libbpf_strerror_r": "char * libbpf_strerror_r ( int err , char * dst , int len ) { int ret = strerror_r ( err < 0 ? - err : err , dst , len ) ; if ( ret ) snprintf ( dst , len , \"ERROR: strerror_r(%d)=%d\" , err , ret ) ; return dst ; }",
    "resources/libbpf/src/relo_core.c@btf_kind": "static inline __u16 btf_kind ( const struct btf_type * t ) { return BTF_INFO_KIND ( t -> info ) ; }",
    "resources/libbpf/src/relo_core.c@btf_vlen": "static inline __u16 btf_vlen ( const struct btf_type * t ) { return BTF_INFO_VLEN ( t -> info ) ; }",
    "resources/libbpf/src/relo_core.c@btf_kflag": "static inline bool btf_kflag ( const struct btf_type * t ) { return BTF_INFO_KFLAG ( t -> info ) ; }",
    "resources/libbpf/src/relo_core.c@btf_is_void": "static inline bool btf_is_void ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_UNKN ; }",
    "resources/libbpf/src/relo_core.c@btf_is_int": "static inline bool btf_is_int ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_INT ; }",
    "resources/libbpf/src/relo_core.c@btf_is_ptr": "static inline bool btf_is_ptr ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_PTR ; }",
    "resources/libbpf/src/relo_core.c@btf_is_array": "static inline bool btf_is_array ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_ARRAY ; }",
    "resources/libbpf/src/relo_core.c@btf_is_struct": "static inline bool btf_is_struct ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_STRUCT ; }",
    "resources/libbpf/src/relo_core.c@btf_is_union": "static inline bool btf_is_union ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_UNION ; }",
    "resources/libbpf/src/relo_core.c@btf_is_composite": "static inline bool btf_is_composite ( const struct btf_type * t ) { __u16 kind = btf_kind ( t ) ; return kind == BTF_KIND_STRUCT || kind == BTF_KIND_UNION ; }",
    "resources/libbpf/src/relo_core.c@btf_is_enum": "static inline bool btf_is_enum ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_ENUM ; }",
    "resources/libbpf/src/relo_core.c@btf_is_enum64": "static inline bool btf_is_enum64 ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_ENUM64 ; }",
    "resources/libbpf/src/relo_core.c@btf_is_fwd": "static inline bool btf_is_fwd ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_FWD ; }",
    "resources/libbpf/src/relo_core.c@btf_is_typedef": "static inline bool btf_is_typedef ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_TYPEDEF ; }",
    "resources/libbpf/src/relo_core.c@btf_is_volatile": "static inline bool btf_is_volatile ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_VOLATILE ; }",
    "resources/libbpf/src/relo_core.c@btf_is_const": "static inline bool btf_is_const ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_CONST ; }",
    "resources/libbpf/src/relo_core.c@btf_is_restrict": "static inline bool btf_is_restrict ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_RESTRICT ; }",
    "resources/libbpf/src/relo_core.c@btf_is_mod": "static inline bool btf_is_mod ( const struct btf_type * t ) { __u16 kind = btf_kind ( t ) ; return kind == BTF_KIND_VOLATILE || kind == BTF_KIND_CONST || kind == BTF_KIND_RESTRICT || kind == BTF_KIND_TYPE_TAG ; }",
    "resources/libbpf/src/relo_core.c@btf_is_func": "static inline bool btf_is_func ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_FUNC ; }",
    "resources/libbpf/src/relo_core.c@btf_is_func_proto": "static inline bool btf_is_func_proto ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_FUNC_PROTO ; }",
    "resources/libbpf/src/relo_core.c@btf_is_var": "static inline bool btf_is_var ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_VAR ; }",
    "resources/libbpf/src/relo_core.c@btf_is_datasec": "static inline bool btf_is_datasec ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_DATASEC ; }",
    "resources/libbpf/src/relo_core.c@btf_is_float": "static inline bool btf_is_float ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_FLOAT ; }",
    "resources/libbpf/src/relo_core.c@btf_is_decl_tag": "static inline bool btf_is_decl_tag ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_DECL_TAG ; }",
    "resources/libbpf/src/relo_core.c@btf_is_type_tag": "static inline bool btf_is_type_tag ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_TYPE_TAG ; }",
    "resources/libbpf/src/relo_core.c@btf_is_any_enum": "static inline bool btf_is_any_enum ( const struct btf_type * t ) { return btf_is_enum ( t ) || btf_is_enum64 ( t ) ; }",
    "resources/libbpf/src/relo_core.c@btf_kind_core_compat": "static inline bool btf_kind_core_compat ( const struct btf_type * t1 , const struct btf_type * t2 ) { return btf_kind ( t1 ) == btf_kind ( t2 ) || ( btf_is_any_enum ( t1 ) && btf_is_any_enum ( t2 ) ) ; }",
    "resources/libbpf/src/relo_core.c@btf_int_encoding": "static inline __u8 btf_int_encoding ( const struct btf_type * t ) { return BTF_INT_ENCODING ( * ( __u32 * ) ( t + 1 ) ) ; }",
    "resources/libbpf/src/relo_core.c@btf_int_offset": "static inline __u8 btf_int_offset ( const struct btf_type * t ) { return BTF_INT_OFFSET ( * ( __u32 * ) ( t + 1 ) ) ; }",
    "resources/libbpf/src/relo_core.c@btf_int_bits": "static inline __u8 btf_int_bits ( const struct btf_type * t ) { return BTF_INT_BITS ( * ( __u32 * ) ( t + 1 ) ) ; }",
    "resources/libbpf/src/relo_core.c@btf_array": "static inline struct btf_array * btf_array ( const struct btf_type * t ) { return ( struct btf_array * ) ( t + 1 ) ; }",
    "resources/libbpf/src/relo_core.c@btf_enum": "static inline struct btf_enum * btf_enum ( const struct btf_type * t ) { return ( struct btf_enum * ) ( t + 1 ) ; }",
    "resources/libbpf/src/relo_core.c@btf_enum64": "static inline struct btf_enum64 * btf_enum64 ( const struct btf_type * t ) { return ( struct btf_enum64 * ) ( t + 1 ) ; }",
    "resources/libbpf/src/relo_core.c@btf_enum64_value": "static inline __u64 btf_enum64_value ( const struct btf_enum64 * e ) { /* struct btf_enum64 is introduced in Linux 6.0, which is very\n\t * bleeding-edge. Here we are avoiding relying on struct btf_enum64\n\t * definition coming from kernel UAPI headers to support wider range\n\t * of system-wide kernel headers.\n\t *\n\t * Given this header can be also included from C++ applications, that\n\t * further restricts C tricks we can use (like using compatible\n\t * anonymous struct). So just treat struct btf_enum64 as\n\t * a three-element array of u32 and access second (lo32) and third\n\t * (hi32) elements directly.\n\t *\n\t * For reference, here is a struct btf_enum64 definition:\n\t *\n\t * const struct btf_enum64 {\n\t *\t__u32\tname_off;\n\t *\t__u32\tval_lo32;\n\t *\t__u32\tval_hi32;\n\t * };\n\t */ const __u32 * e64 = ( const __u32 * ) e ; return ( ( __u64 ) e64 [ 2 ] << 32 ) | e64 [ 1 ] ; }",
    "resources/libbpf/src/relo_core.c@btf_members": "static inline struct btf_member * btf_members ( const struct btf_type * t ) { return ( struct btf_member * ) ( t + 1 ) ; }",
    "resources/libbpf/src/relo_core.c@btf_member_bit_offset": "static inline __u32 btf_member_bit_offset ( const struct btf_type * t , __u32 member_idx ) { const struct btf_member * m = btf_members ( t ) + member_idx ; bool kflag = btf_kflag ( t ) ; return kflag ? BTF_MEMBER_BIT_OFFSET ( m -> offset ) : m -> offset ; }",
    "resources/libbpf/src/relo_core.c@btf_member_bitfield_size": "static inline __u32 btf_member_bitfield_size ( const struct btf_type * t , __u32 member_idx ) { const struct btf_member * m = btf_members ( t ) + member_idx ; bool kflag = btf_kflag ( t ) ; return kflag ? BTF_MEMBER_BITFIELD_SIZE ( m -> offset ) : 0 ; }",
    "resources/libbpf/src/relo_core.c@btf_params": "static inline struct btf_param * btf_params ( const struct btf_type * t ) { return ( struct btf_param * ) ( t + 1 ) ; }",
    "resources/libbpf/src/relo_core.c@btf_var": "static inline struct btf_var * btf_var ( const struct btf_type * t ) { return ( struct btf_var * ) ( t + 1 ) ; }",
    "resources/libbpf/src/relo_core.c@btf_var_secinfos": "static inline struct btf_var_secinfo * btf_var_secinfos ( const struct btf_type * t ) { return ( struct btf_var_secinfo * ) ( t + 1 ) ; }",
    "resources/libbpf/src/relo_core.c@btf_decl_tag": "static inline struct btf_decl_tag * btf_decl_tag ( const struct btf_type * t ) { return ( struct btf_decl_tag * ) ( t + 1 ) ; }",
    "resources/libbpf/src/relo_core.c@str_has_sfx": "static inline bool str_has_sfx ( const char * str , const char * sfx ) { size_t str_len = strlen ( str ) ; size_t sfx_len = strlen ( sfx ) ; if ( sfx_len > str_len ) return false ; return strcmp ( str + str_len - sfx_len , sfx ) == 0 ; }",
    "resources/libbpf/src/relo_core.c@libbpf_reallocarray": "static inline void * libbpf_reallocarray ( void * ptr , size_t nmemb , size_t size ) { size_t total ; # if __has_builtin ( __builtin_mul_overflow ) if ( unlikely ( __builtin_mul_overflow ( nmemb , size , & total ) ) ) return NULL ; # else if ( size == 0 || nmemb > ULONG_MAX / size ) return NULL ; total = nmemb * size ; # endif return realloc ( ptr , total ) ; }",
    "resources/libbpf/src/relo_core.c@libbpf_strlcpy": "static inline void libbpf_strlcpy ( char * dst , const char * src , size_t sz ) { size_t i ; if ( sz == 0 ) return ; sz -- ; for ( i = 0 ; i < sz && src [ i ] ; i ++ ) dst [ i ] = src [ i ] ; dst [ i ] = '\\0' ; }",
    "resources/libbpf/src/relo_core.c@btf_func_linkage": "static inline enum btf_func_linkage btf_func_linkage ( const struct btf_type * t ) { return ( enum btf_func_linkage ) ( int ) btf_vlen ( t ) ; }",
    "resources/libbpf/src/relo_core.c@btf_type_info": "static inline __u32 btf_type_info ( int kind , int vlen , int kflag ) { return ( kflag << 31 ) | ( kind << 24 ) | vlen ; }",
    "resources/libbpf/src/relo_core.c@libbpf_is_mem_zeroed": "static inline bool libbpf_is_mem_zeroed ( const char * p , ssize_t len ) { while ( len > 0 ) { if ( * p ) return false ; p ++ ; len -- ; } return true ; }",
    "resources/libbpf/src/relo_core.c@libbpf_validate_opts": "static inline bool libbpf_validate_opts ( const char * opts , size_t opts_sz , size_t user_sz , const char * type_name ) { if ( user_sz < sizeof ( size_t ) ) { pr_warn ( \"%s size (%zu) is too small\\n\" , type_name , user_sz ) ; return false ; } if ( ! libbpf_is_mem_zeroed ( opts + opts_sz , ( ssize_t ) user_sz - opts_sz ) ) { pr_warn ( \"%s has non-zero extra bytes\\n\" , type_name ) ; return false ; } return true ; }",
    "resources/libbpf/src/relo_core.c@libbpf_err": "static inline int libbpf_err ( int ret ) { if ( ret < 0 ) errno = - ret ; return ret ; }",
    "resources/libbpf/src/relo_core.c@libbpf_err_errno": "static inline int libbpf_err_errno ( int ret ) { /* errno is already assumed to be set on error */ return ret < 0 ? - errno : ret ; }",
    "resources/libbpf/src/relo_core.c@libbpf_err_ptr": "static inline void * libbpf_err_ptr ( int err ) { /* set errno on error, this doesn't break anything */ errno = - err ; return NULL ; }",
    "resources/libbpf/src/relo_core.c@libbpf_ptr": "static inline void * libbpf_ptr ( void * ret ) { /* set errno on error, this doesn't break anything */ if ( IS_ERR ( ret ) ) errno = - PTR_ERR ( ret ) ; return IS_ERR ( ret ) ? NULL : ret ; }",
    "resources/libbpf/src/relo_core.c@str_is_empty": "static inline bool str_is_empty ( const char * s ) { return ! s || ! s [ 0 ] ; }",
    "resources/libbpf/src/relo_core.c@is_ldimm64_insn": "static inline bool is_ldimm64_insn ( struct bpf_insn * insn ) { return insn -> code == ( BPF_LD | BPF_IMM | BPF_DW ) ; }",
    "resources/libbpf/src/relo_core.c@dup_good_fd": "static inline int dup_good_fd ( int fd ) { if ( fd < 0 ) return fd ; return fcntl ( fd , F_DUPFD_CLOEXEC , 3 ) ; }",
    "resources/libbpf/src/relo_core.c@ensure_good_fd": "static inline int ensure_good_fd ( int fd ) { int old_fd = fd , saved_errno ; if ( fd < 0 ) return fd ; if ( fd < 3 ) { fd = dup_good_fd ( fd ) ; saved_errno = errno ; close ( old_fd ) ; errno = saved_errno ; if ( fd < 0 ) { pr_warn ( \"failed to dup FD %d to FD > 2: %d\\n\" , old_fd , - saved_errno ) ; errno = saved_errno ; } } return fd ; }",
    "resources/libbpf/src/relo_core.c@sys_dup2": "static inline int sys_dup2 ( int oldfd , int newfd ) { # ifdef __NR_dup2 return syscall ( __NR_dup2 , oldfd , newfd ) ; # else return syscall ( __NR_dup3 , oldfd , newfd , 0 ) ; # endif }",
    "resources/libbpf/src/relo_core.c@reuse_fd": "static inline int reuse_fd ( int fixed_fd , int tmp_fd ) { int err ; err = sys_dup2 ( tmp_fd , fixed_fd ) ; err = err < 0 ? - errno : 0 ; close ( tmp_fd ) ; /* clean up temporary FD */ return err ; }",
    "resources/libbpf/src/relo_core.c@is_pow_of_2": "static inline bool is_pow_of_2 ( size_t x ) { return x && ( x & ( x - 1 ) ) == 0 ; }",
    "resources/libbpf/src/relo_core.c@is_flex_arr": "static bool is_flex_arr ( const struct btf * btf , const struct bpf_core_accessor * acc , const struct btf_array * arr ) { const struct btf_type * t ; /* not a flexible array, if not inside a struct or has non-zero size */ if ( ! acc -> name || arr -> nelems > 0 ) return false ; /* has to be the last member of enclosing struct */ t = btf_type_by_id ( btf , acc -> type_id ) ; return acc -> idx == btf_vlen ( t ) - 1 ; }",
    "resources/libbpf/src/relo_core.c@core_relo_kind_str": "static const char * core_relo_kind_str ( enum bpf_core_relo_kind kind ) { switch ( kind ) { case BPF_CORE_FIELD_BYTE_OFFSET : return \"byte_off\" ; case BPF_CORE_FIELD_BYTE_SIZE : return \"byte_sz\" ; case BPF_CORE_FIELD_EXISTS : return \"field_exists\" ; case BPF_CORE_FIELD_SIGNED : return \"signed\" ; case BPF_CORE_FIELD_LSHIFT_U64 : return \"lshift_u64\" ; case BPF_CORE_FIELD_RSHIFT_U64 : return \"rshift_u64\" ; case BPF_CORE_TYPE_ID_LOCAL : return \"local_type_id\" ; case BPF_CORE_TYPE_ID_TARGET : return \"target_type_id\" ; case BPF_CORE_TYPE_EXISTS : return \"type_exists\" ; case BPF_CORE_TYPE_MATCHES : return \"type_matches\" ; case BPF_CORE_TYPE_SIZE : return \"type_size\" ; case BPF_CORE_ENUMVAL_EXISTS : return \"enumval_exists\" ; case BPF_CORE_ENUMVAL_VALUE : return \"enumval_value\" ; default : return \"unknown\" ; } }",
    "resources/libbpf/src/relo_core.c@core_relo_is_field_based": "static bool core_relo_is_field_based ( enum bpf_core_relo_kind kind ) { switch ( kind ) { case BPF_CORE_FIELD_BYTE_OFFSET : case BPF_CORE_FIELD_BYTE_SIZE : case BPF_CORE_FIELD_EXISTS : case BPF_CORE_FIELD_SIGNED : case BPF_CORE_FIELD_LSHIFT_U64 : case BPF_CORE_FIELD_RSHIFT_U64 : return true ; default : return false ; } }",
    "resources/libbpf/src/relo_core.c@core_relo_is_type_based": "static bool core_relo_is_type_based ( enum bpf_core_relo_kind kind ) { switch ( kind ) { case BPF_CORE_TYPE_ID_LOCAL : case BPF_CORE_TYPE_ID_TARGET : case BPF_CORE_TYPE_EXISTS : case BPF_CORE_TYPE_MATCHES : case BPF_CORE_TYPE_SIZE : return true ; default : return false ; } }",
    "resources/libbpf/src/relo_core.c@core_relo_is_enumval_based": "static bool core_relo_is_enumval_based ( enum bpf_core_relo_kind kind ) { switch ( kind ) { case BPF_CORE_ENUMVAL_EXISTS : case BPF_CORE_ENUMVAL_VALUE : return true ; default : return false ; } }",
    "resources/libbpf/src/relo_core.c@__bpf_core_types_are_compat": "int __bpf_core_types_are_compat ( const struct btf * local_btf , __u32 local_id , const struct btf * targ_btf , __u32 targ_id , int level ) { const struct btf_type * local_type , * targ_type ; int depth = 32 ; /* max recursion depth */ /* caller made sure that names match (ignoring flavor suffix) */ local_type = btf_type_by_id ( local_btf , local_id ) ; targ_type = btf_type_by_id ( targ_btf , targ_id ) ; if ( ! btf_kind_core_compat ( local_type , targ_type ) ) return 0 ; recur : depth -- ; if ( depth < 0 ) return - EINVAL ; local_type = skip_mods_and_typedefs ( local_btf , local_id , & local_id ) ; targ_type = skip_mods_and_typedefs ( targ_btf , targ_id , & targ_id ) ; if ( ! local_type || ! targ_type ) return - EINVAL ; if ( ! btf_kind_core_compat ( local_type , targ_type ) ) return 0 ; switch ( btf_kind ( local_type ) ) { case BTF_KIND_UNKN : case BTF_KIND_STRUCT : case BTF_KIND_UNION : case BTF_KIND_ENUM : case BTF_KIND_FWD : case BTF_KIND_ENUM64 : return 1 ; case BTF_KIND_INT : /* just reject deprecated bitfield-like integers; all other\n\t\t * integers are by default compatible between each other\n\t\t */ return btf_int_offset ( local_type ) == 0 && btf_int_offset ( targ_type ) == 0 ; case BTF_KIND_PTR : local_id = local_type -> type ; targ_id = targ_type -> type ; goto recur ; case BTF_KIND_ARRAY : local_id = btf_array ( local_type ) -> type ; targ_id = btf_array ( targ_type ) -> type ; goto recur ; case BTF_KIND_FUNC_PROTO : { struct btf_param * local_p = btf_params ( local_type ) ; struct btf_param * targ_p = btf_params ( targ_type ) ; __u16 local_vlen = btf_vlen ( local_type ) ; __u16 targ_vlen = btf_vlen ( targ_type ) ; int i , err ; if ( local_vlen != targ_vlen ) return 0 ; for ( i = 0 ; i < local_vlen ; i ++ , local_p ++ , targ_p ++ ) { if ( level <= 0 ) return - EINVAL ; skip_mods_and_typedefs ( local_btf , local_p -> type , & local_id ) ; skip_mods_and_typedefs ( targ_btf , targ_p -> type , & targ_id ) ; err = __bpf_core_types_are_compat ( local_btf , local_id , targ_btf , targ_id , level - 1 ) ; if ( err <= 0 ) return err ; } /* tail recurse for return type check */ skip_mods_and_typedefs ( local_btf , local_type -> type , & local_id ) ; skip_mods_and_typedefs ( targ_btf , targ_type -> type , & targ_id ) ; goto recur ; } default : pr_warn ( \"unexpected kind %s relocated, local [%d], target [%d]\\n\" , btf_kind_str ( local_type ) , local_id , targ_id ) ; return 0 ; } }",
    "resources/libbpf/src/relo_core.c@bpf_core_parse_spec": "int bpf_core_parse_spec ( const char * prog_name , const struct btf * btf , const struct bpf_core_relo * relo , struct bpf_core_spec * spec ) { int access_idx , parsed_len , i ; struct bpf_core_accessor * acc ; const struct btf_type * t ; const char * name , * spec_str ; __u32 id , name_off ; __s64 sz ; spec_str = btf__name_by_offset ( btf , relo -> access_str_off ) ; if ( str_is_empty ( spec_str ) || * spec_str == ':' ) return - EINVAL ; memset ( spec , 0 , sizeof ( * spec ) ) ; spec -> btf = btf ; spec -> root_type_id = relo -> type_id ; spec -> relo_kind = relo -> kind ; /* type-based relocations don't have a field access string */ if ( core_relo_is_type_based ( relo -> kind ) ) { if ( strcmp ( spec_str , \"0\" ) ) return - EINVAL ; return 0 ; } /* parse spec_str=\"0:1:2:3:4\" into array raw_spec=[0, 1, 2, 3, 4] */ while ( * spec_str ) { if ( * spec_str == ':' ) ++ spec_str ; if ( sscanf ( spec_str , \"%d%n\" , & access_idx , & parsed_len ) != 1 ) return - EINVAL ; if ( spec -> raw_len == BPF_CORE_SPEC_MAX_LEN ) return - E2BIG ; spec_str += parsed_len ; spec -> raw_spec [ spec -> raw_len ++ ] = access_idx ; } if ( spec -> raw_len == 0 ) return - EINVAL ; t = skip_mods_and_typedefs ( btf , relo -> type_id , & id ) ; if ( ! t ) return - EINVAL ; access_idx = spec -> raw_spec [ 0 ] ; acc = & spec -> spec [ 0 ] ; acc -> type_id = id ; acc -> idx = access_idx ; spec -> len ++ ; if ( core_relo_is_enumval_based ( relo -> kind ) ) { if ( ! btf_is_any_enum ( t ) || spec -> raw_len > 1 || access_idx >= btf_vlen ( t ) ) return - EINVAL ; /* record enumerator name in a first accessor */ name_off = btf_is_enum ( t ) ? btf_enum ( t ) [ access_idx ] . name_off : btf_enum64 ( t ) [ access_idx ] . name_off ; acc -> name = btf__name_by_offset ( btf , name_off ) ; return 0 ; } if ( ! core_relo_is_field_based ( relo -> kind ) ) return - EINVAL ; sz = btf__resolve_size ( btf , id ) ; if ( sz < 0 ) return sz ; spec -> bit_offset = access_idx * sz * 8 ; for ( i = 1 ; i < spec -> raw_len ; i ++ ) { t = skip_mods_and_typedefs ( btf , id , & id ) ; if ( ! t ) return - EINVAL ; access_idx = spec -> raw_spec [ i ] ; acc = & spec -> spec [ spec -> len ] ; if ( btf_is_composite ( t ) ) { const struct btf_member * m ; __u32 bit_offset ; if ( access_idx >= btf_vlen ( t ) ) return - EINVAL ; bit_offset = btf_member_bit_offset ( t , access_idx ) ; spec -> bit_offset += bit_offset ; m = btf_members ( t ) + access_idx ; if ( m -> name_off ) { name = btf__name_by_offset ( btf , m -> name_off ) ; if ( str_is_empty ( name ) ) return - EINVAL ; acc -> type_id = id ; acc -> idx = access_idx ; acc -> name = name ; spec -> len ++ ; } id = m -> type ; } else if ( btf_is_array ( t ) ) { const struct btf_array * a = btf_array ( t ) ; bool flex ; t = skip_mods_and_typedefs ( btf , a -> type , & id ) ; if ( ! t ) return - EINVAL ; flex = is_flex_arr ( btf , acc - 1 , a ) ; if ( ! flex && access_idx >= a -> nelems ) return - EINVAL ; spec -> spec [ spec -> len ] . type_id = id ; spec -> spec [ spec -> len ] . idx = access_idx ; spec -> len ++ ; sz = btf__resolve_size ( btf , id ) ; if ( sz < 0 ) return sz ; spec -> bit_offset += access_idx * sz * 8 ; } else { pr_warn ( \"prog '%s': relo for [%u] %s (at idx %d) captures type [%d] of unexpected kind %s\\n\" , prog_name , relo -> type_id , spec_str , i , id , btf_kind_str ( t ) ) ; return - EINVAL ; } } return 0 ; }",
    "resources/libbpf/src/relo_core.c@bpf_core_fields_are_compat": "static int bpf_core_fields_are_compat ( const struct btf * local_btf , __u32 local_id , const struct btf * targ_btf , __u32 targ_id ) { const struct btf_type * local_type , * targ_type ; recur : local_type = skip_mods_and_typedefs ( local_btf , local_id , & local_id ) ; targ_type = skip_mods_and_typedefs ( targ_btf , targ_id , & targ_id ) ; if ( ! local_type || ! targ_type ) return - EINVAL ; if ( btf_is_composite ( local_type ) && btf_is_composite ( targ_type ) ) return 1 ; if ( ! btf_kind_core_compat ( local_type , targ_type ) ) return 0 ; switch ( btf_kind ( local_type ) ) { case BTF_KIND_PTR : case BTF_KIND_FLOAT : return 1 ; case BTF_KIND_FWD : case BTF_KIND_ENUM64 : case BTF_KIND_ENUM : { const char * local_name , * targ_name ; size_t local_len , targ_len ; local_name = btf__name_by_offset ( local_btf , local_type -> name_off ) ; targ_name = btf__name_by_offset ( targ_btf , targ_type -> name_off ) ; local_len = bpf_core_essential_name_len ( local_name ) ; targ_len = bpf_core_essential_name_len ( targ_name ) ; /* one of them is anonymous or both w/ same flavor-less names */ return local_len == 0 || targ_len == 0 || ( local_len == targ_len && strncmp ( local_name , targ_name , local_len ) == 0 ) ; } case BTF_KIND_INT : /* just reject deprecated bitfield-like integers; all other\n\t\t * integers are by default compatible between each other\n\t\t */ return btf_int_offset ( local_type ) == 0 && btf_int_offset ( targ_type ) == 0 ; case BTF_KIND_ARRAY : local_id = btf_array ( local_type ) -> type ; targ_id = btf_array ( targ_type ) -> type ; goto recur ; default : return 0 ; } }",
    "resources/libbpf/src/relo_core.c@bpf_core_match_member": "static int bpf_core_match_member ( const struct btf * local_btf , const struct bpf_core_accessor * local_acc , const struct btf * targ_btf , __u32 targ_id , struct bpf_core_spec * spec , __u32 * next_targ_id ) { const struct btf_type * local_type , * targ_type ; const struct btf_member * local_member , * m ; const char * local_name , * targ_name ; __u32 local_id ; int i , n , found ; targ_type = skip_mods_and_typedefs ( targ_btf , targ_id , & targ_id ) ; if ( ! targ_type ) return - EINVAL ; if ( ! btf_is_composite ( targ_type ) ) return 0 ; local_id = local_acc -> type_id ; local_type = btf_type_by_id ( local_btf , local_id ) ; local_member = btf_members ( local_type ) + local_acc -> idx ; local_name = btf__name_by_offset ( local_btf , local_member -> name_off ) ; n = btf_vlen ( targ_type ) ; m = btf_members ( targ_type ) ; for ( i = 0 ; i < n ; i ++ , m ++ ) { __u32 bit_offset ; bit_offset = btf_member_bit_offset ( targ_type , i ) ; /* too deep struct/union/array nesting */ if ( spec -> raw_len == BPF_CORE_SPEC_MAX_LEN ) return - E2BIG ; /* speculate this member will be the good one */ spec -> bit_offset += bit_offset ; spec -> raw_spec [ spec -> raw_len ++ ] = i ; targ_name = btf__name_by_offset ( targ_btf , m -> name_off ) ; if ( str_is_empty ( targ_name ) ) { /* embedded struct/union, we need to go deeper */ found = bpf_core_match_member ( local_btf , local_acc , targ_btf , m -> type , spec , next_targ_id ) ; if ( found ) /* either found or error */ return found ; } else if ( strcmp ( local_name , targ_name ) == 0 ) { /* matching named field */ struct bpf_core_accessor * targ_acc ; targ_acc = & spec -> spec [ spec -> len ++ ] ; targ_acc -> type_id = targ_id ; targ_acc -> idx = i ; targ_acc -> name = targ_name ; * next_targ_id = m -> type ; found = bpf_core_fields_are_compat ( local_btf , local_member -> type , targ_btf , m -> type ) ; if ( ! found ) spec -> len -- ; /* pop accessor */ return found ; } /* member turned out not to be what we looked for */ spec -> bit_offset -= bit_offset ; spec -> raw_len -- ; } return 0 ; }",
    "resources/libbpf/src/relo_core.c@bpf_core_spec_match": "static int bpf_core_spec_match ( struct bpf_core_spec * local_spec , const struct btf * targ_btf , __u32 targ_id , struct bpf_core_spec * targ_spec ) { const struct btf_type * targ_type ; const struct bpf_core_accessor * local_acc ; struct bpf_core_accessor * targ_acc ; int i , sz , matched ; __u32 name_off ; memset ( targ_spec , 0 , sizeof ( * targ_spec ) ) ; targ_spec -> btf = targ_btf ; targ_spec -> root_type_id = targ_id ; targ_spec -> relo_kind = local_spec -> relo_kind ; if ( core_relo_is_type_based ( local_spec -> relo_kind ) ) { if ( local_spec -> relo_kind == BPF_CORE_TYPE_MATCHES ) return bpf_core_types_match ( local_spec -> btf , local_spec -> root_type_id , targ_btf , targ_id ) ; else return bpf_core_types_are_compat ( local_spec -> btf , local_spec -> root_type_id , targ_btf , targ_id ) ; } local_acc = & local_spec -> spec [ 0 ] ; targ_acc = & targ_spec -> spec [ 0 ] ; if ( core_relo_is_enumval_based ( local_spec -> relo_kind ) ) { size_t local_essent_len , targ_essent_len ; const char * targ_name ; /* has to resolve to an enum */ targ_type = skip_mods_and_typedefs ( targ_spec -> btf , targ_id , & targ_id ) ; if ( ! btf_is_any_enum ( targ_type ) ) return 0 ; local_essent_len = bpf_core_essential_name_len ( local_acc -> name ) ; for ( i = 0 ; i < btf_vlen ( targ_type ) ; i ++ ) { if ( btf_is_enum ( targ_type ) ) name_off = btf_enum ( targ_type ) [ i ] . name_off ; else name_off = btf_enum64 ( targ_type ) [ i ] . name_off ; targ_name = btf__name_by_offset ( targ_spec -> btf , name_off ) ; targ_essent_len = bpf_core_essential_name_len ( targ_name ) ; if ( targ_essent_len != local_essent_len ) continue ; if ( strncmp ( local_acc -> name , targ_name , local_essent_len ) == 0 ) { targ_acc -> type_id = targ_id ; targ_acc -> idx = i ; targ_acc -> name = targ_name ; targ_spec -> len ++ ; targ_spec -> raw_spec [ targ_spec -> raw_len ] = targ_acc -> idx ; targ_spec -> raw_len ++ ; return 1 ; } } return 0 ; } if ( ! core_relo_is_field_based ( local_spec -> relo_kind ) ) return - EINVAL ; for ( i = 0 ; i < local_spec -> len ; i ++ , local_acc ++ , targ_acc ++ ) { targ_type = skip_mods_and_typedefs ( targ_spec -> btf , targ_id , & targ_id ) ; if ( ! targ_type ) return - EINVAL ; if ( local_acc -> name ) { matched = bpf_core_match_member ( local_spec -> btf , local_acc , targ_btf , targ_id , targ_spec , & targ_id ) ; if ( matched <= 0 ) return matched ; } else { /* for i=0, targ_id is already treated as array element\n\t\t\t * type (because it's the original struct), for others\n\t\t\t * we should find array element type first\n\t\t\t */ if ( i > 0 ) { const struct btf_array * a ; bool flex ; if ( ! btf_is_array ( targ_type ) ) return 0 ; a = btf_array ( targ_type ) ; flex = is_flex_arr ( targ_btf , targ_acc - 1 , a ) ; if ( ! flex && local_acc -> idx >= a -> nelems ) return 0 ; if ( ! skip_mods_and_typedefs ( targ_btf , a -> type , & targ_id ) ) return - EINVAL ; } /* too deep struct/union/array nesting */ if ( targ_spec -> raw_len == BPF_CORE_SPEC_MAX_LEN ) return - E2BIG ; targ_acc -> type_id = targ_id ; targ_acc -> idx = local_acc -> idx ; targ_acc -> name = NULL ; targ_spec -> len ++ ; targ_spec -> raw_spec [ targ_spec -> raw_len ] = targ_acc -> idx ; targ_spec -> raw_len ++ ; sz = btf__resolve_size ( targ_btf , targ_id ) ; if ( sz < 0 ) return sz ; targ_spec -> bit_offset += local_acc -> idx * sz * 8 ; } } return 1 ; }",
    "resources/libbpf/src/relo_core.c@bpf_core_calc_field_relo": "static int bpf_core_calc_field_relo ( const char * prog_name , const struct bpf_core_relo * relo , const struct bpf_core_spec * spec , __u64 * val , __u32 * field_sz , __u32 * type_id , bool * validate ) { const struct bpf_core_accessor * acc ; const struct btf_type * t ; __u32 byte_off , byte_sz , bit_off , bit_sz , field_type_id ; const struct btf_member * m ; const struct btf_type * mt ; bool bitfield ; __s64 sz ; * field_sz = 0 ; if ( relo -> kind == BPF_CORE_FIELD_EXISTS ) { * val = spec ? 1 : 0 ; return 0 ; } if ( ! spec ) return - EUCLEAN ; /* request instruction poisoning */ acc = & spec -> spec [ spec -> len - 1 ] ; t = btf_type_by_id ( spec -> btf , acc -> type_id ) ; /* a[n] accessor needs special handling */ if ( ! acc -> name ) { if ( relo -> kind == BPF_CORE_FIELD_BYTE_OFFSET ) { * val = spec -> bit_offset / 8 ; /* remember field size for load/store mem size */ sz = btf__resolve_size ( spec -> btf , acc -> type_id ) ; if ( sz < 0 ) return - EINVAL ; * field_sz = sz ; * type_id = acc -> type_id ; } else if ( relo -> kind == BPF_CORE_FIELD_BYTE_SIZE ) { sz = btf__resolve_size ( spec -> btf , acc -> type_id ) ; if ( sz < 0 ) return - EINVAL ; * val = sz ; } else { pr_warn ( \"prog '%s': relo %d at insn #%d can't be applied to array access\\n\" , prog_name , relo -> kind , relo -> insn_off / 8 ) ; return - EINVAL ; } if ( validate ) * validate = true ; return 0 ; } m = btf_members ( t ) + acc -> idx ; mt = skip_mods_and_typedefs ( spec -> btf , m -> type , & field_type_id ) ; bit_off = spec -> bit_offset ; bit_sz = btf_member_bitfield_size ( t , acc -> idx ) ; bitfield = bit_sz > 0 ; if ( bitfield ) { byte_sz = mt -> size ; byte_off = bit_off / 8 / byte_sz * byte_sz ; /* figure out smallest int size necessary for bitfield load */ while ( bit_off + bit_sz - byte_off * 8 > byte_sz * 8 ) { if ( byte_sz >= 8 ) { /* bitfield can't be read with 64-bit read */ pr_warn ( \"prog '%s': relo %d at insn #%d can't be satisfied for bitfield\\n\" , prog_name , relo -> kind , relo -> insn_off / 8 ) ; return - E2BIG ; } byte_sz *= 2 ; byte_off = bit_off / 8 / byte_sz * byte_sz ; } } else { sz = btf__resolve_size ( spec -> btf , field_type_id ) ; if ( sz < 0 ) return - EINVAL ; byte_sz = sz ; byte_off = spec -> bit_offset / 8 ; bit_sz = byte_sz * 8 ; } /* for bitfields, all the relocatable aspects are ambiguous and we\n\t * might disagree with compiler, so turn off validation of expected\n\t * value, except for signedness\n\t */ if ( validate ) * validate = ! bitfield ; switch ( relo -> kind ) { case BPF_CORE_FIELD_BYTE_OFFSET : * val = byte_off ; if ( ! bitfield ) { * field_sz = byte_sz ; * type_id = field_type_id ; } break ; case BPF_CORE_FIELD_BYTE_SIZE : * val = byte_sz ; break ; case BPF_CORE_FIELD_SIGNED : * val = ( btf_is_any_enum ( mt ) && BTF_INFO_KFLAG ( mt -> info ) ) || ( btf_is_int ( mt ) && ( btf_int_encoding ( mt ) & BTF_INT_SIGNED ) ) ; if ( validate ) * validate = true ; /* signedness is never ambiguous */ break ; case BPF_CORE_FIELD_LSHIFT_U64 : # if __BYTE_ORDER__ == __ORDER_LITTLE_ENDIAN__ * val = 64 - ( bit_off + bit_sz - byte_off * 8 ) ; # else * val = ( 8 - byte_sz ) * 8 + ( bit_off - byte_off * 8 ) ; # endif break ; case BPF_CORE_FIELD_RSHIFT_U64 : * val = 64 - bit_sz ; if ( validate ) * validate = true ; /* right shift is never ambiguous */ break ; case BPF_CORE_FIELD_EXISTS : default : return - EOPNOTSUPP ; } return 0 ; }",
    "resources/libbpf/src/relo_core.c@bpf_core_calc_type_relo": "static int bpf_core_calc_type_relo ( const struct bpf_core_relo * relo , const struct bpf_core_spec * spec , __u64 * val , bool * validate ) { __s64 sz ; /* by default, always check expected value in bpf_insn */ if ( validate ) * validate = true ; /* type-based relos return zero when target type is not found */ if ( ! spec ) { * val = 0 ; return 0 ; } switch ( relo -> kind ) { case BPF_CORE_TYPE_ID_TARGET : * val = spec -> root_type_id ; /* type ID, embedded in bpf_insn, might change during linking,\n\t\t * so enforcing it is pointless\n\t\t */ if ( validate ) * validate = false ; break ; case BPF_CORE_TYPE_EXISTS : case BPF_CORE_TYPE_MATCHES : * val = 1 ; break ; case BPF_CORE_TYPE_SIZE : sz = btf__resolve_size ( spec -> btf , spec -> root_type_id ) ; if ( sz < 0 ) return - EINVAL ; * val = sz ; break ; case BPF_CORE_TYPE_ID_LOCAL : /* BPF_CORE_TYPE_ID_LOCAL is handled specially and shouldn't get here */ default : return - EOPNOTSUPP ; } return 0 ; }",
    "resources/libbpf/src/relo_core.c@bpf_core_calc_enumval_relo": "static int bpf_core_calc_enumval_relo ( const struct bpf_core_relo * relo , const struct bpf_core_spec * spec , __u64 * val ) { const struct btf_type * t ; switch ( relo -> kind ) { case BPF_CORE_ENUMVAL_EXISTS : * val = spec ? 1 : 0 ; break ; case BPF_CORE_ENUMVAL_VALUE : if ( ! spec ) return - EUCLEAN ; /* request instruction poisoning */ t = btf_type_by_id ( spec -> btf , spec -> spec [ 0 ] . type_id ) ; if ( btf_is_enum ( t ) ) * val = btf_enum ( t ) [ spec -> spec [ 0 ] . idx ] . val ; else * val = btf_enum64_value ( btf_enum64 ( t ) + spec -> spec [ 0 ] . idx ) ; break ; default : return - EOPNOTSUPP ; } return 0 ; }",
    "resources/libbpf/src/relo_core.c@bpf_core_calc_relo": "static int bpf_core_calc_relo ( const char * prog_name , const struct bpf_core_relo * relo , int relo_idx , const struct bpf_core_spec * local_spec , const struct bpf_core_spec * targ_spec , struct bpf_core_relo_res * res ) { int err = - EOPNOTSUPP ; res -> orig_val = 0 ; res -> new_val = 0 ; res -> poison = false ; res -> validate = true ; res -> fail_memsz_adjust = false ; res -> orig_sz = res -> new_sz = 0 ; res -> orig_type_id = res -> new_type_id = 0 ; if ( core_relo_is_field_based ( relo -> kind ) ) { err = bpf_core_calc_field_relo ( prog_name , relo , local_spec , & res -> orig_val , & res -> orig_sz , & res -> orig_type_id , & res -> validate ) ; err = err ? : bpf_core_calc_field_relo ( prog_name , relo , targ_spec , & res -> new_val , & res -> new_sz , & res -> new_type_id , NULL ) ; if ( err ) goto done ; /* Validate if it's safe to adjust load/store memory size.\n\t\t * Adjustments are performed only if original and new memory\n\t\t * sizes differ.\n\t\t */ res -> fail_memsz_adjust = false ; if ( res -> orig_sz != res -> new_sz ) { const struct btf_type * orig_t , * new_t ; orig_t = btf_type_by_id ( local_spec -> btf , res -> orig_type_id ) ; new_t = btf_type_by_id ( targ_spec -> btf , res -> new_type_id ) ; /* There are two use cases in which it's safe to\n\t\t\t * adjust load/store's mem size:\n\t\t\t *   - reading a 32-bit kernel pointer, while on BPF\n\t\t\t *   size pointers are always 64-bit; in this case\n\t\t\t *   it's safe to \"downsize\" instruction size due to\n\t\t\t *   pointer being treated as unsigned integer with\n\t\t\t *   zero-extended upper 32-bits;\n\t\t\t *   - reading unsigned integers, again due to\n\t\t\t *   zero-extension is preserving the value correctly.\n\t\t\t *\n\t\t\t * In all other cases it's incorrect to attempt to\n\t\t\t * load/store field because read value will be\n\t\t\t * incorrect, so we poison relocated instruction.\n\t\t\t */ if ( btf_is_ptr ( orig_t ) && btf_is_ptr ( new_t ) ) goto done ; if ( btf_is_int ( orig_t ) && btf_is_int ( new_t ) && btf_int_encoding ( orig_t ) != BTF_INT_SIGNED && btf_int_encoding ( new_t ) != BTF_INT_SIGNED ) goto done ; /* mark as invalid mem size adjustment, but this will\n\t\t\t * only be checked for LDX/STX/ST insns\n\t\t\t */ res -> fail_memsz_adjust = true ; } } else if ( core_relo_is_type_based ( relo -> kind ) ) { err = bpf_core_calc_type_relo ( relo , local_spec , & res -> orig_val , & res -> validate ) ; err = err ? : bpf_core_calc_type_relo ( relo , targ_spec , & res -> new_val , NULL ) ; } else if ( core_relo_is_enumval_based ( relo -> kind ) ) { err = bpf_core_calc_enumval_relo ( relo , local_spec , & res -> orig_val ) ; err = err ? : bpf_core_calc_enumval_relo ( relo , targ_spec , & res -> new_val ) ; } done : if ( err == - EUCLEAN ) { /* EUCLEAN is used to signal instruction poisoning request */ res -> poison = true ; err = 0 ; } else if ( err == - EOPNOTSUPP ) { /* EOPNOTSUPP means unknown/unsupported relocation */ pr_warn ( \"prog '%s': relo #%d: unrecognized CO-RE relocation %s (%d) at insn #%d\\n\" , prog_name , relo_idx , core_relo_kind_str ( relo -> kind ) , relo -> kind , relo -> insn_off / 8 ) ; } return err ; }",
    "resources/libbpf/src/relo_core.c@bpf_core_poison_insn": "static void bpf_core_poison_insn ( const char * prog_name , int relo_idx , int insn_idx , struct bpf_insn * insn ) { pr_debug ( \"prog '%s': relo #%d: substituting insn #%d w/ invalid insn\\n\" , prog_name , relo_idx , insn_idx ) ; insn -> code = BPF_JMP | BPF_CALL ; insn -> dst_reg = 0 ; insn -> src_reg = 0 ; insn -> off = 0 ; /* if this instruction is reachable (not a dead code),\n\t * verifier will complain with the following message:\n\t * invalid func unknown#195896080\n\t */ insn -> imm = 195896080 ; /* => 0xbad2310 => \"bad relo\" */ }",
    "resources/libbpf/src/relo_core.c@insn_bpf_size_to_bytes": "static int insn_bpf_size_to_bytes ( struct bpf_insn * insn ) { switch ( BPF_SIZE ( insn -> code ) ) { case BPF_DW : return 8 ; case BPF_W : return 4 ; case BPF_H : return 2 ; case BPF_B : return 1 ; default : return - 1 ; } }",
    "resources/libbpf/src/relo_core.c@insn_bytes_to_bpf_size": "static int insn_bytes_to_bpf_size ( __u32 sz ) { switch ( sz ) { case 8 : return BPF_DW ; case 4 : return BPF_W ; case 2 : return BPF_H ; case 1 : return BPF_B ; default : return - 1 ; } }",
    "resources/libbpf/src/relo_core.c@bpf_core_patch_insn": "int bpf_core_patch_insn ( const char * prog_name , struct bpf_insn * insn , int insn_idx , const struct bpf_core_relo * relo , int relo_idx , const struct bpf_core_relo_res * res ) { __u64 orig_val , new_val ; __u8 class ; class = BPF_CLASS ( insn -> code ) ; if ( res -> poison ) { poison : /* poison second part of ldimm64 to avoid confusing error from\n\t\t * verifier about \"unknown opcode 00\"\n\t\t */ if ( is_ldimm64_insn ( insn ) ) bpf_core_poison_insn ( prog_name , relo_idx , insn_idx + 1 , insn + 1 ) ; bpf_core_poison_insn ( prog_name , relo_idx , insn_idx , insn ) ; return 0 ; } orig_val = res -> orig_val ; new_val = res -> new_val ; switch ( class ) { case BPF_ALU : case BPF_ALU64 : if ( BPF_SRC ( insn -> code ) != BPF_K ) return - EINVAL ; if ( res -> validate && insn -> imm != orig_val ) { pr_warn ( \"prog '%s': relo #%d: unexpected insn #%d (ALU/ALU64) value: got %u, exp %llu -> %llu\\n\" , prog_name , relo_idx , insn_idx , insn -> imm , ( unsigned long long ) orig_val , ( unsigned long long ) new_val ) ; return - EINVAL ; } orig_val = insn -> imm ; insn -> imm = new_val ; pr_debug ( \"prog '%s': relo #%d: patched insn #%d (ALU/ALU64) imm %llu -> %llu\\n\" , prog_name , relo_idx , insn_idx , ( unsigned long long ) orig_val , ( unsigned long long ) new_val ) ; break ; case BPF_LDX : case BPF_ST : case BPF_STX : if ( res -> validate && insn -> off != orig_val ) { pr_warn ( \"prog '%s': relo #%d: unexpected insn #%d (LDX/ST/STX) value: got %u, exp %llu -> %llu\\n\" , prog_name , relo_idx , insn_idx , insn -> off , ( unsigned long long ) orig_val , ( unsigned long long ) new_val ) ; return - EINVAL ; } if ( new_val > SHRT_MAX ) { pr_warn ( \"prog '%s': relo #%d: insn #%d (LDX/ST/STX) value too big: %llu\\n\" , prog_name , relo_idx , insn_idx , ( unsigned long long ) new_val ) ; return - ERANGE ; } if ( res -> fail_memsz_adjust ) { pr_warn ( \"prog '%s': relo #%d: insn #%d (LDX/ST/STX) accesses field incorrectly. \" \"Make sure you are accessing pointers, unsigned integers, or fields of matching type and size.\\n\" , prog_name , relo_idx , insn_idx ) ; goto poison ; } orig_val = insn -> off ; insn -> off = new_val ; pr_debug ( \"prog '%s': relo #%d: patched insn #%d (LDX/ST/STX) off %llu -> %llu\\n\" , prog_name , relo_idx , insn_idx , ( unsigned long long ) orig_val , ( unsigned long long ) new_val ) ; if ( res -> new_sz != res -> orig_sz ) { int insn_bytes_sz , insn_bpf_sz ; insn_bytes_sz = insn_bpf_size_to_bytes ( insn ) ; if ( insn_bytes_sz != res -> orig_sz ) { pr_warn ( \"prog '%s': relo #%d: insn #%d (LDX/ST/STX) unexpected mem size: got %d, exp %u\\n\" , prog_name , relo_idx , insn_idx , insn_bytes_sz , res -> orig_sz ) ; return - EINVAL ; } insn_bpf_sz = insn_bytes_to_bpf_size ( res -> new_sz ) ; if ( insn_bpf_sz < 0 ) { pr_warn ( \"prog '%s': relo #%d: insn #%d (LDX/ST/STX) invalid new mem size: %u\\n\" , prog_name , relo_idx , insn_idx , res -> new_sz ) ; return - EINVAL ; } insn -> code = BPF_MODE ( insn -> code ) | insn_bpf_sz | BPF_CLASS ( insn -> code ) ; pr_debug ( \"prog '%s': relo #%d: patched insn #%d (LDX/ST/STX) mem_sz %u -> %u\\n\" , prog_name , relo_idx , insn_idx , res -> orig_sz , res -> new_sz ) ; } break ; case BPF_LD : { __u64 imm ; if ( ! is_ldimm64_insn ( insn ) || insn [ 0 ] . src_reg != 0 || insn [ 0 ] . off != 0 || insn [ 1 ] . code != 0 || insn [ 1 ] . dst_reg != 0 || insn [ 1 ] . src_reg != 0 || insn [ 1 ] . off != 0 ) { pr_warn ( \"prog '%s': relo #%d: insn #%d (LDIMM64) has unexpected form\\n\" , prog_name , relo_idx , insn_idx ) ; return - EINVAL ; } imm = ( __u32 ) insn [ 0 ] . imm | ( ( __u64 ) insn [ 1 ] . imm << 32 ) ; if ( res -> validate && imm != orig_val ) { pr_warn ( \"prog '%s': relo #%d: unexpected insn #%d (LDIMM64) value: got %llu, exp %llu -> %llu\\n\" , prog_name , relo_idx , insn_idx , ( unsigned long long ) imm , ( unsigned long long ) orig_val , ( unsigned long long ) new_val ) ; return - EINVAL ; } insn [ 0 ] . imm = new_val ; insn [ 1 ] . imm = new_val >> 32 ; pr_debug ( \"prog '%s': relo #%d: patched insn #%d (LDIMM64) imm64 %llu -> %llu\\n\" , prog_name , relo_idx , insn_idx , ( unsigned long long ) imm , ( unsigned long long ) new_val ) ; break ; } default : pr_warn ( \"prog '%s': relo #%d: trying to relocate unrecognized insn #%d, code:0x%x, src:0x%x, dst:0x%x, off:0x%x, imm:0x%x\\n\" , prog_name , relo_idx , insn_idx , insn -> code , insn -> src_reg , insn -> dst_reg , insn -> off , insn -> imm ) ; return - EINVAL ; } return 0 ; }",
    "resources/libbpf/src/relo_core.c@bpf_core_format_spec": "int bpf_core_format_spec ( char * buf , size_t buf_sz , const struct bpf_core_spec * spec ) { const struct btf_type * t ; const char * s ; __u32 type_id ; int i , len = 0 ; # define append_buf ( fmt , args ... ) ( { int r ; r = snprintf ( buf , buf_sz , fmt , ## args ) ; len += r ; if ( r >= buf_sz ) r = buf_sz ; buf += r ; buf_sz -= r ; } ) type_id = spec -> root_type_id ; t = btf_type_by_id ( spec -> btf , type_id ) ; s = btf__name_by_offset ( spec -> btf , t -> name_off ) ; append_buf ( \"<%s> [%u] %s %s\" , core_relo_kind_str ( spec -> relo_kind ) , type_id , btf_kind_str ( t ) , str_is_empty ( s ) ? \"<anon>\" : s ) ; if ( core_relo_is_type_based ( spec -> relo_kind ) ) return len ; if ( core_relo_is_enumval_based ( spec -> relo_kind ) ) { t = skip_mods_and_typedefs ( spec -> btf , type_id , NULL ) ; if ( btf_is_enum ( t ) ) { const struct btf_enum * e ; const char * fmt_str ; e = btf_enum ( t ) + spec -> raw_spec [ 0 ] ; s = btf__name_by_offset ( spec -> btf , e -> name_off ) ; fmt_str = BTF_INFO_KFLAG ( t -> info ) ? \"::%s = %d\" : \"::%s = %u\" ; append_buf ( fmt_str , s , e -> val ) ; } else { const struct btf_enum64 * e ; const char * fmt_str ; e = btf_enum64 ( t ) + spec -> raw_spec [ 0 ] ; s = btf__name_by_offset ( spec -> btf , e -> name_off ) ; fmt_str = BTF_INFO_KFLAG ( t -> info ) ? \"::%s = %lld\" : \"::%s = %llu\" ; append_buf ( fmt_str , s , ( unsigned long long ) btf_enum64_value ( e ) ) ; } return len ; } if ( core_relo_is_field_based ( spec -> relo_kind ) ) { for ( i = 0 ; i < spec -> len ; i ++ ) { if ( spec -> spec [ i ] . name ) append_buf ( \".%s\" , spec -> spec [ i ] . name ) ; else if ( i > 0 || spec -> spec [ i ] . idx > 0 ) append_buf ( \"[%u]\" , spec -> spec [ i ] . idx ) ; } append_buf ( \" (\" ) ; for ( i = 0 ; i < spec -> raw_len ; i ++ ) append_buf ( \"%s%d\" , i == 0 ? \"\" : \":\" , spec -> raw_spec [ i ] ) ; if ( spec -> bit_offset % 8 ) append_buf ( \" @ offset %u.%u)\" , spec -> bit_offset / 8 , spec -> bit_offset % 8 ) ; else append_buf ( \" @ offset %u)\" , spec -> bit_offset / 8 ) ; return len ; } return len ; # undef append_buf }",
    "resources/libbpf/src/relo_core.c@bpf_core_calc_relo_insn": "int bpf_core_calc_relo_insn ( const char * prog_name , const struct bpf_core_relo * relo , int relo_idx , const struct btf * local_btf , struct bpf_core_cand_list * cands , struct bpf_core_spec * specs_scratch , struct bpf_core_relo_res * targ_res ) { struct bpf_core_spec * local_spec = & specs_scratch [ 0 ] ; struct bpf_core_spec * cand_spec = & specs_scratch [ 1 ] ; struct bpf_core_spec * targ_spec = & specs_scratch [ 2 ] ; struct bpf_core_relo_res cand_res ; const struct btf_type * local_type ; const char * local_name ; __u32 local_id ; char spec_buf [ 256 ] ; int i , j , err ; local_id = relo -> type_id ; local_type = btf_type_by_id ( local_btf , local_id ) ; local_name = btf__name_by_offset ( local_btf , local_type -> name_off ) ; if ( ! local_name ) return - EINVAL ; err = bpf_core_parse_spec ( prog_name , local_btf , relo , local_spec ) ; if ( err ) { const char * spec_str ; spec_str = btf__name_by_offset ( local_btf , relo -> access_str_off ) ; pr_warn ( \"prog '%s': relo #%d: parsing [%d] %s %s + %s failed: %d\\n\" , prog_name , relo_idx , local_id , btf_kind_str ( local_type ) , str_is_empty ( local_name ) ? \"<anon>\" : local_name , spec_str ? : \"<?>\" , err ) ; return - EINVAL ; } bpf_core_format_spec ( spec_buf , sizeof ( spec_buf ) , local_spec ) ; pr_debug ( \"prog '%s': relo #%d: %s\\n\" , prog_name , relo_idx , spec_buf ) ; /* TYPE_ID_LOCAL relo is special and doesn't need candidate search */ if ( relo -> kind == BPF_CORE_TYPE_ID_LOCAL ) { /* bpf_insn's imm value could get out of sync during linking */ memset ( targ_res , 0 , sizeof ( * targ_res ) ) ; targ_res -> validate = false ; targ_res -> poison = false ; targ_res -> orig_val = local_spec -> root_type_id ; targ_res -> new_val = local_spec -> root_type_id ; return 0 ; } /* libbpf doesn't support candidate search for anonymous types */ if ( str_is_empty ( local_name ) ) { pr_warn ( \"prog '%s': relo #%d: <%s> (%d) relocation doesn't support anonymous types\\n\" , prog_name , relo_idx , core_relo_kind_str ( relo -> kind ) , relo -> kind ) ; return - EOPNOTSUPP ; } for ( i = 0 , j = 0 ; i < cands -> len ; i ++ ) { err = bpf_core_spec_match ( local_spec , cands -> cands [ i ] . btf , cands -> cands [ i ] . id , cand_spec ) ; if ( err < 0 ) { bpf_core_format_spec ( spec_buf , sizeof ( spec_buf ) , cand_spec ) ; pr_warn ( \"prog '%s': relo #%d: error matching candidate #%d %s: %d\\n \" , prog_name , relo_idx , i , spec_buf , err ) ; return err ; } bpf_core_format_spec ( spec_buf , sizeof ( spec_buf ) , cand_spec ) ; pr_debug ( \"prog '%s': relo #%d: %s candidate #%d %s\\n\" , prog_name , relo_idx , err == 0 ? \"non-matching\" : \"matching\" , i , spec_buf ) ; if ( err == 0 ) continue ; err = bpf_core_calc_relo ( prog_name , relo , relo_idx , local_spec , cand_spec , & cand_res ) ; if ( err ) return err ; if ( j == 0 ) { * targ_res = cand_res ; * targ_spec = * cand_spec ; } else if ( cand_spec -> bit_offset != targ_spec -> bit_offset ) { /* if there are many field relo candidates, they\n\t\t\t * should all resolve to the same bit offset\n\t\t\t */ pr_warn ( \"prog '%s': relo #%d: field offset ambiguity: %u != %u\\n\" , prog_name , relo_idx , cand_spec -> bit_offset , targ_spec -> bit_offset ) ; return - EINVAL ; } else if ( cand_res . poison != targ_res -> poison || cand_res . new_val != targ_res -> new_val ) { /* all candidates should result in the same relocation\n\t\t\t * decision and value, otherwise it's dangerous to\n\t\t\t * proceed due to ambiguity\n\t\t\t */ pr_warn ( \"prog '%s': relo #%d: relocation decision ambiguity: %s %llu != %s %llu\\n\" , prog_name , relo_idx , cand_res . poison ? \"failure\" : \"success\" , ( unsigned long long ) cand_res . new_val , targ_res -> poison ? \"failure\" : \"success\" , ( unsigned long long ) targ_res -> new_val ) ; return - EINVAL ; } cands -> cands [ j ++ ] = cands -> cands [ i ] ; } /*\n\t * For BPF_CORE_FIELD_EXISTS relo or when used BPF program has field\n\t * existence checks or kernel version/config checks, it's expected\n\t * that we might not find any candidates. In this case, if field\n\t * wasn't found in any candidate, the list of candidates shouldn't\n\t * change at all, we'll just handle relocating appropriately,\n\t * depending on relo's kind.\n\t */ if ( j > 0 ) cands -> len = j ; /*\n\t * If no candidates were found, it might be both a programmer error,\n\t * as well as expected case, depending whether instruction w/\n\t * relocation is guarded in some way that makes it unreachable (dead\n\t * code) if relocation can't be resolved. This is handled in\n\t * bpf_core_patch_insn() uniformly by replacing that instruction with\n\t * BPF helper call insn (using invalid helper ID). If that instruction\n\t * is indeed unreachable, then it will be ignored and eliminated by\n\t * verifier. If it was an error, then verifier will complain and point\n\t * to a specific instruction number in its log.\n\t */ if ( j == 0 ) { pr_debug ( \"prog '%s': relo #%d: no matching targets found\\n\" , prog_name , relo_idx ) ; /* calculate single target relo result explicitly */ err = bpf_core_calc_relo ( prog_name , relo , relo_idx , local_spec , NULL , targ_res ) ; if ( err ) return err ; } return 0 ; }",
    "resources/libbpf/src/relo_core.c@bpf_core_names_match": "static bool bpf_core_names_match ( const struct btf * local_btf , size_t local_name_off , const struct btf * targ_btf , size_t targ_name_off ) { const char * local_n , * targ_n ; size_t local_len , targ_len ; local_n = btf__name_by_offset ( local_btf , local_name_off ) ; targ_n = btf__name_by_offset ( targ_btf , targ_name_off ) ; if ( str_is_empty ( targ_n ) ) return str_is_empty ( local_n ) ; targ_len = bpf_core_essential_name_len ( targ_n ) ; local_len = bpf_core_essential_name_len ( local_n ) ; return targ_len == local_len && strncmp ( local_n , targ_n , local_len ) == 0 ; }",
    "resources/libbpf/src/relo_core.c@bpf_core_enums_match": "static int bpf_core_enums_match ( const struct btf * local_btf , const struct btf_type * local_t , const struct btf * targ_btf , const struct btf_type * targ_t ) { __u16 local_vlen = btf_vlen ( local_t ) ; __u16 targ_vlen = btf_vlen ( targ_t ) ; int i , j ; if ( local_t -> size != targ_t -> size ) return 0 ; if ( local_vlen > targ_vlen ) return 0 ; /* iterate over the local enum's variants and make sure each has\n\t * a symbolic name correspondent in the target\n\t */ for ( i = 0 ; i < local_vlen ; i ++ ) { bool matched = false ; __u32 local_n_off , targ_n_off ; local_n_off = btf_is_enum ( local_t ) ? btf_enum ( local_t ) [ i ] . name_off : btf_enum64 ( local_t ) [ i ] . name_off ; for ( j = 0 ; j < targ_vlen ; j ++ ) { targ_n_off = btf_is_enum ( targ_t ) ? btf_enum ( targ_t ) [ j ] . name_off : btf_enum64 ( targ_t ) [ j ] . name_off ; if ( bpf_core_names_match ( local_btf , local_n_off , targ_btf , targ_n_off ) ) { matched = true ; break ; } } if ( ! matched ) return 0 ; } return 1 ; }",
    "resources/libbpf/src/relo_core.c@bpf_core_composites_match": "static int bpf_core_composites_match ( const struct btf * local_btf , const struct btf_type * local_t , const struct btf * targ_btf , const struct btf_type * targ_t , bool behind_ptr , int level ) { const struct btf_member * local_m = btf_members ( local_t ) ; __u16 local_vlen = btf_vlen ( local_t ) ; __u16 targ_vlen = btf_vlen ( targ_t ) ; int i , j , err ; if ( local_vlen > targ_vlen ) return 0 ; /* check that all local members have a match in the target */ for ( i = 0 ; i < local_vlen ; i ++ , local_m ++ ) { const struct btf_member * targ_m = btf_members ( targ_t ) ; bool matched = false ; for ( j = 0 ; j < targ_vlen ; j ++ , targ_m ++ ) { if ( ! bpf_core_names_match ( local_btf , local_m -> name_off , targ_btf , targ_m -> name_off ) ) continue ; err = __bpf_core_types_match ( local_btf , local_m -> type , targ_btf , targ_m -> type , behind_ptr , level - 1 ) ; if ( err < 0 ) return err ; if ( err > 0 ) { matched = true ; break ; } } if ( ! matched ) return 0 ; } return 1 ; }",
    "resources/libbpf/src/relo_core.c@__bpf_core_types_match": "int __bpf_core_types_match ( const struct btf * local_btf , __u32 local_id , const struct btf * targ_btf , __u32 targ_id , bool behind_ptr , int level ) { const struct btf_type * local_t , * targ_t ; int depth = 32 ; /* max recursion depth */ __u16 local_k , targ_k ; if ( level <= 0 ) return - EINVAL ; recur : depth -- ; if ( depth < 0 ) return - EINVAL ; local_t = skip_mods_and_typedefs ( local_btf , local_id , & local_id ) ; targ_t = skip_mods_and_typedefs ( targ_btf , targ_id , & targ_id ) ; if ( ! local_t || ! targ_t ) return - EINVAL ; /* While the name check happens after typedefs are skipped, root-level\n\t * typedefs would still be name-matched as that's the contract with\n\t * callers.\n\t */ if ( ! bpf_core_names_match ( local_btf , local_t -> name_off , targ_btf , targ_t -> name_off ) ) return 0 ; local_k = btf_kind ( local_t ) ; targ_k = btf_kind ( targ_t ) ; switch ( local_k ) { case BTF_KIND_UNKN : return local_k == targ_k ; case BTF_KIND_FWD : { bool local_f = BTF_INFO_KFLAG ( local_t -> info ) ; if ( behind_ptr ) { if ( local_k == targ_k ) return local_f == BTF_INFO_KFLAG ( targ_t -> info ) ; /* for forward declarations kflag dictates whether the\n\t\t\t * target is a struct (0) or union (1)\n\t\t\t */ return ( targ_k == BTF_KIND_STRUCT && ! local_f ) || ( targ_k == BTF_KIND_UNION && local_f ) ; } else { if ( local_k != targ_k ) return 0 ; /* match if the forward declaration is for the same kind */ return local_f == BTF_INFO_KFLAG ( targ_t -> info ) ; } } case BTF_KIND_ENUM : case BTF_KIND_ENUM64 : if ( ! btf_is_any_enum ( targ_t ) ) return 0 ; return bpf_core_enums_match ( local_btf , local_t , targ_btf , targ_t ) ; case BTF_KIND_STRUCT : case BTF_KIND_UNION : if ( behind_ptr ) { bool targ_f = BTF_INFO_KFLAG ( targ_t -> info ) ; if ( local_k == targ_k ) return 1 ; if ( targ_k != BTF_KIND_FWD ) return 0 ; return ( local_k == BTF_KIND_UNION ) == targ_f ; } else { if ( local_k != targ_k ) return 0 ; return bpf_core_composites_match ( local_btf , local_t , targ_btf , targ_t , behind_ptr , level ) ; } case BTF_KIND_INT : { __u8 local_sgn ; __u8 targ_sgn ; if ( local_k != targ_k ) return 0 ; local_sgn = btf_int_encoding ( local_t ) & BTF_INT_SIGNED ; targ_sgn = btf_int_encoding ( targ_t ) & BTF_INT_SIGNED ; return local_t -> size == targ_t -> size && local_sgn == targ_sgn ; } case BTF_KIND_PTR : if ( local_k != targ_k ) return 0 ; behind_ptr = true ; local_id = local_t -> type ; targ_id = targ_t -> type ; goto recur ; case BTF_KIND_ARRAY : { const struct btf_array * local_array = btf_array ( local_t ) ; const struct btf_array * targ_array = btf_array ( targ_t ) ; if ( local_k != targ_k ) return 0 ; if ( local_array -> nelems != targ_array -> nelems ) return 0 ; local_id = local_array -> type ; targ_id = targ_array -> type ; goto recur ; } case BTF_KIND_FUNC_PROTO : { struct btf_param * local_p = btf_params ( local_t ) ; struct btf_param * targ_p = btf_params ( targ_t ) ; __u16 local_vlen = btf_vlen ( local_t ) ; __u16 targ_vlen = btf_vlen ( targ_t ) ; int i , err ; if ( local_k != targ_k ) return 0 ; if ( local_vlen != targ_vlen ) return 0 ; for ( i = 0 ; i < local_vlen ; i ++ , local_p ++ , targ_p ++ ) { err = __bpf_core_types_match ( local_btf , local_p -> type , targ_btf , targ_p -> type , behind_ptr , level - 1 ) ; if ( err <= 0 ) return err ; } /* tail recurse for return type check */ local_id = local_t -> type ; targ_id = targ_t -> type ; goto recur ; } default : pr_warn ( \"unexpected kind %s relocated, local [%d], target [%d]\\n\" , btf_kind_str ( local_t ) , local_id , targ_id ) ; return 0 ; } }",
    "resources/libbpf/src/linker.c@btf_kind": "static inline __u16 btf_kind ( const struct btf_type * t ) { return BTF_INFO_KIND ( t -> info ) ; }",
    "resources/libbpf/src/linker.c@btf_vlen": "static inline __u16 btf_vlen ( const struct btf_type * t ) { return BTF_INFO_VLEN ( t -> info ) ; }",
    "resources/libbpf/src/linker.c@btf_kflag": "static inline bool btf_kflag ( const struct btf_type * t ) { return BTF_INFO_KFLAG ( t -> info ) ; }",
    "resources/libbpf/src/linker.c@btf_is_void": "static inline bool btf_is_void ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_UNKN ; }",
    "resources/libbpf/src/linker.c@btf_is_int": "static inline bool btf_is_int ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_INT ; }",
    "resources/libbpf/src/linker.c@btf_is_ptr": "static inline bool btf_is_ptr ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_PTR ; }",
    "resources/libbpf/src/linker.c@btf_is_array": "static inline bool btf_is_array ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_ARRAY ; }",
    "resources/libbpf/src/linker.c@btf_is_struct": "static inline bool btf_is_struct ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_STRUCT ; }",
    "resources/libbpf/src/linker.c@btf_is_union": "static inline bool btf_is_union ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_UNION ; }",
    "resources/libbpf/src/linker.c@btf_is_composite": "static inline bool btf_is_composite ( const struct btf_type * t ) { __u16 kind = btf_kind ( t ) ; return kind == BTF_KIND_STRUCT || kind == BTF_KIND_UNION ; }",
    "resources/libbpf/src/linker.c@btf_is_enum": "static inline bool btf_is_enum ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_ENUM ; }",
    "resources/libbpf/src/linker.c@btf_is_enum64": "static inline bool btf_is_enum64 ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_ENUM64 ; }",
    "resources/libbpf/src/linker.c@btf_is_fwd": "static inline bool btf_is_fwd ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_FWD ; }",
    "resources/libbpf/src/linker.c@btf_is_typedef": "static inline bool btf_is_typedef ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_TYPEDEF ; }",
    "resources/libbpf/src/linker.c@btf_is_volatile": "static inline bool btf_is_volatile ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_VOLATILE ; }",
    "resources/libbpf/src/linker.c@btf_is_const": "static inline bool btf_is_const ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_CONST ; }",
    "resources/libbpf/src/linker.c@btf_is_restrict": "static inline bool btf_is_restrict ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_RESTRICT ; }",
    "resources/libbpf/src/linker.c@btf_is_mod": "static inline bool btf_is_mod ( const struct btf_type * t ) { __u16 kind = btf_kind ( t ) ; return kind == BTF_KIND_VOLATILE || kind == BTF_KIND_CONST || kind == BTF_KIND_RESTRICT || kind == BTF_KIND_TYPE_TAG ; }",
    "resources/libbpf/src/linker.c@btf_is_func": "static inline bool btf_is_func ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_FUNC ; }",
    "resources/libbpf/src/linker.c@btf_is_func_proto": "static inline bool btf_is_func_proto ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_FUNC_PROTO ; }",
    "resources/libbpf/src/linker.c@btf_is_var": "static inline bool btf_is_var ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_VAR ; }",
    "resources/libbpf/src/linker.c@btf_is_datasec": "static inline bool btf_is_datasec ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_DATASEC ; }",
    "resources/libbpf/src/linker.c@btf_is_float": "static inline bool btf_is_float ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_FLOAT ; }",
    "resources/libbpf/src/linker.c@btf_is_decl_tag": "static inline bool btf_is_decl_tag ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_DECL_TAG ; }",
    "resources/libbpf/src/linker.c@btf_is_type_tag": "static inline bool btf_is_type_tag ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_TYPE_TAG ; }",
    "resources/libbpf/src/linker.c@btf_is_any_enum": "static inline bool btf_is_any_enum ( const struct btf_type * t ) { return btf_is_enum ( t ) || btf_is_enum64 ( t ) ; }",
    "resources/libbpf/src/linker.c@btf_kind_core_compat": "static inline bool btf_kind_core_compat ( const struct btf_type * t1 , const struct btf_type * t2 ) { return btf_kind ( t1 ) == btf_kind ( t2 ) || ( btf_is_any_enum ( t1 ) && btf_is_any_enum ( t2 ) ) ; }",
    "resources/libbpf/src/linker.c@btf_int_encoding": "static inline __u8 btf_int_encoding ( const struct btf_type * t ) { return BTF_INT_ENCODING ( * ( __u32 * ) ( t + 1 ) ) ; }",
    "resources/libbpf/src/linker.c@btf_int_offset": "static inline __u8 btf_int_offset ( const struct btf_type * t ) { return BTF_INT_OFFSET ( * ( __u32 * ) ( t + 1 ) ) ; }",
    "resources/libbpf/src/linker.c@btf_int_bits": "static inline __u8 btf_int_bits ( const struct btf_type * t ) { return BTF_INT_BITS ( * ( __u32 * ) ( t + 1 ) ) ; }",
    "resources/libbpf/src/linker.c@btf_array": "static inline struct btf_array * btf_array ( const struct btf_type * t ) { return ( struct btf_array * ) ( t + 1 ) ; }",
    "resources/libbpf/src/linker.c@btf_enum": "static inline struct btf_enum * btf_enum ( const struct btf_type * t ) { return ( struct btf_enum * ) ( t + 1 ) ; }",
    "resources/libbpf/src/linker.c@btf_enum64": "static inline struct btf_enum64 * btf_enum64 ( const struct btf_type * t ) { return ( struct btf_enum64 * ) ( t + 1 ) ; }",
    "resources/libbpf/src/linker.c@btf_enum64_value": "static inline __u64 btf_enum64_value ( const struct btf_enum64 * e ) { /* struct btf_enum64 is introduced in Linux 6.0, which is very\n\t * bleeding-edge. Here we are avoiding relying on struct btf_enum64\n\t * definition coming from kernel UAPI headers to support wider range\n\t * of system-wide kernel headers.\n\t *\n\t * Given this header can be also included from C++ applications, that\n\t * further restricts C tricks we can use (like using compatible\n\t * anonymous struct). So just treat struct btf_enum64 as\n\t * a three-element array of u32 and access second (lo32) and third\n\t * (hi32) elements directly.\n\t *\n\t * For reference, here is a struct btf_enum64 definition:\n\t *\n\t * const struct btf_enum64 {\n\t *\t__u32\tname_off;\n\t *\t__u32\tval_lo32;\n\t *\t__u32\tval_hi32;\n\t * };\n\t */ const __u32 * e64 = ( const __u32 * ) e ; return ( ( __u64 ) e64 [ 2 ] << 32 ) | e64 [ 1 ] ; }",
    "resources/libbpf/src/linker.c@btf_members": "static inline struct btf_member * btf_members ( const struct btf_type * t ) { return ( struct btf_member * ) ( t + 1 ) ; }",
    "resources/libbpf/src/linker.c@btf_member_bit_offset": "static inline __u32 btf_member_bit_offset ( const struct btf_type * t , __u32 member_idx ) { const struct btf_member * m = btf_members ( t ) + member_idx ; bool kflag = btf_kflag ( t ) ; return kflag ? BTF_MEMBER_BIT_OFFSET ( m -> offset ) : m -> offset ; }",
    "resources/libbpf/src/linker.c@btf_member_bitfield_size": "static inline __u32 btf_member_bitfield_size ( const struct btf_type * t , __u32 member_idx ) { const struct btf_member * m = btf_members ( t ) + member_idx ; bool kflag = btf_kflag ( t ) ; return kflag ? BTF_MEMBER_BITFIELD_SIZE ( m -> offset ) : 0 ; }",
    "resources/libbpf/src/linker.c@btf_params": "static inline struct btf_param * btf_params ( const struct btf_type * t ) { return ( struct btf_param * ) ( t + 1 ) ; }",
    "resources/libbpf/src/linker.c@btf_var": "static inline struct btf_var * btf_var ( const struct btf_type * t ) { return ( struct btf_var * ) ( t + 1 ) ; }",
    "resources/libbpf/src/linker.c@btf_var_secinfos": "static inline struct btf_var_secinfo * btf_var_secinfos ( const struct btf_type * t ) { return ( struct btf_var_secinfo * ) ( t + 1 ) ; }",
    "resources/libbpf/src/linker.c@btf_decl_tag": "static inline struct btf_decl_tag * btf_decl_tag ( const struct btf_type * t ) { return ( struct btf_decl_tag * ) ( t + 1 ) ; }",
    "resources/libbpf/src/linker.c@str_has_sfx": "static inline bool str_has_sfx ( const char * str , const char * sfx ) { size_t str_len = strlen ( str ) ; size_t sfx_len = strlen ( sfx ) ; if ( sfx_len > str_len ) return false ; return strcmp ( str + str_len - sfx_len , sfx ) == 0 ; }",
    "resources/libbpf/src/linker.c@libbpf_reallocarray": "static inline void * libbpf_reallocarray ( void * ptr , size_t nmemb , size_t size ) { size_t total ; # if __has_builtin ( __builtin_mul_overflow ) if ( unlikely ( __builtin_mul_overflow ( nmemb , size , & total ) ) ) return NULL ; # else if ( size == 0 || nmemb > ULONG_MAX / size ) return NULL ; total = nmemb * size ; # endif return realloc ( ptr , total ) ; }",
    "resources/libbpf/src/linker.c@libbpf_strlcpy": "static inline void libbpf_strlcpy ( char * dst , const char * src , size_t sz ) { size_t i ; if ( sz == 0 ) return ; sz -- ; for ( i = 0 ; i < sz && src [ i ] ; i ++ ) dst [ i ] = src [ i ] ; dst [ i ] = '\\0' ; }",
    "resources/libbpf/src/linker.c@btf_func_linkage": "static inline enum btf_func_linkage btf_func_linkage ( const struct btf_type * t ) { return ( enum btf_func_linkage ) ( int ) btf_vlen ( t ) ; }",
    "resources/libbpf/src/linker.c@btf_type_info": "static inline __u32 btf_type_info ( int kind , int vlen , int kflag ) { return ( kflag << 31 ) | ( kind << 24 ) | vlen ; }",
    "resources/libbpf/src/linker.c@libbpf_is_mem_zeroed": "static inline bool libbpf_is_mem_zeroed ( const char * p , ssize_t len ) { while ( len > 0 ) { if ( * p ) return false ; p ++ ; len -- ; } return true ; }",
    "resources/libbpf/src/linker.c@libbpf_validate_opts": "static inline bool libbpf_validate_opts ( const char * opts , size_t opts_sz , size_t user_sz , const char * type_name ) { if ( user_sz < sizeof ( size_t ) ) { pr_warn ( \"%s size (%zu) is too small\\n\" , type_name , user_sz ) ; return false ; } if ( ! libbpf_is_mem_zeroed ( opts + opts_sz , ( ssize_t ) user_sz - opts_sz ) ) { pr_warn ( \"%s has non-zero extra bytes\\n\" , type_name ) ; return false ; } return true ; }",
    "resources/libbpf/src/linker.c@libbpf_err": "static inline int libbpf_err ( int ret ) { if ( ret < 0 ) errno = - ret ; return ret ; }",
    "resources/libbpf/src/linker.c@libbpf_err_errno": "static inline int libbpf_err_errno ( int ret ) { /* errno is already assumed to be set on error */ return ret < 0 ? - errno : ret ; }",
    "resources/libbpf/src/linker.c@libbpf_err_ptr": "static inline void * libbpf_err_ptr ( int err ) { /* set errno on error, this doesn't break anything */ errno = - err ; return NULL ; }",
    "resources/libbpf/src/linker.c@libbpf_ptr": "static inline void * libbpf_ptr ( void * ret ) { /* set errno on error, this doesn't break anything */ if ( IS_ERR ( ret ) ) errno = - PTR_ERR ( ret ) ; return IS_ERR ( ret ) ? NULL : ret ; }",
    "resources/libbpf/src/linker.c@str_is_empty": "static inline bool str_is_empty ( const char * s ) { return ! s || ! s [ 0 ] ; }",
    "resources/libbpf/src/linker.c@is_ldimm64_insn": "static inline bool is_ldimm64_insn ( struct bpf_insn * insn ) { return insn -> code == ( BPF_LD | BPF_IMM | BPF_DW ) ; }",
    "resources/libbpf/src/linker.c@dup_good_fd": "static inline int dup_good_fd ( int fd ) { if ( fd < 0 ) return fd ; return fcntl ( fd , F_DUPFD_CLOEXEC , 3 ) ; }",
    "resources/libbpf/src/linker.c@ensure_good_fd": "static inline int ensure_good_fd ( int fd ) { int old_fd = fd , saved_errno ; if ( fd < 0 ) return fd ; if ( fd < 3 ) { fd = dup_good_fd ( fd ) ; saved_errno = errno ; close ( old_fd ) ; errno = saved_errno ; if ( fd < 0 ) { pr_warn ( \"failed to dup FD %d to FD > 2: %d\\n\" , old_fd , - saved_errno ) ; errno = saved_errno ; } } return fd ; }",
    "resources/libbpf/src/linker.c@sys_dup2": "static inline int sys_dup2 ( int oldfd , int newfd ) { # ifdef __NR_dup2 return syscall ( __NR_dup2 , oldfd , newfd ) ; # else return syscall ( __NR_dup3 , oldfd , newfd , 0 ) ; # endif }",
    "resources/libbpf/src/linker.c@reuse_fd": "static inline int reuse_fd ( int fixed_fd , int tmp_fd ) { int err ; err = sys_dup2 ( tmp_fd , fixed_fd ) ; err = err < 0 ? - errno : 0 ; close ( tmp_fd ) ; /* clean up temporary FD */ return err ; }",
    "resources/libbpf/src/linker.c@is_pow_of_2": "static inline bool is_pow_of_2 ( size_t x ) { return x && ( x & ( x - 1 ) ) == 0 ; }",
    "resources/libbpf/src/linker.c@bpf_linker__free": "void bpf_linker__free ( struct bpf_linker * linker ) { int i ; if ( ! linker ) return ; free ( linker -> filename ) ; if ( linker -> elf ) elf_end ( linker -> elf ) ; if ( linker -> fd >= 0 ) close ( linker -> fd ) ; strset__free ( linker -> strtab_strs ) ; btf__free ( linker -> btf ) ; btf_ext__free ( linker -> btf_ext ) ; for ( i = 1 ; i < linker -> sec_cnt ; i ++ ) { struct dst_sec * sec = & linker -> secs [ i ] ; free ( sec -> sec_name ) ; free ( sec -> raw_data ) ; free ( sec -> sec_vars ) ; free ( sec -> func_info . recs ) ; free ( sec -> line_info . recs ) ; free ( sec -> core_relo_info . recs ) ; } free ( linker -> secs ) ; free ( linker -> glob_syms ) ; free ( linker ) ; }",
    "resources/libbpf/src/linker.c@bpf_linker__new": "struct bpf_linker * bpf_linker__new ( const char * filename , struct bpf_linker_opts * opts ) { struct bpf_linker * linker ; int err ; if ( ! OPTS_VALID ( opts , bpf_linker_opts ) ) return errno = EINVAL , NULL ; if ( elf_version ( EV_CURRENT ) == EV_NONE ) { pr_warn_elf ( \"libelf initialization failed\" ) ; return errno = EINVAL , NULL ; } linker = calloc ( 1 , sizeof ( * linker ) ) ; if ( ! linker ) return errno = ENOMEM , NULL ; linker -> fd = - 1 ; err = init_output_elf ( linker , filename ) ; if ( err ) goto err_out ; return linker ; err_out : bpf_linker__free ( linker ) ; return errno = - err , NULL ; }",
    "resources/libbpf/src/linker.c@add_dst_sec": "static struct dst_sec * add_dst_sec ( struct bpf_linker * linker , const char * sec_name ) { struct dst_sec * secs = linker -> secs , * sec ; size_t new_cnt = linker -> sec_cnt ? linker -> sec_cnt + 1 : 2 ; secs = libbpf_reallocarray ( secs , new_cnt , sizeof ( * secs ) ) ; if ( ! secs ) return NULL ; /* zero out newly allocated memory */ memset ( secs + linker -> sec_cnt , 0 , ( new_cnt - linker -> sec_cnt ) * sizeof ( * secs ) ) ; linker -> secs = secs ; linker -> sec_cnt = new_cnt ; sec = & linker -> secs [ new_cnt - 1 ] ; sec -> id = new_cnt - 1 ; sec -> sec_name = strdup ( sec_name ) ; if ( ! sec -> sec_name ) return NULL ; return sec ; }",
    "resources/libbpf/src/linker.c@add_new_sym": "static Elf64_Sym * add_new_sym ( struct bpf_linker * linker , size_t * sym_idx ) { struct dst_sec * symtab = & linker -> secs [ linker -> symtab_sec_idx ] ; Elf64_Sym * syms , * sym ; size_t sym_cnt = symtab -> sec_sz / sizeof ( * sym ) ; syms = libbpf_reallocarray ( symtab -> raw_data , sym_cnt + 1 , sizeof ( * sym ) ) ; if ( ! syms ) return NULL ; sym = & syms [ sym_cnt ] ; memset ( sym , 0 , sizeof ( * sym ) ) ; symtab -> raw_data = syms ; symtab -> sec_sz += sizeof ( * sym ) ; symtab -> shdr -> sh_size += sizeof ( * sym ) ; symtab -> data -> d_size += sizeof ( * sym ) ; if ( sym_idx ) * sym_idx = sym_cnt ; return sym ; }",
    "resources/libbpf/src/linker.c@init_output_elf": "static int init_output_elf ( struct bpf_linker * linker , const char * file ) { int err , str_off ; Elf64_Sym * init_sym ; struct dst_sec * sec ; linker -> filename = strdup ( file ) ; if ( ! linker -> filename ) return - ENOMEM ; linker -> fd = open ( file , O_WRONLY | O_CREAT | O_TRUNC | O_CLOEXEC , 0644 ) ; if ( linker -> fd < 0 ) { err = - errno ; pr_warn ( \"failed to create '%s': %d\\n\" , file , err ) ; return err ; } linker -> elf = elf_begin ( linker -> fd , ELF_C_WRITE , NULL ) ; if ( ! linker -> elf ) { pr_warn_elf ( \"failed to create ELF object\" ) ; return - EINVAL ; } /* ELF header */ linker -> elf_hdr = elf64_newehdr ( linker -> elf ) ; if ( ! linker -> elf_hdr ) { pr_warn_elf ( \"failed to create ELF header\" ) ; return - EINVAL ; } linker -> elf_hdr -> e_machine = EM_BPF ; linker -> elf_hdr -> e_type = ET_REL ; # if __BYTE_ORDER__ == __ORDER_LITTLE_ENDIAN__ linker -> elf_hdr -> e_ident [ EI_DATA ] = ELFDATA2LSB ; # elif __BYTE_ORDER__ == __ORDER_BIG_ENDIAN__ linker -> elf_hdr -> e_ident [ EI_DATA ] = ELFDATA2MSB ; # else # error \"Unknown __BYTE_ORDER__\" # endif /* STRTAB */ /* initialize strset with an empty string to conform to ELF */ linker -> strtab_strs = strset__new ( INT_MAX , \"\" , sizeof ( \"\" ) ) ; if ( libbpf_get_error ( linker -> strtab_strs ) ) return libbpf_get_error ( linker -> strtab_strs ) ; sec = add_dst_sec ( linker , \".strtab\" ) ; if ( ! sec ) return - ENOMEM ; sec -> scn = elf_newscn ( linker -> elf ) ; if ( ! sec -> scn ) { pr_warn_elf ( \"failed to create STRTAB section\" ) ; return - EINVAL ; } sec -> shdr = elf64_getshdr ( sec -> scn ) ; if ( ! sec -> shdr ) return - EINVAL ; sec -> data = elf_newdata ( sec -> scn ) ; if ( ! sec -> data ) { pr_warn_elf ( \"failed to create STRTAB data\" ) ; return - EINVAL ; } str_off = strset__add_str ( linker -> strtab_strs , sec -> sec_name ) ; if ( str_off < 0 ) return str_off ; sec -> sec_idx = elf_ndxscn ( sec -> scn ) ; linker -> elf_hdr -> e_shstrndx = sec -> sec_idx ; linker -> strtab_sec_idx = sec -> sec_idx ; sec -> shdr -> sh_name = str_off ; sec -> shdr -> sh_type = SHT_STRTAB ; sec -> shdr -> sh_flags = SHF_STRINGS ; sec -> shdr -> sh_offset = 0 ; sec -> shdr -> sh_link = 0 ; sec -> shdr -> sh_info = 0 ; sec -> shdr -> sh_addralign = 1 ; sec -> shdr -> sh_size = sec -> sec_sz = 0 ; sec -> shdr -> sh_entsize = 0 ; /* SYMTAB */ sec = add_dst_sec ( linker , \".symtab\" ) ; if ( ! sec ) return - ENOMEM ; sec -> scn = elf_newscn ( linker -> elf ) ; if ( ! sec -> scn ) { pr_warn_elf ( \"failed to create SYMTAB section\" ) ; return - EINVAL ; } sec -> shdr = elf64_getshdr ( sec -> scn ) ; if ( ! sec -> shdr ) return - EINVAL ; sec -> data = elf_newdata ( sec -> scn ) ; if ( ! sec -> data ) { pr_warn_elf ( \"failed to create SYMTAB data\" ) ; return - EINVAL ; } str_off = strset__add_str ( linker -> strtab_strs , sec -> sec_name ) ; if ( str_off < 0 ) return str_off ; sec -> sec_idx = elf_ndxscn ( sec -> scn ) ; linker -> symtab_sec_idx = sec -> sec_idx ; sec -> shdr -> sh_name = str_off ; sec -> shdr -> sh_type = SHT_SYMTAB ; sec -> shdr -> sh_flags = 0 ; sec -> shdr -> sh_offset = 0 ; sec -> shdr -> sh_link = linker -> strtab_sec_idx ; /* sh_info should be one greater than the index of the last local\n\t * symbol (i.e., binding is STB_LOCAL). But why and who cares?\n\t */ sec -> shdr -> sh_info = 0 ; sec -> shdr -> sh_addralign = 8 ; sec -> shdr -> sh_entsize = sizeof ( Elf64_Sym ) ; /* .BTF */ linker -> btf = btf__new_empty ( ) ; err = libbpf_get_error ( linker -> btf ) ; if ( err ) return err ; /* add the special all-zero symbol */ init_sym = add_new_sym ( linker , NULL ) ; if ( ! init_sym ) return - EINVAL ; init_sym -> st_name = 0 ; init_sym -> st_info = 0 ; init_sym -> st_other = 0 ; init_sym -> st_shndx = SHN_UNDEF ; init_sym -> st_value = 0 ; init_sym -> st_size = 0 ; return 0 ; }",
    "resources/libbpf/src/linker.c@bpf_linker__add_file": "int bpf_linker__add_file ( struct bpf_linker * linker , const char * filename , const struct bpf_linker_file_opts * opts ) { struct src_obj obj = { } ; int err = 0 ; if ( ! OPTS_VALID ( opts , bpf_linker_file_opts ) ) return libbpf_err ( - EINVAL ) ; if ( ! linker -> elf ) return libbpf_err ( - EINVAL ) ; err = err ? : linker_load_obj_file ( linker , filename , opts , & obj ) ; err = err ? : linker_append_sec_data ( linker , & obj ) ; err = err ? : linker_append_elf_syms ( linker , & obj ) ; err = err ? : linker_append_elf_relos ( linker , & obj ) ; err = err ? : linker_append_btf ( linker , & obj ) ; err = err ? : linker_append_btf_ext ( linker , & obj ) ; /* free up src_obj resources */ free ( obj . btf_type_map ) ; btf__free ( obj . btf ) ; btf_ext__free ( obj . btf_ext ) ; free ( obj . secs ) ; free ( obj . sym_map ) ; if ( obj . elf ) elf_end ( obj . elf ) ; if ( obj . fd >= 0 ) close ( obj . fd ) ; return libbpf_err ( err ) ; }",
    "resources/libbpf/src/linker.c@is_dwarf_sec_name": "static bool is_dwarf_sec_name ( const char * name ) { /* approximation, but the actual list is too long */ return strncmp ( name , \".debug_\" , sizeof ( \".debug_\" ) - 1 ) == 0 ; }",
    "resources/libbpf/src/linker.c@is_ignored_sec": "static bool is_ignored_sec ( struct src_sec * sec ) { Elf64_Shdr * shdr = sec -> shdr ; const char * name = sec -> sec_name ; /* no special handling of .strtab */ if ( shdr -> sh_type == SHT_STRTAB ) return true ; /* ignore .llvm_addrsig section as well */ if ( shdr -> sh_type == SHT_LLVM_ADDRSIG ) return true ; /* no subprograms will lead to an empty .text section, ignore it */ if ( shdr -> sh_type == SHT_PROGBITS && shdr -> sh_size == 0 && strcmp ( sec -> sec_name , \".text\" ) == 0 ) return true ; /* DWARF sections */ if ( is_dwarf_sec_name ( sec -> sec_name ) ) return true ; if ( strncmp ( name , \".rel\" , sizeof ( \".rel\" ) - 1 ) == 0 ) { name += sizeof ( \".rel\" ) - 1 ; /* DWARF section relocations */ if ( is_dwarf_sec_name ( name ) ) return true ; /* .BTF and .BTF.ext don't need relocations */ if ( strcmp ( name , BTF_ELF_SEC ) == 0 || strcmp ( name , BTF_EXT_ELF_SEC ) == 0 ) return true ; } return false ; }",
    "resources/libbpf/src/linker.c@add_src_sec": "static struct src_sec * add_src_sec ( struct src_obj * obj , const char * sec_name ) { struct src_sec * secs = obj -> secs , * sec ; size_t new_cnt = obj -> sec_cnt ? obj -> sec_cnt + 1 : 2 ; secs = libbpf_reallocarray ( secs , new_cnt , sizeof ( * secs ) ) ; if ( ! secs ) return NULL ; /* zero out newly allocated memory */ memset ( secs + obj -> sec_cnt , 0 , ( new_cnt - obj -> sec_cnt ) * sizeof ( * secs ) ) ; obj -> secs = secs ; obj -> sec_cnt = new_cnt ; sec = & obj -> secs [ new_cnt - 1 ] ; sec -> id = new_cnt - 1 ; sec -> sec_name = sec_name ; return sec ; }",
    "resources/libbpf/src/linker.c@linker_load_obj_file": "static int linker_load_obj_file ( struct bpf_linker * linker , const char * filename , const struct bpf_linker_file_opts * opts , struct src_obj * obj ) { # if __BYTE_ORDER__ == __ORDER_LITTLE_ENDIAN__ const int host_endianness = ELFDATA2LSB ; # elif __BYTE_ORDER__ == __ORDER_BIG_ENDIAN__ const int host_endianness = ELFDATA2MSB ; # else # error \"Unknown __BYTE_ORDER__\" # endif int err = 0 ; Elf_Scn * scn ; Elf_Data * data ; Elf64_Ehdr * ehdr ; Elf64_Shdr * shdr ; struct src_sec * sec ; pr_debug ( \"linker: adding object file '%s'...\\n\" , filename ) ; obj -> filename = filename ; obj -> fd = open ( filename , O_RDONLY | O_CLOEXEC ) ; if ( obj -> fd < 0 ) { err = - errno ; pr_warn ( \"failed to open file '%s': %d\\n\" , filename , err ) ; return err ; } obj -> elf = elf_begin ( obj -> fd , ELF_C_READ_MMAP , NULL ) ; if ( ! obj -> elf ) { err = - errno ; pr_warn_elf ( \"failed to parse ELF file '%s'\" , filename ) ; return err ; } /* Sanity check ELF file high-level properties */ ehdr = elf64_getehdr ( obj -> elf ) ; if ( ! ehdr ) { err = - errno ; pr_warn_elf ( \"failed to get ELF header for %s\" , filename ) ; return err ; } if ( ehdr -> e_ident [ EI_DATA ] != host_endianness ) { err = - EOPNOTSUPP ; pr_warn_elf ( \"unsupported byte order of ELF file %s\" , filename ) ; return err ; } if ( ehdr -> e_type != ET_REL || ehdr -> e_machine != EM_BPF || ehdr -> e_ident [ EI_CLASS ] != ELFCLASS64 ) { err = - EOPNOTSUPP ; pr_warn_elf ( \"unsupported kind of ELF file %s\" , filename ) ; return err ; } if ( elf_getshdrstrndx ( obj -> elf , & obj -> shstrs_sec_idx ) ) { err = - errno ; pr_warn_elf ( \"failed to get SHSTRTAB section index for %s\" , filename ) ; return err ; } scn = NULL ; while ( ( scn = elf_nextscn ( obj -> elf , scn ) ) != NULL ) { size_t sec_idx = elf_ndxscn ( scn ) ; const char * sec_name ; shdr = elf64_getshdr ( scn ) ; if ( ! shdr ) { err = - errno ; pr_warn_elf ( \"failed to get section #%zu header for %s\" , sec_idx , filename ) ; return err ; } sec_name = elf_strptr ( obj -> elf , obj -> shstrs_sec_idx , shdr -> sh_name ) ; if ( ! sec_name ) { err = - errno ; pr_warn_elf ( \"failed to get section #%zu name for %s\" , sec_idx , filename ) ; return err ; } data = elf_getdata ( scn , 0 ) ; if ( ! data ) { err = - errno ; pr_warn_elf ( \"failed to get section #%zu (%s) data from %s\" , sec_idx , sec_name , filename ) ; return err ; } sec = add_src_sec ( obj , sec_name ) ; if ( ! sec ) return - ENOMEM ; sec -> scn = scn ; sec -> shdr = shdr ; sec -> data = data ; sec -> sec_idx = elf_ndxscn ( scn ) ; if ( is_ignored_sec ( sec ) ) { sec -> skipped = true ; continue ; } switch ( shdr -> sh_type ) { case SHT_SYMTAB : if ( obj -> symtab_sec_idx ) { err = - EOPNOTSUPP ; pr_warn ( \"multiple SYMTAB sections found, not supported\\n\" ) ; return err ; } obj -> symtab_sec_idx = sec_idx ; break ; case SHT_STRTAB : /* we'll construct our own string table */ break ; case SHT_PROGBITS : if ( strcmp ( sec_name , BTF_ELF_SEC ) == 0 ) { obj -> btf = btf__new ( data -> d_buf , shdr -> sh_size ) ; err = libbpf_get_error ( obj -> btf ) ; if ( err ) { pr_warn ( \"failed to parse .BTF from %s: %d\\n\" , filename , err ) ; return err ; } sec -> skipped = true ; continue ; } if ( strcmp ( sec_name , BTF_EXT_ELF_SEC ) == 0 ) { obj -> btf_ext = btf_ext__new ( data -> d_buf , shdr -> sh_size ) ; err = libbpf_get_error ( obj -> btf_ext ) ; if ( err ) { pr_warn ( \"failed to parse .BTF.ext from '%s': %d\\n\" , filename , err ) ; return err ; } sec -> skipped = true ; continue ; } /* data & code */ break ; case SHT_NOBITS : /* BSS */ break ; case SHT_REL : /* relocations */ break ; default : pr_warn ( \"unrecognized section #%zu (%s) in %s\\n\" , sec_idx , sec_name , filename ) ; err = - EINVAL ; return err ; } } err = err ? : linker_sanity_check_elf ( obj ) ; err = err ? : linker_sanity_check_btf ( obj ) ; err = err ? : linker_sanity_check_btf_ext ( obj ) ; err = err ? : linker_fixup_btf ( obj ) ; return err ; }",
    "resources/libbpf/src/linker.c@linker_sanity_check_elf": "static int linker_sanity_check_elf ( struct src_obj * obj ) { struct src_sec * sec ; int i , err ; if ( ! obj -> symtab_sec_idx ) { pr_warn ( \"ELF is missing SYMTAB section in %s\\n\" , obj -> filename ) ; return - EINVAL ; } if ( ! obj -> shstrs_sec_idx ) { pr_warn ( \"ELF is missing section headers STRTAB section in %s\\n\" , obj -> filename ) ; return - EINVAL ; } for ( i = 1 ; i < obj -> sec_cnt ; i ++ ) { sec = & obj -> secs [ i ] ; if ( sec -> sec_name [ 0 ] == '\\0' ) { pr_warn ( \"ELF section #%zu has empty name in %s\\n\" , sec -> sec_idx , obj -> filename ) ; return - EINVAL ; } if ( is_dwarf_sec_name ( sec -> sec_name ) ) continue ; if ( sec -> shdr -> sh_addralign && ! is_pow_of_2 ( sec -> shdr -> sh_addralign ) ) { pr_warn ( \"ELF section #%zu alignment %llu is non pow-of-2 alignment in %s\\n\" , sec -> sec_idx , ( long long unsigned ) sec -> shdr -> sh_addralign , obj -> filename ) ; return - EINVAL ; } if ( sec -> shdr -> sh_addralign != sec -> data -> d_align ) { pr_warn ( \"ELF section #%zu has inconsistent alignment addr=%llu != d=%llu in %s\\n\" , sec -> sec_idx , ( long long unsigned ) sec -> shdr -> sh_addralign , ( long long unsigned ) sec -> data -> d_align , obj -> filename ) ; return - EINVAL ; } if ( sec -> shdr -> sh_size != sec -> data -> d_size ) { pr_warn ( \"ELF section #%zu has inconsistent section size sh=%llu != d=%llu in %s\\n\" , sec -> sec_idx , ( long long unsigned ) sec -> shdr -> sh_size , ( long long unsigned ) sec -> data -> d_size , obj -> filename ) ; return - EINVAL ; } switch ( sec -> shdr -> sh_type ) { case SHT_SYMTAB : err = linker_sanity_check_elf_symtab ( obj , sec ) ; if ( err ) return err ; break ; case SHT_STRTAB : break ; case SHT_PROGBITS : if ( sec -> shdr -> sh_flags & SHF_EXECINSTR ) { if ( sec -> shdr -> sh_size % sizeof ( struct bpf_insn ) != 0 ) { pr_warn ( \"ELF section #%zu has unexpected size alignment %llu in %s\\n\" , sec -> sec_idx , ( long long unsigned ) sec -> shdr -> sh_size , obj -> filename ) ; return - EINVAL ; } } break ; case SHT_NOBITS : break ; case SHT_REL : err = linker_sanity_check_elf_relos ( obj , sec ) ; if ( err ) return err ; break ; case SHT_LLVM_ADDRSIG : break ; default : pr_warn ( \"ELF section #%zu (%s) has unrecognized type %zu in %s\\n\" , sec -> sec_idx , sec -> sec_name , ( size_t ) sec -> shdr -> sh_type , obj -> filename ) ; return - EINVAL ; } } return 0 ; }",
    "resources/libbpf/src/linker.c@linker_sanity_check_elf_symtab": "static int linker_sanity_check_elf_symtab ( struct src_obj * obj , struct src_sec * sec ) { struct src_sec * link_sec ; Elf64_Sym * sym ; int i , n ; if ( sec -> shdr -> sh_entsize != sizeof ( Elf64_Sym ) ) return - EINVAL ; if ( sec -> shdr -> sh_size % sec -> shdr -> sh_entsize != 0 ) return - EINVAL ; if ( ! sec -> shdr -> sh_link || sec -> shdr -> sh_link >= obj -> sec_cnt ) { pr_warn ( \"ELF SYMTAB section #%zu points to missing STRTAB section #%zu in %s\\n\" , sec -> sec_idx , ( size_t ) sec -> shdr -> sh_link , obj -> filename ) ; return - EINVAL ; } link_sec = & obj -> secs [ sec -> shdr -> sh_link ] ; if ( link_sec -> shdr -> sh_type != SHT_STRTAB ) { pr_warn ( \"ELF SYMTAB section #%zu points to invalid STRTAB section #%zu in %s\\n\" , sec -> sec_idx , ( size_t ) sec -> shdr -> sh_link , obj -> filename ) ; return - EINVAL ; } n = sec -> shdr -> sh_size / sec -> shdr -> sh_entsize ; sym = sec -> data -> d_buf ; for ( i = 0 ; i < n ; i ++ , sym ++ ) { int sym_type = ELF64_ST_TYPE ( sym -> st_info ) ; int sym_bind = ELF64_ST_BIND ( sym -> st_info ) ; int sym_vis = ELF64_ST_VISIBILITY ( sym -> st_other ) ; if ( i == 0 ) { if ( sym -> st_name != 0 || sym -> st_info != 0 || sym -> st_other != 0 || sym -> st_shndx != 0 || sym -> st_value != 0 || sym -> st_size != 0 ) { pr_warn ( \"ELF sym #0 is invalid in %s\\n\" , obj -> filename ) ; return - EINVAL ; } continue ; } if ( sym_bind != STB_LOCAL && sym_bind != STB_GLOBAL && sym_bind != STB_WEAK ) { pr_warn ( \"ELF sym #%d in section #%zu has unsupported symbol binding %d\\n\" , i , sec -> sec_idx , sym_bind ) ; return - EINVAL ; } if ( sym_vis != STV_DEFAULT && sym_vis != STV_HIDDEN ) { pr_warn ( \"ELF sym #%d in section #%zu has unsupported symbol visibility %d\\n\" , i , sec -> sec_idx , sym_vis ) ; return - EINVAL ; } if ( sym -> st_shndx == 0 ) { if ( sym_type != STT_NOTYPE || sym_bind == STB_LOCAL || sym -> st_value != 0 || sym -> st_size != 0 ) { pr_warn ( \"ELF sym #%d is invalid extern symbol in %s\\n\" , i , obj -> filename ) ; return - EINVAL ; } continue ; } if ( sym -> st_shndx < SHN_LORESERVE && sym -> st_shndx >= obj -> sec_cnt ) { pr_warn ( \"ELF sym #%d in section #%zu points to missing section #%zu in %s\\n\" , i , sec -> sec_idx , ( size_t ) sym -> st_shndx , obj -> filename ) ; return - EINVAL ; } if ( sym_type == STT_SECTION ) { if ( sym -> st_value != 0 ) return - EINVAL ; continue ; } } return 0 ; }",
    "resources/libbpf/src/linker.c@linker_sanity_check_elf_relos": "static int linker_sanity_check_elf_relos ( struct src_obj * obj , struct src_sec * sec ) { struct src_sec * link_sec , * sym_sec ; Elf64_Rel * relo ; int i , n ; if ( sec -> shdr -> sh_entsize != sizeof ( Elf64_Rel ) ) return - EINVAL ; if ( sec -> shdr -> sh_size % sec -> shdr -> sh_entsize != 0 ) return - EINVAL ; /* SHT_REL's sh_link should point to SYMTAB */ if ( sec -> shdr -> sh_link != obj -> symtab_sec_idx ) { pr_warn ( \"ELF relo section #%zu points to invalid SYMTAB section #%zu in %s\\n\" , sec -> sec_idx , ( size_t ) sec -> shdr -> sh_link , obj -> filename ) ; return - EINVAL ; } /* SHT_REL's sh_info points to relocated section */ if ( ! sec -> shdr -> sh_info || sec -> shdr -> sh_info >= obj -> sec_cnt ) { pr_warn ( \"ELF relo section #%zu points to missing section #%zu in %s\\n\" , sec -> sec_idx , ( size_t ) sec -> shdr -> sh_info , obj -> filename ) ; return - EINVAL ; } link_sec = & obj -> secs [ sec -> shdr -> sh_info ] ; /* .rel<secname> -> <secname> pattern is followed */ if ( strncmp ( sec -> sec_name , \".rel\" , sizeof ( \".rel\" ) - 1 ) != 0 || strcmp ( sec -> sec_name + sizeof ( \".rel\" ) - 1 , link_sec -> sec_name ) != 0 ) { pr_warn ( \"ELF relo section #%zu name has invalid name in %s\\n\" , sec -> sec_idx , obj -> filename ) ; return - EINVAL ; } /* don't further validate relocations for ignored sections */ if ( link_sec -> skipped ) return 0 ; /* relocatable section is data or instructions */ if ( link_sec -> shdr -> sh_type != SHT_PROGBITS && link_sec -> shdr -> sh_type != SHT_NOBITS ) { pr_warn ( \"ELF relo section #%zu points to invalid section #%zu in %s\\n\" , sec -> sec_idx , ( size_t ) sec -> shdr -> sh_info , obj -> filename ) ; return - EINVAL ; } /* check sanity of each relocation */ n = sec -> shdr -> sh_size / sec -> shdr -> sh_entsize ; relo = sec -> data -> d_buf ; sym_sec = & obj -> secs [ obj -> symtab_sec_idx ] ; for ( i = 0 ; i < n ; i ++ , relo ++ ) { size_t sym_idx = ELF64_R_SYM ( relo -> r_info ) ; size_t sym_type = ELF64_R_TYPE ( relo -> r_info ) ; if ( sym_type != R_BPF_64_64 && sym_type != R_BPF_64_32 && sym_type != R_BPF_64_ABS64 && sym_type != R_BPF_64_ABS32 ) { pr_warn ( \"ELF relo #%d in section #%zu has unexpected type %zu in %s\\n\" , i , sec -> sec_idx , sym_type , obj -> filename ) ; return - EINVAL ; } if ( ! sym_idx || sym_idx * sizeof ( Elf64_Sym ) >= sym_sec -> shdr -> sh_size ) { pr_warn ( \"ELF relo #%d in section #%zu points to invalid symbol #%zu in %s\\n\" , i , sec -> sec_idx , sym_idx , obj -> filename ) ; return - EINVAL ; } if ( link_sec -> shdr -> sh_flags & SHF_EXECINSTR ) { if ( relo -> r_offset % sizeof ( struct bpf_insn ) != 0 ) { pr_warn ( \"ELF relo #%d in section #%zu points to missing symbol #%zu in %s\\n\" , i , sec -> sec_idx , sym_idx , obj -> filename ) ; return - EINVAL ; } } } return 0 ; }",
    "resources/libbpf/src/linker.c@check_btf_type_id": "static int check_btf_type_id ( __u32 * type_id , void * ctx ) { struct btf * btf = ctx ; if ( * type_id >= btf__type_cnt ( btf ) ) return - EINVAL ; return 0 ; }",
    "resources/libbpf/src/linker.c@check_btf_str_off": "static int check_btf_str_off ( __u32 * str_off , void * ctx ) { struct btf * btf = ctx ; const char * s ; s = btf__str_by_offset ( btf , * str_off ) ; if ( ! s ) return - EINVAL ; return 0 ; }",
    "resources/libbpf/src/linker.c@linker_sanity_check_btf": "static int linker_sanity_check_btf ( struct src_obj * obj ) { struct btf_type * t ; int i , n , err = 0 ; if ( ! obj -> btf ) return 0 ; n = btf__type_cnt ( obj -> btf ) ; for ( i = 1 ; i < n ; i ++ ) { t = btf_type_by_id ( obj -> btf , i ) ; err = err ? : btf_type_visit_type_ids ( t , check_btf_type_id , obj -> btf ) ; err = err ? : btf_type_visit_str_offs ( t , check_btf_str_off , obj -> btf ) ; if ( err ) return err ; } return 0 ; }",
    "resources/libbpf/src/linker.c@linker_sanity_check_btf_ext": "static int linker_sanity_check_btf_ext ( struct src_obj * obj ) { int err = 0 ; if ( ! obj -> btf_ext ) return 0 ; /* can't use .BTF.ext without .BTF */ if ( ! obj -> btf ) return - EINVAL ; err = err ? : btf_ext_visit_type_ids ( obj -> btf_ext , check_btf_type_id , obj -> btf ) ; err = err ? : btf_ext_visit_str_offs ( obj -> btf_ext , check_btf_str_off , obj -> btf ) ; if ( err ) return err ; return 0 ; }",
    "resources/libbpf/src/linker.c@init_sec": "static int init_sec ( struct bpf_linker * linker , struct dst_sec * dst_sec , struct src_sec * src_sec ) { Elf_Scn * scn ; Elf_Data * data ; Elf64_Shdr * shdr ; int name_off ; dst_sec -> sec_sz = 0 ; dst_sec -> sec_idx = 0 ; dst_sec -> ephemeral = src_sec -> ephemeral ; /* ephemeral sections are just thin section shells lacking most parts */ if ( src_sec -> ephemeral ) return 0 ; scn = elf_newscn ( linker -> elf ) ; if ( ! scn ) return - ENOMEM ; data = elf_newdata ( scn ) ; if ( ! data ) return - ENOMEM ; shdr = elf64_getshdr ( scn ) ; if ( ! shdr ) return - ENOMEM ; dst_sec -> scn = scn ; dst_sec -> shdr = shdr ; dst_sec -> data = data ; dst_sec -> sec_idx = elf_ndxscn ( scn ) ; name_off = strset__add_str ( linker -> strtab_strs , src_sec -> sec_name ) ; if ( name_off < 0 ) return name_off ; shdr -> sh_name = name_off ; shdr -> sh_type = src_sec -> shdr -> sh_type ; shdr -> sh_flags = src_sec -> shdr -> sh_flags ; shdr -> sh_size = 0 ; /* sh_link and sh_info have different meaning for different types of\n\t * sections, so we leave it up to the caller code to fill them in, if\n\t * necessary\n\t */ shdr -> sh_link = 0 ; shdr -> sh_info = 0 ; shdr -> sh_addralign = src_sec -> shdr -> sh_addralign ; shdr -> sh_entsize = src_sec -> shdr -> sh_entsize ; data -> d_type = src_sec -> data -> d_type ; data -> d_size = 0 ; data -> d_buf = NULL ; data -> d_align = src_sec -> data -> d_align ; data -> d_off = 0 ; return 0 ; }",
    "resources/libbpf/src/linker.c@find_dst_sec_by_name": "static struct dst_sec * find_dst_sec_by_name ( struct bpf_linker * linker , const char * sec_name ) { struct dst_sec * sec ; int i ; for ( i = 1 ; i < linker -> sec_cnt ; i ++ ) { sec = & linker -> secs [ i ] ; if ( strcmp ( sec -> sec_name , sec_name ) == 0 ) return sec ; } return NULL ; }",
    "resources/libbpf/src/linker.c@secs_match": "static bool secs_match ( struct dst_sec * dst , struct src_sec * src ) { if ( dst -> ephemeral || src -> ephemeral ) return true ; if ( dst -> shdr -> sh_type != src -> shdr -> sh_type ) { pr_warn ( \"sec %s types mismatch\\n\" , dst -> sec_name ) ; return false ; } if ( dst -> shdr -> sh_flags != src -> shdr -> sh_flags ) { pr_warn ( \"sec %s flags mismatch\\n\" , dst -> sec_name ) ; return false ; } if ( dst -> shdr -> sh_entsize != src -> shdr -> sh_entsize ) { pr_warn ( \"sec %s entsize mismatch\\n\" , dst -> sec_name ) ; return false ; } return true ; }",
    "resources/libbpf/src/linker.c@sec_content_is_same": "static bool sec_content_is_same ( struct dst_sec * dst_sec , struct src_sec * src_sec ) { if ( dst_sec -> sec_sz != src_sec -> shdr -> sh_size ) return false ; if ( memcmp ( dst_sec -> raw_data , src_sec -> data -> d_buf , dst_sec -> sec_sz ) != 0 ) return false ; return true ; }",
    "resources/libbpf/src/linker.c@extend_sec": "static int extend_sec ( struct bpf_linker * linker , struct dst_sec * dst , struct src_sec * src ) { void * tmp ; size_t dst_align , src_align ; size_t dst_align_sz , dst_final_sz ; int err ; /* Ephemeral source section doesn't contribute anything to ELF\n\t * section data.\n\t */ if ( src -> ephemeral ) return 0 ; /* Some sections (like .maps) can contain both externs (and thus be\n\t * ephemeral) and non-externs (map definitions). So it's possible that\n\t * it has to be \"upgraded\" from ephemeral to non-ephemeral when the\n\t * first non-ephemeral entity appears. In such case, we add ELF\n\t * section, data, etc.\n\t */ if ( dst -> ephemeral ) { err = init_sec ( linker , dst , src ) ; if ( err ) return err ; } dst_align = dst -> shdr -> sh_addralign ; src_align = src -> shdr -> sh_addralign ; if ( dst_align == 0 ) dst_align = 1 ; if ( dst_align < src_align ) dst_align = src_align ; dst_align_sz = ( dst -> sec_sz + dst_align - 1 ) / dst_align * dst_align ; /* no need to re-align final size */ dst_final_sz = dst_align_sz + src -> shdr -> sh_size ; if ( src -> shdr -> sh_type != SHT_NOBITS ) { tmp = realloc ( dst -> raw_data , dst_final_sz ) ; /* If dst_align_sz == 0, realloc() behaves in a special way:\n\t\t * 1. When dst->raw_data is NULL it returns:\n\t\t *    \"either NULL or a pointer suitable to be passed to free()\" [1].\n\t\t * 2. When dst->raw_data is not-NULL it frees dst->raw_data and returns NULL,\n\t\t *    thus invalidating any \"pointer suitable to be passed to free()\" obtained\n\t\t *    at step (1).\n\t\t *\n\t\t * The dst_align_sz > 0 check avoids error exit after (2), otherwise\n\t\t * dst->raw_data would be freed again in bpf_linker__free().\n\t\t *\n\t\t * [1] man 3 realloc\n\t\t */ if ( ! tmp && dst_align_sz > 0 ) return - ENOMEM ; dst -> raw_data = tmp ; /* pad dst section, if it's alignment forced size increase */ memset ( dst -> raw_data + dst -> sec_sz , 0 , dst_align_sz - dst -> sec_sz ) ; /* now copy src data at a properly aligned offset */ memcpy ( dst -> raw_data + dst_align_sz , src -> data -> d_buf , src -> shdr -> sh_size ) ; } dst -> sec_sz = dst_final_sz ; dst -> shdr -> sh_size = dst_final_sz ; dst -> data -> d_size = dst_final_sz ; dst -> shdr -> sh_addralign = dst_align ; dst -> data -> d_align = dst_align ; src -> dst_off = dst_align_sz ; return 0 ; }",
    "resources/libbpf/src/linker.c@is_data_sec": "static bool is_data_sec ( struct src_sec * sec ) { if ( ! sec || sec -> skipped ) return false ; /* ephemeral sections are data sections, e.g., .kconfig, .ksyms */ if ( sec -> ephemeral ) return true ; return sec -> shdr -> sh_type == SHT_PROGBITS || sec -> shdr -> sh_type == SHT_NOBITS ; }",
    "resources/libbpf/src/linker.c@is_relo_sec": "static bool is_relo_sec ( struct src_sec * sec ) { if ( ! sec || sec -> skipped || sec -> ephemeral ) return false ; return sec -> shdr -> sh_type == SHT_REL ; }",
    "resources/libbpf/src/linker.c@linker_append_sec_data": "static int linker_append_sec_data ( struct bpf_linker * linker , struct src_obj * obj ) { int i , err ; for ( i = 1 ; i < obj -> sec_cnt ; i ++ ) { struct src_sec * src_sec ; struct dst_sec * dst_sec ; src_sec = & obj -> secs [ i ] ; if ( ! is_data_sec ( src_sec ) ) continue ; dst_sec = find_dst_sec_by_name ( linker , src_sec -> sec_name ) ; if ( ! dst_sec ) { dst_sec = add_dst_sec ( linker , src_sec -> sec_name ) ; if ( ! dst_sec ) return - ENOMEM ; err = init_sec ( linker , dst_sec , src_sec ) ; if ( err ) { pr_warn ( \"failed to init section '%s'\\n\" , src_sec -> sec_name ) ; return err ; } } else { if ( ! secs_match ( dst_sec , src_sec ) ) { pr_warn ( \"ELF sections %s are incompatible\\n\" , src_sec -> sec_name ) ; return - 1 ; } /* \"license\" and \"version\" sections are deduped */ if ( strcmp ( src_sec -> sec_name , \"license\" ) == 0 || strcmp ( src_sec -> sec_name , \"version\" ) == 0 ) { if ( ! sec_content_is_same ( dst_sec , src_sec ) ) { pr_warn ( \"non-identical contents of section '%s' are not supported\\n\" , src_sec -> sec_name ) ; return - EINVAL ; } src_sec -> skipped = true ; src_sec -> dst_id = dst_sec -> id ; continue ; } } /* record mapped section index */ src_sec -> dst_id = dst_sec -> id ; err = extend_sec ( linker , dst_sec , src_sec ) ; if ( err ) return err ; } return 0 ; }",
    "resources/libbpf/src/linker.c@linker_append_elf_syms": "static int linker_append_elf_syms ( struct bpf_linker * linker , struct src_obj * obj ) { struct src_sec * symtab = & obj -> secs [ obj -> symtab_sec_idx ] ; Elf64_Sym * sym = symtab -> data -> d_buf ; int i , n = symtab -> shdr -> sh_size / symtab -> shdr -> sh_entsize , err ; int str_sec_idx = symtab -> shdr -> sh_link ; const char * sym_name ; obj -> sym_map = calloc ( n + 1 , sizeof ( * obj -> sym_map ) ) ; if ( ! obj -> sym_map ) return - ENOMEM ; for ( i = 0 ; i < n ; i ++ , sym ++ ) { /* We already validated all-zero symbol #0 and we already\n\t\t * appended it preventively to the final SYMTAB, so skip it.\n\t\t */ if ( i == 0 ) continue ; sym_name = elf_strptr ( obj -> elf , str_sec_idx , sym -> st_name ) ; if ( ! sym_name ) { pr_warn ( \"can't fetch symbol name for symbol #%d in '%s'\\n\" , i , obj -> filename ) ; return - EINVAL ; } err = linker_append_elf_sym ( linker , obj , sym , sym_name , i ) ; if ( err ) return err ; } return 0 ; }",
    "resources/libbpf/src/linker.c@get_sym_by_idx": "static Elf64_Sym * get_sym_by_idx ( struct bpf_linker * linker , size_t sym_idx ) { struct dst_sec * symtab = & linker -> secs [ linker -> symtab_sec_idx ] ; Elf64_Sym * syms = symtab -> raw_data ; return & syms [ sym_idx ] ; }",
    "resources/libbpf/src/linker.c@find_glob_sym": "static struct glob_sym * find_glob_sym ( struct bpf_linker * linker , const char * sym_name ) { struct glob_sym * glob_sym ; const char * name ; int i ; for ( i = 0 ; i < linker -> glob_sym_cnt ; i ++ ) { glob_sym = & linker -> glob_syms [ i ] ; name = strset__data ( linker -> strtab_strs ) + glob_sym -> name_off ; if ( strcmp ( name , sym_name ) == 0 ) return glob_sym ; } return NULL ; }",
    "resources/libbpf/src/linker.c@add_glob_sym": "static struct glob_sym * add_glob_sym ( struct bpf_linker * linker ) { struct glob_sym * syms , * sym ; syms = libbpf_reallocarray ( linker -> glob_syms , linker -> glob_sym_cnt + 1 , sizeof ( * linker -> glob_syms ) ) ; if ( ! syms ) return NULL ; sym = & syms [ linker -> glob_sym_cnt ] ; memset ( sym , 0 , sizeof ( * sym ) ) ; sym -> var_idx = - 1 ; linker -> glob_syms = syms ; linker -> glob_sym_cnt ++ ; return sym ; }",
    "resources/libbpf/src/linker.c@glob_sym_btf_matches": "static bool glob_sym_btf_matches ( const char * sym_name , bool exact , const struct btf * btf1 , __u32 id1 , const struct btf * btf2 , __u32 id2 ) { const struct btf_type * t1 , * t2 ; bool is_static1 , is_static2 ; const char * n1 , * n2 ; int i , n ; recur : n1 = n2 = NULL ; t1 = skip_mods_and_typedefs ( btf1 , id1 , & id1 ) ; t2 = skip_mods_and_typedefs ( btf2 , id2 , & id2 ) ; /* check if only one side is FWD, otherwise handle with common logic */ if ( ! exact && btf_is_fwd ( t1 ) != btf_is_fwd ( t2 ) ) { n1 = btf__str_by_offset ( btf1 , t1 -> name_off ) ; n2 = btf__str_by_offset ( btf2 , t2 -> name_off ) ; if ( strcmp ( n1 , n2 ) != 0 ) { pr_warn ( \"global '%s': incompatible forward declaration names '%s' and '%s'\\n\" , sym_name , n1 , n2 ) ; return false ; } /* validate if FWD kind matches concrete kind */ if ( btf_is_fwd ( t1 ) ) { if ( btf_kflag ( t1 ) && btf_is_union ( t2 ) ) return true ; if ( ! btf_kflag ( t1 ) && btf_is_struct ( t2 ) ) return true ; pr_warn ( \"global '%s': incompatible %s forward declaration and concrete kind %s\\n\" , sym_name , btf_kflag ( t1 ) ? \"union\" : \"struct\" , btf_kind_str ( t2 ) ) ; } else { if ( btf_kflag ( t2 ) && btf_is_union ( t1 ) ) return true ; if ( ! btf_kflag ( t2 ) && btf_is_struct ( t1 ) ) return true ; pr_warn ( \"global '%s': incompatible %s forward declaration and concrete kind %s\\n\" , sym_name , btf_kflag ( t2 ) ? \"union\" : \"struct\" , btf_kind_str ( t1 ) ) ; } return false ; } if ( btf_kind ( t1 ) != btf_kind ( t2 ) ) { pr_warn ( \"global '%s': incompatible BTF kinds %s and %s\\n\" , sym_name , btf_kind_str ( t1 ) , btf_kind_str ( t2 ) ) ; return false ; } switch ( btf_kind ( t1 ) ) { case BTF_KIND_STRUCT : case BTF_KIND_UNION : case BTF_KIND_ENUM : case BTF_KIND_ENUM64 : case BTF_KIND_FWD : case BTF_KIND_FUNC : case BTF_KIND_VAR : n1 = btf__str_by_offset ( btf1 , t1 -> name_off ) ; n2 = btf__str_by_offset ( btf2 , t2 -> name_off ) ; if ( strcmp ( n1 , n2 ) != 0 ) { pr_warn ( \"global '%s': incompatible %s names '%s' and '%s'\\n\" , sym_name , btf_kind_str ( t1 ) , n1 , n2 ) ; return false ; } break ; default : break ; } switch ( btf_kind ( t1 ) ) { case BTF_KIND_UNKN : /* void */ case BTF_KIND_FWD : return true ; case BTF_KIND_INT : case BTF_KIND_FLOAT : case BTF_KIND_ENUM : case BTF_KIND_ENUM64 : /* ignore encoding for int and enum values for enum */ if ( t1 -> size != t2 -> size ) { pr_warn ( \"global '%s': incompatible %s '%s' size %u and %u\\n\" , sym_name , btf_kind_str ( t1 ) , n1 , t1 -> size , t2 -> size ) ; return false ; } return true ; case BTF_KIND_PTR : /* just validate overall shape of the referenced type, so no\n\t\t * contents comparison for struct/union, and allowd fwd vs\n\t\t * struct/union\n\t\t */ exact = false ; id1 = t1 -> type ; id2 = t2 -> type ; goto recur ; case BTF_KIND_ARRAY : /* ignore index type and array size */ id1 = btf_array ( t1 ) -> type ; id2 = btf_array ( t2 ) -> type ; goto recur ; case BTF_KIND_FUNC : /* extern and global linkages are compatible */ is_static1 = btf_func_linkage ( t1 ) == BTF_FUNC_STATIC ; is_static2 = btf_func_linkage ( t2 ) == BTF_FUNC_STATIC ; if ( is_static1 != is_static2 ) { pr_warn ( \"global '%s': incompatible func '%s' linkage\\n\" , sym_name , n1 ) ; return false ; } id1 = t1 -> type ; id2 = t2 -> type ; goto recur ; case BTF_KIND_VAR : /* extern and global linkages are compatible */ is_static1 = btf_var ( t1 ) -> linkage == BTF_VAR_STATIC ; is_static2 = btf_var ( t2 ) -> linkage == BTF_VAR_STATIC ; if ( is_static1 != is_static2 ) { pr_warn ( \"global '%s': incompatible var '%s' linkage\\n\" , sym_name , n1 ) ; return false ; } id1 = t1 -> type ; id2 = t2 -> type ; goto recur ; case BTF_KIND_STRUCT : case BTF_KIND_UNION : { const struct btf_member * m1 , * m2 ; if ( ! exact ) return true ; if ( btf_vlen ( t1 ) != btf_vlen ( t2 ) ) { pr_warn ( \"global '%s': incompatible number of %s fields %u and %u\\n\" , sym_name , btf_kind_str ( t1 ) , btf_vlen ( t1 ) , btf_vlen ( t2 ) ) ; return false ; } n = btf_vlen ( t1 ) ; m1 = btf_members ( t1 ) ; m2 = btf_members ( t2 ) ; for ( i = 0 ; i < n ; i ++ , m1 ++ , m2 ++ ) { n1 = btf__str_by_offset ( btf1 , m1 -> name_off ) ; n2 = btf__str_by_offset ( btf2 , m2 -> name_off ) ; if ( strcmp ( n1 , n2 ) != 0 ) { pr_warn ( \"global '%s': incompatible field #%d names '%s' and '%s'\\n\" , sym_name , i , n1 , n2 ) ; return false ; } if ( m1 -> offset != m2 -> offset ) { pr_warn ( \"global '%s': incompatible field #%d ('%s') offsets\\n\" , sym_name , i , n1 ) ; return false ; } if ( ! glob_sym_btf_matches ( sym_name , exact , btf1 , m1 -> type , btf2 , m2 -> type ) ) return false ; } return true ; } case BTF_KIND_FUNC_PROTO : { const struct btf_param * m1 , * m2 ; if ( btf_vlen ( t1 ) != btf_vlen ( t2 ) ) { pr_warn ( \"global '%s': incompatible number of %s params %u and %u\\n\" , sym_name , btf_kind_str ( t1 ) , btf_vlen ( t1 ) , btf_vlen ( t2 ) ) ; return false ; } n = btf_vlen ( t1 ) ; m1 = btf_params ( t1 ) ; m2 = btf_params ( t2 ) ; for ( i = 0 ; i < n ; i ++ , m1 ++ , m2 ++ ) { /* ignore func arg names */ if ( ! glob_sym_btf_matches ( sym_name , exact , btf1 , m1 -> type , btf2 , m2 -> type ) ) return false ; } /* now check return type as well */ id1 = t1 -> type ; id2 = t2 -> type ; goto recur ; } /* skip_mods_and_typedefs() make this impossible */ case BTF_KIND_TYPEDEF : case BTF_KIND_VOLATILE : case BTF_KIND_CONST : case BTF_KIND_RESTRICT : /* DATASECs are never compared with each other */ case BTF_KIND_DATASEC : default : pr_warn ( \"global '%s': unsupported BTF kind %s\\n\" , sym_name , btf_kind_str ( t1 ) ) ; return false ; } }",
    "resources/libbpf/src/linker.c@map_defs_match": "static bool map_defs_match ( const char * sym_name , const struct btf * main_btf , const struct btf_map_def * main_def , const struct btf_map_def * main_inner_def , const struct btf * extra_btf , const struct btf_map_def * extra_def , const struct btf_map_def * extra_inner_def ) { const char * reason ; if ( main_def -> map_type != extra_def -> map_type ) { reason = \"type\" ; goto mismatch ; } /* check key type/size match */ if ( main_def -> key_size != extra_def -> key_size ) { reason = \"key_size\" ; goto mismatch ; } if ( ! ! main_def -> key_type_id != ! ! extra_def -> key_type_id ) { reason = \"key type\" ; goto mismatch ; } if ( ( main_def -> parts & MAP_DEF_KEY_TYPE ) && ! glob_sym_btf_matches ( sym_name , true /*exact*/ , main_btf , main_def -> key_type_id , extra_btf , extra_def -> key_type_id ) ) { reason = \"key type\" ; goto mismatch ; } /* validate value type/size match */ if ( main_def -> value_size != extra_def -> value_size ) { reason = \"value_size\" ; goto mismatch ; } if ( ! ! main_def -> value_type_id != ! ! extra_def -> value_type_id ) { reason = \"value type\" ; goto mismatch ; } if ( ( main_def -> parts & MAP_DEF_VALUE_TYPE ) && ! glob_sym_btf_matches ( sym_name , true /*exact*/ , main_btf , main_def -> value_type_id , extra_btf , extra_def -> value_type_id ) ) { reason = \"key type\" ; goto mismatch ; } if ( main_def -> max_entries != extra_def -> max_entries ) { reason = \"max_entries\" ; goto mismatch ; } if ( main_def -> map_flags != extra_def -> map_flags ) { reason = \"map_flags\" ; goto mismatch ; } if ( main_def -> numa_node != extra_def -> numa_node ) { reason = \"numa_node\" ; goto mismatch ; } if ( main_def -> pinning != extra_def -> pinning ) { reason = \"pinning\" ; goto mismatch ; } if ( ( main_def -> parts & MAP_DEF_INNER_MAP ) != ( extra_def -> parts & MAP_DEF_INNER_MAP ) ) { reason = \"inner map\" ; goto mismatch ; } if ( main_def -> parts & MAP_DEF_INNER_MAP ) { char inner_map_name [ 128 ] ; snprintf ( inner_map_name , sizeof ( inner_map_name ) , \"%s.inner\" , sym_name ) ; return map_defs_match ( inner_map_name , main_btf , main_inner_def , NULL , extra_btf , extra_inner_def , NULL ) ; } return true ; mismatch : pr_warn ( \"global '%s': map %s mismatch\\n\" , sym_name , reason ) ; return false ; }",
    "resources/libbpf/src/linker.c@glob_map_defs_match": "static bool glob_map_defs_match ( const char * sym_name , struct bpf_linker * linker , struct glob_sym * glob_sym , struct src_obj * obj , Elf64_Sym * sym , int btf_id ) { struct btf_map_def dst_def = { } , dst_inner_def = { } ; struct btf_map_def src_def = { } , src_inner_def = { } ; const struct btf_type * t ; int err ; t = btf__type_by_id ( obj -> btf , btf_id ) ; if ( ! btf_is_var ( t ) ) { pr_warn ( \"global '%s': invalid map definition type [%d]\\n\" , sym_name , btf_id ) ; return false ; } t = skip_mods_and_typedefs ( obj -> btf , t -> type , NULL ) ; err = parse_btf_map_def ( sym_name , obj -> btf , t , true /*strict*/ , & src_def , & src_inner_def ) ; if ( err ) { pr_warn ( \"global '%s': invalid map definition\\n\" , sym_name ) ; return false ; } /* re-parse existing map definition */ t = btf__type_by_id ( linker -> btf , glob_sym -> btf_id ) ; t = skip_mods_and_typedefs ( linker -> btf , t -> type , NULL ) ; err = parse_btf_map_def ( sym_name , linker -> btf , t , true /*strict*/ , & dst_def , & dst_inner_def ) ; if ( err ) { /* this should not happen, because we already validated it */ pr_warn ( \"global '%s': invalid dst map definition\\n\" , sym_name ) ; return false ; } /* Currently extern map definition has to be complete and match\n\t * concrete map definition exactly. This restriction might be lifted\n\t * in the future.\n\t */ return map_defs_match ( sym_name , linker -> btf , & dst_def , & dst_inner_def , obj -> btf , & src_def , & src_inner_def ) ; }",
    "resources/libbpf/src/linker.c@glob_syms_match": "static bool glob_syms_match ( const char * sym_name , struct bpf_linker * linker , struct glob_sym * glob_sym , struct src_obj * obj , Elf64_Sym * sym , size_t sym_idx , int btf_id ) { const struct btf_type * src_t ; /* if we are dealing with externs, BTF types describing both global\n\t * and extern VARs/FUNCs should be completely present in all files\n\t */ if ( ! glob_sym -> btf_id || ! btf_id ) { pr_warn ( \"BTF info is missing for global symbol '%s'\\n\" , sym_name ) ; return false ; } src_t = btf__type_by_id ( obj -> btf , btf_id ) ; if ( ! btf_is_var ( src_t ) && ! btf_is_func ( src_t ) ) { pr_warn ( \"only extern variables and functions are supported, but got '%s' for '%s'\\n\" , btf_kind_str ( src_t ) , sym_name ) ; return false ; } /* deal with .maps definitions specially */ if ( glob_sym -> sec_id && strcmp ( linker -> secs [ glob_sym -> sec_id ] . sec_name , MAPS_ELF_SEC ) == 0 ) return glob_map_defs_match ( sym_name , linker , glob_sym , obj , sym , btf_id ) ; if ( ! glob_sym_btf_matches ( sym_name , true /*exact*/ , linker -> btf , glob_sym -> btf_id , obj -> btf , btf_id ) ) return false ; return true ; }",
    "resources/libbpf/src/linker.c@btf_is_non_static": "static bool btf_is_non_static ( const struct btf_type * t ) { return ( btf_is_var ( t ) && btf_var ( t ) -> linkage != BTF_VAR_STATIC ) || ( btf_is_func ( t ) && btf_func_linkage ( t ) != BTF_FUNC_STATIC ) ; }",
    "resources/libbpf/src/linker.c@find_glob_sym_btf": "static int find_glob_sym_btf ( struct src_obj * obj , Elf64_Sym * sym , const char * sym_name , int * out_btf_sec_id , int * out_btf_id ) { int i , j , n , m , btf_id = 0 ; const struct btf_type * t ; const struct btf_var_secinfo * vi ; const char * name ; if ( ! obj -> btf ) { pr_warn ( \"failed to find BTF info for object '%s'\\n\" , obj -> filename ) ; return - EINVAL ; } n = btf__type_cnt ( obj -> btf ) ; for ( i = 1 ; i < n ; i ++ ) { t = btf__type_by_id ( obj -> btf , i ) ; /* some global and extern FUNCs and VARs might not be associated with any\n\t\t * DATASEC, so try to detect them in the same pass\n\t\t */ if ( btf_is_non_static ( t ) ) { name = btf__str_by_offset ( obj -> btf , t -> name_off ) ; if ( strcmp ( name , sym_name ) != 0 ) continue ; /* remember and still try to find DATASEC */ btf_id = i ; continue ; } if ( ! btf_is_datasec ( t ) ) continue ; vi = btf_var_secinfos ( t ) ; for ( j = 0 , m = btf_vlen ( t ) ; j < m ; j ++ , vi ++ ) { t = btf__type_by_id ( obj -> btf , vi -> type ) ; name = btf__str_by_offset ( obj -> btf , t -> name_off ) ; if ( strcmp ( name , sym_name ) != 0 ) continue ; if ( btf_is_var ( t ) && btf_var ( t ) -> linkage == BTF_VAR_STATIC ) continue ; if ( btf_is_func ( t ) && btf_func_linkage ( t ) == BTF_FUNC_STATIC ) continue ; if ( btf_id && btf_id != vi -> type ) { pr_warn ( \"global/extern '%s' BTF is ambiguous: both types #%d and #%u match\\n\" , sym_name , btf_id , vi -> type ) ; return - EINVAL ; } * out_btf_sec_id = i ; * out_btf_id = vi -> type ; return 0 ; } } /* free-floating extern or global FUNC */ if ( btf_id ) { * out_btf_sec_id = 0 ; * out_btf_id = btf_id ; return 0 ; } pr_warn ( \"failed to find BTF info for global/extern symbol '%s'\\n\" , sym_name ) ; return - ENOENT ; }",
    "resources/libbpf/src/linker.c@find_src_sec_by_name": "static struct src_sec * find_src_sec_by_name ( struct src_obj * obj , const char * sec_name ) { struct src_sec * sec ; int i ; for ( i = 1 ; i < obj -> sec_cnt ; i ++ ) { sec = & obj -> secs [ i ] ; if ( strcmp ( sec -> sec_name , sec_name ) == 0 ) return sec ; } return NULL ; }",
    "resources/libbpf/src/linker.c@complete_extern_btf_info": "static int complete_extern_btf_info ( struct btf * dst_btf , int dst_id , struct btf * src_btf , int src_id ) { struct btf_type * dst_t = btf_type_by_id ( dst_btf , dst_id ) ; struct btf_type * src_t = btf_type_by_id ( src_btf , src_id ) ; struct btf_param * src_p , * dst_p ; const char * s ; int i , n , off ; /* We already made sure that source and destination types (FUNC or\n\t * VAR) match in terms of types and argument names.\n\t */ if ( btf_is_var ( dst_t ) ) { btf_var ( dst_t ) -> linkage = BTF_VAR_GLOBAL_ALLOCATED ; return 0 ; } dst_t -> info = btf_type_info ( BTF_KIND_FUNC , BTF_FUNC_GLOBAL , 0 ) ; /* now onto FUNC_PROTO types */ src_t = btf_type_by_id ( src_btf , src_t -> type ) ; dst_t = btf_type_by_id ( dst_btf , dst_t -> type ) ; /* Fill in all the argument names, which for extern FUNCs are missing.\n\t * We'll end up with two copies of FUNCs/VARs for externs, but that\n\t * will be taken care of by BTF dedup at the very end.\n\t * It might be that BTF types for extern in one file has less/more BTF\n\t * information (e.g., FWD instead of full STRUCT/UNION information),\n\t * but that should be (in most cases, subject to BTF dedup rules)\n\t * handled and resolved by BTF dedup algorithm as well, so we won't\n\t * worry about it. Our only job is to make sure that argument names\n\t * are populated on both sides, otherwise BTF dedup will pedantically\n\t * consider them different.\n\t */ src_p = btf_params ( src_t ) ; dst_p = btf_params ( dst_t ) ; for ( i = 0 , n = btf_vlen ( dst_t ) ; i < n ; i ++ , src_p ++ , dst_p ++ ) { if ( ! src_p -> name_off ) continue ; /* src_btf has more complete info, so add name to dst_btf */ s = btf__str_by_offset ( src_btf , src_p -> name_off ) ; off = btf__add_str ( dst_btf , s ) ; if ( off < 0 ) return off ; dst_p -> name_off = off ; } return 0 ; }",
    "resources/libbpf/src/linker.c@sym_update_bind": "static void sym_update_bind ( Elf64_Sym * sym , int sym_bind ) { sym -> st_info = ELF64_ST_INFO ( sym_bind , ELF64_ST_TYPE ( sym -> st_info ) ) ; }",
    "resources/libbpf/src/linker.c@sym_update_type": "static void sym_update_type ( Elf64_Sym * sym , int sym_type ) { sym -> st_info = ELF64_ST_INFO ( ELF64_ST_BIND ( sym -> st_info ) , sym_type ) ; }",
    "resources/libbpf/src/linker.c@sym_update_visibility": "static void sym_update_visibility ( Elf64_Sym * sym , int sym_vis ) { /* libelf doesn't provide setters for ST_VISIBILITY,\n\t * but it is stored in the lower 2 bits of st_other\n\t */ sym -> st_other &= ~ 0x03 ; sym -> st_other |= sym_vis ; }",
    "resources/libbpf/src/linker.c@linker_append_elf_sym": "static int linker_append_elf_sym ( struct bpf_linker * linker , struct src_obj * obj , Elf64_Sym * sym , const char * sym_name , int src_sym_idx ) { struct src_sec * src_sec = NULL ; struct dst_sec * dst_sec = NULL ; struct glob_sym * glob_sym = NULL ; int name_off , sym_type , sym_bind , sym_vis , err ; int btf_sec_id = 0 , btf_id = 0 ; size_t dst_sym_idx ; Elf64_Sym * dst_sym ; bool sym_is_extern ; sym_type = ELF64_ST_TYPE ( sym -> st_info ) ; sym_bind = ELF64_ST_BIND ( sym -> st_info ) ; sym_vis = ELF64_ST_VISIBILITY ( sym -> st_other ) ; sym_is_extern = sym -> st_shndx == SHN_UNDEF ; if ( sym_is_extern ) { if ( ! obj -> btf ) { pr_warn ( \"externs without BTF info are not supported\\n\" ) ; return - ENOTSUP ; } } else if ( sym -> st_shndx < SHN_LORESERVE ) { src_sec = & obj -> secs [ sym -> st_shndx ] ; if ( src_sec -> skipped ) return 0 ; dst_sec = & linker -> secs [ src_sec -> dst_id ] ; /* allow only one STT_SECTION symbol per section */ if ( sym_type == STT_SECTION && dst_sec -> sec_sym_idx ) { obj -> sym_map [ src_sym_idx ] = dst_sec -> sec_sym_idx ; return 0 ; } } if ( sym_bind == STB_LOCAL ) goto add_sym ; /* find matching BTF info */ err = find_glob_sym_btf ( obj , sym , sym_name , & btf_sec_id , & btf_id ) ; if ( err ) return err ; if ( sym_is_extern && btf_sec_id ) { const char * sec_name = NULL ; const struct btf_type * t ; t = btf__type_by_id ( obj -> btf , btf_sec_id ) ; sec_name = btf__str_by_offset ( obj -> btf , t -> name_off ) ; /* Clang puts unannotated extern vars into\n\t\t * '.extern' BTF DATASEC. Treat them the same\n\t\t * as unannotated extern funcs (which are\n\t\t * currently not put into any DATASECs).\n\t\t * Those don't have associated src_sec/dst_sec.\n\t\t */ if ( strcmp ( sec_name , BTF_EXTERN_SEC ) != 0 ) { src_sec = find_src_sec_by_name ( obj , sec_name ) ; if ( ! src_sec ) { pr_warn ( \"failed to find matching ELF sec '%s'\\n\" , sec_name ) ; return - ENOENT ; } dst_sec = & linker -> secs [ src_sec -> dst_id ] ; } } glob_sym = find_glob_sym ( linker , sym_name ) ; if ( glob_sym ) { /* Preventively resolve to existing symbol. This is\n\t\t * needed for further relocation symbol remapping in\n\t\t * the next step of linking.\n\t\t */ obj -> sym_map [ src_sym_idx ] = glob_sym -> sym_idx ; /* If both symbols are non-externs, at least one of\n\t\t * them has to be STB_WEAK, otherwise they are in\n\t\t * a conflict with each other.\n\t\t */ if ( ! sym_is_extern && ! glob_sym -> is_extern && ! glob_sym -> is_weak && sym_bind != STB_WEAK ) { pr_warn ( \"conflicting non-weak symbol #%d (%s) definition in '%s'\\n\" , src_sym_idx , sym_name , obj -> filename ) ; return - EINVAL ; } if ( ! glob_syms_match ( sym_name , linker , glob_sym , obj , sym , src_sym_idx , btf_id ) ) return - EINVAL ; dst_sym = get_sym_by_idx ( linker , glob_sym -> sym_idx ) ; /* If new symbol is strong, then force dst_sym to be strong as\n\t\t * well; this way a mix of weak and non-weak extern\n\t\t * definitions will end up being strong.\n\t\t */ if ( sym_bind == STB_GLOBAL ) { /* We still need to preserve type (NOTYPE or\n\t\t\t * OBJECT/FUNC, depending on whether the symbol is\n\t\t\t * extern or not)\n\t\t\t */ sym_update_bind ( dst_sym , STB_GLOBAL ) ; glob_sym -> is_weak = false ; } /* Non-default visibility is \"contaminating\", with stricter\n\t\t * visibility overwriting more permissive ones, even if more\n\t\t * permissive visibility comes from just an extern definition.\n\t\t * Currently only STV_DEFAULT and STV_HIDDEN are allowed and\n\t\t * ensured by ELF symbol sanity checks above.\n\t\t */ if ( sym_vis > ELF64_ST_VISIBILITY ( dst_sym -> st_other ) ) sym_update_visibility ( dst_sym , sym_vis ) ; /* If the new symbol is extern, then regardless if\n\t\t * existing symbol is extern or resolved global, just\n\t\t * keep the existing one untouched.\n\t\t */ if ( sym_is_extern ) return 0 ; /* If existing symbol is a strong resolved symbol, bail out,\n\t\t * because we lost resolution battle have nothing to\n\t\t * contribute. We already checked abover that there is no\n\t\t * strong-strong conflict. We also already tightened binding\n\t\t * and visibility, so nothing else to contribute at that point.\n\t\t */ if ( ! glob_sym -> is_extern && sym_bind == STB_WEAK ) return 0 ; /* At this point, new symbol is strong non-extern,\n\t\t * so overwrite glob_sym with new symbol information.\n\t\t * Preserve binding and visibility.\n\t\t */ sym_update_type ( dst_sym , sym_type ) ; dst_sym -> st_shndx = dst_sec -> sec_idx ; dst_sym -> st_value = src_sec -> dst_off + sym -> st_value ; dst_sym -> st_size = sym -> st_size ; /* see comment below about dst_sec->id vs dst_sec->sec_idx */ glob_sym -> sec_id = dst_sec -> id ; glob_sym -> is_extern = false ; if ( complete_extern_btf_info ( linker -> btf , glob_sym -> btf_id , obj -> btf , btf_id ) ) return - EINVAL ; /* request updating VAR's/FUNC's underlying BTF type when appending BTF type */ glob_sym -> underlying_btf_id = 0 ; obj -> sym_map [ src_sym_idx ] = glob_sym -> sym_idx ; return 0 ; } add_sym : name_off = strset__add_str ( linker -> strtab_strs , sym_name ) ; if ( name_off < 0 ) return name_off ; dst_sym = add_new_sym ( linker , & dst_sym_idx ) ; if ( ! dst_sym ) return - ENOMEM ; dst_sym -> st_name = name_off ; dst_sym -> st_info = sym -> st_info ; dst_sym -> st_other = sym -> st_other ; dst_sym -> st_shndx = dst_sec ? dst_sec -> sec_idx : sym -> st_shndx ; dst_sym -> st_value = ( src_sec ? src_sec -> dst_off : 0 ) + sym -> st_value ; dst_sym -> st_size = sym -> st_size ; obj -> sym_map [ src_sym_idx ] = dst_sym_idx ; if ( sym_type == STT_SECTION && dst_sym ) { dst_sec -> sec_sym_idx = dst_sym_idx ; dst_sym -> st_value = 0 ; } if ( sym_bind != STB_LOCAL ) { glob_sym = add_glob_sym ( linker ) ; if ( ! glob_sym ) return - ENOMEM ; glob_sym -> sym_idx = dst_sym_idx ; /* we use dst_sec->id (and not dst_sec->sec_idx), because\n\t\t * ephemeral sections (.kconfig, .ksyms, etc) don't have\n\t\t * sec_idx (as they don't have corresponding ELF section), but\n\t\t * still have id. .extern doesn't have even ephemeral section\n\t\t * associated with it, so dst_sec->id == dst_sec->sec_idx == 0.\n\t\t */ glob_sym -> sec_id = dst_sec ? dst_sec -> id : 0 ; glob_sym -> name_off = name_off ; /* we will fill btf_id in during BTF merging step */ glob_sym -> btf_id = 0 ; glob_sym -> is_extern = sym_is_extern ; glob_sym -> is_weak = sym_bind == STB_WEAK ; } return 0 ; }",
    "resources/libbpf/src/linker.c@linker_append_elf_relos": "static int linker_append_elf_relos ( struct bpf_linker * linker , struct src_obj * obj ) { struct src_sec * src_symtab = & obj -> secs [ obj -> symtab_sec_idx ] ; int i , err ; for ( i = 1 ; i < obj -> sec_cnt ; i ++ ) { struct src_sec * src_sec , * src_linked_sec ; struct dst_sec * dst_sec , * dst_linked_sec ; Elf64_Rel * src_rel , * dst_rel ; int j , n ; src_sec = & obj -> secs [ i ] ; if ( ! is_relo_sec ( src_sec ) ) continue ; /* shdr->sh_info points to relocatable section */ src_linked_sec = & obj -> secs [ src_sec -> shdr -> sh_info ] ; if ( src_linked_sec -> skipped ) continue ; dst_sec = find_dst_sec_by_name ( linker , src_sec -> sec_name ) ; if ( ! dst_sec ) { dst_sec = add_dst_sec ( linker , src_sec -> sec_name ) ; if ( ! dst_sec ) return - ENOMEM ; err = init_sec ( linker , dst_sec , src_sec ) ; if ( err ) { pr_warn ( \"failed to init section '%s'\\n\" , src_sec -> sec_name ) ; return err ; } } else if ( ! secs_match ( dst_sec , src_sec ) ) { pr_warn ( \"sections %s are not compatible\\n\" , src_sec -> sec_name ) ; return - 1 ; } /* shdr->sh_link points to SYMTAB */ dst_sec -> shdr -> sh_link = linker -> symtab_sec_idx ; /* shdr->sh_info points to relocated section */ dst_linked_sec = & linker -> secs [ src_linked_sec -> dst_id ] ; dst_sec -> shdr -> sh_info = dst_linked_sec -> sec_idx ; src_sec -> dst_id = dst_sec -> id ; err = extend_sec ( linker , dst_sec , src_sec ) ; if ( err ) return err ; src_rel = src_sec -> data -> d_buf ; dst_rel = dst_sec -> raw_data + src_sec -> dst_off ; n = src_sec -> shdr -> sh_size / src_sec -> shdr -> sh_entsize ; for ( j = 0 ; j < n ; j ++ , src_rel ++ , dst_rel ++ ) { size_t src_sym_idx , dst_sym_idx , sym_type ; Elf64_Sym * src_sym ; src_sym_idx = ELF64_R_SYM ( src_rel -> r_info ) ; src_sym = src_symtab -> data -> d_buf + sizeof ( * src_sym ) * src_sym_idx ; dst_sym_idx = obj -> sym_map [ src_sym_idx ] ; dst_rel -> r_offset += src_linked_sec -> dst_off ; sym_type = ELF64_R_TYPE ( src_rel -> r_info ) ; dst_rel -> r_info = ELF64_R_INFO ( dst_sym_idx , sym_type ) ; if ( ELF64_ST_TYPE ( src_sym -> st_info ) == STT_SECTION ) { struct src_sec * sec = & obj -> secs [ src_sym -> st_shndx ] ; struct bpf_insn * insn ; if ( src_linked_sec -> shdr -> sh_flags & SHF_EXECINSTR ) { /* calls to the very first static function inside\n\t\t\t\t\t * .text section at offset 0 will\n\t\t\t\t\t * reference section symbol, not the\n\t\t\t\t\t * function symbol. Fix that up,\n\t\t\t\t\t * otherwise it won't be possible to\n\t\t\t\t\t * relocate calls to two different\n\t\t\t\t\t * static functions with the same name\n\t\t\t\t\t * (rom two different object files)\n\t\t\t\t\t */ insn = dst_linked_sec -> raw_data + dst_rel -> r_offset ; if ( insn -> code == ( BPF_JMP | BPF_CALL ) ) insn -> imm += sec -> dst_off / sizeof ( struct bpf_insn ) ; else insn -> imm += sec -> dst_off ; } else { pr_warn ( \"relocation against STT_SECTION in non-exec section is not supported!\\n\" ) ; return - EINVAL ; } } } } return 0 ; }",
    "resources/libbpf/src/linker.c@find_sym_by_name": "static Elf64_Sym * find_sym_by_name ( struct src_obj * obj , size_t sec_idx , int sym_type , const char * sym_name ) { struct src_sec * symtab = & obj -> secs [ obj -> symtab_sec_idx ] ; Elf64_Sym * sym = symtab -> data -> d_buf ; int i , n = symtab -> shdr -> sh_size / symtab -> shdr -> sh_entsize ; int str_sec_idx = symtab -> shdr -> sh_link ; const char * name ; for ( i = 0 ; i < n ; i ++ , sym ++ ) { if ( sym -> st_shndx != sec_idx ) continue ; if ( ELF64_ST_TYPE ( sym -> st_info ) != sym_type ) continue ; name = elf_strptr ( obj -> elf , str_sec_idx , sym -> st_name ) ; if ( ! name ) return NULL ; if ( strcmp ( sym_name , name ) != 0 ) continue ; return sym ; } return NULL ; }",
    "resources/libbpf/src/linker.c@linker_fixup_btf": "static int linker_fixup_btf ( struct src_obj * obj ) { const char * sec_name ; struct src_sec * sec ; int i , j , n , m ; if ( ! obj -> btf ) return 0 ; n = btf__type_cnt ( obj -> btf ) ; for ( i = 1 ; i < n ; i ++ ) { struct btf_var_secinfo * vi ; struct btf_type * t ; t = btf_type_by_id ( obj -> btf , i ) ; if ( btf_kind ( t ) != BTF_KIND_DATASEC ) continue ; sec_name = btf__str_by_offset ( obj -> btf , t -> name_off ) ; sec = find_src_sec_by_name ( obj , sec_name ) ; if ( sec ) { /* record actual section size, unless ephemeral */ if ( sec -> shdr ) t -> size = sec -> shdr -> sh_size ; } else { /* BTF can have some sections that are not represented\n\t\t\t * in ELF, e.g., .kconfig, .ksyms, .extern, which are used\n\t\t\t * for special extern variables.\n\t\t\t *\n\t\t\t * For all but one such special (ephemeral)\n\t\t\t * sections, we pre-create \"section shells\" to be able\n\t\t\t * to keep track of extra per-section metadata later\n\t\t\t * (e.g., those BTF extern variables).\n\t\t\t *\n\t\t\t * .extern is even more special, though, because it\n\t\t\t * contains extern variables that need to be resolved\n\t\t\t * by static linker, not libbpf and kernel. When such\n\t\t\t * externs are resolved, we are going to remove them\n\t\t\t * from .extern BTF section and might end up not\n\t\t\t * needing it at all. Each resolved extern should have\n\t\t\t * matching non-extern VAR/FUNC in other sections.\n\t\t\t *\n\t\t\t * We do support leaving some of the externs\n\t\t\t * unresolved, though, to support cases of building\n\t\t\t * libraries, which will later be linked against final\n\t\t\t * BPF applications. So if at finalization we still\n\t\t\t * see unresolved externs, we'll create .extern\n\t\t\t * section on our own.\n\t\t\t */ if ( strcmp ( sec_name , BTF_EXTERN_SEC ) == 0 ) continue ; sec = add_src_sec ( obj , sec_name ) ; if ( ! sec ) return - ENOMEM ; sec -> ephemeral = true ; sec -> sec_idx = 0 ; /* will match UNDEF shndx in ELF */ } /* remember ELF section and its BTF type ID match */ sec -> sec_type_id = i ; /* fix up variable offsets */ vi = btf_var_secinfos ( t ) ; for ( j = 0 , m = btf_vlen ( t ) ; j < m ; j ++ , vi ++ ) { const struct btf_type * vt = btf__type_by_id ( obj -> btf , vi -> type ) ; const char * var_name = btf__str_by_offset ( obj -> btf , vt -> name_off ) ; int var_linkage = btf_var ( vt ) -> linkage ; Elf64_Sym * sym ; /* no need to patch up static or extern vars */ if ( var_linkage != BTF_VAR_GLOBAL_ALLOCATED ) continue ; sym = find_sym_by_name ( obj , sec -> sec_idx , STT_OBJECT , var_name ) ; if ( ! sym ) { pr_warn ( \"failed to find symbol for variable '%s' in section '%s'\\n\" , var_name , sec_name ) ; return - ENOENT ; } vi -> offset = sym -> st_value ; } } return 0 ; }",
    "resources/libbpf/src/linker.c@remap_type_id": "static int remap_type_id ( __u32 * type_id , void * ctx ) { int * id_map = ctx ; int new_id = id_map [ * type_id ] ; /* Error out if the type wasn't remapped. Ignore VOID which stays VOID. */ if ( new_id == 0 && * type_id != 0 ) { pr_warn ( \"failed to find new ID mapping for original BTF type ID %u\\n\" , * type_id ) ; return - EINVAL ; } * type_id = id_map [ * type_id ] ; return 0 ; }",
    "resources/libbpf/src/linker.c@linker_append_btf": "static int linker_append_btf ( struct bpf_linker * linker , struct src_obj * obj ) { const struct btf_type * t ; int i , j , n , start_id , id ; const char * name ; if ( ! obj -> btf ) return 0 ; start_id = btf__type_cnt ( linker -> btf ) ; n = btf__type_cnt ( obj -> btf ) ; obj -> btf_type_map = calloc ( n + 1 , sizeof ( int ) ) ; if ( ! obj -> btf_type_map ) return - ENOMEM ; for ( i = 1 ; i < n ; i ++ ) { struct glob_sym * glob_sym = NULL ; t = btf__type_by_id ( obj -> btf , i ) ; /* DATASECs are handled specially below */ if ( btf_kind ( t ) == BTF_KIND_DATASEC ) continue ; if ( btf_is_non_static ( t ) ) { /* there should be glob_sym already */ name = btf__str_by_offset ( obj -> btf , t -> name_off ) ; glob_sym = find_glob_sym ( linker , name ) ; /* VARs without corresponding glob_sym are those that\n\t\t\t * belong to skipped/deduplicated sections (i.e.,\n\t\t\t * license and version), so just skip them\n\t\t\t */ if ( ! glob_sym ) continue ; /* linker_append_elf_sym() might have requested\n\t\t\t * updating underlying type ID, if extern was resolved\n\t\t\t * to strong symbol or weak got upgraded to non-weak\n\t\t\t */ if ( glob_sym -> underlying_btf_id == 0 ) glob_sym -> underlying_btf_id = - t -> type ; /* globals from previous object files that match our\n\t\t\t * VAR/FUNC already have a corresponding associated\n\t\t\t * BTF type, so just make sure to use it\n\t\t\t */ if ( glob_sym -> btf_id ) { /* reuse existing BTF type for global var/func */ obj -> btf_type_map [ i ] = glob_sym -> btf_id ; continue ; } } id = btf__add_type ( linker -> btf , obj -> btf , t ) ; if ( id < 0 ) { pr_warn ( \"failed to append BTF type #%d from file '%s'\\n\" , i , obj -> filename ) ; return id ; } obj -> btf_type_map [ i ] = id ; /* record just appended BTF type for var/func */ if ( glob_sym ) { glob_sym -> btf_id = id ; glob_sym -> underlying_btf_id = - t -> type ; } } /* remap all the types except DATASECs */ n = btf__type_cnt ( linker -> btf ) ; for ( i = start_id ; i < n ; i ++ ) { struct btf_type * dst_t = btf_type_by_id ( linker -> btf , i ) ; if ( btf_type_visit_type_ids ( dst_t , remap_type_id , obj -> btf_type_map ) ) return - EINVAL ; } /* Rewrite VAR/FUNC underlying types (i.e., FUNC's FUNC_PROTO and VAR's\n\t * actual type), if necessary\n\t */ for ( i = 0 ; i < linker -> glob_sym_cnt ; i ++ ) { struct glob_sym * glob_sym = & linker -> glob_syms [ i ] ; struct btf_type * glob_t ; if ( glob_sym -> underlying_btf_id >= 0 ) continue ; glob_sym -> underlying_btf_id = obj -> btf_type_map [ - glob_sym -> underlying_btf_id ] ; glob_t = btf_type_by_id ( linker -> btf , glob_sym -> btf_id ) ; glob_t -> type = glob_sym -> underlying_btf_id ; } /* append DATASEC info */ for ( i = 1 ; i < obj -> sec_cnt ; i ++ ) { struct src_sec * src_sec ; struct dst_sec * dst_sec ; const struct btf_var_secinfo * src_var ; struct btf_var_secinfo * dst_var ; src_sec = & obj -> secs [ i ] ; if ( ! src_sec -> sec_type_id || src_sec -> skipped ) continue ; dst_sec = & linker -> secs [ src_sec -> dst_id ] ; /* Mark section as having BTF regardless of the presence of\n\t\t * variables. In some cases compiler might generate empty BTF\n\t\t * with no variables information. E.g., when promoting local\n\t\t * array/structure variable initial values and BPF object\n\t\t * file otherwise has no read-only static variables in\n\t\t * .rodata. We need to preserve such empty BTF and just set\n\t\t * correct section size.\n\t\t */ dst_sec -> has_btf = true ; t = btf__type_by_id ( obj -> btf , src_sec -> sec_type_id ) ; src_var = btf_var_secinfos ( t ) ; n = btf_vlen ( t ) ; for ( j = 0 ; j < n ; j ++ , src_var ++ ) { void * sec_vars = dst_sec -> sec_vars ; int new_id = obj -> btf_type_map [ src_var -> type ] ; struct glob_sym * glob_sym = NULL ; t = btf_type_by_id ( linker -> btf , new_id ) ; if ( btf_is_non_static ( t ) ) { name = btf__str_by_offset ( linker -> btf , t -> name_off ) ; glob_sym = find_glob_sym ( linker , name ) ; if ( glob_sym -> sec_id != dst_sec -> id ) { pr_warn ( \"global '%s': section mismatch %d vs %d\\n\" , name , glob_sym -> sec_id , dst_sec -> id ) ; return - EINVAL ; } } /* If there is already a member (VAR or FUNC) mapped\n\t\t\t * to the same type, don't add a duplicate entry.\n\t\t\t * This will happen when multiple object files define\n\t\t\t * the same extern VARs/FUNCs.\n\t\t\t */ if ( glob_sym && glob_sym -> var_idx >= 0 ) { __s64 sz ; dst_var = & dst_sec -> sec_vars [ glob_sym -> var_idx ] ; /* Because underlying BTF type might have\n\t\t\t\t * changed, so might its size have changed, so\n\t\t\t\t * re-calculate and update it in sec_var.\n\t\t\t\t */ sz = btf__resolve_size ( linker -> btf , glob_sym -> underlying_btf_id ) ; if ( sz < 0 ) { pr_warn ( \"global '%s': failed to resolve size of underlying type: %d\\n\" , name , ( int ) sz ) ; return - EINVAL ; } dst_var -> size = sz ; continue ; } sec_vars = libbpf_reallocarray ( sec_vars , dst_sec -> sec_var_cnt + 1 , sizeof ( * dst_sec -> sec_vars ) ) ; if ( ! sec_vars ) return - ENOMEM ; dst_sec -> sec_vars = sec_vars ; dst_sec -> sec_var_cnt ++ ; dst_var = & dst_sec -> sec_vars [ dst_sec -> sec_var_cnt - 1 ] ; dst_var -> type = obj -> btf_type_map [ src_var -> type ] ; dst_var -> size = src_var -> size ; dst_var -> offset = src_sec -> dst_off + src_var -> offset ; if ( glob_sym ) glob_sym -> var_idx = dst_sec -> sec_var_cnt - 1 ; } } return 0 ; }",
    "resources/libbpf/src/linker.c@add_btf_ext_rec": "static void * add_btf_ext_rec ( struct btf_ext_sec_data * ext_data , const void * src_rec ) { void * tmp ; tmp = libbpf_reallocarray ( ext_data -> recs , ext_data -> rec_cnt + 1 , ext_data -> rec_sz ) ; if ( ! tmp ) return NULL ; ext_data -> recs = tmp ; tmp += ext_data -> rec_cnt * ext_data -> rec_sz ; memcpy ( tmp , src_rec , ext_data -> rec_sz ) ; ext_data -> rec_cnt ++ ; return tmp ; }",
    "resources/libbpf/src/linker.c@linker_append_btf_ext": "static int linker_append_btf_ext ( struct bpf_linker * linker , struct src_obj * obj ) { const struct btf_ext_info_sec * ext_sec ; const char * sec_name , * s ; struct src_sec * src_sec ; struct dst_sec * dst_sec ; int rec_sz , str_off , i ; if ( ! obj -> btf_ext ) return 0 ; rec_sz = obj -> btf_ext -> func_info . rec_size ; for_each_btf_ext_sec ( & obj -> btf_ext -> func_info , ext_sec ) { struct bpf_func_info_min * src_rec , * dst_rec ; sec_name = btf__name_by_offset ( obj -> btf , ext_sec -> sec_name_off ) ; src_sec = find_src_sec_by_name ( obj , sec_name ) ; if ( ! src_sec ) { pr_warn ( \"can't find section '%s' referenced from .BTF.ext\\n\" , sec_name ) ; return - EINVAL ; } dst_sec = & linker -> secs [ src_sec -> dst_id ] ; if ( dst_sec -> func_info . rec_sz == 0 ) dst_sec -> func_info . rec_sz = rec_sz ; if ( dst_sec -> func_info . rec_sz != rec_sz ) { pr_warn ( \"incompatible .BTF.ext record sizes for section '%s'\\n\" , sec_name ) ; return - EINVAL ; } for_each_btf_ext_rec ( & obj -> btf_ext -> func_info , ext_sec , i , src_rec ) { dst_rec = add_btf_ext_rec ( & dst_sec -> func_info , src_rec ) ; if ( ! dst_rec ) return - ENOMEM ; dst_rec -> insn_off += src_sec -> dst_off ; dst_rec -> type_id = obj -> btf_type_map [ dst_rec -> type_id ] ; } } rec_sz = obj -> btf_ext -> line_info . rec_size ; for_each_btf_ext_sec ( & obj -> btf_ext -> line_info , ext_sec ) { struct bpf_line_info_min * src_rec , * dst_rec ; sec_name = btf__name_by_offset ( obj -> btf , ext_sec -> sec_name_off ) ; src_sec = find_src_sec_by_name ( obj , sec_name ) ; if ( ! src_sec ) { pr_warn ( \"can't find section '%s' referenced from .BTF.ext\\n\" , sec_name ) ; return - EINVAL ; } dst_sec = & linker -> secs [ src_sec -> dst_id ] ; if ( dst_sec -> line_info . rec_sz == 0 ) dst_sec -> line_info . rec_sz = rec_sz ; if ( dst_sec -> line_info . rec_sz != rec_sz ) { pr_warn ( \"incompatible .BTF.ext record sizes for section '%s'\\n\" , sec_name ) ; return - EINVAL ; } for_each_btf_ext_rec ( & obj -> btf_ext -> line_info , ext_sec , i , src_rec ) { dst_rec = add_btf_ext_rec ( & dst_sec -> line_info , src_rec ) ; if ( ! dst_rec ) return - ENOMEM ; dst_rec -> insn_off += src_sec -> dst_off ; s = btf__str_by_offset ( obj -> btf , src_rec -> file_name_off ) ; str_off = btf__add_str ( linker -> btf , s ) ; if ( str_off < 0 ) return - ENOMEM ; dst_rec -> file_name_off = str_off ; s = btf__str_by_offset ( obj -> btf , src_rec -> line_off ) ; str_off = btf__add_str ( linker -> btf , s ) ; if ( str_off < 0 ) return - ENOMEM ; dst_rec -> line_off = str_off ; /* dst_rec->line_col is fine */ } } rec_sz = obj -> btf_ext -> core_relo_info . rec_size ; for_each_btf_ext_sec ( & obj -> btf_ext -> core_relo_info , ext_sec ) { struct bpf_core_relo * src_rec , * dst_rec ; sec_name = btf__name_by_offset ( obj -> btf , ext_sec -> sec_name_off ) ; src_sec = find_src_sec_by_name ( obj , sec_name ) ; if ( ! src_sec ) { pr_warn ( \"can't find section '%s' referenced from .BTF.ext\\n\" , sec_name ) ; return - EINVAL ; } dst_sec = & linker -> secs [ src_sec -> dst_id ] ; if ( dst_sec -> core_relo_info . rec_sz == 0 ) dst_sec -> core_relo_info . rec_sz = rec_sz ; if ( dst_sec -> core_relo_info . rec_sz != rec_sz ) { pr_warn ( \"incompatible .BTF.ext record sizes for section '%s'\\n\" , sec_name ) ; return - EINVAL ; } for_each_btf_ext_rec ( & obj -> btf_ext -> core_relo_info , ext_sec , i , src_rec ) { dst_rec = add_btf_ext_rec ( & dst_sec -> core_relo_info , src_rec ) ; if ( ! dst_rec ) return - ENOMEM ; dst_rec -> insn_off += src_sec -> dst_off ; dst_rec -> type_id = obj -> btf_type_map [ dst_rec -> type_id ] ; s = btf__str_by_offset ( obj -> btf , src_rec -> access_str_off ) ; str_off = btf__add_str ( linker -> btf , s ) ; if ( str_off < 0 ) return - ENOMEM ; dst_rec -> access_str_off = str_off ; /* dst_rec->kind is fine */ } } return 0 ; }",
    "resources/libbpf/src/linker.c@bpf_linker__finalize": "int bpf_linker__finalize ( struct bpf_linker * linker ) { struct dst_sec * sec ; size_t strs_sz ; const void * strs ; int err , i ; if ( ! linker -> elf ) return libbpf_err ( - EINVAL ) ; err = finalize_btf ( linker ) ; if ( err ) return libbpf_err ( err ) ; /* Finalize strings */ strs_sz = strset__data_size ( linker -> strtab_strs ) ; strs = strset__data ( linker -> strtab_strs ) ; sec = & linker -> secs [ linker -> strtab_sec_idx ] ; sec -> data -> d_align = 1 ; sec -> data -> d_off = 0LL ; sec -> data -> d_buf = ( void * ) strs ; sec -> data -> d_type = ELF_T_BYTE ; sec -> data -> d_size = strs_sz ; sec -> shdr -> sh_size = strs_sz ; for ( i = 1 ; i < linker -> sec_cnt ; i ++ ) { sec = & linker -> secs [ i ] ; /* STRTAB is handled specially above */ if ( sec -> sec_idx == linker -> strtab_sec_idx ) continue ; /* special ephemeral sections (.ksyms, .kconfig, etc) */ if ( ! sec -> scn ) continue ; sec -> data -> d_buf = sec -> raw_data ; } /* Finalize ELF layout */ if ( elf_update ( linker -> elf , ELF_C_NULL ) < 0 ) { err = - errno ; pr_warn_elf ( \"failed to finalize ELF layout\" ) ; return libbpf_err ( err ) ; } /* Write out final ELF contents */ if ( elf_update ( linker -> elf , ELF_C_WRITE ) < 0 ) { err = - errno ; pr_warn_elf ( \"failed to write ELF contents\" ) ; return libbpf_err ( err ) ; } elf_end ( linker -> elf ) ; close ( linker -> fd ) ; linker -> elf = NULL ; linker -> fd = - 1 ; return 0 ; }",
    "resources/libbpf/src/linker.c@emit_elf_data_sec": "static int emit_elf_data_sec ( struct bpf_linker * linker , const char * sec_name , size_t align , const void * raw_data , size_t raw_sz ) { Elf_Scn * scn ; Elf_Data * data ; Elf64_Shdr * shdr ; int name_off ; name_off = strset__add_str ( linker -> strtab_strs , sec_name ) ; if ( name_off < 0 ) return name_off ; scn = elf_newscn ( linker -> elf ) ; if ( ! scn ) return - ENOMEM ; data = elf_newdata ( scn ) ; if ( ! data ) return - ENOMEM ; shdr = elf64_getshdr ( scn ) ; if ( ! shdr ) return - EINVAL ; shdr -> sh_name = name_off ; shdr -> sh_type = SHT_PROGBITS ; shdr -> sh_flags = 0 ; shdr -> sh_size = raw_sz ; shdr -> sh_link = 0 ; shdr -> sh_info = 0 ; shdr -> sh_addralign = align ; shdr -> sh_entsize = 0 ; data -> d_type = ELF_T_BYTE ; data -> d_size = raw_sz ; data -> d_buf = ( void * ) raw_data ; data -> d_align = align ; data -> d_off = 0 ; return 0 ; }",
    "resources/libbpf/src/linker.c@finalize_btf": "static int finalize_btf ( struct bpf_linker * linker ) { LIBBPF_OPTS ( btf_dedup_opts , opts ) ; struct btf * btf = linker -> btf ; const void * raw_data ; int i , j , id , err ; __u32 raw_sz ; /* bail out if no BTF data was produced */ if ( btf__type_cnt ( linker -> btf ) == 1 ) return 0 ; for ( i = 1 ; i < linker -> sec_cnt ; i ++ ) { struct dst_sec * sec = & linker -> secs [ i ] ; if ( ! sec -> has_btf ) continue ; id = btf__add_datasec ( btf , sec -> sec_name , sec -> sec_sz ) ; if ( id < 0 ) { pr_warn ( \"failed to add consolidated BTF type for datasec '%s': %d\\n\" , sec -> sec_name , id ) ; return id ; } for ( j = 0 ; j < sec -> sec_var_cnt ; j ++ ) { struct btf_var_secinfo * vi = & sec -> sec_vars [ j ] ; if ( btf__add_datasec_var_info ( btf , vi -> type , vi -> offset , vi -> size ) ) return - EINVAL ; } } err = finalize_btf_ext ( linker ) ; if ( err ) { pr_warn ( \".BTF.ext generation failed: %d\\n\" , err ) ; return err ; } opts . btf_ext = linker -> btf_ext ; err = btf__dedup ( linker -> btf , & opts ) ; if ( err ) { pr_warn ( \"BTF dedup failed: %d\\n\" , err ) ; return err ; } /* Emit .BTF section */ raw_data = btf__raw_data ( linker -> btf , & raw_sz ) ; if ( ! raw_data ) return - ENOMEM ; err = emit_elf_data_sec ( linker , BTF_ELF_SEC , 8 , raw_data , raw_sz ) ; if ( err ) { pr_warn ( \"failed to write out .BTF ELF section: %d\\n\" , err ) ; return err ; } /* Emit .BTF.ext section */ if ( linker -> btf_ext ) { raw_data = btf_ext__raw_data ( linker -> btf_ext , & raw_sz ) ; if ( ! raw_data ) return - ENOMEM ; err = emit_elf_data_sec ( linker , BTF_EXT_ELF_SEC , 8 , raw_data , raw_sz ) ; if ( err ) { pr_warn ( \"failed to write out .BTF.ext ELF section: %d\\n\" , err ) ; return err ; } } return 0 ; }",
    "resources/libbpf/src/linker.c@emit_btf_ext_data": "static int emit_btf_ext_data ( struct bpf_linker * linker , void * output , const char * sec_name , struct btf_ext_sec_data * sec_data ) { struct btf_ext_info_sec * sec_info ; void * cur = output ; int str_off ; size_t sz ; if ( ! sec_data -> rec_cnt ) return 0 ; str_off = btf__add_str ( linker -> btf , sec_name ) ; if ( str_off < 0 ) return - ENOMEM ; sec_info = cur ; sec_info -> sec_name_off = str_off ; sec_info -> num_info = sec_data -> rec_cnt ; cur += sizeof ( struct btf_ext_info_sec ) ; sz = sec_data -> rec_cnt * sec_data -> rec_sz ; memcpy ( cur , sec_data -> recs , sz ) ; cur += sz ; return cur - output ; }",
    "resources/libbpf/src/linker.c@finalize_btf_ext": "static int finalize_btf_ext ( struct bpf_linker * linker ) { size_t funcs_sz = 0 , lines_sz = 0 , core_relos_sz = 0 , total_sz = 0 ; size_t func_rec_sz = 0 , line_rec_sz = 0 , core_relo_rec_sz = 0 ; struct btf_ext_header * hdr ; void * data , * cur ; int i , err , sz ; /* validate that all sections have the same .BTF.ext record sizes\n\t * and calculate total data size for each type of data (func info,\n\t * line info, core relos)\n\t */ for ( i = 1 ; i < linker -> sec_cnt ; i ++ ) { struct dst_sec * sec = & linker -> secs [ i ] ; if ( sec -> func_info . rec_cnt ) { if ( func_rec_sz == 0 ) func_rec_sz = sec -> func_info . rec_sz ; if ( func_rec_sz != sec -> func_info . rec_sz ) { pr_warn ( \"mismatch in func_info record size %zu != %u\\n\" , func_rec_sz , sec -> func_info . rec_sz ) ; return - EINVAL ; } funcs_sz += sizeof ( struct btf_ext_info_sec ) + func_rec_sz * sec -> func_info . rec_cnt ; } if ( sec -> line_info . rec_cnt ) { if ( line_rec_sz == 0 ) line_rec_sz = sec -> line_info . rec_sz ; if ( line_rec_sz != sec -> line_info . rec_sz ) { pr_warn ( \"mismatch in line_info record size %zu != %u\\n\" , line_rec_sz , sec -> line_info . rec_sz ) ; return - EINVAL ; } lines_sz += sizeof ( struct btf_ext_info_sec ) + line_rec_sz * sec -> line_info . rec_cnt ; } if ( sec -> core_relo_info . rec_cnt ) { if ( core_relo_rec_sz == 0 ) core_relo_rec_sz = sec -> core_relo_info . rec_sz ; if ( core_relo_rec_sz != sec -> core_relo_info . rec_sz ) { pr_warn ( \"mismatch in core_relo_info record size %zu != %u\\n\" , core_relo_rec_sz , sec -> core_relo_info . rec_sz ) ; return - EINVAL ; } core_relos_sz += sizeof ( struct btf_ext_info_sec ) + core_relo_rec_sz * sec -> core_relo_info . rec_cnt ; } } if ( ! funcs_sz && ! lines_sz && ! core_relos_sz ) return 0 ; total_sz += sizeof ( struct btf_ext_header ) ; if ( funcs_sz ) { funcs_sz += sizeof ( __u32 ) ; /* record size prefix */ total_sz += funcs_sz ; } if ( lines_sz ) { lines_sz += sizeof ( __u32 ) ; /* record size prefix */ total_sz += lines_sz ; } if ( core_relos_sz ) { core_relos_sz += sizeof ( __u32 ) ; /* record size prefix */ total_sz += core_relos_sz ; } cur = data = calloc ( 1 , total_sz ) ; if ( ! data ) return - ENOMEM ; hdr = cur ; hdr -> magic = BTF_MAGIC ; hdr -> version = BTF_VERSION ; hdr -> flags = 0 ; hdr -> hdr_len = sizeof ( struct btf_ext_header ) ; cur += sizeof ( struct btf_ext_header ) ; /* All offsets are in bytes relative to the end of this header */ hdr -> func_info_off = 0 ; hdr -> func_info_len = funcs_sz ; hdr -> line_info_off = funcs_sz ; hdr -> line_info_len = lines_sz ; hdr -> core_relo_off = funcs_sz + lines_sz ; hdr -> core_relo_len = core_relos_sz ; if ( funcs_sz ) { * ( __u32 * ) cur = func_rec_sz ; cur += sizeof ( __u32 ) ; for ( i = 1 ; i < linker -> sec_cnt ; i ++ ) { struct dst_sec * sec = & linker -> secs [ i ] ; sz = emit_btf_ext_data ( linker , cur , sec -> sec_name , & sec -> func_info ) ; if ( sz < 0 ) { err = sz ; goto out ; } cur += sz ; } } if ( lines_sz ) { * ( __u32 * ) cur = line_rec_sz ; cur += sizeof ( __u32 ) ; for ( i = 1 ; i < linker -> sec_cnt ; i ++ ) { struct dst_sec * sec = & linker -> secs [ i ] ; sz = emit_btf_ext_data ( linker , cur , sec -> sec_name , & sec -> line_info ) ; if ( sz < 0 ) { err = sz ; goto out ; } cur += sz ; } } if ( core_relos_sz ) { * ( __u32 * ) cur = core_relo_rec_sz ; cur += sizeof ( __u32 ) ; for ( i = 1 ; i < linker -> sec_cnt ; i ++ ) { struct dst_sec * sec = & linker -> secs [ i ] ; sz = emit_btf_ext_data ( linker , cur , sec -> sec_name , & sec -> core_relo_info ) ; if ( sz < 0 ) { err = sz ; goto out ; } cur += sz ; } } linker -> btf_ext = btf_ext__new ( data , total_sz ) ; err = libbpf_get_error ( linker -> btf_ext ) ; if ( err ) { linker -> btf_ext = NULL ; pr_warn ( \"failed to parse final .BTF.ext data: %d\\n\" , err ) ; goto out ; } out : free ( data ) ; return err ; }",
    "resources/libbpf/src/zip.c@btf_kind": "static inline __u16 btf_kind ( const struct btf_type * t ) { return BTF_INFO_KIND ( t -> info ) ; }",
    "resources/libbpf/src/zip.c@btf_vlen": "static inline __u16 btf_vlen ( const struct btf_type * t ) { return BTF_INFO_VLEN ( t -> info ) ; }",
    "resources/libbpf/src/zip.c@btf_kflag": "static inline bool btf_kflag ( const struct btf_type * t ) { return BTF_INFO_KFLAG ( t -> info ) ; }",
    "resources/libbpf/src/zip.c@btf_is_void": "static inline bool btf_is_void ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_UNKN ; }",
    "resources/libbpf/src/zip.c@btf_is_int": "static inline bool btf_is_int ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_INT ; }",
    "resources/libbpf/src/zip.c@btf_is_ptr": "static inline bool btf_is_ptr ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_PTR ; }",
    "resources/libbpf/src/zip.c@btf_is_array": "static inline bool btf_is_array ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_ARRAY ; }",
    "resources/libbpf/src/zip.c@btf_is_struct": "static inline bool btf_is_struct ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_STRUCT ; }",
    "resources/libbpf/src/zip.c@btf_is_union": "static inline bool btf_is_union ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_UNION ; }",
    "resources/libbpf/src/zip.c@btf_is_composite": "static inline bool btf_is_composite ( const struct btf_type * t ) { __u16 kind = btf_kind ( t ) ; return kind == BTF_KIND_STRUCT || kind == BTF_KIND_UNION ; }",
    "resources/libbpf/src/zip.c@btf_is_enum": "static inline bool btf_is_enum ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_ENUM ; }",
    "resources/libbpf/src/zip.c@btf_is_enum64": "static inline bool btf_is_enum64 ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_ENUM64 ; }",
    "resources/libbpf/src/zip.c@btf_is_fwd": "static inline bool btf_is_fwd ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_FWD ; }",
    "resources/libbpf/src/zip.c@btf_is_typedef": "static inline bool btf_is_typedef ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_TYPEDEF ; }",
    "resources/libbpf/src/zip.c@btf_is_volatile": "static inline bool btf_is_volatile ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_VOLATILE ; }",
    "resources/libbpf/src/zip.c@btf_is_const": "static inline bool btf_is_const ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_CONST ; }",
    "resources/libbpf/src/zip.c@btf_is_restrict": "static inline bool btf_is_restrict ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_RESTRICT ; }",
    "resources/libbpf/src/zip.c@btf_is_mod": "static inline bool btf_is_mod ( const struct btf_type * t ) { __u16 kind = btf_kind ( t ) ; return kind == BTF_KIND_VOLATILE || kind == BTF_KIND_CONST || kind == BTF_KIND_RESTRICT || kind == BTF_KIND_TYPE_TAG ; }",
    "resources/libbpf/src/zip.c@btf_is_func": "static inline bool btf_is_func ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_FUNC ; }",
    "resources/libbpf/src/zip.c@btf_is_func_proto": "static inline bool btf_is_func_proto ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_FUNC_PROTO ; }",
    "resources/libbpf/src/zip.c@btf_is_var": "static inline bool btf_is_var ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_VAR ; }",
    "resources/libbpf/src/zip.c@btf_is_datasec": "static inline bool btf_is_datasec ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_DATASEC ; }",
    "resources/libbpf/src/zip.c@btf_is_float": "static inline bool btf_is_float ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_FLOAT ; }",
    "resources/libbpf/src/zip.c@btf_is_decl_tag": "static inline bool btf_is_decl_tag ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_DECL_TAG ; }",
    "resources/libbpf/src/zip.c@btf_is_type_tag": "static inline bool btf_is_type_tag ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_TYPE_TAG ; }",
    "resources/libbpf/src/zip.c@btf_is_any_enum": "static inline bool btf_is_any_enum ( const struct btf_type * t ) { return btf_is_enum ( t ) || btf_is_enum64 ( t ) ; }",
    "resources/libbpf/src/zip.c@btf_kind_core_compat": "static inline bool btf_kind_core_compat ( const struct btf_type * t1 , const struct btf_type * t2 ) { return btf_kind ( t1 ) == btf_kind ( t2 ) || ( btf_is_any_enum ( t1 ) && btf_is_any_enum ( t2 ) ) ; }",
    "resources/libbpf/src/zip.c@btf_int_encoding": "static inline __u8 btf_int_encoding ( const struct btf_type * t ) { return BTF_INT_ENCODING ( * ( __u32 * ) ( t + 1 ) ) ; }",
    "resources/libbpf/src/zip.c@btf_int_offset": "static inline __u8 btf_int_offset ( const struct btf_type * t ) { return BTF_INT_OFFSET ( * ( __u32 * ) ( t + 1 ) ) ; }",
    "resources/libbpf/src/zip.c@btf_int_bits": "static inline __u8 btf_int_bits ( const struct btf_type * t ) { return BTF_INT_BITS ( * ( __u32 * ) ( t + 1 ) ) ; }",
    "resources/libbpf/src/zip.c@btf_array": "static inline struct btf_array * btf_array ( const struct btf_type * t ) { return ( struct btf_array * ) ( t + 1 ) ; }",
    "resources/libbpf/src/zip.c@btf_enum": "static inline struct btf_enum * btf_enum ( const struct btf_type * t ) { return ( struct btf_enum * ) ( t + 1 ) ; }",
    "resources/libbpf/src/zip.c@btf_enum64": "static inline struct btf_enum64 * btf_enum64 ( const struct btf_type * t ) { return ( struct btf_enum64 * ) ( t + 1 ) ; }",
    "resources/libbpf/src/zip.c@btf_enum64_value": "static inline __u64 btf_enum64_value ( const struct btf_enum64 * e ) { /* struct btf_enum64 is introduced in Linux 6.0, which is very\n\t * bleeding-edge. Here we are avoiding relying on struct btf_enum64\n\t * definition coming from kernel UAPI headers to support wider range\n\t * of system-wide kernel headers.\n\t *\n\t * Given this header can be also included from C++ applications, that\n\t * further restricts C tricks we can use (like using compatible\n\t * anonymous struct). So just treat struct btf_enum64 as\n\t * a three-element array of u32 and access second (lo32) and third\n\t * (hi32) elements directly.\n\t *\n\t * For reference, here is a struct btf_enum64 definition:\n\t *\n\t * const struct btf_enum64 {\n\t *\t__u32\tname_off;\n\t *\t__u32\tval_lo32;\n\t *\t__u32\tval_hi32;\n\t * };\n\t */ const __u32 * e64 = ( const __u32 * ) e ; return ( ( __u64 ) e64 [ 2 ] << 32 ) | e64 [ 1 ] ; }",
    "resources/libbpf/src/zip.c@btf_members": "static inline struct btf_member * btf_members ( const struct btf_type * t ) { return ( struct btf_member * ) ( t + 1 ) ; }",
    "resources/libbpf/src/zip.c@btf_member_bit_offset": "static inline __u32 btf_member_bit_offset ( const struct btf_type * t , __u32 member_idx ) { const struct btf_member * m = btf_members ( t ) + member_idx ; bool kflag = btf_kflag ( t ) ; return kflag ? BTF_MEMBER_BIT_OFFSET ( m -> offset ) : m -> offset ; }",
    "resources/libbpf/src/zip.c@btf_member_bitfield_size": "static inline __u32 btf_member_bitfield_size ( const struct btf_type * t , __u32 member_idx ) { const struct btf_member * m = btf_members ( t ) + member_idx ; bool kflag = btf_kflag ( t ) ; return kflag ? BTF_MEMBER_BITFIELD_SIZE ( m -> offset ) : 0 ; }",
    "resources/libbpf/src/zip.c@btf_params": "static inline struct btf_param * btf_params ( const struct btf_type * t ) { return ( struct btf_param * ) ( t + 1 ) ; }",
    "resources/libbpf/src/zip.c@btf_var": "static inline struct btf_var * btf_var ( const struct btf_type * t ) { return ( struct btf_var * ) ( t + 1 ) ; }",
    "resources/libbpf/src/zip.c@btf_var_secinfos": "static inline struct btf_var_secinfo * btf_var_secinfos ( const struct btf_type * t ) { return ( struct btf_var_secinfo * ) ( t + 1 ) ; }",
    "resources/libbpf/src/zip.c@btf_decl_tag": "static inline struct btf_decl_tag * btf_decl_tag ( const struct btf_type * t ) { return ( struct btf_decl_tag * ) ( t + 1 ) ; }",
    "resources/libbpf/src/zip.c@str_has_sfx": "static inline bool str_has_sfx ( const char * str , const char * sfx ) { size_t str_len = strlen ( str ) ; size_t sfx_len = strlen ( sfx ) ; if ( sfx_len > str_len ) return false ; return strcmp ( str + str_len - sfx_len , sfx ) == 0 ; }",
    "resources/libbpf/src/zip.c@libbpf_reallocarray": "static inline void * libbpf_reallocarray ( void * ptr , size_t nmemb , size_t size ) { size_t total ; # if __has_builtin ( __builtin_mul_overflow ) if ( unlikely ( __builtin_mul_overflow ( nmemb , size , & total ) ) ) return NULL ; # else if ( size == 0 || nmemb > ULONG_MAX / size ) return NULL ; total = nmemb * size ; # endif return realloc ( ptr , total ) ; }",
    "resources/libbpf/src/zip.c@libbpf_strlcpy": "static inline void libbpf_strlcpy ( char * dst , const char * src , size_t sz ) { size_t i ; if ( sz == 0 ) return ; sz -- ; for ( i = 0 ; i < sz && src [ i ] ; i ++ ) dst [ i ] = src [ i ] ; dst [ i ] = '\\0' ; }",
    "resources/libbpf/src/zip.c@btf_func_linkage": "static inline enum btf_func_linkage btf_func_linkage ( const struct btf_type * t ) { return ( enum btf_func_linkage ) ( int ) btf_vlen ( t ) ; }",
    "resources/libbpf/src/zip.c@btf_type_info": "static inline __u32 btf_type_info ( int kind , int vlen , int kflag ) { return ( kflag << 31 ) | ( kind << 24 ) | vlen ; }",
    "resources/libbpf/src/zip.c@libbpf_is_mem_zeroed": "static inline bool libbpf_is_mem_zeroed ( const char * p , ssize_t len ) { while ( len > 0 ) { if ( * p ) return false ; p ++ ; len -- ; } return true ; }",
    "resources/libbpf/src/zip.c@libbpf_validate_opts": "static inline bool libbpf_validate_opts ( const char * opts , size_t opts_sz , size_t user_sz , const char * type_name ) { if ( user_sz < sizeof ( size_t ) ) { pr_warn ( \"%s size (%zu) is too small\\n\" , type_name , user_sz ) ; return false ; } if ( ! libbpf_is_mem_zeroed ( opts + opts_sz , ( ssize_t ) user_sz - opts_sz ) ) { pr_warn ( \"%s has non-zero extra bytes\\n\" , type_name ) ; return false ; } return true ; }",
    "resources/libbpf/src/zip.c@libbpf_err": "static inline int libbpf_err ( int ret ) { if ( ret < 0 ) errno = - ret ; return ret ; }",
    "resources/libbpf/src/zip.c@libbpf_err_errno": "static inline int libbpf_err_errno ( int ret ) { /* errno is already assumed to be set on error */ return ret < 0 ? - errno : ret ; }",
    "resources/libbpf/src/zip.c@libbpf_err_ptr": "static inline void * libbpf_err_ptr ( int err ) { /* set errno on error, this doesn't break anything */ errno = - err ; return NULL ; }",
    "resources/libbpf/src/zip.c@libbpf_ptr": "static inline void * libbpf_ptr ( void * ret ) { /* set errno on error, this doesn't break anything */ if ( IS_ERR ( ret ) ) errno = - PTR_ERR ( ret ) ; return IS_ERR ( ret ) ? NULL : ret ; }",
    "resources/libbpf/src/zip.c@str_is_empty": "static inline bool str_is_empty ( const char * s ) { return ! s || ! s [ 0 ] ; }",
    "resources/libbpf/src/zip.c@is_ldimm64_insn": "static inline bool is_ldimm64_insn ( struct bpf_insn * insn ) { return insn -> code == ( BPF_LD | BPF_IMM | BPF_DW ) ; }",
    "resources/libbpf/src/zip.c@dup_good_fd": "static inline int dup_good_fd ( int fd ) { if ( fd < 0 ) return fd ; return fcntl ( fd , F_DUPFD_CLOEXEC , 3 ) ; }",
    "resources/libbpf/src/zip.c@ensure_good_fd": "static inline int ensure_good_fd ( int fd ) { int old_fd = fd , saved_errno ; if ( fd < 0 ) return fd ; if ( fd < 3 ) { fd = dup_good_fd ( fd ) ; saved_errno = errno ; close ( old_fd ) ; errno = saved_errno ; if ( fd < 0 ) { pr_warn ( \"failed to dup FD %d to FD > 2: %d\\n\" , old_fd , - saved_errno ) ; errno = saved_errno ; } } return fd ; }",
    "resources/libbpf/src/zip.c@sys_dup2": "static inline int sys_dup2 ( int oldfd , int newfd ) { # ifdef __NR_dup2 return syscall ( __NR_dup2 , oldfd , newfd ) ; # else return syscall ( __NR_dup3 , oldfd , newfd , 0 ) ; # endif }",
    "resources/libbpf/src/zip.c@reuse_fd": "static inline int reuse_fd ( int fixed_fd , int tmp_fd ) { int err ; err = sys_dup2 ( tmp_fd , fixed_fd ) ; err = err < 0 ? - errno : 0 ; close ( tmp_fd ) ; /* clean up temporary FD */ return err ; }",
    "resources/libbpf/src/zip.c@is_pow_of_2": "static inline bool is_pow_of_2 ( size_t x ) { return x && ( x & ( x - 1 ) ) == 0 ; }",
    "resources/libbpf/src/zip.c@check_access": "static void * check_access ( struct zip_archive * archive , __u32 offset , __u32 size ) { if ( offset + size > archive -> size || offset > offset + size ) return NULL ; return archive -> data + offset ; }",
    "resources/libbpf/src/zip.c@try_parse_end_of_cd": "static int try_parse_end_of_cd ( struct zip_archive * archive , __u32 offset ) { __u16 comment_length , cd_records ; struct end_of_cd_record * eocd ; __u32 cd_offset , cd_size ; eocd = check_access ( archive , offset , sizeof ( * eocd ) ) ; if ( ! eocd || eocd -> magic != END_OF_CD_RECORD_MAGIC ) return - EINVAL ; comment_length = eocd -> comment_length ; if ( offset + sizeof ( * eocd ) + comment_length != archive -> size ) return - EINVAL ; cd_records = eocd -> cd_records ; if ( eocd -> this_disk != 0 || eocd -> cd_disk != 0 || eocd -> cd_records_total != cd_records ) /* This is a valid eocd, but we only support single-file non-ZIP64 archives. */ return - ENOTSUP ; cd_offset = eocd -> cd_offset ; cd_size = eocd -> cd_size ; if ( ! check_access ( archive , cd_offset , cd_size ) ) return - EINVAL ; archive -> cd_offset = cd_offset ; archive -> cd_records = cd_records ; return 0 ; }",
    "resources/libbpf/src/zip.c@find_cd": "static int find_cd ( struct zip_archive * archive ) { int64_t limit , offset ; int rc = - EINVAL ; if ( archive -> size <= sizeof ( struct end_of_cd_record ) ) return - EINVAL ; /* Because the end of central directory ends with a variable length array of\n\t * up to 0xFFFF bytes we can't know exactly where it starts and need to\n\t * search for it at the end of the file, scanning the (limit, offset] range.\n\t */ offset = archive -> size - sizeof ( struct end_of_cd_record ) ; limit = ( int64_t ) offset - ( 1 << 16 ) ; for ( ; offset >= 0 && offset > limit && rc != 0 ; offset -- ) { rc = try_parse_end_of_cd ( archive , offset ) ; if ( rc == - ENOTSUP ) break ; } return rc ; }",
    "resources/libbpf/src/zip.c@zip_archive_open": "struct zip_archive * zip_archive_open ( const char * path ) { struct zip_archive * archive ; int err , fd ; off_t size ; void * data ; fd = open ( path , O_RDONLY | O_CLOEXEC ) ; if ( fd < 0 ) return ERR_PTR ( - errno ) ; size = lseek ( fd , 0 , SEEK_END ) ; if ( size == ( off_t ) - 1 || size > UINT32_MAX ) { close ( fd ) ; return ERR_PTR ( - EINVAL ) ; } data = mmap ( NULL , size , PROT_READ , MAP_PRIVATE , fd , 0 ) ; err = - errno ; close ( fd ) ; if ( data == MAP_FAILED ) return ERR_PTR ( err ) ; archive = malloc ( sizeof ( * archive ) ) ; if ( ! archive ) { munmap ( data , size ) ; return ERR_PTR ( - ENOMEM ) ; } ; archive -> data = data ; archive -> size = size ; err = find_cd ( archive ) ; if ( err ) { munmap ( data , size ) ; free ( archive ) ; return ERR_PTR ( err ) ; } return archive ; }",
    "resources/libbpf/src/zip.c@zip_archive_close": "void zip_archive_close ( struct zip_archive * archive ) { munmap ( archive -> data , archive -> size ) ; free ( archive ) ; }",
    "resources/libbpf/src/zip.c@local_file_header_at_offset": "static struct local_file_header * local_file_header_at_offset ( struct zip_archive * archive , __u32 offset ) { struct local_file_header * lfh ; lfh = check_access ( archive , offset , sizeof ( * lfh ) ) ; if ( ! lfh || lfh -> magic != LOCAL_FILE_HEADER_MAGIC ) return NULL ; return lfh ; }",
    "resources/libbpf/src/zip.c@get_entry_at_offset": "static int get_entry_at_offset ( struct zip_archive * archive , __u32 offset , struct zip_entry * out ) { struct local_file_header * lfh ; __u32 compressed_size ; const char * name ; void * data ; lfh = local_file_header_at_offset ( archive , offset ) ; if ( ! lfh ) return - EINVAL ; offset += sizeof ( * lfh ) ; if ( ( lfh -> flags & FLAG_ENCRYPTED ) || ( lfh -> flags & FLAG_HAS_DATA_DESCRIPTOR ) ) return - EINVAL ; name = check_access ( archive , offset , lfh -> file_name_length ) ; if ( ! name ) return - EINVAL ; offset += lfh -> file_name_length ; if ( ! check_access ( archive , offset , lfh -> extra_field_length ) ) return - EINVAL ; offset += lfh -> extra_field_length ; compressed_size = lfh -> compressed_size ; data = check_access ( archive , offset , compressed_size ) ; if ( ! data ) return - EINVAL ; out -> compression = lfh -> compression ; out -> name_length = lfh -> file_name_length ; out -> name = name ; out -> data = data ; out -> data_length = compressed_size ; out -> data_offset = offset ; return 0 ; }",
    "resources/libbpf/src/zip.c@zip_archive_find_entry": "int zip_archive_find_entry ( struct zip_archive * archive , const char * file_name , struct zip_entry * out ) { size_t file_name_length = strlen ( file_name ) ; __u32 i , offset = archive -> cd_offset ; for ( i = 0 ; i < archive -> cd_records ; ++ i ) { __u16 cdfh_name_length , cdfh_flags ; struct cd_file_header * cdfh ; const char * cdfh_name ; cdfh = check_access ( archive , offset , sizeof ( * cdfh ) ) ; if ( ! cdfh || cdfh -> magic != CD_FILE_HEADER_MAGIC ) return - EINVAL ; offset += sizeof ( * cdfh ) ; cdfh_name_length = cdfh -> file_name_length ; cdfh_name = check_access ( archive , offset , cdfh_name_length ) ; if ( ! cdfh_name ) return - EINVAL ; cdfh_flags = cdfh -> flags ; if ( ( cdfh_flags & FLAG_ENCRYPTED ) == 0 && ( cdfh_flags & FLAG_HAS_DATA_DESCRIPTOR ) == 0 && file_name_length == cdfh_name_length && memcmp ( file_name , archive -> data + offset , file_name_length ) == 0 ) { return get_entry_at_offset ( archive , cdfh -> offset , out ) ; } offset += cdfh_name_length ; offset += cdfh -> extra_field_length ; offset += cdfh -> file_comment_length ; } return - ENOENT ; }",
    "resources/libbpf/src/libbpf_probes.c@btf_kind": "static inline __u16 btf_kind ( const struct btf_type * t ) { return BTF_INFO_KIND ( t -> info ) ; }",
    "resources/libbpf/src/libbpf_probes.c@btf_vlen": "static inline __u16 btf_vlen ( const struct btf_type * t ) { return BTF_INFO_VLEN ( t -> info ) ; }",
    "resources/libbpf/src/libbpf_probes.c@btf_kflag": "static inline bool btf_kflag ( const struct btf_type * t ) { return BTF_INFO_KFLAG ( t -> info ) ; }",
    "resources/libbpf/src/libbpf_probes.c@btf_is_void": "static inline bool btf_is_void ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_UNKN ; }",
    "resources/libbpf/src/libbpf_probes.c@btf_is_int": "static inline bool btf_is_int ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_INT ; }",
    "resources/libbpf/src/libbpf_probes.c@btf_is_ptr": "static inline bool btf_is_ptr ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_PTR ; }",
    "resources/libbpf/src/libbpf_probes.c@btf_is_array": "static inline bool btf_is_array ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_ARRAY ; }",
    "resources/libbpf/src/libbpf_probes.c@btf_is_struct": "static inline bool btf_is_struct ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_STRUCT ; }",
    "resources/libbpf/src/libbpf_probes.c@btf_is_union": "static inline bool btf_is_union ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_UNION ; }",
    "resources/libbpf/src/libbpf_probes.c@btf_is_composite": "static inline bool btf_is_composite ( const struct btf_type * t ) { __u16 kind = btf_kind ( t ) ; return kind == BTF_KIND_STRUCT || kind == BTF_KIND_UNION ; }",
    "resources/libbpf/src/libbpf_probes.c@btf_is_enum": "static inline bool btf_is_enum ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_ENUM ; }",
    "resources/libbpf/src/libbpf_probes.c@btf_is_enum64": "static inline bool btf_is_enum64 ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_ENUM64 ; }",
    "resources/libbpf/src/libbpf_probes.c@btf_is_fwd": "static inline bool btf_is_fwd ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_FWD ; }",
    "resources/libbpf/src/libbpf_probes.c@btf_is_typedef": "static inline bool btf_is_typedef ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_TYPEDEF ; }",
    "resources/libbpf/src/libbpf_probes.c@btf_is_volatile": "static inline bool btf_is_volatile ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_VOLATILE ; }",
    "resources/libbpf/src/libbpf_probes.c@btf_is_const": "static inline bool btf_is_const ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_CONST ; }",
    "resources/libbpf/src/libbpf_probes.c@btf_is_restrict": "static inline bool btf_is_restrict ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_RESTRICT ; }",
    "resources/libbpf/src/libbpf_probes.c@btf_is_mod": "static inline bool btf_is_mod ( const struct btf_type * t ) { __u16 kind = btf_kind ( t ) ; return kind == BTF_KIND_VOLATILE || kind == BTF_KIND_CONST || kind == BTF_KIND_RESTRICT || kind == BTF_KIND_TYPE_TAG ; }",
    "resources/libbpf/src/libbpf_probes.c@btf_is_func": "static inline bool btf_is_func ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_FUNC ; }",
    "resources/libbpf/src/libbpf_probes.c@btf_is_func_proto": "static inline bool btf_is_func_proto ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_FUNC_PROTO ; }",
    "resources/libbpf/src/libbpf_probes.c@btf_is_var": "static inline bool btf_is_var ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_VAR ; }",
    "resources/libbpf/src/libbpf_probes.c@btf_is_datasec": "static inline bool btf_is_datasec ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_DATASEC ; }",
    "resources/libbpf/src/libbpf_probes.c@btf_is_float": "static inline bool btf_is_float ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_FLOAT ; }",
    "resources/libbpf/src/libbpf_probes.c@btf_is_decl_tag": "static inline bool btf_is_decl_tag ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_DECL_TAG ; }",
    "resources/libbpf/src/libbpf_probes.c@btf_is_type_tag": "static inline bool btf_is_type_tag ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_TYPE_TAG ; }",
    "resources/libbpf/src/libbpf_probes.c@btf_is_any_enum": "static inline bool btf_is_any_enum ( const struct btf_type * t ) { return btf_is_enum ( t ) || btf_is_enum64 ( t ) ; }",
    "resources/libbpf/src/libbpf_probes.c@btf_kind_core_compat": "static inline bool btf_kind_core_compat ( const struct btf_type * t1 , const struct btf_type * t2 ) { return btf_kind ( t1 ) == btf_kind ( t2 ) || ( btf_is_any_enum ( t1 ) && btf_is_any_enum ( t2 ) ) ; }",
    "resources/libbpf/src/libbpf_probes.c@btf_int_encoding": "static inline __u8 btf_int_encoding ( const struct btf_type * t ) { return BTF_INT_ENCODING ( * ( __u32 * ) ( t + 1 ) ) ; }",
    "resources/libbpf/src/libbpf_probes.c@btf_int_offset": "static inline __u8 btf_int_offset ( const struct btf_type * t ) { return BTF_INT_OFFSET ( * ( __u32 * ) ( t + 1 ) ) ; }",
    "resources/libbpf/src/libbpf_probes.c@btf_int_bits": "static inline __u8 btf_int_bits ( const struct btf_type * t ) { return BTF_INT_BITS ( * ( __u32 * ) ( t + 1 ) ) ; }",
    "resources/libbpf/src/libbpf_probes.c@btf_array": "static inline struct btf_array * btf_array ( const struct btf_type * t ) { return ( struct btf_array * ) ( t + 1 ) ; }",
    "resources/libbpf/src/libbpf_probes.c@btf_enum": "static inline struct btf_enum * btf_enum ( const struct btf_type * t ) { return ( struct btf_enum * ) ( t + 1 ) ; }",
    "resources/libbpf/src/libbpf_probes.c@btf_enum64": "static inline struct btf_enum64 * btf_enum64 ( const struct btf_type * t ) { return ( struct btf_enum64 * ) ( t + 1 ) ; }",
    "resources/libbpf/src/libbpf_probes.c@btf_enum64_value": "static inline __u64 btf_enum64_value ( const struct btf_enum64 * e ) { /* struct btf_enum64 is introduced in Linux 6.0, which is very\n\t * bleeding-edge. Here we are avoiding relying on struct btf_enum64\n\t * definition coming from kernel UAPI headers to support wider range\n\t * of system-wide kernel headers.\n\t *\n\t * Given this header can be also included from C++ applications, that\n\t * further restricts C tricks we can use (like using compatible\n\t * anonymous struct). So just treat struct btf_enum64 as\n\t * a three-element array of u32 and access second (lo32) and third\n\t * (hi32) elements directly.\n\t *\n\t * For reference, here is a struct btf_enum64 definition:\n\t *\n\t * const struct btf_enum64 {\n\t *\t__u32\tname_off;\n\t *\t__u32\tval_lo32;\n\t *\t__u32\tval_hi32;\n\t * };\n\t */ const __u32 * e64 = ( const __u32 * ) e ; return ( ( __u64 ) e64 [ 2 ] << 32 ) | e64 [ 1 ] ; }",
    "resources/libbpf/src/libbpf_probes.c@btf_members": "static inline struct btf_member * btf_members ( const struct btf_type * t ) { return ( struct btf_member * ) ( t + 1 ) ; }",
    "resources/libbpf/src/libbpf_probes.c@btf_member_bit_offset": "static inline __u32 btf_member_bit_offset ( const struct btf_type * t , __u32 member_idx ) { const struct btf_member * m = btf_members ( t ) + member_idx ; bool kflag = btf_kflag ( t ) ; return kflag ? BTF_MEMBER_BIT_OFFSET ( m -> offset ) : m -> offset ; }",
    "resources/libbpf/src/libbpf_probes.c@btf_member_bitfield_size": "static inline __u32 btf_member_bitfield_size ( const struct btf_type * t , __u32 member_idx ) { const struct btf_member * m = btf_members ( t ) + member_idx ; bool kflag = btf_kflag ( t ) ; return kflag ? BTF_MEMBER_BITFIELD_SIZE ( m -> offset ) : 0 ; }",
    "resources/libbpf/src/libbpf_probes.c@btf_params": "static inline struct btf_param * btf_params ( const struct btf_type * t ) { return ( struct btf_param * ) ( t + 1 ) ; }",
    "resources/libbpf/src/libbpf_probes.c@btf_var": "static inline struct btf_var * btf_var ( const struct btf_type * t ) { return ( struct btf_var * ) ( t + 1 ) ; }",
    "resources/libbpf/src/libbpf_probes.c@btf_var_secinfos": "static inline struct btf_var_secinfo * btf_var_secinfos ( const struct btf_type * t ) { return ( struct btf_var_secinfo * ) ( t + 1 ) ; }",
    "resources/libbpf/src/libbpf_probes.c@btf_decl_tag": "static inline struct btf_decl_tag * btf_decl_tag ( const struct btf_type * t ) { return ( struct btf_decl_tag * ) ( t + 1 ) ; }",
    "resources/libbpf/src/libbpf_probes.c@str_has_sfx": "static inline bool str_has_sfx ( const char * str , const char * sfx ) { size_t str_len = strlen ( str ) ; size_t sfx_len = strlen ( sfx ) ; if ( sfx_len > str_len ) return false ; return strcmp ( str + str_len - sfx_len , sfx ) == 0 ; }",
    "resources/libbpf/src/libbpf_probes.c@libbpf_reallocarray": "static inline void * libbpf_reallocarray ( void * ptr , size_t nmemb , size_t size ) { size_t total ; # if __has_builtin ( __builtin_mul_overflow ) if ( unlikely ( __builtin_mul_overflow ( nmemb , size , & total ) ) ) return NULL ; # else if ( size == 0 || nmemb > ULONG_MAX / size ) return NULL ; total = nmemb * size ; # endif return realloc ( ptr , total ) ; }",
    "resources/libbpf/src/libbpf_probes.c@libbpf_strlcpy": "static inline void libbpf_strlcpy ( char * dst , const char * src , size_t sz ) { size_t i ; if ( sz == 0 ) return ; sz -- ; for ( i = 0 ; i < sz && src [ i ] ; i ++ ) dst [ i ] = src [ i ] ; dst [ i ] = '\\0' ; }",
    "resources/libbpf/src/libbpf_probes.c@btf_func_linkage": "static inline enum btf_func_linkage btf_func_linkage ( const struct btf_type * t ) { return ( enum btf_func_linkage ) ( int ) btf_vlen ( t ) ; }",
    "resources/libbpf/src/libbpf_probes.c@btf_type_info": "static inline __u32 btf_type_info ( int kind , int vlen , int kflag ) { return ( kflag << 31 ) | ( kind << 24 ) | vlen ; }",
    "resources/libbpf/src/libbpf_probes.c@libbpf_is_mem_zeroed": "static inline bool libbpf_is_mem_zeroed ( const char * p , ssize_t len ) { while ( len > 0 ) { if ( * p ) return false ; p ++ ; len -- ; } return true ; }",
    "resources/libbpf/src/libbpf_probes.c@libbpf_validate_opts": "static inline bool libbpf_validate_opts ( const char * opts , size_t opts_sz , size_t user_sz , const char * type_name ) { if ( user_sz < sizeof ( size_t ) ) { pr_warn ( \"%s size (%zu) is too small\\n\" , type_name , user_sz ) ; return false ; } if ( ! libbpf_is_mem_zeroed ( opts + opts_sz , ( ssize_t ) user_sz - opts_sz ) ) { pr_warn ( \"%s has non-zero extra bytes\\n\" , type_name ) ; return false ; } return true ; }",
    "resources/libbpf/src/libbpf_probes.c@libbpf_err": "static inline int libbpf_err ( int ret ) { if ( ret < 0 ) errno = - ret ; return ret ; }",
    "resources/libbpf/src/libbpf_probes.c@libbpf_err_errno": "static inline int libbpf_err_errno ( int ret ) { /* errno is already assumed to be set on error */ return ret < 0 ? - errno : ret ; }",
    "resources/libbpf/src/libbpf_probes.c@libbpf_err_ptr": "static inline void * libbpf_err_ptr ( int err ) { /* set errno on error, this doesn't break anything */ errno = - err ; return NULL ; }",
    "resources/libbpf/src/libbpf_probes.c@libbpf_ptr": "static inline void * libbpf_ptr ( void * ret ) { /* set errno on error, this doesn't break anything */ if ( IS_ERR ( ret ) ) errno = - PTR_ERR ( ret ) ; return IS_ERR ( ret ) ? NULL : ret ; }",
    "resources/libbpf/src/libbpf_probes.c@str_is_empty": "static inline bool str_is_empty ( const char * s ) { return ! s || ! s [ 0 ] ; }",
    "resources/libbpf/src/libbpf_probes.c@is_ldimm64_insn": "static inline bool is_ldimm64_insn ( struct bpf_insn * insn ) { return insn -> code == ( BPF_LD | BPF_IMM | BPF_DW ) ; }",
    "resources/libbpf/src/libbpf_probes.c@dup_good_fd": "static inline int dup_good_fd ( int fd ) { if ( fd < 0 ) return fd ; return fcntl ( fd , F_DUPFD_CLOEXEC , 3 ) ; }",
    "resources/libbpf/src/libbpf_probes.c@ensure_good_fd": "static inline int ensure_good_fd ( int fd ) { int old_fd = fd , saved_errno ; if ( fd < 0 ) return fd ; if ( fd < 3 ) { fd = dup_good_fd ( fd ) ; saved_errno = errno ; close ( old_fd ) ; errno = saved_errno ; if ( fd < 0 ) { pr_warn ( \"failed to dup FD %d to FD > 2: %d\\n\" , old_fd , - saved_errno ) ; errno = saved_errno ; } } return fd ; }",
    "resources/libbpf/src/libbpf_probes.c@sys_dup2": "static inline int sys_dup2 ( int oldfd , int newfd ) { # ifdef __NR_dup2 return syscall ( __NR_dup2 , oldfd , newfd ) ; # else return syscall ( __NR_dup3 , oldfd , newfd , 0 ) ; # endif }",
    "resources/libbpf/src/libbpf_probes.c@reuse_fd": "static inline int reuse_fd ( int fixed_fd , int tmp_fd ) { int err ; err = sys_dup2 ( tmp_fd , fixed_fd ) ; err = err < 0 ? - errno : 0 ; close ( tmp_fd ) ; /* clean up temporary FD */ return err ; }",
    "resources/libbpf/src/libbpf_probes.c@is_pow_of_2": "static inline bool is_pow_of_2 ( size_t x ) { return x && ( x & ( x - 1 ) ) == 0 ; }",
    "resources/libbpf/src/libbpf_probes.c@get_ubuntu_kernel_version": "static __u32 get_ubuntu_kernel_version ( void ) { const char * ubuntu_kver_file = \"/proc/version_signature\" ; __u32 major , minor , patch ; int ret ; FILE * f ; if ( faccessat ( AT_FDCWD , ubuntu_kver_file , R_OK , AT_EACCESS ) != 0 ) return 0 ; f = fopen ( ubuntu_kver_file , \"re\" ) ; if ( ! f ) return 0 ; ret = fscanf ( f , \"%*s %*s %u.%u.%u\\n\" , & major , & minor , & patch ) ; fclose ( f ) ; if ( ret != 3 ) return 0 ; return KERNEL_VERSION ( major , minor , patch ) ; }",
    "resources/libbpf/src/libbpf_probes.c@get_debian_kernel_version": "static __u32 get_debian_kernel_version ( struct utsname * info ) { __u32 major , minor , patch ; char * p ; p = strstr ( info -> version , \"Debian \" ) ; if ( ! p ) { /* This is not a Debian kernel. */ return 0 ; } if ( sscanf ( p , \"Debian %u.%u.%u\" , & major , & minor , & patch ) != 3 ) return 0 ; return KERNEL_VERSION ( major , minor , patch ) ; }",
    "resources/libbpf/src/libbpf_probes.c@get_kernel_version": "__u32 get_kernel_version ( void ) { __u32 major , minor , patch , version ; struct utsname info ; /* Check if this is an Ubuntu kernel. */ version = get_ubuntu_kernel_version ( ) ; if ( version != 0 ) return version ; uname ( & info ) ; /* Check if this is a Debian kernel. */ version = get_debian_kernel_version ( & info ) ; if ( version != 0 ) return version ; if ( sscanf ( info . release , \"%u.%u.%u\" , & major , & minor , & patch ) != 3 ) return 0 ; return KERNEL_VERSION ( major , minor , patch ) ; }",
    "resources/libbpf/src/libbpf_probes.c@probe_prog_load": "static int probe_prog_load ( enum bpf_prog_type prog_type , const struct bpf_insn * insns , size_t insns_cnt , char * log_buf , size_t log_buf_sz ) { LIBBPF_OPTS ( bpf_prog_load_opts , opts , . log_buf = log_buf , . log_size = log_buf_sz , . log_level = log_buf ? 1 : 0 , ) ; int fd , err , exp_err = 0 ; const char * exp_msg = NULL ; char buf [ 4096 ] ; switch ( prog_type ) { case BPF_PROG_TYPE_CGROUP_SOCK_ADDR : opts . expected_attach_type = BPF_CGROUP_INET4_CONNECT ; break ; case BPF_PROG_TYPE_CGROUP_SOCKOPT : opts . expected_attach_type = BPF_CGROUP_GETSOCKOPT ; break ; case BPF_PROG_TYPE_SK_LOOKUP : opts . expected_attach_type = BPF_SK_LOOKUP ; break ; case BPF_PROG_TYPE_KPROBE : opts . kern_version = get_kernel_version ( ) ; break ; case BPF_PROG_TYPE_LIRC_MODE2 : opts . expected_attach_type = BPF_LIRC_MODE2 ; break ; case BPF_PROG_TYPE_TRACING : case BPF_PROG_TYPE_LSM : opts . log_buf = buf ; opts . log_size = sizeof ( buf ) ; opts . log_level = 1 ; if ( prog_type == BPF_PROG_TYPE_TRACING ) opts . expected_attach_type = BPF_TRACE_FENTRY ; else opts . expected_attach_type = BPF_MODIFY_RETURN ; opts . attach_btf_id = 1 ; exp_err = - EINVAL ; exp_msg = \"attach_btf_id 1 is not a function\" ; break ; case BPF_PROG_TYPE_EXT : opts . log_buf = buf ; opts . log_size = sizeof ( buf ) ; opts . log_level = 1 ; opts . attach_btf_id = 1 ; exp_err = - EINVAL ; exp_msg = \"Cannot replace kernel functions\" ; break ; case BPF_PROG_TYPE_SYSCALL : opts . prog_flags = BPF_F_SLEEPABLE ; break ; case BPF_PROG_TYPE_STRUCT_OPS : exp_err = - 524 ; /* -ENOTSUPP */ break ; case BPF_PROG_TYPE_UNSPEC : case BPF_PROG_TYPE_SOCKET_FILTER : case BPF_PROG_TYPE_SCHED_CLS : case BPF_PROG_TYPE_SCHED_ACT : case BPF_PROG_TYPE_TRACEPOINT : case BPF_PROG_TYPE_XDP : case BPF_PROG_TYPE_PERF_EVENT : case BPF_PROG_TYPE_CGROUP_SKB : case BPF_PROG_TYPE_CGROUP_SOCK : case BPF_PROG_TYPE_LWT_IN : case BPF_PROG_TYPE_LWT_OUT : case BPF_PROG_TYPE_LWT_XMIT : case BPF_PROG_TYPE_SOCK_OPS : case BPF_PROG_TYPE_SK_SKB : case BPF_PROG_TYPE_CGROUP_DEVICE : case BPF_PROG_TYPE_SK_MSG : case BPF_PROG_TYPE_RAW_TRACEPOINT : case BPF_PROG_TYPE_RAW_TRACEPOINT_WRITABLE : case BPF_PROG_TYPE_LWT_SEG6LOCAL : case BPF_PROG_TYPE_SK_REUSEPORT : case BPF_PROG_TYPE_FLOW_DISSECTOR : case BPF_PROG_TYPE_CGROUP_SYSCTL : break ; case BPF_PROG_TYPE_NETFILTER : opts . expected_attach_type = BPF_NETFILTER ; break ; default : return - EOPNOTSUPP ; } fd = bpf_prog_load ( prog_type , NULL , \"GPL\" , insns , insns_cnt , & opts ) ; err = - errno ; if ( fd >= 0 ) close ( fd ) ; if ( exp_err ) { if ( fd >= 0 || err != exp_err ) return 0 ; if ( exp_msg && ! strstr ( buf , exp_msg ) ) return 0 ; return 1 ; } return fd >= 0 ? 1 : 0 ; }",
    "resources/libbpf/src/libbpf_probes.c@libbpf_probe_bpf_prog_type": "int libbpf_probe_bpf_prog_type ( enum bpf_prog_type prog_type , const void * opts ) { struct bpf_insn insns [ ] = { BPF_MOV64_IMM ( BPF_REG_0 , 0 ) , BPF_EXIT_INSN ( ) } ; const size_t insn_cnt = ARRAY_SIZE ( insns ) ; int ret ; if ( opts ) return libbpf_err ( - EINVAL ) ; ret = probe_prog_load ( prog_type , insns , insn_cnt , NULL , 0 ) ; return libbpf_err ( ret ) ; }",
    "resources/libbpf/src/libbpf_probes.c@libbpf__load_raw_btf": "int libbpf__load_raw_btf ( const char * raw_types , size_t types_len , const char * str_sec , size_t str_len , int token_fd ) { struct btf_header hdr = { . magic = BTF_MAGIC , . version = BTF_VERSION , . hdr_len = sizeof ( struct btf_header ) , . type_len = types_len , . str_off = types_len , . str_len = str_len , } ; LIBBPF_OPTS ( bpf_btf_load_opts , opts , . token_fd = token_fd , . btf_flags = token_fd ? BPF_F_TOKEN_FD : 0 , ) ; int btf_fd , btf_len ; __u8 * raw_btf ; btf_len = hdr . hdr_len + hdr . type_len + hdr . str_len ; raw_btf = malloc ( btf_len ) ; if ( ! raw_btf ) return - ENOMEM ; memcpy ( raw_btf , & hdr , sizeof ( hdr ) ) ; memcpy ( raw_btf + hdr . hdr_len , raw_types , hdr . type_len ) ; memcpy ( raw_btf + hdr . hdr_len + hdr . type_len , str_sec , hdr . str_len ) ; btf_fd = bpf_btf_load ( raw_btf , btf_len , & opts ) ; free ( raw_btf ) ; return btf_fd ; }",
    "resources/libbpf/src/libbpf_probes.c@load_local_storage_btf": "static int load_local_storage_btf ( void ) { const char strs [ ] = \"\\0bpf_spin_lock\\0val\\0cnt\\0l\" ; /* struct bpf_spin_lock {\n\t *   int val;\n\t * };\n\t * struct val {\n\t *   int cnt;\n\t *   struct bpf_spin_lock l;\n\t * };\n\t */ __u32 types [ ] = { /* int */ BTF_TYPE_INT_ENC ( 0 , BTF_INT_SIGNED , 0 , 32 , 4 ) , /* [1] */ /* struct bpf_spin_lock */ /* [2] */ BTF_TYPE_ENC ( 1 , BTF_INFO_ENC ( BTF_KIND_STRUCT , 0 , 1 ) , 4 ) , BTF_MEMBER_ENC ( 15 , 1 , 0 ) , /* int val; */ /* struct val */ /* [3] */ BTF_TYPE_ENC ( 15 , BTF_INFO_ENC ( BTF_KIND_STRUCT , 0 , 2 ) , 8 ) , BTF_MEMBER_ENC ( 19 , 1 , 0 ) , /* int cnt; */ BTF_MEMBER_ENC ( 23 , 2 , 32 ) , /* struct bpf_spin_lock l; */ } ; return libbpf__load_raw_btf ( ( char * ) types , sizeof ( types ) , strs , sizeof ( strs ) , 0 ) ; }",
    "resources/libbpf/src/libbpf_probes.c@probe_map_create": "static int probe_map_create ( enum bpf_map_type map_type ) { LIBBPF_OPTS ( bpf_map_create_opts , opts ) ; int key_size , value_size , max_entries ; __u32 btf_key_type_id = 0 , btf_value_type_id = 0 ; int fd = - 1 , btf_fd = - 1 , fd_inner = - 1 , exp_err = 0 , err = 0 ; key_size = sizeof ( __u32 ) ; value_size = sizeof ( __u32 ) ; max_entries = 1 ; switch ( map_type ) { case BPF_MAP_TYPE_STACK_TRACE : value_size = sizeof ( __u64 ) ; break ; case BPF_MAP_TYPE_LPM_TRIE : key_size = sizeof ( __u64 ) ; value_size = sizeof ( __u64 ) ; opts . map_flags = BPF_F_NO_PREALLOC ; break ; case BPF_MAP_TYPE_CGROUP_STORAGE : case BPF_MAP_TYPE_PERCPU_CGROUP_STORAGE : key_size = sizeof ( struct bpf_cgroup_storage_key ) ; value_size = sizeof ( __u64 ) ; max_entries = 0 ; break ; case BPF_MAP_TYPE_QUEUE : case BPF_MAP_TYPE_STACK : key_size = 0 ; break ; case BPF_MAP_TYPE_SK_STORAGE : case BPF_MAP_TYPE_INODE_STORAGE : case BPF_MAP_TYPE_TASK_STORAGE : case BPF_MAP_TYPE_CGRP_STORAGE : btf_key_type_id = 1 ; btf_value_type_id = 3 ; value_size = 8 ; max_entries = 0 ; opts . map_flags = BPF_F_NO_PREALLOC ; btf_fd = load_local_storage_btf ( ) ; if ( btf_fd < 0 ) return btf_fd ; break ; case BPF_MAP_TYPE_RINGBUF : case BPF_MAP_TYPE_USER_RINGBUF : key_size = 0 ; value_size = 0 ; max_entries = sysconf ( _SC_PAGE_SIZE ) ; break ; case BPF_MAP_TYPE_STRUCT_OPS : /* we'll get -ENOTSUPP for invalid BTF type ID for struct_ops */ opts . btf_vmlinux_value_type_id = 1 ; opts . value_type_btf_obj_fd = - 1 ; exp_err = - 524 ; /* -ENOTSUPP */ break ; case BPF_MAP_TYPE_BLOOM_FILTER : key_size = 0 ; max_entries = 1 ; break ; case BPF_MAP_TYPE_ARENA : key_size = 0 ; value_size = 0 ; max_entries = 1 ; /* one page */ opts . map_extra = 0 ; /* can mmap() at any address */ opts . map_flags = BPF_F_MMAPABLE ; break ; case BPF_MAP_TYPE_HASH : case BPF_MAP_TYPE_ARRAY : case BPF_MAP_TYPE_PROG_ARRAY : case BPF_MAP_TYPE_PERF_EVENT_ARRAY : case BPF_MAP_TYPE_PERCPU_HASH : case BPF_MAP_TYPE_PERCPU_ARRAY : case BPF_MAP_TYPE_CGROUP_ARRAY : case BPF_MAP_TYPE_LRU_HASH : case BPF_MAP_TYPE_LRU_PERCPU_HASH : case BPF_MAP_TYPE_ARRAY_OF_MAPS : case BPF_MAP_TYPE_HASH_OF_MAPS : case BPF_MAP_TYPE_DEVMAP : case BPF_MAP_TYPE_DEVMAP_HASH : case BPF_MAP_TYPE_SOCKMAP : case BPF_MAP_TYPE_CPUMAP : case BPF_MAP_TYPE_XSKMAP : case BPF_MAP_TYPE_SOCKHASH : case BPF_MAP_TYPE_REUSEPORT_SOCKARRAY : break ; case BPF_MAP_TYPE_UNSPEC : default : return - EOPNOTSUPP ; } if ( map_type == BPF_MAP_TYPE_ARRAY_OF_MAPS || map_type == BPF_MAP_TYPE_HASH_OF_MAPS ) { fd_inner = bpf_map_create ( BPF_MAP_TYPE_HASH , NULL , sizeof ( __u32 ) , sizeof ( __u32 ) , 1 , NULL ) ; if ( fd_inner < 0 ) goto cleanup ; opts . inner_map_fd = fd_inner ; } if ( btf_fd >= 0 ) { opts . btf_fd = btf_fd ; opts . btf_key_type_id = btf_key_type_id ; opts . btf_value_type_id = btf_value_type_id ; } fd = bpf_map_create ( map_type , NULL , key_size , value_size , max_entries , & opts ) ; err = - errno ; cleanup : if ( fd >= 0 ) close ( fd ) ; if ( fd_inner >= 0 ) close ( fd_inner ) ; if ( btf_fd >= 0 ) close ( btf_fd ) ; if ( exp_err ) return fd < 0 && err == exp_err ? 1 : 0 ; else return fd >= 0 ? 1 : 0 ; }",
    "resources/libbpf/src/libbpf_probes.c@libbpf_probe_bpf_map_type": "int libbpf_probe_bpf_map_type ( enum bpf_map_type map_type , const void * opts ) { int ret ; if ( opts ) return libbpf_err ( - EINVAL ) ; ret = probe_map_create ( map_type ) ; return libbpf_err ( ret ) ; }",
    "resources/libbpf/src/libbpf_probes.c@libbpf_probe_bpf_helper": "int libbpf_probe_bpf_helper ( enum bpf_prog_type prog_type , enum bpf_func_id helper_id , const void * opts ) { struct bpf_insn insns [ ] = { BPF_EMIT_CALL ( ( __u32 ) helper_id ) , BPF_EXIT_INSN ( ) , } ; const size_t insn_cnt = ARRAY_SIZE ( insns ) ; char buf [ 4096 ] ; int ret ; if ( opts ) return libbpf_err ( - EINVAL ) ; /* we can't successfully load all prog types to check for BPF helper\n\t * support, so bail out with -EOPNOTSUPP error\n\t */ switch ( prog_type ) { case BPF_PROG_TYPE_TRACING : case BPF_PROG_TYPE_EXT : case BPF_PROG_TYPE_LSM : case BPF_PROG_TYPE_STRUCT_OPS : return - EOPNOTSUPP ; default : break ; } buf [ 0 ] = '\\0' ; ret = probe_prog_load ( prog_type , insns , insn_cnt , buf , sizeof ( buf ) ) ; if ( ret < 0 ) return libbpf_err ( ret ) ; /* If BPF verifier doesn't recognize BPF helper ID (enum bpf_func_id)\n\t * at all, it will emit something like \"invalid func unknown#181\".\n\t * If BPF verifier recognizes BPF helper but it's not supported for\n\t * given BPF program type, it will emit \"unknown func bpf_sys_bpf#166\".\n\t * In both cases, provided combination of BPF program type and BPF\n\t * helper is not supported by the kernel.\n\t * In all other cases, probe_prog_load() above will either succeed (e.g.,\n\t * because BPF helper happens to accept no input arguments or it\n\t * accepts one input argument and initial PTR_TO_CTX is fine for\n\t * that), or we'll get some more specific BPF verifier error about\n\t * some unsatisfied conditions.\n\t */ if ( ret == 0 && ( strstr ( buf , \"invalid func \" ) || strstr ( buf , \"unknown func \" ) ) ) return 0 ; return 1 ; /* assume supported */ }",
    "resources/libbpf/src/libbpf_errno.c@btf_kind": "static inline __u16 btf_kind ( const struct btf_type * t ) { return BTF_INFO_KIND ( t -> info ) ; }",
    "resources/libbpf/src/libbpf_errno.c@btf_vlen": "static inline __u16 btf_vlen ( const struct btf_type * t ) { return BTF_INFO_VLEN ( t -> info ) ; }",
    "resources/libbpf/src/libbpf_errno.c@btf_kflag": "static inline bool btf_kflag ( const struct btf_type * t ) { return BTF_INFO_KFLAG ( t -> info ) ; }",
    "resources/libbpf/src/libbpf_errno.c@btf_is_void": "static inline bool btf_is_void ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_UNKN ; }",
    "resources/libbpf/src/libbpf_errno.c@btf_is_int": "static inline bool btf_is_int ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_INT ; }",
    "resources/libbpf/src/libbpf_errno.c@btf_is_ptr": "static inline bool btf_is_ptr ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_PTR ; }",
    "resources/libbpf/src/libbpf_errno.c@btf_is_array": "static inline bool btf_is_array ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_ARRAY ; }",
    "resources/libbpf/src/libbpf_errno.c@btf_is_struct": "static inline bool btf_is_struct ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_STRUCT ; }",
    "resources/libbpf/src/libbpf_errno.c@btf_is_union": "static inline bool btf_is_union ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_UNION ; }",
    "resources/libbpf/src/libbpf_errno.c@btf_is_composite": "static inline bool btf_is_composite ( const struct btf_type * t ) { __u16 kind = btf_kind ( t ) ; return kind == BTF_KIND_STRUCT || kind == BTF_KIND_UNION ; }",
    "resources/libbpf/src/libbpf_errno.c@btf_is_enum": "static inline bool btf_is_enum ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_ENUM ; }",
    "resources/libbpf/src/libbpf_errno.c@btf_is_enum64": "static inline bool btf_is_enum64 ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_ENUM64 ; }",
    "resources/libbpf/src/libbpf_errno.c@btf_is_fwd": "static inline bool btf_is_fwd ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_FWD ; }",
    "resources/libbpf/src/libbpf_errno.c@btf_is_typedef": "static inline bool btf_is_typedef ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_TYPEDEF ; }",
    "resources/libbpf/src/libbpf_errno.c@btf_is_volatile": "static inline bool btf_is_volatile ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_VOLATILE ; }",
    "resources/libbpf/src/libbpf_errno.c@btf_is_const": "static inline bool btf_is_const ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_CONST ; }",
    "resources/libbpf/src/libbpf_errno.c@btf_is_restrict": "static inline bool btf_is_restrict ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_RESTRICT ; }",
    "resources/libbpf/src/libbpf_errno.c@btf_is_mod": "static inline bool btf_is_mod ( const struct btf_type * t ) { __u16 kind = btf_kind ( t ) ; return kind == BTF_KIND_VOLATILE || kind == BTF_KIND_CONST || kind == BTF_KIND_RESTRICT || kind == BTF_KIND_TYPE_TAG ; }",
    "resources/libbpf/src/libbpf_errno.c@btf_is_func": "static inline bool btf_is_func ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_FUNC ; }",
    "resources/libbpf/src/libbpf_errno.c@btf_is_func_proto": "static inline bool btf_is_func_proto ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_FUNC_PROTO ; }",
    "resources/libbpf/src/libbpf_errno.c@btf_is_var": "static inline bool btf_is_var ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_VAR ; }",
    "resources/libbpf/src/libbpf_errno.c@btf_is_datasec": "static inline bool btf_is_datasec ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_DATASEC ; }",
    "resources/libbpf/src/libbpf_errno.c@btf_is_float": "static inline bool btf_is_float ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_FLOAT ; }",
    "resources/libbpf/src/libbpf_errno.c@btf_is_decl_tag": "static inline bool btf_is_decl_tag ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_DECL_TAG ; }",
    "resources/libbpf/src/libbpf_errno.c@btf_is_type_tag": "static inline bool btf_is_type_tag ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_TYPE_TAG ; }",
    "resources/libbpf/src/libbpf_errno.c@btf_is_any_enum": "static inline bool btf_is_any_enum ( const struct btf_type * t ) { return btf_is_enum ( t ) || btf_is_enum64 ( t ) ; }",
    "resources/libbpf/src/libbpf_errno.c@btf_kind_core_compat": "static inline bool btf_kind_core_compat ( const struct btf_type * t1 , const struct btf_type * t2 ) { return btf_kind ( t1 ) == btf_kind ( t2 ) || ( btf_is_any_enum ( t1 ) && btf_is_any_enum ( t2 ) ) ; }",
    "resources/libbpf/src/libbpf_errno.c@btf_int_encoding": "static inline __u8 btf_int_encoding ( const struct btf_type * t ) { return BTF_INT_ENCODING ( * ( __u32 * ) ( t + 1 ) ) ; }",
    "resources/libbpf/src/libbpf_errno.c@btf_int_offset": "static inline __u8 btf_int_offset ( const struct btf_type * t ) { return BTF_INT_OFFSET ( * ( __u32 * ) ( t + 1 ) ) ; }",
    "resources/libbpf/src/libbpf_errno.c@btf_int_bits": "static inline __u8 btf_int_bits ( const struct btf_type * t ) { return BTF_INT_BITS ( * ( __u32 * ) ( t + 1 ) ) ; }",
    "resources/libbpf/src/libbpf_errno.c@btf_array": "static inline struct btf_array * btf_array ( const struct btf_type * t ) { return ( struct btf_array * ) ( t + 1 ) ; }",
    "resources/libbpf/src/libbpf_errno.c@btf_enum": "static inline struct btf_enum * btf_enum ( const struct btf_type * t ) { return ( struct btf_enum * ) ( t + 1 ) ; }",
    "resources/libbpf/src/libbpf_errno.c@btf_enum64": "static inline struct btf_enum64 * btf_enum64 ( const struct btf_type * t ) { return ( struct btf_enum64 * ) ( t + 1 ) ; }",
    "resources/libbpf/src/libbpf_errno.c@btf_enum64_value": "static inline __u64 btf_enum64_value ( const struct btf_enum64 * e ) { /* struct btf_enum64 is introduced in Linux 6.0, which is very\n\t * bleeding-edge. Here we are avoiding relying on struct btf_enum64\n\t * definition coming from kernel UAPI headers to support wider range\n\t * of system-wide kernel headers.\n\t *\n\t * Given this header can be also included from C++ applications, that\n\t * further restricts C tricks we can use (like using compatible\n\t * anonymous struct). So just treat struct btf_enum64 as\n\t * a three-element array of u32 and access second (lo32) and third\n\t * (hi32) elements directly.\n\t *\n\t * For reference, here is a struct btf_enum64 definition:\n\t *\n\t * const struct btf_enum64 {\n\t *\t__u32\tname_off;\n\t *\t__u32\tval_lo32;\n\t *\t__u32\tval_hi32;\n\t * };\n\t */ const __u32 * e64 = ( const __u32 * ) e ; return ( ( __u64 ) e64 [ 2 ] << 32 ) | e64 [ 1 ] ; }",
    "resources/libbpf/src/libbpf_errno.c@btf_members": "static inline struct btf_member * btf_members ( const struct btf_type * t ) { return ( struct btf_member * ) ( t + 1 ) ; }",
    "resources/libbpf/src/libbpf_errno.c@btf_member_bit_offset": "static inline __u32 btf_member_bit_offset ( const struct btf_type * t , __u32 member_idx ) { const struct btf_member * m = btf_members ( t ) + member_idx ; bool kflag = btf_kflag ( t ) ; return kflag ? BTF_MEMBER_BIT_OFFSET ( m -> offset ) : m -> offset ; }",
    "resources/libbpf/src/libbpf_errno.c@btf_member_bitfield_size": "static inline __u32 btf_member_bitfield_size ( const struct btf_type * t , __u32 member_idx ) { const struct btf_member * m = btf_members ( t ) + member_idx ; bool kflag = btf_kflag ( t ) ; return kflag ? BTF_MEMBER_BITFIELD_SIZE ( m -> offset ) : 0 ; }",
    "resources/libbpf/src/libbpf_errno.c@btf_params": "static inline struct btf_param * btf_params ( const struct btf_type * t ) { return ( struct btf_param * ) ( t + 1 ) ; }",
    "resources/libbpf/src/libbpf_errno.c@btf_var": "static inline struct btf_var * btf_var ( const struct btf_type * t ) { return ( struct btf_var * ) ( t + 1 ) ; }",
    "resources/libbpf/src/libbpf_errno.c@btf_var_secinfos": "static inline struct btf_var_secinfo * btf_var_secinfos ( const struct btf_type * t ) { return ( struct btf_var_secinfo * ) ( t + 1 ) ; }",
    "resources/libbpf/src/libbpf_errno.c@btf_decl_tag": "static inline struct btf_decl_tag * btf_decl_tag ( const struct btf_type * t ) { return ( struct btf_decl_tag * ) ( t + 1 ) ; }",
    "resources/libbpf/src/libbpf_errno.c@str_has_sfx": "static inline bool str_has_sfx ( const char * str , const char * sfx ) { size_t str_len = strlen ( str ) ; size_t sfx_len = strlen ( sfx ) ; if ( sfx_len > str_len ) return false ; return strcmp ( str + str_len - sfx_len , sfx ) == 0 ; }",
    "resources/libbpf/src/libbpf_errno.c@libbpf_reallocarray": "static inline void * libbpf_reallocarray ( void * ptr , size_t nmemb , size_t size ) { size_t total ; # if __has_builtin ( __builtin_mul_overflow ) if ( unlikely ( __builtin_mul_overflow ( nmemb , size , & total ) ) ) return NULL ; # else if ( size == 0 || nmemb > ULONG_MAX / size ) return NULL ; total = nmemb * size ; # endif return realloc ( ptr , total ) ; }",
    "resources/libbpf/src/libbpf_errno.c@libbpf_strlcpy": "static inline void libbpf_strlcpy ( char * dst , const char * src , size_t sz ) { size_t i ; if ( sz == 0 ) return ; sz -- ; for ( i = 0 ; i < sz && src [ i ] ; i ++ ) dst [ i ] = src [ i ] ; dst [ i ] = '\\0' ; }",
    "resources/libbpf/src/libbpf_errno.c@btf_func_linkage": "static inline enum btf_func_linkage btf_func_linkage ( const struct btf_type * t ) { return ( enum btf_func_linkage ) ( int ) btf_vlen ( t ) ; }",
    "resources/libbpf/src/libbpf_errno.c@btf_type_info": "static inline __u32 btf_type_info ( int kind , int vlen , int kflag ) { return ( kflag << 31 ) | ( kind << 24 ) | vlen ; }",
    "resources/libbpf/src/libbpf_errno.c@libbpf_is_mem_zeroed": "static inline bool libbpf_is_mem_zeroed ( const char * p , ssize_t len ) { while ( len > 0 ) { if ( * p ) return false ; p ++ ; len -- ; } return true ; }",
    "resources/libbpf/src/libbpf_errno.c@libbpf_validate_opts": "static inline bool libbpf_validate_opts ( const char * opts , size_t opts_sz , size_t user_sz , const char * type_name ) { if ( user_sz < sizeof ( size_t ) ) { pr_warn ( \"%s size (%zu) is too small\\n\" , type_name , user_sz ) ; return false ; } if ( ! libbpf_is_mem_zeroed ( opts + opts_sz , ( ssize_t ) user_sz - opts_sz ) ) { pr_warn ( \"%s has non-zero extra bytes\\n\" , type_name ) ; return false ; } return true ; }",
    "resources/libbpf/src/libbpf_errno.c@libbpf_err": "static inline int libbpf_err ( int ret ) { if ( ret < 0 ) errno = - ret ; return ret ; }",
    "resources/libbpf/src/libbpf_errno.c@libbpf_err_errno": "static inline int libbpf_err_errno ( int ret ) { /* errno is already assumed to be set on error */ return ret < 0 ? - errno : ret ; }",
    "resources/libbpf/src/libbpf_errno.c@libbpf_err_ptr": "static inline void * libbpf_err_ptr ( int err ) { /* set errno on error, this doesn't break anything */ errno = - err ; return NULL ; }",
    "resources/libbpf/src/libbpf_errno.c@libbpf_ptr": "static inline void * libbpf_ptr ( void * ret ) { /* set errno on error, this doesn't break anything */ if ( IS_ERR ( ret ) ) errno = - PTR_ERR ( ret ) ; return IS_ERR ( ret ) ? NULL : ret ; }",
    "resources/libbpf/src/libbpf_errno.c@str_is_empty": "static inline bool str_is_empty ( const char * s ) { return ! s || ! s [ 0 ] ; }",
    "resources/libbpf/src/libbpf_errno.c@is_ldimm64_insn": "static inline bool is_ldimm64_insn ( struct bpf_insn * insn ) { return insn -> code == ( BPF_LD | BPF_IMM | BPF_DW ) ; }",
    "resources/libbpf/src/libbpf_errno.c@dup_good_fd": "static inline int dup_good_fd ( int fd ) { if ( fd < 0 ) return fd ; return fcntl ( fd , F_DUPFD_CLOEXEC , 3 ) ; }",
    "resources/libbpf/src/libbpf_errno.c@ensure_good_fd": "static inline int ensure_good_fd ( int fd ) { int old_fd = fd , saved_errno ; if ( fd < 0 ) return fd ; if ( fd < 3 ) { fd = dup_good_fd ( fd ) ; saved_errno = errno ; close ( old_fd ) ; errno = saved_errno ; if ( fd < 0 ) { pr_warn ( \"failed to dup FD %d to FD > 2: %d\\n\" , old_fd , - saved_errno ) ; errno = saved_errno ; } } return fd ; }",
    "resources/libbpf/src/libbpf_errno.c@sys_dup2": "static inline int sys_dup2 ( int oldfd , int newfd ) { # ifdef __NR_dup2 return syscall ( __NR_dup2 , oldfd , newfd ) ; # else return syscall ( __NR_dup3 , oldfd , newfd , 0 ) ; # endif }",
    "resources/libbpf/src/libbpf_errno.c@reuse_fd": "static inline int reuse_fd ( int fixed_fd , int tmp_fd ) { int err ; err = sys_dup2 ( tmp_fd , fixed_fd ) ; err = err < 0 ? - errno : 0 ; close ( tmp_fd ) ; /* clean up temporary FD */ return err ; }",
    "resources/libbpf/src/libbpf_errno.c@is_pow_of_2": "static inline bool is_pow_of_2 ( size_t x ) { return x && ( x & ( x - 1 ) ) == 0 ; }",
    "resources/libbpf/src/libbpf_errno.c@libbpf_strerror": "int libbpf_strerror ( int err , char * buf , size_t size ) { int ret ; if ( ! buf || ! size ) return libbpf_err ( - EINVAL ) ; err = err > 0 ? err : - err ; if ( err < __LIBBPF_ERRNO__START ) { ret = strerror_r ( err , buf , size ) ; buf [ size - 1 ] = '\\0' ; return libbpf_err_errno ( ret ) ; } if ( err < __LIBBPF_ERRNO__END ) { const char * msg ; msg = libbpf_strerror_table [ ERRNO_OFFSET ( err ) ] ; ret = snprintf ( buf , size , \"%s\" , msg ) ; buf [ size - 1 ] = '\\0' ; /* The length of the buf and msg is positive.\n\t\t * A negative number may be returned only when the\n\t\t * size exceeds INT_MAX. Not likely to appear.\n\t\t */ if ( ret >= size ) return libbpf_err ( - ERANGE ) ; return 0 ; } ret = snprintf ( buf , size , \"Unknown libbpf error %d\" , err ) ; buf [ size - 1 ] = '\\0' ; if ( ret >= size ) return libbpf_err ( - ERANGE ) ; return libbpf_err ( - ENOENT ) ; }",
    "resources/libbpf/src/btf_dump.c@btf_kind": "static inline __u16 btf_kind ( const struct btf_type * t ) { return BTF_INFO_KIND ( t -> info ) ; }",
    "resources/libbpf/src/btf_dump.c@btf_vlen": "static inline __u16 btf_vlen ( const struct btf_type * t ) { return BTF_INFO_VLEN ( t -> info ) ; }",
    "resources/libbpf/src/btf_dump.c@btf_kflag": "static inline bool btf_kflag ( const struct btf_type * t ) { return BTF_INFO_KFLAG ( t -> info ) ; }",
    "resources/libbpf/src/btf_dump.c@btf_is_void": "static inline bool btf_is_void ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_UNKN ; }",
    "resources/libbpf/src/btf_dump.c@btf_is_int": "static inline bool btf_is_int ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_INT ; }",
    "resources/libbpf/src/btf_dump.c@btf_is_ptr": "static inline bool btf_is_ptr ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_PTR ; }",
    "resources/libbpf/src/btf_dump.c@btf_is_array": "static inline bool btf_is_array ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_ARRAY ; }",
    "resources/libbpf/src/btf_dump.c@btf_is_struct": "static inline bool btf_is_struct ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_STRUCT ; }",
    "resources/libbpf/src/btf_dump.c@btf_is_union": "static inline bool btf_is_union ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_UNION ; }",
    "resources/libbpf/src/btf_dump.c@btf_is_composite": "static inline bool btf_is_composite ( const struct btf_type * t ) { __u16 kind = btf_kind ( t ) ; return kind == BTF_KIND_STRUCT || kind == BTF_KIND_UNION ; }",
    "resources/libbpf/src/btf_dump.c@btf_is_enum": "static inline bool btf_is_enum ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_ENUM ; }",
    "resources/libbpf/src/btf_dump.c@btf_is_enum64": "static inline bool btf_is_enum64 ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_ENUM64 ; }",
    "resources/libbpf/src/btf_dump.c@btf_is_fwd": "static inline bool btf_is_fwd ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_FWD ; }",
    "resources/libbpf/src/btf_dump.c@btf_is_typedef": "static inline bool btf_is_typedef ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_TYPEDEF ; }",
    "resources/libbpf/src/btf_dump.c@btf_is_volatile": "static inline bool btf_is_volatile ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_VOLATILE ; }",
    "resources/libbpf/src/btf_dump.c@btf_is_const": "static inline bool btf_is_const ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_CONST ; }",
    "resources/libbpf/src/btf_dump.c@btf_is_restrict": "static inline bool btf_is_restrict ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_RESTRICT ; }",
    "resources/libbpf/src/btf_dump.c@btf_is_mod": "static inline bool btf_is_mod ( const struct btf_type * t ) { __u16 kind = btf_kind ( t ) ; return kind == BTF_KIND_VOLATILE || kind == BTF_KIND_CONST || kind == BTF_KIND_RESTRICT || kind == BTF_KIND_TYPE_TAG ; }",
    "resources/libbpf/src/btf_dump.c@btf_is_func": "static inline bool btf_is_func ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_FUNC ; }",
    "resources/libbpf/src/btf_dump.c@btf_is_func_proto": "static inline bool btf_is_func_proto ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_FUNC_PROTO ; }",
    "resources/libbpf/src/btf_dump.c@btf_is_var": "static inline bool btf_is_var ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_VAR ; }",
    "resources/libbpf/src/btf_dump.c@btf_is_datasec": "static inline bool btf_is_datasec ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_DATASEC ; }",
    "resources/libbpf/src/btf_dump.c@btf_is_float": "static inline bool btf_is_float ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_FLOAT ; }",
    "resources/libbpf/src/btf_dump.c@btf_is_decl_tag": "static inline bool btf_is_decl_tag ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_DECL_TAG ; }",
    "resources/libbpf/src/btf_dump.c@btf_is_type_tag": "static inline bool btf_is_type_tag ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_TYPE_TAG ; }",
    "resources/libbpf/src/btf_dump.c@btf_is_any_enum": "static inline bool btf_is_any_enum ( const struct btf_type * t ) { return btf_is_enum ( t ) || btf_is_enum64 ( t ) ; }",
    "resources/libbpf/src/btf_dump.c@btf_kind_core_compat": "static inline bool btf_kind_core_compat ( const struct btf_type * t1 , const struct btf_type * t2 ) { return btf_kind ( t1 ) == btf_kind ( t2 ) || ( btf_is_any_enum ( t1 ) && btf_is_any_enum ( t2 ) ) ; }",
    "resources/libbpf/src/btf_dump.c@btf_int_encoding": "static inline __u8 btf_int_encoding ( const struct btf_type * t ) { return BTF_INT_ENCODING ( * ( __u32 * ) ( t + 1 ) ) ; }",
    "resources/libbpf/src/btf_dump.c@btf_int_offset": "static inline __u8 btf_int_offset ( const struct btf_type * t ) { return BTF_INT_OFFSET ( * ( __u32 * ) ( t + 1 ) ) ; }",
    "resources/libbpf/src/btf_dump.c@btf_int_bits": "static inline __u8 btf_int_bits ( const struct btf_type * t ) { return BTF_INT_BITS ( * ( __u32 * ) ( t + 1 ) ) ; }",
    "resources/libbpf/src/btf_dump.c@btf_array": "static inline struct btf_array * btf_array ( const struct btf_type * t ) { return ( struct btf_array * ) ( t + 1 ) ; }",
    "resources/libbpf/src/btf_dump.c@btf_enum": "static inline struct btf_enum * btf_enum ( const struct btf_type * t ) { return ( struct btf_enum * ) ( t + 1 ) ; }",
    "resources/libbpf/src/btf_dump.c@btf_enum64": "static inline struct btf_enum64 * btf_enum64 ( const struct btf_type * t ) { return ( struct btf_enum64 * ) ( t + 1 ) ; }",
    "resources/libbpf/src/btf_dump.c@btf_enum64_value": "static inline __u64 btf_enum64_value ( const struct btf_enum64 * e ) { /* struct btf_enum64 is introduced in Linux 6.0, which is very\n\t * bleeding-edge. Here we are avoiding relying on struct btf_enum64\n\t * definition coming from kernel UAPI headers to support wider range\n\t * of system-wide kernel headers.\n\t *\n\t * Given this header can be also included from C++ applications, that\n\t * further restricts C tricks we can use (like using compatible\n\t * anonymous struct). So just treat struct btf_enum64 as\n\t * a three-element array of u32 and access second (lo32) and third\n\t * (hi32) elements directly.\n\t *\n\t * For reference, here is a struct btf_enum64 definition:\n\t *\n\t * const struct btf_enum64 {\n\t *\t__u32\tname_off;\n\t *\t__u32\tval_lo32;\n\t *\t__u32\tval_hi32;\n\t * };\n\t */ const __u32 * e64 = ( const __u32 * ) e ; return ( ( __u64 ) e64 [ 2 ] << 32 ) | e64 [ 1 ] ; }",
    "resources/libbpf/src/btf_dump.c@btf_members": "static inline struct btf_member * btf_members ( const struct btf_type * t ) { return ( struct btf_member * ) ( t + 1 ) ; }",
    "resources/libbpf/src/btf_dump.c@btf_member_bit_offset": "static inline __u32 btf_member_bit_offset ( const struct btf_type * t , __u32 member_idx ) { const struct btf_member * m = btf_members ( t ) + member_idx ; bool kflag = btf_kflag ( t ) ; return kflag ? BTF_MEMBER_BIT_OFFSET ( m -> offset ) : m -> offset ; }",
    "resources/libbpf/src/btf_dump.c@btf_member_bitfield_size": "static inline __u32 btf_member_bitfield_size ( const struct btf_type * t , __u32 member_idx ) { const struct btf_member * m = btf_members ( t ) + member_idx ; bool kflag = btf_kflag ( t ) ; return kflag ? BTF_MEMBER_BITFIELD_SIZE ( m -> offset ) : 0 ; }",
    "resources/libbpf/src/btf_dump.c@btf_params": "static inline struct btf_param * btf_params ( const struct btf_type * t ) { return ( struct btf_param * ) ( t + 1 ) ; }",
    "resources/libbpf/src/btf_dump.c@btf_var": "static inline struct btf_var * btf_var ( const struct btf_type * t ) { return ( struct btf_var * ) ( t + 1 ) ; }",
    "resources/libbpf/src/btf_dump.c@btf_var_secinfos": "static inline struct btf_var_secinfo * btf_var_secinfos ( const struct btf_type * t ) { return ( struct btf_var_secinfo * ) ( t + 1 ) ; }",
    "resources/libbpf/src/btf_dump.c@btf_decl_tag": "static inline struct btf_decl_tag * btf_decl_tag ( const struct btf_type * t ) { return ( struct btf_decl_tag * ) ( t + 1 ) ; }",
    "resources/libbpf/src/btf_dump.c@hash_bits": "static inline size_t hash_bits ( size_t h , int bits ) { /* shuffle bits and return requested number of upper bits */ if ( bits == 0 ) return 0 ; # if ( __SIZEOF_SIZE_T__ == __SIZEOF_LONG_LONG__ ) /* LP64 case */ return ( h * 11400714819323198485llu ) >> ( __SIZEOF_LONG_LONG__ * 8 - bits ) ; # elif ( __SIZEOF_SIZE_T__ <= __SIZEOF_LONG__ ) return ( h * 2654435769lu ) >> ( __SIZEOF_LONG__ * 8 - bits ) ; # else # error \"Unsupported size_t size\" # endif }",
    "resources/libbpf/src/btf_dump.c@str_hash": "static inline size_t str_hash ( const char * s ) { size_t h = 0 ; while ( * s ) { h = h * 31 + * s ; s ++ ; } return h ; }",
    "resources/libbpf/src/btf_dump.c@str_has_sfx": "static inline bool str_has_sfx ( const char * str , const char * sfx ) { size_t str_len = strlen ( str ) ; size_t sfx_len = strlen ( sfx ) ; if ( sfx_len > str_len ) return false ; return strcmp ( str + str_len - sfx_len , sfx ) == 0 ; }",
    "resources/libbpf/src/btf_dump.c@libbpf_reallocarray": "static inline void * libbpf_reallocarray ( void * ptr , size_t nmemb , size_t size ) { size_t total ; # if __has_builtin ( __builtin_mul_overflow ) if ( unlikely ( __builtin_mul_overflow ( nmemb , size , & total ) ) ) return NULL ; # else if ( size == 0 || nmemb > ULONG_MAX / size ) return NULL ; total = nmemb * size ; # endif return realloc ( ptr , total ) ; }",
    "resources/libbpf/src/btf_dump.c@libbpf_strlcpy": "static inline void libbpf_strlcpy ( char * dst , const char * src , size_t sz ) { size_t i ; if ( sz == 0 ) return ; sz -- ; for ( i = 0 ; i < sz && src [ i ] ; i ++ ) dst [ i ] = src [ i ] ; dst [ i ] = '\\0' ; }",
    "resources/libbpf/src/btf_dump.c@btf_func_linkage": "static inline enum btf_func_linkage btf_func_linkage ( const struct btf_type * t ) { return ( enum btf_func_linkage ) ( int ) btf_vlen ( t ) ; }",
    "resources/libbpf/src/btf_dump.c@btf_type_info": "static inline __u32 btf_type_info ( int kind , int vlen , int kflag ) { return ( kflag << 31 ) | ( kind << 24 ) | vlen ; }",
    "resources/libbpf/src/btf_dump.c@libbpf_is_mem_zeroed": "static inline bool libbpf_is_mem_zeroed ( const char * p , ssize_t len ) { while ( len > 0 ) { if ( * p ) return false ; p ++ ; len -- ; } return true ; }",
    "resources/libbpf/src/btf_dump.c@libbpf_validate_opts": "static inline bool libbpf_validate_opts ( const char * opts , size_t opts_sz , size_t user_sz , const char * type_name ) { if ( user_sz < sizeof ( size_t ) ) { pr_warn ( \"%s size (%zu) is too small\\n\" , type_name , user_sz ) ; return false ; } if ( ! libbpf_is_mem_zeroed ( opts + opts_sz , ( ssize_t ) user_sz - opts_sz ) ) { pr_warn ( \"%s has non-zero extra bytes\\n\" , type_name ) ; return false ; } return true ; }",
    "resources/libbpf/src/btf_dump.c@libbpf_err": "static inline int libbpf_err ( int ret ) { if ( ret < 0 ) errno = - ret ; return ret ; }",
    "resources/libbpf/src/btf_dump.c@libbpf_err_errno": "static inline int libbpf_err_errno ( int ret ) { /* errno is already assumed to be set on error */ return ret < 0 ? - errno : ret ; }",
    "resources/libbpf/src/btf_dump.c@libbpf_err_ptr": "static inline void * libbpf_err_ptr ( int err ) { /* set errno on error, this doesn't break anything */ errno = - err ; return NULL ; }",
    "resources/libbpf/src/btf_dump.c@libbpf_ptr": "static inline void * libbpf_ptr ( void * ret ) { /* set errno on error, this doesn't break anything */ if ( IS_ERR ( ret ) ) errno = - PTR_ERR ( ret ) ; return IS_ERR ( ret ) ? NULL : ret ; }",
    "resources/libbpf/src/btf_dump.c@str_is_empty": "static inline bool str_is_empty ( const char * s ) { return ! s || ! s [ 0 ] ; }",
    "resources/libbpf/src/btf_dump.c@is_ldimm64_insn": "static inline bool is_ldimm64_insn ( struct bpf_insn * insn ) { return insn -> code == ( BPF_LD | BPF_IMM | BPF_DW ) ; }",
    "resources/libbpf/src/btf_dump.c@dup_good_fd": "static inline int dup_good_fd ( int fd ) { if ( fd < 0 ) return fd ; return fcntl ( fd , F_DUPFD_CLOEXEC , 3 ) ; }",
    "resources/libbpf/src/btf_dump.c@ensure_good_fd": "static inline int ensure_good_fd ( int fd ) { int old_fd = fd , saved_errno ; if ( fd < 0 ) return fd ; if ( fd < 3 ) { fd = dup_good_fd ( fd ) ; saved_errno = errno ; close ( old_fd ) ; errno = saved_errno ; if ( fd < 0 ) { pr_warn ( \"failed to dup FD %d to FD > 2: %d\\n\" , old_fd , - saved_errno ) ; errno = saved_errno ; } } return fd ; }",
    "resources/libbpf/src/btf_dump.c@sys_dup2": "static inline int sys_dup2 ( int oldfd , int newfd ) { # ifdef __NR_dup2 return syscall ( __NR_dup2 , oldfd , newfd ) ; # else return syscall ( __NR_dup3 , oldfd , newfd , 0 ) ; # endif }",
    "resources/libbpf/src/btf_dump.c@reuse_fd": "static inline int reuse_fd ( int fixed_fd , int tmp_fd ) { int err ; err = sys_dup2 ( tmp_fd , fixed_fd ) ; err = err < 0 ? - errno : 0 ; close ( tmp_fd ) ; /* clean up temporary FD */ return err ; }",
    "resources/libbpf/src/btf_dump.c@is_pow_of_2": "static inline bool is_pow_of_2 ( size_t x ) { return x && ( x & ( x - 1 ) ) == 0 ; }",
    "resources/libbpf/src/btf_dump.c@pfx": "static const char * pfx ( int lvl ) { return lvl >= PREFIX_CNT ? PREFIXES : & PREFIXES [ PREFIX_CNT - lvl ] ; }",
    "resources/libbpf/src/btf_dump.c@str_hash_fn": "static size_t str_hash_fn ( long key , void * ctx ) { return str_hash ( ( void * ) key ) ; }",
    "resources/libbpf/src/btf_dump.c@str_equal_fn": "static bool str_equal_fn ( long a , long b , void * ctx ) { return strcmp ( ( void * ) a , ( void * ) b ) == 0 ; }",
    "resources/libbpf/src/btf_dump.c@btf_name_of": "static const char * btf_name_of ( const struct btf_dump * d , __u32 name_off ) { return btf__name_by_offset ( d -> btf , name_off ) ; }",
    "resources/libbpf/src/btf_dump.c@btf_dump_printf": "static void btf_dump_printf ( const struct btf_dump * d , const char * fmt , ... ) { va_list args ; va_start ( args , fmt ) ; d -> printf_fn ( d -> cb_ctx , fmt , args ) ; va_end ( args ) ; }",
    "resources/libbpf/src/btf_dump.c@btf_dump__new": "struct btf_dump * btf_dump__new ( const struct btf * btf , btf_dump_printf_fn_t printf_fn , void * ctx , const struct btf_dump_opts * opts ) { struct btf_dump * d ; int err ; if ( ! OPTS_VALID ( opts , btf_dump_opts ) ) return libbpf_err_ptr ( - EINVAL ) ; if ( ! printf_fn ) return libbpf_err_ptr ( - EINVAL ) ; d = calloc ( 1 , sizeof ( struct btf_dump ) ) ; if ( ! d ) return libbpf_err_ptr ( - ENOMEM ) ; d -> btf = btf ; d -> printf_fn = printf_fn ; d -> cb_ctx = ctx ; d -> ptr_sz = btf__pointer_size ( btf ) ? : sizeof ( void * ) ; d -> type_names = hashmap__new ( str_hash_fn , str_equal_fn , NULL ) ; if ( IS_ERR ( d -> type_names ) ) { err = PTR_ERR ( d -> type_names ) ; d -> type_names = NULL ; goto err ; } d -> ident_names = hashmap__new ( str_hash_fn , str_equal_fn , NULL ) ; if ( IS_ERR ( d -> ident_names ) ) { err = PTR_ERR ( d -> ident_names ) ; d -> ident_names = NULL ; goto err ; } err = btf_dump_resize ( d ) ; if ( err ) goto err ; return d ; err : btf_dump__free ( d ) ; return libbpf_err_ptr ( err ) ; }",
    "resources/libbpf/src/btf_dump.c@btf_dump_resize": "static int btf_dump_resize ( struct btf_dump * d ) { int err , last_id = btf__type_cnt ( d -> btf ) - 1 ; if ( last_id <= d -> last_id ) return 0 ; if ( libbpf_ensure_mem ( ( void * * ) & d -> type_states , & d -> type_states_cap , sizeof ( * d -> type_states ) , last_id + 1 ) ) return - ENOMEM ; if ( libbpf_ensure_mem ( ( void * * ) & d -> cached_names , & d -> cached_names_cap , sizeof ( * d -> cached_names ) , last_id + 1 ) ) return - ENOMEM ; if ( d -> last_id == 0 ) { /* VOID is special */ d -> type_states [ 0 ] . order_state = ORDERED ; d -> type_states [ 0 ] . emit_state = EMITTED ; } /* eagerly determine referenced types for anon enums */ err = btf_dump_mark_referenced ( d ) ; if ( err ) return err ; d -> last_id = last_id ; return 0 ; }",
    "resources/libbpf/src/btf_dump.c@btf_dump_free_names": "static void btf_dump_free_names ( struct hashmap * map ) { size_t bkt ; struct hashmap_entry * cur ; hashmap__for_each_entry ( map , cur , bkt ) free ( ( void * ) cur -> pkey ) ; hashmap__free ( map ) ; }",
    "resources/libbpf/src/btf_dump.c@btf_dump__free": "void btf_dump__free ( struct btf_dump * d ) { int i ; if ( IS_ERR_OR_NULL ( d ) ) return ; free ( d -> type_states ) ; if ( d -> cached_names ) { /* any set cached name is owned by us and should be freed */ for ( i = 0 ; i <= d -> last_id ; i ++ ) { if ( d -> cached_names [ i ] ) free ( ( void * ) d -> cached_names [ i ] ) ; } } free ( d -> cached_names ) ; free ( d -> emit_queue ) ; free ( d -> decl_stack ) ; btf_dump_free_names ( d -> type_names ) ; btf_dump_free_names ( d -> ident_names ) ; free ( d ) ; }",
    "resources/libbpf/src/btf_dump.c@btf_dump__dump_type": "int btf_dump__dump_type ( struct btf_dump * d , __u32 id ) { int err , i ; if ( id >= btf__type_cnt ( d -> btf ) ) return libbpf_err ( - EINVAL ) ; err = btf_dump_resize ( d ) ; if ( err ) return libbpf_err ( err ) ; d -> emit_queue_cnt = 0 ; err = btf_dump_order_type ( d , id , false ) ; if ( err < 0 ) return libbpf_err ( err ) ; for ( i = 0 ; i < d -> emit_queue_cnt ; i ++ ) btf_dump_emit_type ( d , d -> emit_queue [ i ] , 0 /*top-level*/ ) ; return 0 ; }",
    "resources/libbpf/src/btf_dump.c@btf_dump_mark_referenced": "static int btf_dump_mark_referenced ( struct btf_dump * d ) { int i , j , n = btf__type_cnt ( d -> btf ) ; const struct btf_type * t ; __u16 vlen ; for ( i = d -> last_id + 1 ; i < n ; i ++ ) { t = btf__type_by_id ( d -> btf , i ) ; vlen = btf_vlen ( t ) ; switch ( btf_kind ( t ) ) { case BTF_KIND_INT : case BTF_KIND_ENUM : case BTF_KIND_ENUM64 : case BTF_KIND_FWD : case BTF_KIND_FLOAT : break ; case BTF_KIND_VOLATILE : case BTF_KIND_CONST : case BTF_KIND_RESTRICT : case BTF_KIND_PTR : case BTF_KIND_TYPEDEF : case BTF_KIND_FUNC : case BTF_KIND_VAR : case BTF_KIND_DECL_TAG : case BTF_KIND_TYPE_TAG : d -> type_states [ t -> type ] . referenced = 1 ; break ; case BTF_KIND_ARRAY : { const struct btf_array * a = btf_array ( t ) ; d -> type_states [ a -> index_type ] . referenced = 1 ; d -> type_states [ a -> type ] . referenced = 1 ; break ; } case BTF_KIND_STRUCT : case BTF_KIND_UNION : { const struct btf_member * m = btf_members ( t ) ; for ( j = 0 ; j < vlen ; j ++ , m ++ ) d -> type_states [ m -> type ] . referenced = 1 ; break ; } case BTF_KIND_FUNC_PROTO : { const struct btf_param * p = btf_params ( t ) ; for ( j = 0 ; j < vlen ; j ++ , p ++ ) d -> type_states [ p -> type ] . referenced = 1 ; break ; } case BTF_KIND_DATASEC : { const struct btf_var_secinfo * v = btf_var_secinfos ( t ) ; for ( j = 0 ; j < vlen ; j ++ , v ++ ) d -> type_states [ v -> type ] . referenced = 1 ; break ; } default : return - EINVAL ; } } return 0 ; }",
    "resources/libbpf/src/btf_dump.c@btf_dump_add_emit_queue_id": "static int btf_dump_add_emit_queue_id ( struct btf_dump * d , __u32 id ) { __u32 * new_queue ; size_t new_cap ; if ( d -> emit_queue_cnt >= d -> emit_queue_cap ) { new_cap = max ( 16 , d -> emit_queue_cap * 3 / 2 ) ; new_queue = libbpf_reallocarray ( d -> emit_queue , new_cap , sizeof ( new_queue [ 0 ] ) ) ; if ( ! new_queue ) return - ENOMEM ; d -> emit_queue = new_queue ; d -> emit_queue_cap = new_cap ; } d -> emit_queue [ d -> emit_queue_cnt ++ ] = id ; return 0 ; }",
    "resources/libbpf/src/btf_dump.c@btf_dump_order_type": "static int btf_dump_order_type ( struct btf_dump * d , __u32 id , bool through_ptr ) { /*\n\t * Order state is used to detect strong link cycles, but only for BTF\n\t * kinds that are or could be an independent definition (i.e.,\n\t * stand-alone fwd decl, enum, typedef, struct, union). Ptrs, arrays,\n\t * func_protos, modifiers are just means to get to these definitions.\n\t * Int/void don't need definitions, they are assumed to be always\n\t * properly defined.  We also ignore datasec, var, and funcs for now.\n\t * So for all non-defining kinds, we never even set ordering state,\n\t * for defining kinds we set ORDERING and subsequently ORDERED if it\n\t * forms a strong link.\n\t */ struct btf_dump_type_aux_state * tstate = & d -> type_states [ id ] ; const struct btf_type * t ; __u16 vlen ; int err , i ; /* return true, letting typedefs know that it's ok to be emitted */ if ( tstate -> order_state == ORDERED ) return 1 ; t = btf__type_by_id ( d -> btf , id ) ; if ( tstate -> order_state == ORDERING ) { /* type loop, but resolvable through fwd declaration */ if ( btf_is_composite ( t ) && through_ptr && t -> name_off != 0 ) return 0 ; pr_warn ( \"unsatisfiable type cycle, id:[%u]\\n\" , id ) ; return - ELOOP ; } switch ( btf_kind ( t ) ) { case BTF_KIND_INT : case BTF_KIND_FLOAT : tstate -> order_state = ORDERED ; return 0 ; case BTF_KIND_PTR : err = btf_dump_order_type ( d , t -> type , true ) ; tstate -> order_state = ORDERED ; return err ; case BTF_KIND_ARRAY : return btf_dump_order_type ( d , btf_array ( t ) -> type , false ) ; case BTF_KIND_STRUCT : case BTF_KIND_UNION : { const struct btf_member * m = btf_members ( t ) ; /*\n\t\t * struct/union is part of strong link, only if it's embedded\n\t\t * (so no ptr in a path) or it's anonymous (so has to be\n\t\t * defined inline, even if declared through ptr)\n\t\t */ if ( through_ptr && t -> name_off != 0 ) return 0 ; tstate -> order_state = ORDERING ; vlen = btf_vlen ( t ) ; for ( i = 0 ; i < vlen ; i ++ , m ++ ) { err = btf_dump_order_type ( d , m -> type , false ) ; if ( err < 0 ) return err ; } if ( t -> name_off != 0 ) { err = btf_dump_add_emit_queue_id ( d , id ) ; if ( err < 0 ) return err ; } tstate -> order_state = ORDERED ; return 1 ; } case BTF_KIND_ENUM : case BTF_KIND_ENUM64 : case BTF_KIND_FWD : /*\n\t\t * non-anonymous or non-referenced enums are top-level\n\t\t * declarations and should be emitted. Same logic can be\n\t\t * applied to FWDs, it won't hurt anyways.\n\t\t */ if ( t -> name_off != 0 || ! tstate -> referenced ) { err = btf_dump_add_emit_queue_id ( d , id ) ; if ( err ) return err ; } tstate -> order_state = ORDERED ; return 1 ; case BTF_KIND_TYPEDEF : { int is_strong ; is_strong = btf_dump_order_type ( d , t -> type , through_ptr ) ; if ( is_strong < 0 ) return is_strong ; /* typedef is similar to struct/union w.r.t. fwd-decls */ if ( through_ptr && ! is_strong ) return 0 ; /* typedef is always a named definition */ err = btf_dump_add_emit_queue_id ( d , id ) ; if ( err ) return err ; d -> type_states [ id ] . order_state = ORDERED ; return 1 ; } case BTF_KIND_VOLATILE : case BTF_KIND_CONST : case BTF_KIND_RESTRICT : case BTF_KIND_TYPE_TAG : return btf_dump_order_type ( d , t -> type , through_ptr ) ; case BTF_KIND_FUNC_PROTO : { const struct btf_param * p = btf_params ( t ) ; bool is_strong ; err = btf_dump_order_type ( d , t -> type , through_ptr ) ; if ( err < 0 ) return err ; is_strong = err > 0 ; vlen = btf_vlen ( t ) ; for ( i = 0 ; i < vlen ; i ++ , p ++ ) { err = btf_dump_order_type ( d , p -> type , through_ptr ) ; if ( err < 0 ) return err ; if ( err > 0 ) is_strong = true ; } return is_strong ; } case BTF_KIND_FUNC : case BTF_KIND_VAR : case BTF_KIND_DATASEC : case BTF_KIND_DECL_TAG : d -> type_states [ id ] . order_state = ORDERED ; return 0 ; default : return - EINVAL ; } }",
    "resources/libbpf/src/btf_dump.c@btf_dump_is_blacklisted": "static bool btf_dump_is_blacklisted ( struct btf_dump * d , __u32 id ) { const struct btf_type * t = btf__type_by_id ( d -> btf , id ) ; /* __builtin_va_list is a compiler built-in, which causes compilation\n\t * errors, when compiling w/ different compiler, then used to compile\n\t * original code (e.g., GCC to compile kernel, Clang to use generated\n\t * C header from BTF). As it is built-in, it should be already defined\n\t * properly internally in compiler.\n\t */ if ( t -> name_off == 0 ) return false ; return strcmp ( btf_name_of ( d , t -> name_off ) , \"__builtin_va_list\" ) == 0 ; }",
    "resources/libbpf/src/btf_dump.c@btf_dump_emit_type": "static void btf_dump_emit_type ( struct btf_dump * d , __u32 id , __u32 cont_id ) { struct btf_dump_type_aux_state * tstate = & d -> type_states [ id ] ; bool top_level_def = cont_id == 0 ; const struct btf_type * t ; __u16 kind ; if ( tstate -> emit_state == EMITTED ) return ; t = btf__type_by_id ( d -> btf , id ) ; kind = btf_kind ( t ) ; if ( tstate -> emit_state == EMITTING ) { if ( tstate -> fwd_emitted ) return ; switch ( kind ) { case BTF_KIND_STRUCT : case BTF_KIND_UNION : /*\n\t\t\t * if we are referencing a struct/union that we are\n\t\t\t * part of - then no need for fwd declaration\n\t\t\t */ if ( id == cont_id ) return ; if ( t -> name_off == 0 ) { pr_warn ( \"anonymous struct/union loop, id:[%u]\\n\" , id ) ; return ; } btf_dump_emit_struct_fwd ( d , id , t ) ; btf_dump_printf ( d , \";\\n\\n\" ) ; tstate -> fwd_emitted = 1 ; break ; case BTF_KIND_TYPEDEF : /*\n\t\t\t * for typedef fwd_emitted means typedef definition\n\t\t\t * was emitted, but it can be used only for \"weak\"\n\t\t\t * references through pointer only, not for embedding\n\t\t\t */ if ( ! btf_dump_is_blacklisted ( d , id ) ) { btf_dump_emit_typedef_def ( d , id , t , 0 ) ; btf_dump_printf ( d , \";\\n\\n\" ) ; } tstate -> fwd_emitted = 1 ; break ; default : break ; } return ; } switch ( kind ) { case BTF_KIND_INT : /* Emit type alias definitions if necessary */ btf_dump_emit_missing_aliases ( d , id , t ) ; tstate -> emit_state = EMITTED ; break ; case BTF_KIND_ENUM : case BTF_KIND_ENUM64 : if ( top_level_def ) { btf_dump_emit_enum_def ( d , id , t , 0 ) ; btf_dump_printf ( d , \";\\n\\n\" ) ; } tstate -> emit_state = EMITTED ; break ; case BTF_KIND_PTR : case BTF_KIND_VOLATILE : case BTF_KIND_CONST : case BTF_KIND_RESTRICT : case BTF_KIND_TYPE_TAG : btf_dump_emit_type ( d , t -> type , cont_id ) ; break ; case BTF_KIND_ARRAY : btf_dump_emit_type ( d , btf_array ( t ) -> type , cont_id ) ; break ; case BTF_KIND_FWD : btf_dump_emit_fwd_def ( d , id , t ) ; btf_dump_printf ( d , \";\\n\\n\" ) ; tstate -> emit_state = EMITTED ; break ; case BTF_KIND_TYPEDEF : tstate -> emit_state = EMITTING ; btf_dump_emit_type ( d , t -> type , id ) ; /*\n\t\t * typedef can server as both definition and forward\n\t\t * declaration; at this stage someone depends on\n\t\t * typedef as a forward declaration (refers to it\n\t\t * through pointer), so unless we already did it,\n\t\t * emit typedef as a forward declaration\n\t\t */ if ( ! tstate -> fwd_emitted && ! btf_dump_is_blacklisted ( d , id ) ) { btf_dump_emit_typedef_def ( d , id , t , 0 ) ; btf_dump_printf ( d , \";\\n\\n\" ) ; } tstate -> emit_state = EMITTED ; break ; case BTF_KIND_STRUCT : case BTF_KIND_UNION : tstate -> emit_state = EMITTING ; /* if it's a top-level struct/union definition or struct/union\n\t\t * is anonymous, then in C we'll be emitting all fields and\n\t\t * their types (as opposed to just `struct X`), so we need to\n\t\t * make sure that all types, referenced from struct/union\n\t\t * members have necessary forward-declarations, where\n\t\t * applicable\n\t\t */ if ( top_level_def || t -> name_off == 0 ) { const struct btf_member * m = btf_members ( t ) ; __u16 vlen = btf_vlen ( t ) ; int i , new_cont_id ; new_cont_id = t -> name_off == 0 ? cont_id : id ; for ( i = 0 ; i < vlen ; i ++ , m ++ ) btf_dump_emit_type ( d , m -> type , new_cont_id ) ; } else if ( ! tstate -> fwd_emitted && id != cont_id ) { btf_dump_emit_struct_fwd ( d , id , t ) ; btf_dump_printf ( d , \";\\n\\n\" ) ; tstate -> fwd_emitted = 1 ; } if ( top_level_def ) { btf_dump_emit_struct_def ( d , id , t , 0 ) ; btf_dump_printf ( d , \";\\n\\n\" ) ; tstate -> emit_state = EMITTED ; } else { tstate -> emit_state = NOT_EMITTED ; } break ; case BTF_KIND_FUNC_PROTO : { const struct btf_param * p = btf_params ( t ) ; __u16 n = btf_vlen ( t ) ; int i ; btf_dump_emit_type ( d , t -> type , cont_id ) ; for ( i = 0 ; i < n ; i ++ , p ++ ) btf_dump_emit_type ( d , p -> type , cont_id ) ; break ; } default : break ; } }",
    "resources/libbpf/src/btf_dump.c@btf_is_struct_packed": "static bool btf_is_struct_packed ( const struct btf * btf , __u32 id , const struct btf_type * t ) { const struct btf_member * m ; int max_align = 1 , align , i , bit_sz ; __u16 vlen ; m = btf_members ( t ) ; vlen = btf_vlen ( t ) ; /* all non-bitfield fields have to be naturally aligned */ for ( i = 0 ; i < vlen ; i ++ , m ++ ) { align = btf__align_of ( btf , m -> type ) ; bit_sz = btf_member_bitfield_size ( t , i ) ; if ( align && bit_sz == 0 && m -> offset % ( 8 * align ) != 0 ) return true ; max_align = max ( align , max_align ) ; } /* size of a non-packed struct has to be a multiple of its alignment */ if ( t -> size % max_align != 0 ) return true ; /*\n\t * if original struct was marked as packed, but its layout is\n\t * naturally aligned, we'll detect that it's not packed\n\t */ return false ; }",
    "resources/libbpf/src/btf_dump.c@btf_dump_emit_bit_padding": "static void btf_dump_emit_bit_padding ( const struct btf_dump * d , int cur_off , int next_off , int next_align , bool in_bitfield , int lvl ) { const struct { const char * name ; int bits ; } pads [ ] = { { \"long\" , d -> ptr_sz * 8 } , { \"int\" , 32 } , { \"short\" , 16 } , { \"char\" , 8 } } ; int new_off , pad_bits , bits , i ; const char * pad_type ; if ( cur_off >= next_off ) return ; /* no gap */ /* For filling out padding we want to take advantage of\n\t * natural alignment rules to minimize unnecessary explicit\n\t * padding. First, we find the largest type (among long, int,\n\t * short, or char) that can be used to force naturally aligned\n\t * boundary. Once determined, we'll use such type to fill in\n\t * the remaining padding gap. In some cases we can rely on\n\t * compiler filling some gaps, but sometimes we need to force\n\t * alignment to close natural alignment with markers like\n\t * `long: 0` (this is always the case for bitfields).  Note\n\t * that even if struct itself has, let's say 4-byte alignment\n\t * (i.e., it only uses up to int-aligned types), using `long:\n\t * X;` explicit padding doesn't actually change struct's\n\t * overall alignment requirements, but compiler does take into\n\t * account that type's (long, in this example) natural\n\t * alignment requirements when adding implicit padding. We use\n\t * this fact heavily and don't worry about ruining correct\n\t * struct alignment requirement.\n\t */ for ( i = 0 ; i < ARRAY_SIZE ( pads ) ; i ++ ) { pad_bits = pads [ i ] . bits ; pad_type = pads [ i ] . name ; new_off = roundup ( cur_off , pad_bits ) ; if ( new_off <= next_off ) break ; } if ( new_off > cur_off && new_off <= next_off ) { /* We need explicit `<type>: 0` aligning mark if next\n\t\t * field is right on alignment offset and its\n\t\t * alignment requirement is less strict than <type>'s\n\t\t * alignment (so compiler won't naturally align to the\n\t\t * offset we expect), or if subsequent `<type>: X`,\n\t\t * will actually completely fit in the remaining hole,\n\t\t * making compiler basically ignore `<type>: X`\n\t\t * completely.\n\t\t */ if ( in_bitfield || ( new_off == next_off && roundup ( cur_off , next_align * 8 ) != new_off ) || ( new_off != next_off && next_off - new_off <= new_off - cur_off ) ) /* but for bitfields we'll emit explicit bit count */ btf_dump_printf ( d , \"\\n%s%s: %d;\" , pfx ( lvl ) , pad_type , in_bitfield ? new_off - cur_off : 0 ) ; cur_off = new_off ; } /* Now we know we start at naturally aligned offset for a chosen\n\t * padding type (long, int, short, or char), and so the rest is just\n\t * a straightforward filling of remaining padding gap with full\n\t * `<type>: sizeof(<type>);` markers, except for the last one, which\n\t * might need smaller than sizeof(<type>) padding.\n\t */ while ( cur_off != next_off ) { bits = min ( next_off - cur_off , pad_bits ) ; if ( bits == pad_bits ) { btf_dump_printf ( d , \"\\n%s%s: %d;\" , pfx ( lvl ) , pad_type , pad_bits ) ; cur_off += bits ; continue ; } /* For the remainder padding that doesn't cover entire\n\t\t * pad_type bit length, we pick the smallest necessary type.\n\t\t * This is pure aesthetics, we could have just used `long`,\n\t\t * but having smallest necessary one communicates better the\n\t\t * scale of the padding gap.\n\t\t */ for ( i = ARRAY_SIZE ( pads ) - 1 ; i >= 0 ; i -- ) { pad_type = pads [ i ] . name ; pad_bits = pads [ i ] . bits ; if ( pad_bits < bits ) continue ; btf_dump_printf ( d , \"\\n%s%s: %d;\" , pfx ( lvl ) , pad_type , bits ) ; cur_off += bits ; break ; } } }",
    "resources/libbpf/src/btf_dump.c@btf_dump_emit_struct_fwd": "static void btf_dump_emit_struct_fwd ( struct btf_dump * d , __u32 id , const struct btf_type * t ) { btf_dump_printf ( d , \"%s%s%s\" , btf_is_struct ( t ) ? \"struct\" : \"union\" , t -> name_off ? \" \" : \"\" , btf_dump_type_name ( d , id ) ) ; }",
    "resources/libbpf/src/btf_dump.c@btf_dump_emit_struct_def": "static void btf_dump_emit_struct_def ( struct btf_dump * d , __u32 id , const struct btf_type * t , int lvl ) { const struct btf_member * m = btf_members ( t ) ; bool is_struct = btf_is_struct ( t ) ; bool packed , prev_bitfield = false ; int align , i , off = 0 ; __u16 vlen = btf_vlen ( t ) ; align = btf__align_of ( d -> btf , id ) ; packed = is_struct ? btf_is_struct_packed ( d -> btf , id , t ) : 0 ; btf_dump_printf ( d , \"%s%s%s {\" , is_struct ? \"struct\" : \"union\" , t -> name_off ? \" \" : \"\" , btf_dump_type_name ( d , id ) ) ; for ( i = 0 ; i < vlen ; i ++ , m ++ ) { const char * fname ; int m_off , m_sz , m_align ; bool in_bitfield ; fname = btf_name_of ( d , m -> name_off ) ; m_sz = btf_member_bitfield_size ( t , i ) ; m_off = btf_member_bit_offset ( t , i ) ; m_align = packed ? 1 : btf__align_of ( d -> btf , m -> type ) ; in_bitfield = prev_bitfield && m_sz != 0 ; btf_dump_emit_bit_padding ( d , off , m_off , m_align , in_bitfield , lvl + 1 ) ; btf_dump_printf ( d , \"\\n%s\" , pfx ( lvl + 1 ) ) ; btf_dump_emit_type_decl ( d , m -> type , fname , lvl + 1 ) ; if ( m_sz ) { btf_dump_printf ( d , \": %d\" , m_sz ) ; off = m_off + m_sz ; prev_bitfield = true ; } else { m_sz = max ( ( __s64 ) 0 , btf__resolve_size ( d -> btf , m -> type ) ) ; off = m_off + m_sz * 8 ; prev_bitfield = false ; } btf_dump_printf ( d , \";\" ) ; } /* pad at the end, if necessary */ if ( is_struct ) btf_dump_emit_bit_padding ( d , off , t -> size * 8 , align , false , lvl + 1 ) ; /*\n\t * Keep `struct empty {}` on a single line,\n\t * only print newline when there are regular or padding fields.\n\t */ if ( vlen || t -> size ) { btf_dump_printf ( d , \"\\n\" ) ; btf_dump_printf ( d , \"%s}\" , pfx ( lvl ) ) ; } else { btf_dump_printf ( d , \"}\" ) ; } if ( packed ) btf_dump_printf ( d , \" __attribute__((packed))\" ) ; }",
    "resources/libbpf/src/btf_dump.c@btf_dump_emit_missing_aliases": "static void btf_dump_emit_missing_aliases ( struct btf_dump * d , __u32 id , const struct btf_type * t ) { const char * name = btf_dump_type_name ( d , id ) ; int i ; for ( i = 0 ; i < ARRAY_SIZE ( missing_base_types ) ; i ++ ) { if ( strcmp ( name , missing_base_types [ i ] [ 0 ] ) == 0 ) { btf_dump_printf ( d , \"typedef %s %s;\\n\\n\" , missing_base_types [ i ] [ 1 ] , name ) ; break ; } } }",
    "resources/libbpf/src/btf_dump.c@btf_dump_emit_enum_fwd": "static void btf_dump_emit_enum_fwd ( struct btf_dump * d , __u32 id , const struct btf_type * t ) { btf_dump_printf ( d , \"enum %s\" , btf_dump_type_name ( d , id ) ) ; }",
    "resources/libbpf/src/btf_dump.c@btf_dump_emit_enum32_val": "static void btf_dump_emit_enum32_val ( struct btf_dump * d , const struct btf_type * t , int lvl , __u16 vlen ) { const struct btf_enum * v = btf_enum ( t ) ; bool is_signed = btf_kflag ( t ) ; const char * fmt_str ; const char * name ; size_t dup_cnt ; int i ; for ( i = 0 ; i < vlen ; i ++ , v ++ ) { name = btf_name_of ( d , v -> name_off ) ; /* enumerators share namespace with typedef idents */ dup_cnt = btf_dump_name_dups ( d , d -> ident_names , name ) ; if ( dup_cnt > 1 ) { fmt_str = is_signed ? \"\\n%s%s___%zd = %d,\" : \"\\n%s%s___%zd = %u,\" ; btf_dump_printf ( d , fmt_str , pfx ( lvl + 1 ) , name , dup_cnt , v -> val ) ; } else { fmt_str = is_signed ? \"\\n%s%s = %d,\" : \"\\n%s%s = %u,\" ; btf_dump_printf ( d , fmt_str , pfx ( lvl + 1 ) , name , v -> val ) ; } } }",
    "resources/libbpf/src/btf_dump.c@btf_dump_emit_enum64_val": "static void btf_dump_emit_enum64_val ( struct btf_dump * d , const struct btf_type * t , int lvl , __u16 vlen ) { const struct btf_enum64 * v = btf_enum64 ( t ) ; bool is_signed = btf_kflag ( t ) ; const char * fmt_str ; const char * name ; size_t dup_cnt ; __u64 val ; int i ; for ( i = 0 ; i < vlen ; i ++ , v ++ ) { name = btf_name_of ( d , v -> name_off ) ; dup_cnt = btf_dump_name_dups ( d , d -> ident_names , name ) ; val = btf_enum64_value ( v ) ; if ( dup_cnt > 1 ) { fmt_str = is_signed ? \"\\n%s%s___%zd = %lldLL,\" : \"\\n%s%s___%zd = %lluULL,\" ; btf_dump_printf ( d , fmt_str , pfx ( lvl + 1 ) , name , dup_cnt , ( unsigned long long ) val ) ; } else { fmt_str = is_signed ? \"\\n%s%s = %lldLL,\" : \"\\n%s%s = %lluULL,\" ; btf_dump_printf ( d , fmt_str , pfx ( lvl + 1 ) , name , ( unsigned long long ) val ) ; } } }",
    "resources/libbpf/src/btf_dump.c@btf_dump_emit_enum_def": "static void btf_dump_emit_enum_def ( struct btf_dump * d , __u32 id , const struct btf_type * t , int lvl ) { __u16 vlen = btf_vlen ( t ) ; btf_dump_printf ( d , \"enum%s%s\" , t -> name_off ? \" \" : \"\" , btf_dump_type_name ( d , id ) ) ; if ( ! vlen ) return ; btf_dump_printf ( d , \" {\" ) ; if ( btf_is_enum ( t ) ) btf_dump_emit_enum32_val ( d , t , lvl , vlen ) ; else btf_dump_emit_enum64_val ( d , t , lvl , vlen ) ; btf_dump_printf ( d , \"\\n%s}\" , pfx ( lvl ) ) ; /* special case enums with special sizes */ if ( t -> size == 1 ) { /* one-byte enums can be forced with mode(byte) attribute */ btf_dump_printf ( d , \" __attribute__((mode(byte)))\" ) ; } else if ( t -> size == 8 && d -> ptr_sz == 8 ) { /* enum can be 8-byte sized if one of the enumerator values\n\t\t * doesn't fit in 32-bit integer, or by adding mode(word)\n\t\t * attribute (but probably only on 64-bit architectures); do\n\t\t * our best here to try to satisfy the contract without adding\n\t\t * unnecessary attributes\n\t\t */ bool needs_word_mode ; if ( btf_is_enum ( t ) ) { /* enum can't represent 64-bit values, so we need word mode */ needs_word_mode = true ; } else { /* enum64 needs mode(word) if none of its values has\n\t\t\t * non-zero upper 32-bits (which means that all values\n\t\t\t * fit in 32-bit integers and won't cause compiler to\n\t\t\t * bump enum to be 64-bit naturally\n\t\t\t */ int i ; needs_word_mode = true ; for ( i = 0 ; i < vlen ; i ++ ) { if ( btf_enum64 ( t ) [ i ] . val_hi32 != 0 ) { needs_word_mode = false ; break ; } } } if ( needs_word_mode ) btf_dump_printf ( d , \" __attribute__((mode(word)))\" ) ; } }",
    "resources/libbpf/src/btf_dump.c@btf_dump_emit_fwd_def": "static void btf_dump_emit_fwd_def ( struct btf_dump * d , __u32 id , const struct btf_type * t ) { const char * name = btf_dump_type_name ( d , id ) ; if ( btf_kflag ( t ) ) btf_dump_printf ( d , \"union %s\" , name ) ; else btf_dump_printf ( d , \"struct %s\" , name ) ; }",
    "resources/libbpf/src/btf_dump.c@btf_dump_emit_typedef_def": "static void btf_dump_emit_typedef_def ( struct btf_dump * d , __u32 id , const struct btf_type * t , int lvl ) { const char * name = btf_dump_ident_name ( d , id ) ; /*\n\t * Old GCC versions are emitting invalid typedef for __gnuc_va_list\n\t * pointing to VOID. This generates warnings from btf_dump() and\n\t * results in uncompilable header file, so we are fixing it up here\n\t * with valid typedef into __builtin_va_list.\n\t */ if ( t -> type == 0 && strcmp ( name , \"__gnuc_va_list\" ) == 0 ) { btf_dump_printf ( d , \"typedef __builtin_va_list __gnuc_va_list\" ) ; return ; } btf_dump_printf ( d , \"typedef \" ) ; btf_dump_emit_type_decl ( d , t -> type , name , lvl ) ; }",
    "resources/libbpf/src/btf_dump.c@btf_dump_push_decl_stack_id": "static int btf_dump_push_decl_stack_id ( struct btf_dump * d , __u32 id ) { __u32 * new_stack ; size_t new_cap ; if ( d -> decl_stack_cnt >= d -> decl_stack_cap ) { new_cap = max ( 16 , d -> decl_stack_cap * 3 / 2 ) ; new_stack = libbpf_reallocarray ( d -> decl_stack , new_cap , sizeof ( new_stack [ 0 ] ) ) ; if ( ! new_stack ) return - ENOMEM ; d -> decl_stack = new_stack ; d -> decl_stack_cap = new_cap ; } d -> decl_stack [ d -> decl_stack_cnt ++ ] = id ; return 0 ; }",
    "resources/libbpf/src/btf_dump.c@btf_dump__emit_type_decl": "int btf_dump__emit_type_decl ( struct btf_dump * d , __u32 id , const struct btf_dump_emit_type_decl_opts * opts ) { const char * fname ; int lvl , err ; if ( ! OPTS_VALID ( opts , btf_dump_emit_type_decl_opts ) ) return libbpf_err ( - EINVAL ) ; err = btf_dump_resize ( d ) ; if ( err ) return libbpf_err ( err ) ; fname = OPTS_GET ( opts , field_name , \"\" ) ; lvl = OPTS_GET ( opts , indent_level , 0 ) ; d -> strip_mods = OPTS_GET ( opts , strip_mods , false ) ; btf_dump_emit_type_decl ( d , id , fname , lvl ) ; d -> strip_mods = false ; return 0 ; }",
    "resources/libbpf/src/btf_dump.c@btf_dump_emit_type_decl": "static void btf_dump_emit_type_decl ( struct btf_dump * d , __u32 id , const char * fname , int lvl ) { struct id_stack decl_stack ; const struct btf_type * t ; int err , stack_start ; stack_start = d -> decl_stack_cnt ; for ( ; ; ) { t = btf__type_by_id ( d -> btf , id ) ; if ( d -> strip_mods && btf_is_mod ( t ) ) goto skip_mod ; err = btf_dump_push_decl_stack_id ( d , id ) ; if ( err < 0 ) { /*\n\t\t\t * if we don't have enough memory for entire type decl\n\t\t\t * chain, restore stack, emit warning, and try to\n\t\t\t * proceed nevertheless\n\t\t\t */ pr_warn ( \"not enough memory for decl stack:%d\" , err ) ; d -> decl_stack_cnt = stack_start ; return ; } skip_mod : /* VOID */ if ( id == 0 ) break ; switch ( btf_kind ( t ) ) { case BTF_KIND_PTR : case BTF_KIND_VOLATILE : case BTF_KIND_CONST : case BTF_KIND_RESTRICT : case BTF_KIND_FUNC_PROTO : case BTF_KIND_TYPE_TAG : id = t -> type ; break ; case BTF_KIND_ARRAY : id = btf_array ( t ) -> type ; break ; case BTF_KIND_INT : case BTF_KIND_ENUM : case BTF_KIND_ENUM64 : case BTF_KIND_FWD : case BTF_KIND_STRUCT : case BTF_KIND_UNION : case BTF_KIND_TYPEDEF : case BTF_KIND_FLOAT : goto done ; default : pr_warn ( \"unexpected type in decl chain, kind:%u, id:[%u]\\n\" , btf_kind ( t ) , id ) ; goto done ; } } done : /*\n\t * We might be inside a chain of declarations (e.g., array of function\n\t * pointers returning anonymous (so inlined) structs, having another\n\t * array field). Each of those needs its own \"stack frame\" to handle\n\t * emitting of declarations. Those stack frames are non-overlapping\n\t * portions of shared btf_dump->decl_stack. To make it a bit nicer to\n\t * handle this set of nested stacks, we create a view corresponding to\n\t * our own \"stack frame\" and work with it as an independent stack.\n\t * We'll need to clean up after emit_type_chain() returns, though.\n\t */ decl_stack . ids = d -> decl_stack + stack_start ; decl_stack . cnt = d -> decl_stack_cnt - stack_start ; btf_dump_emit_type_chain ( d , & decl_stack , fname , lvl ) ; /*\n\t * emit_type_chain() guarantees that it will pop its entire decl_stack\n\t * frame before returning. But it works with a read-only view into\n\t * decl_stack, so it doesn't actually pop anything from the\n\t * perspective of shared btf_dump->decl_stack, per se. We need to\n\t * reset decl_stack state to how it was before us to avoid it growing\n\t * all the time.\n\t */ d -> decl_stack_cnt = stack_start ; }",
    "resources/libbpf/src/btf_dump.c@btf_dump_emit_mods": "static void btf_dump_emit_mods ( struct btf_dump * d , struct id_stack * decl_stack ) { const struct btf_type * t ; __u32 id ; while ( decl_stack -> cnt ) { id = decl_stack -> ids [ decl_stack -> cnt - 1 ] ; t = btf__type_by_id ( d -> btf , id ) ; switch ( btf_kind ( t ) ) { case BTF_KIND_VOLATILE : btf_dump_printf ( d , \"volatile \" ) ; break ; case BTF_KIND_CONST : btf_dump_printf ( d , \"const \" ) ; break ; case BTF_KIND_RESTRICT : btf_dump_printf ( d , \"restrict \" ) ; break ; default : return ; } decl_stack -> cnt -- ; } }",
    "resources/libbpf/src/btf_dump.c@btf_dump_drop_mods": "static void btf_dump_drop_mods ( struct btf_dump * d , struct id_stack * decl_stack ) { const struct btf_type * t ; __u32 id ; while ( decl_stack -> cnt ) { id = decl_stack -> ids [ decl_stack -> cnt - 1 ] ; t = btf__type_by_id ( d -> btf , id ) ; if ( ! btf_is_mod ( t ) ) return ; decl_stack -> cnt -- ; } }",
    "resources/libbpf/src/btf_dump.c@btf_dump_emit_name": "static void btf_dump_emit_name ( const struct btf_dump * d , const char * name , bool last_was_ptr ) { bool separate = name [ 0 ] && ! last_was_ptr ; btf_dump_printf ( d , \"%s%s\" , separate ? \" \" : \"\" , name ) ; }",
    "resources/libbpf/src/btf_dump.c@btf_dump_emit_type_chain": "static void btf_dump_emit_type_chain ( struct btf_dump * d , struct id_stack * decls , const char * fname , int lvl ) { /*\n\t * last_was_ptr is used to determine if we need to separate pointer\n\t * asterisk (*) from previous part of type signature with space, so\n\t * that we get `int ***`, instead of `int * * *`. We default to true\n\t * for cases where we have single pointer in a chain. E.g., in ptr ->\n\t * func_proto case. func_proto will start a new emit_type_chain call\n\t * with just ptr, which should be emitted as (*) or (*<fname>), so we\n\t * don't want to prepend space for that last pointer.\n\t */ bool last_was_ptr = true ; const struct btf_type * t ; const char * name ; __u16 kind ; __u32 id ; while ( decls -> cnt ) { id = decls -> ids [ -- decls -> cnt ] ; if ( id == 0 ) { /* VOID is a special snowflake */ btf_dump_emit_mods ( d , decls ) ; btf_dump_printf ( d , \"void\" ) ; last_was_ptr = false ; continue ; } t = btf__type_by_id ( d -> btf , id ) ; kind = btf_kind ( t ) ; switch ( kind ) { case BTF_KIND_INT : case BTF_KIND_FLOAT : btf_dump_emit_mods ( d , decls ) ; name = btf_name_of ( d , t -> name_off ) ; btf_dump_printf ( d , \"%s\" , name ) ; break ; case BTF_KIND_STRUCT : case BTF_KIND_UNION : btf_dump_emit_mods ( d , decls ) ; /* inline anonymous struct/union */ if ( t -> name_off == 0 && ! d -> skip_anon_defs ) btf_dump_emit_struct_def ( d , id , t , lvl ) ; else btf_dump_emit_struct_fwd ( d , id , t ) ; break ; case BTF_KIND_ENUM : case BTF_KIND_ENUM64 : btf_dump_emit_mods ( d , decls ) ; /* inline anonymous enum */ if ( t -> name_off == 0 && ! d -> skip_anon_defs ) btf_dump_emit_enum_def ( d , id , t , lvl ) ; else btf_dump_emit_enum_fwd ( d , id , t ) ; break ; case BTF_KIND_FWD : btf_dump_emit_mods ( d , decls ) ; btf_dump_emit_fwd_def ( d , id , t ) ; break ; case BTF_KIND_TYPEDEF : btf_dump_emit_mods ( d , decls ) ; btf_dump_printf ( d , \"%s\" , btf_dump_ident_name ( d , id ) ) ; break ; case BTF_KIND_PTR : btf_dump_printf ( d , \"%s\" , last_was_ptr ? \"*\" : \" *\" ) ; break ; case BTF_KIND_VOLATILE : btf_dump_printf ( d , \" volatile\" ) ; break ; case BTF_KIND_CONST : btf_dump_printf ( d , \" const\" ) ; break ; case BTF_KIND_RESTRICT : btf_dump_printf ( d , \" restrict\" ) ; break ; case BTF_KIND_TYPE_TAG : btf_dump_emit_mods ( d , decls ) ; name = btf_name_of ( d , t -> name_off ) ; btf_dump_printf ( d , \" __attribute__((btf_type_tag(\\\"%s\\\")))\" , name ) ; break ; case BTF_KIND_ARRAY : { const struct btf_array * a = btf_array ( t ) ; const struct btf_type * next_t ; __u32 next_id ; bool multidim ; /*\n\t\t\t * GCC has a bug\n\t\t\t * (https://gcc.gnu.org/bugzilla/show_bug.cgi?id=8354)\n\t\t\t * which causes it to emit extra const/volatile\n\t\t\t * modifiers for an array, if array's element type has\n\t\t\t * const/volatile modifiers. Clang doesn't do that.\n\t\t\t * In general, it doesn't seem very meaningful to have\n\t\t\t * a const/volatile modifier for array, so we are\n\t\t\t * going to silently skip them here.\n\t\t\t */ btf_dump_drop_mods ( d , decls ) ; if ( decls -> cnt == 0 ) { btf_dump_emit_name ( d , fname , last_was_ptr ) ; btf_dump_printf ( d , \"[%u]\" , a -> nelems ) ; return ; } next_id = decls -> ids [ decls -> cnt - 1 ] ; next_t = btf__type_by_id ( d -> btf , next_id ) ; multidim = btf_is_array ( next_t ) ; /* we need space if we have named non-pointer */ if ( fname [ 0 ] && ! last_was_ptr ) btf_dump_printf ( d , \" \" ) ; /* no parentheses for multi-dimensional array */ if ( ! multidim ) btf_dump_printf ( d , \"(\" ) ; btf_dump_emit_type_chain ( d , decls , fname , lvl ) ; if ( ! multidim ) btf_dump_printf ( d , \")\" ) ; btf_dump_printf ( d , \"[%u]\" , a -> nelems ) ; return ; } case BTF_KIND_FUNC_PROTO : { const struct btf_param * p = btf_params ( t ) ; __u16 vlen = btf_vlen ( t ) ; int i ; /*\n\t\t\t * GCC emits extra volatile qualifier for\n\t\t\t * __attribute__((noreturn)) function pointers. Clang\n\t\t\t * doesn't do it. It's a GCC quirk for backwards\n\t\t\t * compatibility with code written for GCC <2.5. So,\n\t\t\t * similarly to extra qualifiers for array, just drop\n\t\t\t * them, instead of handling them.\n\t\t\t */ btf_dump_drop_mods ( d , decls ) ; if ( decls -> cnt ) { btf_dump_printf ( d , \" (\" ) ; btf_dump_emit_type_chain ( d , decls , fname , lvl ) ; btf_dump_printf ( d , \")\" ) ; } else { btf_dump_emit_name ( d , fname , last_was_ptr ) ; } btf_dump_printf ( d , \"(\" ) ; /*\n\t\t\t * Clang for BPF target generates func_proto with no\n\t\t\t * args as a func_proto with a single void arg (e.g.,\n\t\t\t * `int (*f)(void)` vs just `int (*f)()`). We are\n\t\t\t * going to pretend there are no args for such case.\n\t\t\t */ if ( vlen == 1 && p -> type == 0 ) { btf_dump_printf ( d , \")\" ) ; return ; } for ( i = 0 ; i < vlen ; i ++ , p ++ ) { if ( i > 0 ) btf_dump_printf ( d , \", \" ) ; /* last arg of type void is vararg */ if ( i == vlen - 1 && p -> type == 0 ) { btf_dump_printf ( d , \"...\" ) ; break ; } name = btf_name_of ( d , p -> name_off ) ; btf_dump_emit_type_decl ( d , p -> type , name , lvl ) ; } btf_dump_printf ( d , \")\" ) ; return ; } default : pr_warn ( \"unexpected type in decl chain, kind:%u, id:[%u]\\n\" , kind , id ) ; return ; } last_was_ptr = kind == BTF_KIND_PTR ; } btf_dump_emit_name ( d , fname , last_was_ptr ) ; }",
    "resources/libbpf/src/btf_dump.c@btf_dump_emit_type_cast": "static void btf_dump_emit_type_cast ( struct btf_dump * d , __u32 id , bool top_level ) { const struct btf_type * t ; /* for array members, we don't bother emitting type name for each\n\t * member to avoid the redundancy of\n\t * .name = (char[4])[(char)'f',(char)'o',(char)'o',]\n\t */ if ( d -> typed_dump -> is_array_member ) return ; /* avoid type name specification for variable/section; it will be done\n\t * for the associated variable value(s).\n\t */ t = btf__type_by_id ( d -> btf , id ) ; if ( btf_is_var ( t ) || btf_is_datasec ( t ) ) return ; if ( top_level ) btf_dump_printf ( d , \"(\" ) ; d -> skip_anon_defs = true ; d -> strip_mods = true ; btf_dump_emit_type_decl ( d , id , \"\" , 0 ) ; d -> strip_mods = false ; d -> skip_anon_defs = false ; if ( top_level ) btf_dump_printf ( d , \")\" ) ; }",
    "resources/libbpf/src/btf_dump.c@btf_dump_name_dups": "static size_t btf_dump_name_dups ( struct btf_dump * d , struct hashmap * name_map , const char * orig_name ) { char * old_name , * new_name ; size_t dup_cnt = 0 ; int err ; new_name = strdup ( orig_name ) ; if ( ! new_name ) return 1 ; ( void ) hashmap__find ( name_map , orig_name , & dup_cnt ) ; dup_cnt ++ ; err = hashmap__set ( name_map , new_name , dup_cnt , & old_name , NULL ) ; if ( err ) free ( new_name ) ; free ( old_name ) ; return dup_cnt ; }",
    "resources/libbpf/src/btf_dump.c@btf_dump_resolve_name": "static const char * btf_dump_resolve_name ( struct btf_dump * d , __u32 id , struct hashmap * name_map ) { struct btf_dump_type_aux_state * s = & d -> type_states [ id ] ; const struct btf_type * t = btf__type_by_id ( d -> btf , id ) ; const char * orig_name = btf_name_of ( d , t -> name_off ) ; const char * * cached_name = & d -> cached_names [ id ] ; size_t dup_cnt ; if ( t -> name_off == 0 ) return \"\" ; if ( s -> name_resolved ) return * cached_name ? * cached_name : orig_name ; if ( btf_is_fwd ( t ) || ( btf_is_enum ( t ) && btf_vlen ( t ) == 0 ) ) { s -> name_resolved = 1 ; return orig_name ; } dup_cnt = btf_dump_name_dups ( d , name_map , orig_name ) ; if ( dup_cnt > 1 ) { const size_t max_len = 256 ; char new_name [ max_len ] ; snprintf ( new_name , max_len , \"%s___%zu\" , orig_name , dup_cnt ) ; * cached_name = strdup ( new_name ) ; } s -> name_resolved = 1 ; return * cached_name ? * cached_name : orig_name ; }",
    "resources/libbpf/src/btf_dump.c@btf_dump_type_name": "static const char * btf_dump_type_name ( struct btf_dump * d , __u32 id ) { return btf_dump_resolve_name ( d , id , d -> type_names ) ; }",
    "resources/libbpf/src/btf_dump.c@btf_dump_ident_name": "static const char * btf_dump_ident_name ( struct btf_dump * d , __u32 id ) { return btf_dump_resolve_name ( d , id , d -> ident_names ) ; }",
    "resources/libbpf/src/btf_dump.c@btf_dump_data_newline": "static const char * btf_dump_data_newline ( struct btf_dump * d ) { return d -> typed_dump -> compact || d -> typed_dump -> depth == 0 ? \"\" : \"\\n\" ; }",
    "resources/libbpf/src/btf_dump.c@btf_dump_data_delim": "static const char * btf_dump_data_delim ( struct btf_dump * d ) { return d -> typed_dump -> depth == 0 ? \"\" : \",\" ; }",
    "resources/libbpf/src/btf_dump.c@btf_dump_data_pfx": "static void btf_dump_data_pfx ( struct btf_dump * d ) { int i , lvl = d -> typed_dump -> indent_lvl + d -> typed_dump -> depth ; if ( d -> typed_dump -> compact ) return ; for ( i = 0 ; i < lvl ; i ++ ) btf_dump_printf ( d , \"%s\" , d -> typed_dump -> indent_str ) ; }",
    "resources/libbpf/src/btf_dump.c@btf_dump_unsupported_data": "static int btf_dump_unsupported_data ( struct btf_dump * d , const struct btf_type * t , __u32 id ) { btf_dump_printf ( d , \"<unsupported kind:%u>\" , btf_kind ( t ) ) ; return - ENOTSUP ; }",
    "resources/libbpf/src/btf_dump.c@btf_dump_get_bitfield_value": "static int btf_dump_get_bitfield_value ( struct btf_dump * d , const struct btf_type * t , const void * data , __u8 bits_offset , __u8 bit_sz , __u64 * value ) { __u16 left_shift_bits , right_shift_bits ; const __u8 * bytes = data ; __u8 nr_copy_bits ; __u64 num = 0 ; int i ; /* Maximum supported bitfield size is 64 bits */ if ( t -> size > 8 ) { pr_warn ( \"unexpected bitfield size %d\\n\" , t -> size ) ; return - EINVAL ; } /* Bitfield value retrieval is done in two steps; first relevant bytes are\n\t * stored in num, then we left/right shift num to eliminate irrelevant bits.\n\t */ # if __BYTE_ORDER__ == __ORDER_LITTLE_ENDIAN__ for ( i = t -> size - 1 ; i >= 0 ; i -- ) num = num * 256 + bytes [ i ] ; nr_copy_bits = bit_sz + bits_offset ; # elif __BYTE_ORDER__ == __ORDER_BIG_ENDIAN__ for ( i = 0 ; i < t -> size ; i ++ ) num = num * 256 + bytes [ i ] ; nr_copy_bits = t -> size * 8 - bits_offset ; # else # error \"Unrecognized __BYTE_ORDER__\" # endif left_shift_bits = 64 - nr_copy_bits ; right_shift_bits = 64 - bit_sz ; * value = ( num << left_shift_bits ) >> right_shift_bits ; return 0 ; }",
    "resources/libbpf/src/btf_dump.c@btf_dump_bitfield_check_zero": "static int btf_dump_bitfield_check_zero ( struct btf_dump * d , const struct btf_type * t , const void * data , __u8 bits_offset , __u8 bit_sz ) { __u64 check_num ; int err ; err = btf_dump_get_bitfield_value ( d , t , data , bits_offset , bit_sz , & check_num ) ; if ( err ) return err ; if ( check_num == 0 ) return - ENODATA ; return 0 ; }",
    "resources/libbpf/src/btf_dump.c@btf_dump_bitfield_data": "static int btf_dump_bitfield_data ( struct btf_dump * d , const struct btf_type * t , const void * data , __u8 bits_offset , __u8 bit_sz ) { __u64 print_num ; int err ; err = btf_dump_get_bitfield_value ( d , t , data , bits_offset , bit_sz , & print_num ) ; if ( err ) return err ; btf_dump_type_values ( d , \"0x%llx\" , ( unsigned long long ) print_num ) ; return 0 ; }",
    "resources/libbpf/src/btf_dump.c@btf_dump_base_type_check_zero": "static int btf_dump_base_type_check_zero ( struct btf_dump * d , const struct btf_type * t , __u32 id , const void * data ) { static __u8 bytecmp [ 16 ] = { } ; int nr_bytes ; /* For pointer types, pointer size is not defined on a per-type basis.\n\t * On dump creation however, we store the pointer size.\n\t */ if ( btf_kind ( t ) == BTF_KIND_PTR ) nr_bytes = d -> ptr_sz ; else nr_bytes = t -> size ; if ( nr_bytes < 1 || nr_bytes > 16 ) { pr_warn ( \"unexpected size %d for id [%u]\\n\" , nr_bytes , id ) ; return - EINVAL ; } if ( memcmp ( data , bytecmp , nr_bytes ) == 0 ) return - ENODATA ; return 0 ; }",
    "resources/libbpf/src/btf_dump.c@ptr_is_aligned": "static bool ptr_is_aligned ( const struct btf * btf , __u32 type_id , const void * data ) { int alignment = btf__align_of ( btf , type_id ) ; if ( alignment == 0 ) return false ; return ( ( uintptr_t ) data ) % alignment == 0 ; }",
    "resources/libbpf/src/btf_dump.c@btf_dump_int_data": "static int btf_dump_int_data ( struct btf_dump * d , const struct btf_type * t , __u32 type_id , const void * data , __u8 bits_offset ) { __u8 encoding = btf_int_encoding ( t ) ; bool sign = encoding & BTF_INT_SIGNED ; char buf [ 16 ] __attribute__ ( ( aligned ( 16 ) ) ) ; int sz = t -> size ; if ( sz == 0 || sz > sizeof ( buf ) ) { pr_warn ( \"unexpected size %d for id [%u]\\n\" , sz , type_id ) ; return - EINVAL ; } /* handle packed int data - accesses of integers not aligned on\n\t * int boundaries can cause problems on some platforms.\n\t */ if ( ! ptr_is_aligned ( d -> btf , type_id , data ) ) { memcpy ( buf , data , sz ) ; data = buf ; } switch ( sz ) { case 16 : { const __u64 * ints = data ; __u64 lsi , msi ; /* avoid use of __int128 as some 32-bit platforms do not\n\t\t * support it.\n\t\t */ # if __BYTE_ORDER__ == __ORDER_LITTLE_ENDIAN__ lsi = ints [ 0 ] ; msi = ints [ 1 ] ; # elif __BYTE_ORDER__ == __ORDER_BIG_ENDIAN__ lsi = ints [ 1 ] ; msi = ints [ 0 ] ; # else # error \"Unrecognized __BYTE_ORDER__\" # endif if ( msi == 0 ) btf_dump_type_values ( d , \"0x%llx\" , ( unsigned long long ) lsi ) ; else btf_dump_type_values ( d , \"0x%llx%016llx\" , ( unsigned long long ) msi , ( unsigned long long ) lsi ) ; break ; } case 8 : if ( sign ) btf_dump_type_values ( d , \"%lld\" , * ( long long * ) data ) ; else btf_dump_type_values ( d , \"%llu\" , * ( unsigned long long * ) data ) ; break ; case 4 : if ( sign ) btf_dump_type_values ( d , \"%d\" , * ( __s32 * ) data ) ; else btf_dump_type_values ( d , \"%u\" , * ( __u32 * ) data ) ; break ; case 2 : if ( sign ) btf_dump_type_values ( d , \"%d\" , * ( __s16 * ) data ) ; else btf_dump_type_values ( d , \"%u\" , * ( __u16 * ) data ) ; break ; case 1 : if ( d -> typed_dump -> is_array_char ) { /* check for null terminator */ if ( d -> typed_dump -> is_array_terminated ) break ; if ( * ( char * ) data == '\\0' ) { d -> typed_dump -> is_array_terminated = true ; break ; } if ( isprint ( * ( char * ) data ) ) { btf_dump_type_values ( d , \"'%c'\" , * ( char * ) data ) ; break ; } } if ( sign ) btf_dump_type_values ( d , \"%d\" , * ( __s8 * ) data ) ; else btf_dump_type_values ( d , \"%u\" , * ( __u8 * ) data ) ; break ; default : pr_warn ( \"unexpected sz %d for id [%u]\\n\" , sz , type_id ) ; return - EINVAL ; } return 0 ; }",
    "resources/libbpf/src/btf_dump.c@btf_dump_float_data": "static int btf_dump_float_data ( struct btf_dump * d , const struct btf_type * t , __u32 type_id , const void * data ) { const union float_data * flp = data ; union float_data fl ; int sz = t -> size ; /* handle unaligned data; copy to local union */ if ( ! ptr_is_aligned ( d -> btf , type_id , data ) ) { memcpy ( & fl , data , sz ) ; flp = & fl ; } switch ( sz ) { case 16 : btf_dump_type_values ( d , \"%Lf\" , flp -> ld ) ; break ; case 8 : btf_dump_type_values ( d , \"%lf\" , flp -> d ) ; break ; case 4 : btf_dump_type_values ( d , \"%f\" , flp -> f ) ; break ; default : pr_warn ( \"unexpected size %d for id [%u]\\n\" , sz , type_id ) ; return - EINVAL ; } return 0 ; }",
    "resources/libbpf/src/btf_dump.c@btf_dump_var_data": "static int btf_dump_var_data ( struct btf_dump * d , const struct btf_type * v , __u32 id , const void * data ) { enum btf_func_linkage linkage = btf_var ( v ) -> linkage ; const struct btf_type * t ; const char * l ; __u32 type_id ; switch ( linkage ) { case BTF_FUNC_STATIC : l = \"static \" ; break ; case BTF_FUNC_EXTERN : l = \"extern \" ; break ; case BTF_FUNC_GLOBAL : default : l = \"\" ; break ; } /* format of output here is [linkage] [type] [varname] = (type)value,\n\t * for example \"static int cpu_profile_flip = (int)1\"\n\t */ btf_dump_printf ( d , \"%s\" , l ) ; type_id = v -> type ; t = btf__type_by_id ( d -> btf , type_id ) ; btf_dump_emit_type_cast ( d , type_id , false ) ; btf_dump_printf ( d , \" %s = \" , btf_name_of ( d , v -> name_off ) ) ; return btf_dump_dump_type_data ( d , NULL , t , type_id , data , 0 , 0 ) ; }",
    "resources/libbpf/src/btf_dump.c@btf_dump_array_data": "static int btf_dump_array_data ( struct btf_dump * d , const struct btf_type * t , __u32 id , const void * data ) { const struct btf_array * array = btf_array ( t ) ; const struct btf_type * elem_type ; __u32 i , elem_type_id ; __s64 elem_size ; bool is_array_member ; elem_type_id = array -> type ; elem_type = skip_mods_and_typedefs ( d -> btf , elem_type_id , NULL ) ; elem_size = btf__resolve_size ( d -> btf , elem_type_id ) ; if ( elem_size <= 0 ) { pr_warn ( \"unexpected elem size %zd for array type [%u]\\n\" , ( ssize_t ) elem_size , id ) ; return - EINVAL ; } if ( btf_is_int ( elem_type ) ) { /*\n\t\t * BTF_INT_CHAR encoding never seems to be set for\n\t\t * char arrays, so if size is 1 and element is\n\t\t * printable as a char, we'll do that.\n\t\t */ if ( elem_size == 1 ) d -> typed_dump -> is_array_char = true ; } /* note that we increment depth before calling btf_dump_print() below;\n\t * this is intentional.  btf_dump_data_newline() will not print a\n\t * newline for depth 0 (since this leaves us with trailing newlines\n\t * at the end of typed display), so depth is incremented first.\n\t * For similar reasons, we decrement depth before showing the closing\n\t * parenthesis.\n\t */ d -> typed_dump -> depth ++ ; btf_dump_printf ( d , \"[%s\" , btf_dump_data_newline ( d ) ) ; /* may be a multidimensional array, so store current \"is array member\"\n\t * status so we can restore it correctly later.\n\t */ is_array_member = d -> typed_dump -> is_array_member ; d -> typed_dump -> is_array_member = true ; for ( i = 0 ; i < array -> nelems ; i ++ , data += elem_size ) { if ( d -> typed_dump -> is_array_terminated ) break ; btf_dump_dump_type_data ( d , NULL , elem_type , elem_type_id , data , 0 , 0 ) ; } d -> typed_dump -> is_array_member = is_array_member ; d -> typed_dump -> depth -- ; btf_dump_data_pfx ( d ) ; btf_dump_type_values ( d , \"]\" ) ; return 0 ; }",
    "resources/libbpf/src/btf_dump.c@btf_dump_struct_data": "static int btf_dump_struct_data ( struct btf_dump * d , const struct btf_type * t , __u32 id , const void * data ) { const struct btf_member * m = btf_members ( t ) ; __u16 n = btf_vlen ( t ) ; int i , err = 0 ; /* note that we increment depth before calling btf_dump_print() below;\n\t * this is intentional.  btf_dump_data_newline() will not print a\n\t * newline for depth 0 (since this leaves us with trailing newlines\n\t * at the end of typed display), so depth is incremented first.\n\t * For similar reasons, we decrement depth before showing the closing\n\t * parenthesis.\n\t */ d -> typed_dump -> depth ++ ; btf_dump_printf ( d , \"{%s\" , btf_dump_data_newline ( d ) ) ; for ( i = 0 ; i < n ; i ++ , m ++ ) { const struct btf_type * mtype ; const char * mname ; __u32 moffset ; __u8 bit_sz ; mtype = btf__type_by_id ( d -> btf , m -> type ) ; mname = btf_name_of ( d , m -> name_off ) ; moffset = btf_member_bit_offset ( t , i ) ; bit_sz = btf_member_bitfield_size ( t , i ) ; err = btf_dump_dump_type_data ( d , mname , mtype , m -> type , data + moffset / 8 , moffset % 8 , bit_sz ) ; if ( err < 0 ) return err ; } d -> typed_dump -> depth -- ; btf_dump_data_pfx ( d ) ; btf_dump_type_values ( d , \"}\" ) ; return err ; }",
    "resources/libbpf/src/btf_dump.c@btf_dump_ptr_data": "static int btf_dump_ptr_data ( struct btf_dump * d , const struct btf_type * t , __u32 id , const void * data ) { if ( ptr_is_aligned ( d -> btf , id , data ) && d -> ptr_sz == sizeof ( void * ) ) { btf_dump_type_values ( d , \"%p\" , * ( void * * ) data ) ; } else { union ptr_data pt ; memcpy ( & pt , data , d -> ptr_sz ) ; if ( d -> ptr_sz == 4 ) btf_dump_type_values ( d , \"0x%x\" , pt . p ) ; else btf_dump_type_values ( d , \"0x%llx\" , pt . lp ) ; } return 0 ; }",
    "resources/libbpf/src/btf_dump.c@btf_dump_get_enum_value": "static int btf_dump_get_enum_value ( struct btf_dump * d , const struct btf_type * t , const void * data , __u32 id , __s64 * value ) { bool is_signed = btf_kflag ( t ) ; if ( ! ptr_is_aligned ( d -> btf , id , data ) ) { __u64 val ; int err ; err = btf_dump_get_bitfield_value ( d , t , data , 0 , 0 , & val ) ; if ( err ) return err ; * value = ( __s64 ) val ; return 0 ; } switch ( t -> size ) { case 8 : * value = * ( __s64 * ) data ; return 0 ; case 4 : * value = is_signed ? ( __s64 ) * ( __s32 * ) data : * ( __u32 * ) data ; return 0 ; case 2 : * value = is_signed ? * ( __s16 * ) data : * ( __u16 * ) data ; return 0 ; case 1 : * value = is_signed ? * ( __s8 * ) data : * ( __u8 * ) data ; return 0 ; default : pr_warn ( \"unexpected size %d for enum, id:[%u]\\n\" , t -> size , id ) ; return - EINVAL ; } }",
    "resources/libbpf/src/btf_dump.c@btf_dump_enum_data": "static int btf_dump_enum_data ( struct btf_dump * d , const struct btf_type * t , __u32 id , const void * data ) { bool is_signed ; __s64 value ; int i , err ; err = btf_dump_get_enum_value ( d , t , data , id , & value ) ; if ( err ) return err ; is_signed = btf_kflag ( t ) ; if ( btf_is_enum ( t ) ) { const struct btf_enum * e ; for ( i = 0 , e = btf_enum ( t ) ; i < btf_vlen ( t ) ; i ++ , e ++ ) { if ( value != e -> val ) continue ; btf_dump_type_values ( d , \"%s\" , btf_name_of ( d , e -> name_off ) ) ; return 0 ; } btf_dump_type_values ( d , is_signed ? \"%d\" : \"%u\" , value ) ; } else { const struct btf_enum64 * e ; for ( i = 0 , e = btf_enum64 ( t ) ; i < btf_vlen ( t ) ; i ++ , e ++ ) { if ( value != btf_enum64_value ( e ) ) continue ; btf_dump_type_values ( d , \"%s\" , btf_name_of ( d , e -> name_off ) ) ; return 0 ; } btf_dump_type_values ( d , is_signed ? \"%lldLL\" : \"%lluULL\" , ( unsigned long long ) value ) ; } return 0 ; }",
    "resources/libbpf/src/btf_dump.c@btf_dump_datasec_data": "static int btf_dump_datasec_data ( struct btf_dump * d , const struct btf_type * t , __u32 id , const void * data ) { const struct btf_var_secinfo * vsi ; const struct btf_type * var ; __u32 i ; int err ; btf_dump_type_values ( d , \"SEC(\\\"%s\\\") \" , btf_name_of ( d , t -> name_off ) ) ; for ( i = 0 , vsi = btf_var_secinfos ( t ) ; i < btf_vlen ( t ) ; i ++ , vsi ++ ) { var = btf__type_by_id ( d -> btf , vsi -> type ) ; err = btf_dump_dump_type_data ( d , NULL , var , vsi -> type , data + vsi -> offset , 0 , 0 ) ; if ( err < 0 ) return err ; btf_dump_printf ( d , \";\" ) ; } return 0 ; }",
    "resources/libbpf/src/btf_dump.c@btf_dump_type_data_check_overflow": "static int btf_dump_type_data_check_overflow ( struct btf_dump * d , const struct btf_type * t , __u32 id , const void * data , __u8 bits_offset , __u8 bit_sz ) { __s64 size ; if ( bit_sz ) { /* bits_offset is at most 7. bit_sz is at most 128. */ __u8 nr_bytes = ( bits_offset + bit_sz + 7 ) / 8 ; /* When bit_sz is non zero, it is called from\n\t\t * btf_dump_struct_data() where it only cares about\n\t\t * negative error value.\n\t\t * Return nr_bytes in success case to make it\n\t\t * consistent as the regular integer case below.\n\t\t */ return data + nr_bytes > d -> typed_dump -> data_end ? - E2BIG : nr_bytes ; } size = btf__resolve_size ( d -> btf , id ) ; if ( size < 0 || size >= INT_MAX ) { pr_warn ( \"unexpected size [%zu] for id [%u]\\n\" , ( size_t ) size , id ) ; return - EINVAL ; } /* Only do overflow checking for base types; we do not want to\n\t * avoid showing part of a struct, union or array, even if we\n\t * do not have enough data to show the full object.  By\n\t * restricting overflow checking to base types we can ensure\n\t * that partial display succeeds, while avoiding overflowing\n\t * and using bogus data for display.\n\t */ t = skip_mods_and_typedefs ( d -> btf , id , NULL ) ; if ( ! t ) { pr_warn ( \"unexpected error skipping mods/typedefs for id [%u]\\n\" , id ) ; return - EINVAL ; } switch ( btf_kind ( t ) ) { case BTF_KIND_INT : case BTF_KIND_FLOAT : case BTF_KIND_PTR : case BTF_KIND_ENUM : case BTF_KIND_ENUM64 : if ( data + bits_offset / 8 + size > d -> typed_dump -> data_end ) return - E2BIG ; break ; default : break ; } return ( int ) size ; }",
    "resources/libbpf/src/btf_dump.c@btf_dump_type_data_check_zero": "static int btf_dump_type_data_check_zero ( struct btf_dump * d , const struct btf_type * t , __u32 id , const void * data , __u8 bits_offset , __u8 bit_sz ) { __s64 value ; int i , err ; /* toplevel exceptions; we show zero values if\n\t * - we ask for them (emit_zeros)\n\t * - if we are at top-level so we see \"struct empty { }\"\n\t * - or if we are an array member and the array is non-empty and\n\t *   not a char array; we don't want to be in a situation where we\n\t *   have an integer array 0, 1, 0, 1 and only show non-zero values.\n\t *   If the array contains zeroes only, or is a char array starting\n\t *   with a '\\0', the array-level check_zero() will prevent showing it;\n\t *   we are concerned with determining zero value at the array member\n\t *   level here.\n\t */ if ( d -> typed_dump -> emit_zeroes || d -> typed_dump -> depth == 0 || ( d -> typed_dump -> is_array_member && ! d -> typed_dump -> is_array_char ) ) return 0 ; t = skip_mods_and_typedefs ( d -> btf , id , NULL ) ; switch ( btf_kind ( t ) ) { case BTF_KIND_INT : if ( bit_sz ) return btf_dump_bitfield_check_zero ( d , t , data , bits_offset , bit_sz ) ; return btf_dump_base_type_check_zero ( d , t , id , data ) ; case BTF_KIND_FLOAT : case BTF_KIND_PTR : return btf_dump_base_type_check_zero ( d , t , id , data ) ; case BTF_KIND_ARRAY : { const struct btf_array * array = btf_array ( t ) ; const struct btf_type * elem_type ; __u32 elem_type_id , elem_size ; bool ischar ; elem_type_id = array -> type ; elem_size = btf__resolve_size ( d -> btf , elem_type_id ) ; elem_type = skip_mods_and_typedefs ( d -> btf , elem_type_id , NULL ) ; ischar = btf_is_int ( elem_type ) && elem_size == 1 ; /* check all elements; if _any_ element is nonzero, all\n\t\t * of array is displayed.  We make an exception however\n\t\t * for char arrays where the first element is 0; these\n\t\t * are considered zeroed also, even if later elements are\n\t\t * non-zero because the string is terminated.\n\t\t */ for ( i = 0 ; i < array -> nelems ; i ++ ) { if ( i == 0 && ischar && * ( char * ) data == 0 ) return - ENODATA ; err = btf_dump_type_data_check_zero ( d , elem_type , elem_type_id , data + ( i * elem_size ) , bits_offset , 0 ) ; if ( err != - ENODATA ) return err ; } return - ENODATA ; } case BTF_KIND_STRUCT : case BTF_KIND_UNION : { const struct btf_member * m = btf_members ( t ) ; __u16 n = btf_vlen ( t ) ; /* if any struct/union member is non-zero, the struct/union\n\t\t * is considered non-zero and dumped.\n\t\t */ for ( i = 0 ; i < n ; i ++ , m ++ ) { const struct btf_type * mtype ; __u32 moffset ; mtype = btf__type_by_id ( d -> btf , m -> type ) ; moffset = btf_member_bit_offset ( t , i ) ; /* btf_int_bits() does not store member bitfield size;\n\t\t\t * bitfield size needs to be stored here so int display\n\t\t\t * of member can retrieve it.\n\t\t\t */ bit_sz = btf_member_bitfield_size ( t , i ) ; err = btf_dump_type_data_check_zero ( d , mtype , m -> type , data + moffset / 8 , moffset % 8 , bit_sz ) ; if ( err != ENODATA ) return err ; } return - ENODATA ; } case BTF_KIND_ENUM : case BTF_KIND_ENUM64 : err = btf_dump_get_enum_value ( d , t , data , id , & value ) ; if ( err ) return err ; if ( value == 0 ) return - ENODATA ; return 0 ; default : return 0 ; } }",
    "resources/libbpf/src/btf_dump.c@btf_dump_dump_type_data": "static int btf_dump_dump_type_data ( struct btf_dump * d , const char * fname , const struct btf_type * t , __u32 id , const void * data , __u8 bits_offset , __u8 bit_sz ) { int size , err = 0 ; size = btf_dump_type_data_check_overflow ( d , t , id , data , bits_offset , bit_sz ) ; if ( size < 0 ) return size ; err = btf_dump_type_data_check_zero ( d , t , id , data , bits_offset , bit_sz ) ; if ( err ) { /* zeroed data is expected and not an error, so simply skip\n\t\t * dumping such data.  Record other errors however.\n\t\t */ if ( err == - ENODATA ) return size ; return err ; } btf_dump_data_pfx ( d ) ; if ( ! d -> typed_dump -> skip_names ) { if ( fname && strlen ( fname ) > 0 ) btf_dump_printf ( d , \".%s = \" , fname ) ; btf_dump_emit_type_cast ( d , id , true ) ; } t = skip_mods_and_typedefs ( d -> btf , id , NULL ) ; switch ( btf_kind ( t ) ) { case BTF_KIND_UNKN : case BTF_KIND_FWD : case BTF_KIND_FUNC : case BTF_KIND_FUNC_PROTO : case BTF_KIND_DECL_TAG : err = btf_dump_unsupported_data ( d , t , id ) ; break ; case BTF_KIND_INT : if ( bit_sz ) err = btf_dump_bitfield_data ( d , t , data , bits_offset , bit_sz ) ; else err = btf_dump_int_data ( d , t , id , data , bits_offset ) ; break ; case BTF_KIND_FLOAT : err = btf_dump_float_data ( d , t , id , data ) ; break ; case BTF_KIND_PTR : err = btf_dump_ptr_data ( d , t , id , data ) ; break ; case BTF_KIND_ARRAY : err = btf_dump_array_data ( d , t , id , data ) ; break ; case BTF_KIND_STRUCT : case BTF_KIND_UNION : err = btf_dump_struct_data ( d , t , id , data ) ; break ; case BTF_KIND_ENUM : case BTF_KIND_ENUM64 : /* handle bitfield and int enum values */ if ( bit_sz ) { __u64 print_num ; __s64 enum_val ; err = btf_dump_get_bitfield_value ( d , t , data , bits_offset , bit_sz , & print_num ) ; if ( err ) break ; enum_val = ( __s64 ) print_num ; err = btf_dump_enum_data ( d , t , id , & enum_val ) ; } else err = btf_dump_enum_data ( d , t , id , data ) ; break ; case BTF_KIND_VAR : err = btf_dump_var_data ( d , t , id , data ) ; break ; case BTF_KIND_DATASEC : err = btf_dump_datasec_data ( d , t , id , data ) ; break ; default : pr_warn ( \"unexpected kind [%u] for id [%u]\\n\" , BTF_INFO_KIND ( t -> info ) , id ) ; return - EINVAL ; } if ( err < 0 ) return err ; return size ; }",
    "resources/libbpf/src/btf_dump.c@btf_dump__dump_type_data": "int btf_dump__dump_type_data ( struct btf_dump * d , __u32 id , const void * data , size_t data_sz , const struct btf_dump_type_data_opts * opts ) { struct btf_dump_data typed_dump = { } ; const struct btf_type * t ; int ret ; if ( ! OPTS_VALID ( opts , btf_dump_type_data_opts ) ) return libbpf_err ( - EINVAL ) ; t = btf__type_by_id ( d -> btf , id ) ; if ( ! t ) return libbpf_err ( - ENOENT ) ; d -> typed_dump = & typed_dump ; d -> typed_dump -> data_end = data + data_sz ; d -> typed_dump -> indent_lvl = OPTS_GET ( opts , indent_level , 0 ) ; /* default indent string is a tab */ if ( ! OPTS_GET ( opts , indent_str , NULL ) ) d -> typed_dump -> indent_str [ 0 ] = '\\t' ; else libbpf_strlcpy ( d -> typed_dump -> indent_str , opts -> indent_str , sizeof ( d -> typed_dump -> indent_str ) ) ; d -> typed_dump -> compact = OPTS_GET ( opts , compact , false ) ; d -> typed_dump -> skip_names = OPTS_GET ( opts , skip_names , false ) ; d -> typed_dump -> emit_zeroes = OPTS_GET ( opts , emit_zeroes , false ) ; ret = btf_dump_dump_type_data ( d , NULL , t , id , data , 0 , 0 ) ; d -> typed_dump = NULL ; return libbpf_err ( ret ) ; }",
    "resources/libbpf/src/usdt.c@btf_kind": "static inline __u16 btf_kind ( const struct btf_type * t ) { return BTF_INFO_KIND ( t -> info ) ; }",
    "resources/libbpf/src/usdt.c@btf_vlen": "static inline __u16 btf_vlen ( const struct btf_type * t ) { return BTF_INFO_VLEN ( t -> info ) ; }",
    "resources/libbpf/src/usdt.c@btf_kflag": "static inline bool btf_kflag ( const struct btf_type * t ) { return BTF_INFO_KFLAG ( t -> info ) ; }",
    "resources/libbpf/src/usdt.c@btf_is_void": "static inline bool btf_is_void ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_UNKN ; }",
    "resources/libbpf/src/usdt.c@btf_is_int": "static inline bool btf_is_int ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_INT ; }",
    "resources/libbpf/src/usdt.c@btf_is_ptr": "static inline bool btf_is_ptr ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_PTR ; }",
    "resources/libbpf/src/usdt.c@btf_is_array": "static inline bool btf_is_array ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_ARRAY ; }",
    "resources/libbpf/src/usdt.c@btf_is_struct": "static inline bool btf_is_struct ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_STRUCT ; }",
    "resources/libbpf/src/usdt.c@btf_is_union": "static inline bool btf_is_union ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_UNION ; }",
    "resources/libbpf/src/usdt.c@btf_is_composite": "static inline bool btf_is_composite ( const struct btf_type * t ) { __u16 kind = btf_kind ( t ) ; return kind == BTF_KIND_STRUCT || kind == BTF_KIND_UNION ; }",
    "resources/libbpf/src/usdt.c@btf_is_enum": "static inline bool btf_is_enum ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_ENUM ; }",
    "resources/libbpf/src/usdt.c@btf_is_enum64": "static inline bool btf_is_enum64 ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_ENUM64 ; }",
    "resources/libbpf/src/usdt.c@btf_is_fwd": "static inline bool btf_is_fwd ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_FWD ; }",
    "resources/libbpf/src/usdt.c@btf_is_typedef": "static inline bool btf_is_typedef ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_TYPEDEF ; }",
    "resources/libbpf/src/usdt.c@btf_is_volatile": "static inline bool btf_is_volatile ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_VOLATILE ; }",
    "resources/libbpf/src/usdt.c@btf_is_const": "static inline bool btf_is_const ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_CONST ; }",
    "resources/libbpf/src/usdt.c@btf_is_restrict": "static inline bool btf_is_restrict ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_RESTRICT ; }",
    "resources/libbpf/src/usdt.c@btf_is_mod": "static inline bool btf_is_mod ( const struct btf_type * t ) { __u16 kind = btf_kind ( t ) ; return kind == BTF_KIND_VOLATILE || kind == BTF_KIND_CONST || kind == BTF_KIND_RESTRICT || kind == BTF_KIND_TYPE_TAG ; }",
    "resources/libbpf/src/usdt.c@btf_is_func": "static inline bool btf_is_func ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_FUNC ; }",
    "resources/libbpf/src/usdt.c@btf_is_func_proto": "static inline bool btf_is_func_proto ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_FUNC_PROTO ; }",
    "resources/libbpf/src/usdt.c@btf_is_var": "static inline bool btf_is_var ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_VAR ; }",
    "resources/libbpf/src/usdt.c@btf_is_datasec": "static inline bool btf_is_datasec ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_DATASEC ; }",
    "resources/libbpf/src/usdt.c@btf_is_float": "static inline bool btf_is_float ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_FLOAT ; }",
    "resources/libbpf/src/usdt.c@btf_is_decl_tag": "static inline bool btf_is_decl_tag ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_DECL_TAG ; }",
    "resources/libbpf/src/usdt.c@btf_is_type_tag": "static inline bool btf_is_type_tag ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_TYPE_TAG ; }",
    "resources/libbpf/src/usdt.c@btf_is_any_enum": "static inline bool btf_is_any_enum ( const struct btf_type * t ) { return btf_is_enum ( t ) || btf_is_enum64 ( t ) ; }",
    "resources/libbpf/src/usdt.c@btf_kind_core_compat": "static inline bool btf_kind_core_compat ( const struct btf_type * t1 , const struct btf_type * t2 ) { return btf_kind ( t1 ) == btf_kind ( t2 ) || ( btf_is_any_enum ( t1 ) && btf_is_any_enum ( t2 ) ) ; }",
    "resources/libbpf/src/usdt.c@btf_int_encoding": "static inline __u8 btf_int_encoding ( const struct btf_type * t ) { return BTF_INT_ENCODING ( * ( __u32 * ) ( t + 1 ) ) ; }",
    "resources/libbpf/src/usdt.c@btf_int_offset": "static inline __u8 btf_int_offset ( const struct btf_type * t ) { return BTF_INT_OFFSET ( * ( __u32 * ) ( t + 1 ) ) ; }",
    "resources/libbpf/src/usdt.c@btf_int_bits": "static inline __u8 btf_int_bits ( const struct btf_type * t ) { return BTF_INT_BITS ( * ( __u32 * ) ( t + 1 ) ) ; }",
    "resources/libbpf/src/usdt.c@btf_array": "static inline struct btf_array * btf_array ( const struct btf_type * t ) { return ( struct btf_array * ) ( t + 1 ) ; }",
    "resources/libbpf/src/usdt.c@btf_enum": "static inline struct btf_enum * btf_enum ( const struct btf_type * t ) { return ( struct btf_enum * ) ( t + 1 ) ; }",
    "resources/libbpf/src/usdt.c@btf_enum64": "static inline struct btf_enum64 * btf_enum64 ( const struct btf_type * t ) { return ( struct btf_enum64 * ) ( t + 1 ) ; }",
    "resources/libbpf/src/usdt.c@btf_enum64_value": "static inline __u64 btf_enum64_value ( const struct btf_enum64 * e ) { /* struct btf_enum64 is introduced in Linux 6.0, which is very\n\t * bleeding-edge. Here we are avoiding relying on struct btf_enum64\n\t * definition coming from kernel UAPI headers to support wider range\n\t * of system-wide kernel headers.\n\t *\n\t * Given this header can be also included from C++ applications, that\n\t * further restricts C tricks we can use (like using compatible\n\t * anonymous struct). So just treat struct btf_enum64 as\n\t * a three-element array of u32 and access second (lo32) and third\n\t * (hi32) elements directly.\n\t *\n\t * For reference, here is a struct btf_enum64 definition:\n\t *\n\t * const struct btf_enum64 {\n\t *\t__u32\tname_off;\n\t *\t__u32\tval_lo32;\n\t *\t__u32\tval_hi32;\n\t * };\n\t */ const __u32 * e64 = ( const __u32 * ) e ; return ( ( __u64 ) e64 [ 2 ] << 32 ) | e64 [ 1 ] ; }",
    "resources/libbpf/src/usdt.c@btf_members": "static inline struct btf_member * btf_members ( const struct btf_type * t ) { return ( struct btf_member * ) ( t + 1 ) ; }",
    "resources/libbpf/src/usdt.c@btf_member_bit_offset": "static inline __u32 btf_member_bit_offset ( const struct btf_type * t , __u32 member_idx ) { const struct btf_member * m = btf_members ( t ) + member_idx ; bool kflag = btf_kflag ( t ) ; return kflag ? BTF_MEMBER_BIT_OFFSET ( m -> offset ) : m -> offset ; }",
    "resources/libbpf/src/usdt.c@btf_member_bitfield_size": "static inline __u32 btf_member_bitfield_size ( const struct btf_type * t , __u32 member_idx ) { const struct btf_member * m = btf_members ( t ) + member_idx ; bool kflag = btf_kflag ( t ) ; return kflag ? BTF_MEMBER_BITFIELD_SIZE ( m -> offset ) : 0 ; }",
    "resources/libbpf/src/usdt.c@btf_params": "static inline struct btf_param * btf_params ( const struct btf_type * t ) { return ( struct btf_param * ) ( t + 1 ) ; }",
    "resources/libbpf/src/usdt.c@btf_var": "static inline struct btf_var * btf_var ( const struct btf_type * t ) { return ( struct btf_var * ) ( t + 1 ) ; }",
    "resources/libbpf/src/usdt.c@btf_var_secinfos": "static inline struct btf_var_secinfo * btf_var_secinfos ( const struct btf_type * t ) { return ( struct btf_var_secinfo * ) ( t + 1 ) ; }",
    "resources/libbpf/src/usdt.c@btf_decl_tag": "static inline struct btf_decl_tag * btf_decl_tag ( const struct btf_type * t ) { return ( struct btf_decl_tag * ) ( t + 1 ) ; }",
    "resources/libbpf/src/usdt.c@str_has_sfx": "static inline bool str_has_sfx ( const char * str , const char * sfx ) { size_t str_len = strlen ( str ) ; size_t sfx_len = strlen ( sfx ) ; if ( sfx_len > str_len ) return false ; return strcmp ( str + str_len - sfx_len , sfx ) == 0 ; }",
    "resources/libbpf/src/usdt.c@libbpf_reallocarray": "static inline void * libbpf_reallocarray ( void * ptr , size_t nmemb , size_t size ) { size_t total ; # if __has_builtin ( __builtin_mul_overflow ) if ( unlikely ( __builtin_mul_overflow ( nmemb , size , & total ) ) ) return NULL ; # else if ( size == 0 || nmemb > ULONG_MAX / size ) return NULL ; total = nmemb * size ; # endif return realloc ( ptr , total ) ; }",
    "resources/libbpf/src/usdt.c@libbpf_strlcpy": "static inline void libbpf_strlcpy ( char * dst , const char * src , size_t sz ) { size_t i ; if ( sz == 0 ) return ; sz -- ; for ( i = 0 ; i < sz && src [ i ] ; i ++ ) dst [ i ] = src [ i ] ; dst [ i ] = '\\0' ; }",
    "resources/libbpf/src/usdt.c@btf_func_linkage": "static inline enum btf_func_linkage btf_func_linkage ( const struct btf_type * t ) { return ( enum btf_func_linkage ) ( int ) btf_vlen ( t ) ; }",
    "resources/libbpf/src/usdt.c@btf_type_info": "static inline __u32 btf_type_info ( int kind , int vlen , int kflag ) { return ( kflag << 31 ) | ( kind << 24 ) | vlen ; }",
    "resources/libbpf/src/usdt.c@libbpf_is_mem_zeroed": "static inline bool libbpf_is_mem_zeroed ( const char * p , ssize_t len ) { while ( len > 0 ) { if ( * p ) return false ; p ++ ; len -- ; } return true ; }",
    "resources/libbpf/src/usdt.c@libbpf_validate_opts": "static inline bool libbpf_validate_opts ( const char * opts , size_t opts_sz , size_t user_sz , const char * type_name ) { if ( user_sz < sizeof ( size_t ) ) { pr_warn ( \"%s size (%zu) is too small\\n\" , type_name , user_sz ) ; return false ; } if ( ! libbpf_is_mem_zeroed ( opts + opts_sz , ( ssize_t ) user_sz - opts_sz ) ) { pr_warn ( \"%s has non-zero extra bytes\\n\" , type_name ) ; return false ; } return true ; }",
    "resources/libbpf/src/usdt.c@libbpf_err": "static inline int libbpf_err ( int ret ) { if ( ret < 0 ) errno = - ret ; return ret ; }",
    "resources/libbpf/src/usdt.c@libbpf_err_errno": "static inline int libbpf_err_errno ( int ret ) { /* errno is already assumed to be set on error */ return ret < 0 ? - errno : ret ; }",
    "resources/libbpf/src/usdt.c@libbpf_err_ptr": "static inline void * libbpf_err_ptr ( int err ) { /* set errno on error, this doesn't break anything */ errno = - err ; return NULL ; }",
    "resources/libbpf/src/usdt.c@libbpf_ptr": "static inline void * libbpf_ptr ( void * ret ) { /* set errno on error, this doesn't break anything */ if ( IS_ERR ( ret ) ) errno = - PTR_ERR ( ret ) ; return IS_ERR ( ret ) ? NULL : ret ; }",
    "resources/libbpf/src/usdt.c@str_is_empty": "static inline bool str_is_empty ( const char * s ) { return ! s || ! s [ 0 ] ; }",
    "resources/libbpf/src/usdt.c@is_ldimm64_insn": "static inline bool is_ldimm64_insn ( struct bpf_insn * insn ) { return insn -> code == ( BPF_LD | BPF_IMM | BPF_DW ) ; }",
    "resources/libbpf/src/usdt.c@dup_good_fd": "static inline int dup_good_fd ( int fd ) { if ( fd < 0 ) return fd ; return fcntl ( fd , F_DUPFD_CLOEXEC , 3 ) ; }",
    "resources/libbpf/src/usdt.c@ensure_good_fd": "static inline int ensure_good_fd ( int fd ) { int old_fd = fd , saved_errno ; if ( fd < 0 ) return fd ; if ( fd < 3 ) { fd = dup_good_fd ( fd ) ; saved_errno = errno ; close ( old_fd ) ; errno = saved_errno ; if ( fd < 0 ) { pr_warn ( \"failed to dup FD %d to FD > 2: %d\\n\" , old_fd , - saved_errno ) ; errno = saved_errno ; } } return fd ; }",
    "resources/libbpf/src/usdt.c@sys_dup2": "static inline int sys_dup2 ( int oldfd , int newfd ) { # ifdef __NR_dup2 return syscall ( __NR_dup2 , oldfd , newfd ) ; # else return syscall ( __NR_dup3 , oldfd , newfd , 0 ) ; # endif }",
    "resources/libbpf/src/usdt.c@reuse_fd": "static inline int reuse_fd ( int fixed_fd , int tmp_fd ) { int err ; err = sys_dup2 ( tmp_fd , fixed_fd ) ; err = err < 0 ? - errno : 0 ; close ( tmp_fd ) ; /* clean up temporary FD */ return err ; }",
    "resources/libbpf/src/usdt.c@is_pow_of_2": "static inline bool is_pow_of_2 ( size_t x ) { return x && ( x & ( x - 1 ) ) == 0 ; }",
    "resources/libbpf/src/usdt.c@hash_bits": "static inline size_t hash_bits ( size_t h , int bits ) { /* shuffle bits and return requested number of upper bits */ if ( bits == 0 ) return 0 ; # if ( __SIZEOF_SIZE_T__ == __SIZEOF_LONG_LONG__ ) /* LP64 case */ return ( h * 11400714819323198485llu ) >> ( __SIZEOF_LONG_LONG__ * 8 - bits ) ; # elif ( __SIZEOF_SIZE_T__ <= __SIZEOF_LONG__ ) return ( h * 2654435769lu ) >> ( __SIZEOF_LONG__ * 8 - bits ) ; # else # error \"Unsupported size_t size\" # endif }",
    "resources/libbpf/src/usdt.c@str_hash": "static inline size_t str_hash ( const char * s ) { size_t h = 0 ; while ( * s ) { h = h * 31 + * s ; s ++ ; } return h ; }",
    "resources/libbpf/src/usdt.c@usdt_manager_new": "struct usdt_manager * usdt_manager_new ( struct bpf_object * obj ) { static const char * ref_ctr_sysfs_path = \"/sys/bus/event_source/devices/uprobe/format/ref_ctr_offset\" ; struct usdt_manager * man ; struct bpf_map * specs_map , * ip_to_spec_id_map ; specs_map = bpf_object__find_map_by_name ( obj , \"__bpf_usdt_specs\" ) ; ip_to_spec_id_map = bpf_object__find_map_by_name ( obj , \"__bpf_usdt_ip_to_spec_id\" ) ; if ( ! specs_map || ! ip_to_spec_id_map ) { pr_warn ( \"usdt: failed to find USDT support BPF maps, did you forget to include bpf/usdt.bpf.h?\\n\" ) ; return ERR_PTR ( - ESRCH ) ; } man = calloc ( 1 , sizeof ( * man ) ) ; if ( ! man ) return ERR_PTR ( - ENOMEM ) ; man -> specs_map = specs_map ; man -> ip_to_spec_id_map = ip_to_spec_id_map ; /* Detect if BPF cookie is supported for kprobes.\n\t * We don't need IP-to-ID mapping if we can use BPF cookies.\n\t * Added in: 7adfc6c9b315 (\"bpf: Add bpf_get_attach_cookie() BPF helper to access bpf_cookie value\")\n\t */ man -> has_bpf_cookie = kernel_supports ( obj , FEAT_BPF_COOKIE ) ; /* Detect kernel support for automatic refcounting of USDT semaphore.\n\t * If this is not supported, USDTs with semaphores will not be supported.\n\t * Added in: a6ca88b241d5 (\"trace_uprobe: support reference counter in fd-based uprobe\")\n\t */ man -> has_sema_refcnt = faccessat ( AT_FDCWD , ref_ctr_sysfs_path , F_OK , AT_EACCESS ) == 0 ; /*\n\t * Detect kernel support for uprobe multi link to be used for attaching\n\t * usdt probes.\n\t */ man -> has_uprobe_multi = kernel_supports ( obj , FEAT_UPROBE_MULTI_LINK ) ; return man ; }",
    "resources/libbpf/src/usdt.c@usdt_manager_free": "void usdt_manager_free ( struct usdt_manager * man ) { if ( IS_ERR_OR_NULL ( man ) ) return ; free ( man -> free_spec_ids ) ; free ( man ) ; }",
    "resources/libbpf/src/usdt.c@sanity_check_usdt_elf": "static int sanity_check_usdt_elf ( Elf * elf , const char * path ) { GElf_Ehdr ehdr ; int endianness ; if ( elf_kind ( elf ) != ELF_K_ELF ) { pr_warn ( \"usdt: unrecognized ELF kind %d for '%s'\\n\" , elf_kind ( elf ) , path ) ; return - EBADF ; } switch ( gelf_getclass ( elf ) ) { case ELFCLASS64 : if ( sizeof ( void * ) != 8 ) { pr_warn ( \"usdt: attaching to 64-bit ELF binary '%s' is not supported\\n\" , path ) ; return - EBADF ; } break ; case ELFCLASS32 : if ( sizeof ( void * ) != 4 ) { pr_warn ( \"usdt: attaching to 32-bit ELF binary '%s' is not supported\\n\" , path ) ; return - EBADF ; } break ; default : pr_warn ( \"usdt: unsupported ELF class for '%s'\\n\" , path ) ; return - EBADF ; } if ( ! gelf_getehdr ( elf , & ehdr ) ) return - EINVAL ; if ( ehdr . e_type != ET_EXEC && ehdr . e_type != ET_DYN ) { pr_warn ( \"usdt: unsupported type of ELF binary '%s' (%d), only ET_EXEC and ET_DYN are supported\\n\" , path , ehdr . e_type ) ; return - EBADF ; } # if __BYTE_ORDER__ == __ORDER_LITTLE_ENDIAN__ endianness = ELFDATA2LSB ; # elif __BYTE_ORDER__ == __ORDER_BIG_ENDIAN__ endianness = ELFDATA2MSB ; # else # error \"Unrecognized __BYTE_ORDER__\" # endif if ( endianness != ehdr . e_ident [ EI_DATA ] ) { pr_warn ( \"usdt: ELF endianness mismatch for '%s'\\n\" , path ) ; return - EBADF ; } return 0 ; }",
    "resources/libbpf/src/usdt.c@find_elf_sec_by_name": "static int find_elf_sec_by_name ( Elf * elf , const char * sec_name , GElf_Shdr * shdr , Elf_Scn * * scn ) { Elf_Scn * sec = NULL ; size_t shstrndx ; if ( elf_getshdrstrndx ( elf , & shstrndx ) ) return - EINVAL ; /* check if ELF is corrupted and avoid calling elf_strptr if yes */ if ( ! elf_rawdata ( elf_getscn ( elf , shstrndx ) , NULL ) ) return - EINVAL ; while ( ( sec = elf_nextscn ( elf , sec ) ) != NULL ) { char * name ; if ( ! gelf_getshdr ( sec , shdr ) ) return - EINVAL ; name = elf_strptr ( elf , shstrndx , shdr -> sh_name ) ; if ( name && strcmp ( sec_name , name ) == 0 ) { * scn = sec ; return 0 ; } } return - ENOENT ; }",
    "resources/libbpf/src/usdt.c@cmp_elf_segs": "static int cmp_elf_segs ( const void * _a , const void * _b ) { const struct elf_seg * a = _a ; const struct elf_seg * b = _b ; return a -> start < b -> start ? - 1 : 1 ; }",
    "resources/libbpf/src/usdt.c@parse_elf_segs": "static int parse_elf_segs ( Elf * elf , const char * path , struct elf_seg * * segs , size_t * seg_cnt ) { GElf_Phdr phdr ; size_t n ; int i , err ; struct elf_seg * seg ; void * tmp ; * seg_cnt = 0 ; if ( elf_getphdrnum ( elf , & n ) ) { err = - errno ; return err ; } for ( i = 0 ; i < n ; i ++ ) { if ( ! gelf_getphdr ( elf , i , & phdr ) ) { err = - errno ; return err ; } pr_debug ( \"usdt: discovered PHDR #%d in '%s': vaddr 0x%lx memsz 0x%lx offset 0x%lx type 0x%lx flags 0x%lx\\n\" , i , path , ( long ) phdr . p_vaddr , ( long ) phdr . p_memsz , ( long ) phdr . p_offset , ( long ) phdr . p_type , ( long ) phdr . p_flags ) ; if ( phdr . p_type != PT_LOAD ) continue ; tmp = libbpf_reallocarray ( * segs , * seg_cnt + 1 , sizeof ( * * segs ) ) ; if ( ! tmp ) return - ENOMEM ; * segs = tmp ; seg = * segs + * seg_cnt ; ( * seg_cnt ) ++ ; seg -> start = phdr . p_vaddr ; seg -> end = phdr . p_vaddr + phdr . p_memsz ; seg -> offset = phdr . p_offset ; seg -> is_exec = phdr . p_flags & PF_X ; } if ( * seg_cnt == 0 ) { pr_warn ( \"usdt: failed to find PT_LOAD program headers in '%s'\\n\" , path ) ; return - ESRCH ; } qsort ( * segs , * seg_cnt , sizeof ( * * segs ) , cmp_elf_segs ) ; return 0 ; }",
    "resources/libbpf/src/usdt.c@parse_vma_segs": "static int parse_vma_segs ( int pid , const char * lib_path , struct elf_seg * * segs , size_t * seg_cnt ) { char path [ PATH_MAX ] , line [ PATH_MAX ] , mode [ 16 ] ; size_t seg_start , seg_end , seg_off ; struct elf_seg * seg ; int tmp_pid , i , err ; FILE * f ; * seg_cnt = 0 ; /* Handle containerized binaries only accessible from\n\t * /proc/<pid>/root/<path>. They will be reported as just /<path> in\n\t * /proc/<pid>/maps.\n\t */ if ( sscanf ( lib_path , \"/proc/%d/root%s\" , & tmp_pid , path ) == 2 && pid == tmp_pid ) goto proceed ; if ( ! realpath ( lib_path , path ) ) { pr_warn ( \"usdt: failed to get absolute path of '%s' (err %d), using path as is...\\n\" , lib_path , - errno ) ; libbpf_strlcpy ( path , lib_path , sizeof ( path ) ) ; } proceed : sprintf ( line , \"/proc/%d/maps\" , pid ) ; f = fopen ( line , \"re\" ) ; if ( ! f ) { err = - errno ; pr_warn ( \"usdt: failed to open '%s' to get base addr of '%s': %d\\n\" , line , lib_path , err ) ; return err ; } /* We need to handle lines with no path at the end:\n\t *\n\t * 7f5c6f5d1000-7f5c6f5d3000 rw-p 001c7000 08:04 21238613      /usr/lib64/libc-2.17.so\n\t * 7f5c6f5d3000-7f5c6f5d8000 rw-p 00000000 00:00 0\n\t * 7f5c6f5d8000-7f5c6f5d9000 r-xp 00000000 103:01 362990598    /data/users/andriin/linux/tools/bpf/usdt/libhello_usdt.so\n\t */ while ( fscanf ( f , \"%zx-%zx %s %zx %*s %*d%[^\\n]\\n\" , & seg_start , & seg_end , mode , & seg_off , line ) == 5 ) { void * tmp ; /* to handle no path case (see above) we need to capture line\n\t\t * without skipping any whitespaces. So we need to strip\n\t\t * leading whitespaces manually here\n\t\t */ i = 0 ; while ( isblank ( line [ i ] ) ) i ++ ; if ( strcmp ( line + i , path ) != 0 ) continue ; pr_debug ( \"usdt: discovered segment for lib '%s': addrs %zx-%zx mode %s offset %zx\\n\" , path , seg_start , seg_end , mode , seg_off ) ; /* ignore non-executable sections for shared libs */ if ( mode [ 2 ] != 'x' ) continue ; tmp = libbpf_reallocarray ( * segs , * seg_cnt + 1 , sizeof ( * * segs ) ) ; if ( ! tmp ) { err = - ENOMEM ; goto err_out ; } * segs = tmp ; seg = * segs + * seg_cnt ; * seg_cnt += 1 ; seg -> start = seg_start ; seg -> end = seg_end ; seg -> offset = seg_off ; seg -> is_exec = true ; } if ( * seg_cnt == 0 ) { pr_warn ( \"usdt: failed to find '%s' (resolved to '%s') within PID %d memory mappings\\n\" , lib_path , path , pid ) ; err = - ESRCH ; goto err_out ; } qsort ( * segs , * seg_cnt , sizeof ( * * segs ) , cmp_elf_segs ) ; err = 0 ; err_out : fclose ( f ) ; return err ; }",
    "resources/libbpf/src/usdt.c@find_elf_seg": "static struct elf_seg * find_elf_seg ( struct elf_seg * segs , size_t seg_cnt , long virtaddr ) { struct elf_seg * seg ; int i ; /* for ELF binaries (both executables and shared libraries), we are\n\t * given virtual address (absolute for executables, relative for\n\t * libraries) which should match address range of [seg_start, seg_end)\n\t */ for ( i = 0 , seg = segs ; i < seg_cnt ; i ++ , seg ++ ) { if ( seg -> start <= virtaddr && virtaddr < seg -> end ) return seg ; } return NULL ; }",
    "resources/libbpf/src/usdt.c@find_vma_seg": "static struct elf_seg * find_vma_seg ( struct elf_seg * segs , size_t seg_cnt , long offset ) { struct elf_seg * seg ; int i ; /* for VMA segments from /proc/<pid>/maps file, provided \"address\" is\n\t * actually a file offset, so should be fall within logical\n\t * offset-based range of [offset_start, offset_end)\n\t */ for ( i = 0 , seg = segs ; i < seg_cnt ; i ++ , seg ++ ) { if ( seg -> offset <= offset && offset < seg -> offset + ( seg -> end - seg -> start ) ) return seg ; } return NULL ; }",
    "resources/libbpf/src/usdt.c@collect_usdt_targets": "static int collect_usdt_targets ( struct usdt_manager * man , Elf * elf , const char * path , pid_t pid , const char * usdt_provider , const char * usdt_name , __u64 usdt_cookie , struct usdt_target * * out_targets , size_t * out_target_cnt ) { size_t off , name_off , desc_off , seg_cnt = 0 , vma_seg_cnt = 0 , target_cnt = 0 ; struct elf_seg * segs = NULL , * vma_segs = NULL ; struct usdt_target * targets = NULL , * target ; long base_addr = 0 ; Elf_Scn * notes_scn , * base_scn ; GElf_Shdr base_shdr , notes_shdr ; GElf_Ehdr ehdr ; GElf_Nhdr nhdr ; Elf_Data * data ; int err ; * out_targets = NULL ; * out_target_cnt = 0 ; err = find_elf_sec_by_name ( elf , USDT_NOTE_SEC , & notes_shdr , & notes_scn ) ; if ( err ) { pr_warn ( \"usdt: no USDT notes section (%s) found in '%s'\\n\" , USDT_NOTE_SEC , path ) ; return err ; } if ( notes_shdr . sh_type != SHT_NOTE || ! gelf_getehdr ( elf , & ehdr ) ) { pr_warn ( \"usdt: invalid USDT notes section (%s) in '%s'\\n\" , USDT_NOTE_SEC , path ) ; return - EINVAL ; } err = parse_elf_segs ( elf , path , & segs , & seg_cnt ) ; if ( err ) { pr_warn ( \"usdt: failed to process ELF program segments for '%s': %d\\n\" , path , err ) ; goto err_out ; } /* .stapsdt.base ELF section is optional, but is used for prelink\n\t * offset compensation (see a big comment further below)\n\t */ if ( find_elf_sec_by_name ( elf , USDT_BASE_SEC , & base_shdr , & base_scn ) == 0 ) base_addr = base_shdr . sh_addr ; data = elf_getdata ( notes_scn , 0 ) ; off = 0 ; while ( ( off = gelf_getnote ( data , off , & nhdr , & name_off , & desc_off ) ) > 0 ) { long usdt_abs_ip , usdt_rel_ip , usdt_sema_off = 0 ; struct usdt_note note ; struct elf_seg * seg = NULL ; void * tmp ; err = parse_usdt_note ( elf , path , & nhdr , data -> d_buf , name_off , desc_off , & note ) ; if ( err ) goto err_out ; if ( strcmp ( note . provider , usdt_provider ) != 0 || strcmp ( note . name , usdt_name ) != 0 ) continue ; /* We need to compensate \"prelink effect\". See [0] for details,\n\t\t * relevant parts quoted here:\n\t\t *\n\t\t * Each SDT probe also expands into a non-allocated ELF note. You can\n\t\t * find this by looking at SHT_NOTE sections and decoding the format;\n\t\t * see below for details. Because the note is non-allocated, it means\n\t\t * there is no runtime cost, and also preserved in both stripped files\n\t\t * and .debug files.\n\t\t *\n\t\t * However, this means that prelink won't adjust the note's contents\n\t\t * for address offsets. Instead, this is done via the .stapsdt.base\n\t\t * section. This is a special section that is added to the text. We\n\t\t * will only ever have one of these sections in a final link and it\n\t\t * will only ever be one byte long. Nothing about this section itself\n\t\t * matters, we just use it as a marker to detect prelink address\n\t\t * adjustments.\n\t\t *\n\t\t * Each probe note records the link-time address of the .stapsdt.base\n\t\t * section alongside the probe PC address. The decoder compares the\n\t\t * base address stored in the note with the .stapsdt.base section's\n\t\t * sh_addr. Initially these are the same, but the section header will\n\t\t * be adjusted by prelink. So the decoder applies the difference to\n\t\t * the probe PC address to get the correct prelinked PC address; the\n\t\t * same adjustment is applied to the semaphore address, if any.\n\t\t *\n\t\t *   [0] https://sourceware.org/systemtap/wiki/UserSpaceProbeImplementation\n\t\t */ usdt_abs_ip = note . loc_addr ; if ( base_addr ) usdt_abs_ip += base_addr - note . base_addr ; /* When attaching uprobes (which is what USDTs basically are)\n\t\t * kernel expects file offset to be specified, not a relative\n\t\t * virtual address, so we need to translate virtual address to\n\t\t * file offset, for both ET_EXEC and ET_DYN binaries.\n\t\t */ seg = find_elf_seg ( segs , seg_cnt , usdt_abs_ip ) ; if ( ! seg ) { err = - ESRCH ; pr_warn ( \"usdt: failed to find ELF program segment for '%s:%s' in '%s' at IP 0x%lx\\n\" , usdt_provider , usdt_name , path , usdt_abs_ip ) ; goto err_out ; } if ( ! seg -> is_exec ) { err = - ESRCH ; pr_warn ( \"usdt: matched ELF binary '%s' segment [0x%lx, 0x%lx) for '%s:%s' at IP 0x%lx is not executable\\n\" , path , seg -> start , seg -> end , usdt_provider , usdt_name , usdt_abs_ip ) ; goto err_out ; } /* translate from virtual address to file offset */ usdt_rel_ip = usdt_abs_ip - seg -> start + seg -> offset ; if ( ehdr . e_type == ET_DYN && ! man -> has_bpf_cookie ) { /* If we don't have BPF cookie support but need to\n\t\t\t * attach to a shared library, we'll need to know and\n\t\t\t * record absolute addresses of attach points due to\n\t\t\t * the need to lookup USDT spec by absolute IP of\n\t\t\t * triggered uprobe. Doing this resolution is only\n\t\t\t * possible when we have a specific PID of the process\n\t\t\t * that's using specified shared library. BPF cookie\n\t\t\t * removes the absolute address limitation as we don't\n\t\t\t * need to do this lookup (we just use BPF cookie as\n\t\t\t * an index of USDT spec), so for newer kernels with\n\t\t\t * BPF cookie support libbpf supports USDT attachment\n\t\t\t * to shared libraries with no PID filter.\n\t\t\t */ if ( pid < 0 ) { pr_warn ( \"usdt: attaching to shared libraries without specific PID is not supported on current kernel\\n\" ) ; err = - ENOTSUP ; goto err_out ; } /* vma_segs are lazily initialized only if necessary */ if ( vma_seg_cnt == 0 ) { err = parse_vma_segs ( pid , path , & vma_segs , & vma_seg_cnt ) ; if ( err ) { pr_warn ( \"usdt: failed to get memory segments in PID %d for shared library '%s': %d\\n\" , pid , path , err ) ; goto err_out ; } } seg = find_vma_seg ( vma_segs , vma_seg_cnt , usdt_rel_ip ) ; if ( ! seg ) { err = - ESRCH ; pr_warn ( \"usdt: failed to find shared lib memory segment for '%s:%s' in '%s' at relative IP 0x%lx\\n\" , usdt_provider , usdt_name , path , usdt_rel_ip ) ; goto err_out ; } usdt_abs_ip = seg -> start - seg -> offset + usdt_rel_ip ; } pr_debug ( \"usdt: probe for '%s:%s' in %s '%s': addr 0x%lx base 0x%lx (resolved abs_ip 0x%lx rel_ip 0x%lx) args '%s' in segment [0x%lx, 0x%lx) at offset 0x%lx\\n\" , usdt_provider , usdt_name , ehdr . e_type == ET_EXEC ? \"exec\" : \"lib \" , path , note . loc_addr , note . base_addr , usdt_abs_ip , usdt_rel_ip , note . args , seg ? seg -> start : 0 , seg ? seg -> end : 0 , seg ? seg -> offset : 0 ) ; /* Adjust semaphore address to be a file offset */ if ( note . sema_addr ) { if ( ! man -> has_sema_refcnt ) { pr_warn ( \"usdt: kernel doesn't support USDT semaphore refcounting for '%s:%s' in '%s'\\n\" , usdt_provider , usdt_name , path ) ; err = - ENOTSUP ; goto err_out ; } seg = find_elf_seg ( segs , seg_cnt , note . sema_addr ) ; if ( ! seg ) { err = - ESRCH ; pr_warn ( \"usdt: failed to find ELF loadable segment with semaphore of '%s:%s' in '%s' at 0x%lx\\n\" , usdt_provider , usdt_name , path , note . sema_addr ) ; goto err_out ; } if ( seg -> is_exec ) { err = - ESRCH ; pr_warn ( \"usdt: matched ELF binary '%s' segment [0x%lx, 0x%lx] for semaphore of '%s:%s' at 0x%lx is executable\\n\" , path , seg -> start , seg -> end , usdt_provider , usdt_name , note . sema_addr ) ; goto err_out ; } usdt_sema_off = note . sema_addr - seg -> start + seg -> offset ; pr_debug ( \"usdt: sema  for '%s:%s' in %s '%s': addr 0x%lx base 0x%lx (resolved 0x%lx) in segment [0x%lx, 0x%lx] at offset 0x%lx\\n\" , usdt_provider , usdt_name , ehdr . e_type == ET_EXEC ? \"exec\" : \"lib \" , path , note . sema_addr , note . base_addr , usdt_sema_off , seg -> start , seg -> end , seg -> offset ) ; } /* Record adjusted addresses and offsets and parse USDT spec */ tmp = libbpf_reallocarray ( targets , target_cnt + 1 , sizeof ( * targets ) ) ; if ( ! tmp ) { err = - ENOMEM ; goto err_out ; } targets = tmp ; target = & targets [ target_cnt ] ; memset ( target , 0 , sizeof ( * target ) ) ; target -> abs_ip = usdt_abs_ip ; target -> rel_ip = usdt_rel_ip ; target -> sema_off = usdt_sema_off ; /* notes.args references strings from ELF itself, so they can\n\t\t * be referenced safely until elf_end() call\n\t\t */ target -> spec_str = note . args ; err = parse_usdt_spec ( & target -> spec , & note , usdt_cookie ) ; if ( err ) goto err_out ; target_cnt ++ ; } * out_targets = targets ; * out_target_cnt = target_cnt ; err = target_cnt ; err_out : free ( segs ) ; free ( vma_segs ) ; if ( err < 0 ) free ( targets ) ; return err ; }",
    "resources/libbpf/src/usdt.c@bpf_link_usdt_detach": "static int bpf_link_usdt_detach ( struct bpf_link * link ) { struct bpf_link_usdt * usdt_link = container_of ( link , struct bpf_link_usdt , link ) ; struct usdt_manager * man = usdt_link -> usdt_man ; int i ; bpf_link__destroy ( usdt_link -> multi_link ) ; /* When having multi_link, uprobe_cnt is 0 */ for ( i = 0 ; i < usdt_link -> uprobe_cnt ; i ++ ) { /* detach underlying uprobe link */ bpf_link__destroy ( usdt_link -> uprobes [ i ] . link ) ; /* there is no need to update specs map because it will be\n\t\t * unconditionally overwritten on subsequent USDT attaches,\n\t\t * but if BPF cookies are not used we need to remove entry\n\t\t * from ip_to_spec_id map, otherwise we'll run into false\n\t\t * conflicting IP errors\n\t\t */ if ( ! man -> has_bpf_cookie ) { /* not much we can do about errors here */ ( void ) bpf_map_delete_elem ( bpf_map__fd ( man -> ip_to_spec_id_map ) , & usdt_link -> uprobes [ i ] . abs_ip ) ; } } /* try to return the list of previously used spec IDs to usdt_manager\n\t * for future reuse for subsequent USDT attaches\n\t */ if ( ! man -> free_spec_ids ) { /* if there were no free spec IDs yet, just transfer our IDs */ man -> free_spec_ids = usdt_link -> spec_ids ; man -> free_spec_cnt = usdt_link -> spec_cnt ; usdt_link -> spec_ids = NULL ; } else { /* otherwise concat IDs */ size_t new_cnt = man -> free_spec_cnt + usdt_link -> spec_cnt ; int * new_free_ids ; new_free_ids = libbpf_reallocarray ( man -> free_spec_ids , new_cnt , sizeof ( * new_free_ids ) ) ; /* If we couldn't resize free_spec_ids, we'll just leak\n\t\t * a bunch of free IDs; this is very unlikely to happen and if\n\t\t * system is so exhausted on memory, it's the least of user's\n\t\t * concerns, probably.\n\t\t * So just do our best here to return those IDs to usdt_manager.\n\t\t * Another edge case when we can legitimately get NULL is when\n\t\t * new_cnt is zero, which can happen in some edge cases, so we\n\t\t * need to be careful about that.\n\t\t */ if ( new_free_ids || new_cnt == 0 ) { memcpy ( new_free_ids + man -> free_spec_cnt , usdt_link -> spec_ids , usdt_link -> spec_cnt * sizeof ( * usdt_link -> spec_ids ) ) ; man -> free_spec_ids = new_free_ids ; man -> free_spec_cnt = new_cnt ; } } return 0 ; }",
    "resources/libbpf/src/usdt.c@bpf_link_usdt_dealloc": "static void bpf_link_usdt_dealloc ( struct bpf_link * link ) { struct bpf_link_usdt * usdt_link = container_of ( link , struct bpf_link_usdt , link ) ; free ( usdt_link -> spec_ids ) ; free ( usdt_link -> uprobes ) ; free ( usdt_link ) ; }",
    "resources/libbpf/src/usdt.c@specs_hash_fn": "static size_t specs_hash_fn ( long key , void * ctx ) { return str_hash ( ( char * ) key ) ; }",
    "resources/libbpf/src/usdt.c@specs_equal_fn": "static bool specs_equal_fn ( long key1 , long key2 , void * ctx ) { return strcmp ( ( char * ) key1 , ( char * ) key2 ) == 0 ; }",
    "resources/libbpf/src/usdt.c@allocate_spec_id": "static int allocate_spec_id ( struct usdt_manager * man , struct hashmap * specs_hash , struct bpf_link_usdt * link , struct usdt_target * target , int * spec_id , bool * is_new ) { long tmp ; void * new_ids ; int err ; /* check if we already allocated spec ID for this spec string */ if ( hashmap__find ( specs_hash , target -> spec_str , & tmp ) ) { * spec_id = tmp ; * is_new = false ; return 0 ; } /* otherwise it's a new ID that needs to be set up in specs map and\n\t * returned back to usdt_manager when USDT link is detached\n\t */ new_ids = libbpf_reallocarray ( link -> spec_ids , link -> spec_cnt + 1 , sizeof ( * link -> spec_ids ) ) ; if ( ! new_ids ) return - ENOMEM ; link -> spec_ids = new_ids ; /* get next free spec ID, giving preference to free list, if not empty */ if ( man -> free_spec_cnt ) { * spec_id = man -> free_spec_ids [ man -> free_spec_cnt - 1 ] ; /* cache spec ID for current spec string for future lookups */ err = hashmap__add ( specs_hash , target -> spec_str , * spec_id ) ; if ( err ) return err ; man -> free_spec_cnt -- ; } else { /* don't allocate spec ID bigger than what fits in specs map */ if ( man -> next_free_spec_id >= bpf_map__max_entries ( man -> specs_map ) ) return - E2BIG ; * spec_id = man -> next_free_spec_id ; /* cache spec ID for current spec string for future lookups */ err = hashmap__add ( specs_hash , target -> spec_str , * spec_id ) ; if ( err ) return err ; man -> next_free_spec_id ++ ; } /* remember new spec ID in the link for later return back to free list on detach */ link -> spec_ids [ link -> spec_cnt ] = * spec_id ; link -> spec_cnt ++ ; * is_new = true ; return 0 ; }",
    "resources/libbpf/src/usdt.c@usdt_manager_attach_usdt": "struct bpf_link * usdt_manager_attach_usdt ( struct usdt_manager * man , const struct bpf_program * prog , pid_t pid , const char * path , const char * usdt_provider , const char * usdt_name , __u64 usdt_cookie ) { unsigned long * offsets = NULL , * ref_ctr_offsets = NULL ; int i , err , spec_map_fd , ip_map_fd ; LIBBPF_OPTS ( bpf_uprobe_opts , opts ) ; struct hashmap * specs_hash = NULL ; struct bpf_link_usdt * link = NULL ; struct usdt_target * targets = NULL ; __u64 * cookies = NULL ; struct elf_fd elf_fd ; size_t target_cnt ; spec_map_fd = bpf_map__fd ( man -> specs_map ) ; ip_map_fd = bpf_map__fd ( man -> ip_to_spec_id_map ) ; err = elf_open ( path , & elf_fd ) ; if ( err ) return libbpf_err_ptr ( err ) ; err = sanity_check_usdt_elf ( elf_fd . elf , path ) ; if ( err ) goto err_out ; /* normalize PID filter */ if ( pid < 0 ) pid = - 1 ; else if ( pid == 0 ) pid = getpid ( ) ; /* discover USDT in given binary, optionally limiting\n\t * activations to a given PID, if pid > 0\n\t */ err = collect_usdt_targets ( man , elf_fd . elf , path , pid , usdt_provider , usdt_name , usdt_cookie , & targets , & target_cnt ) ; if ( err <= 0 ) { err = ( err == 0 ) ? - ENOENT : err ; goto err_out ; } specs_hash = hashmap__new ( specs_hash_fn , specs_equal_fn , NULL ) ; if ( IS_ERR ( specs_hash ) ) { err = PTR_ERR ( specs_hash ) ; goto err_out ; } link = calloc ( 1 , sizeof ( * link ) ) ; if ( ! link ) { err = - ENOMEM ; goto err_out ; } link -> usdt_man = man ; link -> link . detach = & bpf_link_usdt_detach ; link -> link . dealloc = & bpf_link_usdt_dealloc ; if ( man -> has_uprobe_multi ) { offsets = calloc ( target_cnt , sizeof ( * offsets ) ) ; cookies = calloc ( target_cnt , sizeof ( * cookies ) ) ; ref_ctr_offsets = calloc ( target_cnt , sizeof ( * ref_ctr_offsets ) ) ; if ( ! offsets || ! ref_ctr_offsets || ! cookies ) { err = - ENOMEM ; goto err_out ; } } else { link -> uprobes = calloc ( target_cnt , sizeof ( * link -> uprobes ) ) ; if ( ! link -> uprobes ) { err = - ENOMEM ; goto err_out ; } } for ( i = 0 ; i < target_cnt ; i ++ ) { struct usdt_target * target = & targets [ i ] ; struct bpf_link * uprobe_link ; bool is_new ; int spec_id ; /* Spec ID can be either reused or newly allocated. If it is\n\t\t * newly allocated, we'll need to fill out spec map, otherwise\n\t\t * entire spec should be valid and can be just used by a new\n\t\t * uprobe. We reuse spec when USDT arg spec is identical. We\n\t\t * also never share specs between two different USDT\n\t\t * attachments (\"links\"), so all the reused specs already\n\t\t * share USDT cookie value implicitly.\n\t\t */ err = allocate_spec_id ( man , specs_hash , link , target , & spec_id , & is_new ) ; if ( err ) goto err_out ; if ( is_new && bpf_map_update_elem ( spec_map_fd , & spec_id , & target -> spec , BPF_ANY ) ) { err = - errno ; pr_warn ( \"usdt: failed to set USDT spec #%d for '%s:%s' in '%s': %d\\n\" , spec_id , usdt_provider , usdt_name , path , err ) ; goto err_out ; } if ( ! man -> has_bpf_cookie && bpf_map_update_elem ( ip_map_fd , & target -> abs_ip , & spec_id , BPF_NOEXIST ) ) { err = - errno ; if ( err == - EEXIST ) { pr_warn ( \"usdt: IP collision detected for spec #%d for '%s:%s' in '%s'\\n\" , spec_id , usdt_provider , usdt_name , path ) ; } else { pr_warn ( \"usdt: failed to map IP 0x%lx to spec #%d for '%s:%s' in '%s': %d\\n\" , target -> abs_ip , spec_id , usdt_provider , usdt_name , path , err ) ; } goto err_out ; } if ( man -> has_uprobe_multi ) { offsets [ i ] = target -> rel_ip ; ref_ctr_offsets [ i ] = target -> sema_off ; cookies [ i ] = spec_id ; } else { opts . ref_ctr_offset = target -> sema_off ; opts . bpf_cookie = man -> has_bpf_cookie ? spec_id : 0 ; uprobe_link = bpf_program__attach_uprobe_opts ( prog , pid , path , target -> rel_ip , & opts ) ; err = libbpf_get_error ( uprobe_link ) ; if ( err ) { pr_warn ( \"usdt: failed to attach uprobe #%d for '%s:%s' in '%s': %d\\n\" , i , usdt_provider , usdt_name , path , err ) ; goto err_out ; } link -> uprobes [ i ] . link = uprobe_link ; link -> uprobes [ i ] . abs_ip = target -> abs_ip ; link -> uprobe_cnt ++ ; } } if ( man -> has_uprobe_multi ) { LIBBPF_OPTS ( bpf_uprobe_multi_opts , opts_multi , . ref_ctr_offsets = ref_ctr_offsets , . offsets = offsets , . cookies = cookies , . cnt = target_cnt , ) ; link -> multi_link = bpf_program__attach_uprobe_multi ( prog , pid , path , NULL , & opts_multi ) ; if ( ! link -> multi_link ) { err = - errno ; pr_warn ( \"usdt: failed to attach uprobe multi for '%s:%s' in '%s': %d\\n\" , usdt_provider , usdt_name , path , err ) ; goto err_out ; } free ( offsets ) ; free ( ref_ctr_offsets ) ; free ( cookies ) ; } free ( targets ) ; hashmap__free ( specs_hash ) ; elf_close ( & elf_fd ) ; return & link -> link ; err_out : free ( offsets ) ; free ( ref_ctr_offsets ) ; free ( cookies ) ; if ( link ) bpf_link__destroy ( & link -> link ) ; free ( targets ) ; hashmap__free ( specs_hash ) ; elf_close ( & elf_fd ) ; return libbpf_err_ptr ( err ) ; }",
    "resources/libbpf/src/usdt.c@parse_usdt_note": "static int parse_usdt_note ( Elf * elf , const char * path , GElf_Nhdr * nhdr , const char * data , size_t name_off , size_t desc_off , struct usdt_note * note ) { const char * provider , * name , * args ; long addrs [ 3 ] ; size_t len ; /* sanity check USDT note name and type first */ if ( strncmp ( data + name_off , USDT_NOTE_NAME , nhdr -> n_namesz ) != 0 ) return - EINVAL ; if ( nhdr -> n_type != USDT_NOTE_TYPE ) return - EINVAL ; /* sanity check USDT note contents (\"description\" in ELF terminology) */ len = nhdr -> n_descsz ; data = data + desc_off ; /* +3 is the very minimum required to store three empty strings */ if ( len < sizeof ( addrs ) + 3 ) return - EINVAL ; /* get location, base, and semaphore addrs */ memcpy ( & addrs , data , sizeof ( addrs ) ) ; /* parse string fields: provider, name, args */ provider = data + sizeof ( addrs ) ; name = ( const char * ) memchr ( provider , '\\0' , data + len - provider ) ; if ( ! name ) /* non-zero-terminated provider */ return - EINVAL ; name ++ ; if ( name >= data + len || * name == '\\0' ) /* missing or empty name */ return - EINVAL ; args = memchr ( name , '\\0' , data + len - name ) ; if ( ! args ) /* non-zero-terminated name */ return - EINVAL ; ++ args ; if ( args >= data + len ) /* missing arguments spec */ return - EINVAL ; note -> provider = provider ; note -> name = name ; if ( * args == '\\0' || * args == ':' ) note -> args = \"\" ; else note -> args = args ; note -> loc_addr = addrs [ 0 ] ; note -> base_addr = addrs [ 1 ] ; note -> sema_addr = addrs [ 2 ] ; return 0 ; }",
    "resources/libbpf/src/usdt.c@parse_usdt_spec": "static int parse_usdt_spec ( struct usdt_spec * spec , const struct usdt_note * note , __u64 usdt_cookie ) { struct usdt_arg_spec * arg ; const char * s ; int arg_sz , len ; spec -> usdt_cookie = usdt_cookie ; spec -> arg_cnt = 0 ; s = note -> args ; while ( s [ 0 ] ) { if ( spec -> arg_cnt >= USDT_MAX_ARG_CNT ) { pr_warn ( \"usdt: too many USDT arguments (> %d) for '%s:%s' with args spec '%s'\\n\" , USDT_MAX_ARG_CNT , note -> provider , note -> name , note -> args ) ; return - E2BIG ; } arg = & spec -> args [ spec -> arg_cnt ] ; len = parse_usdt_arg ( s , spec -> arg_cnt , arg , & arg_sz ) ; if ( len < 0 ) return len ; arg -> arg_signed = arg_sz < 0 ; if ( arg_sz < 0 ) arg_sz = - arg_sz ; switch ( arg_sz ) { case 1 : case 2 : case 4 : case 8 : arg -> arg_bitshift = 64 - arg_sz * 8 ; break ; default : pr_warn ( \"usdt: unsupported arg #%d (spec '%s') size: %d\\n\" , spec -> arg_cnt , s , arg_sz ) ; return - EINVAL ; } s += len ; spec -> arg_cnt ++ ; } return 0 ; }",
    "resources/libbpf/src/usdt.c@calc_pt_regs_off": "static int calc_pt_regs_off ( const char * reg_name ) { int reg_num ; if ( sscanf ( reg_name , \"x%d\" , & reg_num ) == 1 ) { if ( reg_num >= 0 && reg_num < 31 ) return offsetof ( struct user_pt_regs , regs [ reg_num ] ) ; } else if ( strcmp ( reg_name , \"sp\" ) == 0 ) { return offsetof ( struct user_pt_regs , sp ) ; } pr_warn ( \"usdt: unrecognized register '%s'\\n\" , reg_name ) ; return - ENOENT ; }",
    "resources/libbpf/src/usdt.c@parse_usdt_arg": "static int parse_usdt_arg ( const char * arg_str , int arg_num , struct usdt_arg_spec * arg , int * arg_sz ) { char reg_name [ 16 ] ; int len , reg_off ; long off ; if ( sscanf ( arg_str , \" %d @ \\[ %15[a-z0-9] , %ld ] %n\" , arg_sz , reg_name , & off , & len ) == 3 ) { /* Memory dereference case, e.g., -4@[sp, 96] */ arg -> arg_type = USDT_ARG_REG_DEREF ; arg -> val_off = off ; reg_off = calc_pt_regs_off ( reg_name ) ; if ( reg_off < 0 ) return reg_off ; arg -> reg_off = reg_off ; } else if ( sscanf ( arg_str , \" %d @ \\[ %15[a-z0-9] ] %n\" , arg_sz , reg_name , & len ) == 2 ) { /* Memory dereference case, e.g., -4@[sp] */ arg -> arg_type = USDT_ARG_REG_DEREF ; arg -> val_off = 0 ; reg_off = calc_pt_regs_off ( reg_name ) ; if ( reg_off < 0 ) return reg_off ; arg -> reg_off = reg_off ; } else if ( sscanf ( arg_str , \" %d @ %ld %n\" , arg_sz , & off , & len ) == 2 ) { /* Constant value case, e.g., 4@5 */ arg -> arg_type = USDT_ARG_CONST ; arg -> val_off = off ; arg -> reg_off = 0 ; } else if ( sscanf ( arg_str , \" %d @ %15[a-z0-9] %n\" , arg_sz , reg_name , & len ) == 2 ) { /* Register read case, e.g., -8@x4 */ arg -> arg_type = USDT_ARG_REG ; arg -> val_off = 0 ; reg_off = calc_pt_regs_off ( reg_name ) ; if ( reg_off < 0 ) return reg_off ; arg -> reg_off = reg_off ; } else { pr_warn ( \"usdt: unrecognized arg #%d spec '%s'\\n\" , arg_num , arg_str ) ; return - EINVAL ; } return len ; }",
    "resources/libbpf/src/features.c@btf_kind": "static inline __u16 btf_kind ( const struct btf_type * t ) { return BTF_INFO_KIND ( t -> info ) ; }",
    "resources/libbpf/src/features.c@btf_vlen": "static inline __u16 btf_vlen ( const struct btf_type * t ) { return BTF_INFO_VLEN ( t -> info ) ; }",
    "resources/libbpf/src/features.c@btf_kflag": "static inline bool btf_kflag ( const struct btf_type * t ) { return BTF_INFO_KFLAG ( t -> info ) ; }",
    "resources/libbpf/src/features.c@btf_is_void": "static inline bool btf_is_void ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_UNKN ; }",
    "resources/libbpf/src/features.c@btf_is_int": "static inline bool btf_is_int ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_INT ; }",
    "resources/libbpf/src/features.c@btf_is_ptr": "static inline bool btf_is_ptr ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_PTR ; }",
    "resources/libbpf/src/features.c@btf_is_array": "static inline bool btf_is_array ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_ARRAY ; }",
    "resources/libbpf/src/features.c@btf_is_struct": "static inline bool btf_is_struct ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_STRUCT ; }",
    "resources/libbpf/src/features.c@btf_is_union": "static inline bool btf_is_union ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_UNION ; }",
    "resources/libbpf/src/features.c@btf_is_composite": "static inline bool btf_is_composite ( const struct btf_type * t ) { __u16 kind = btf_kind ( t ) ; return kind == BTF_KIND_STRUCT || kind == BTF_KIND_UNION ; }",
    "resources/libbpf/src/features.c@btf_is_enum": "static inline bool btf_is_enum ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_ENUM ; }",
    "resources/libbpf/src/features.c@btf_is_enum64": "static inline bool btf_is_enum64 ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_ENUM64 ; }",
    "resources/libbpf/src/features.c@btf_is_fwd": "static inline bool btf_is_fwd ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_FWD ; }",
    "resources/libbpf/src/features.c@btf_is_typedef": "static inline bool btf_is_typedef ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_TYPEDEF ; }",
    "resources/libbpf/src/features.c@btf_is_volatile": "static inline bool btf_is_volatile ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_VOLATILE ; }",
    "resources/libbpf/src/features.c@btf_is_const": "static inline bool btf_is_const ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_CONST ; }",
    "resources/libbpf/src/features.c@btf_is_restrict": "static inline bool btf_is_restrict ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_RESTRICT ; }",
    "resources/libbpf/src/features.c@btf_is_mod": "static inline bool btf_is_mod ( const struct btf_type * t ) { __u16 kind = btf_kind ( t ) ; return kind == BTF_KIND_VOLATILE || kind == BTF_KIND_CONST || kind == BTF_KIND_RESTRICT || kind == BTF_KIND_TYPE_TAG ; }",
    "resources/libbpf/src/features.c@btf_is_func": "static inline bool btf_is_func ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_FUNC ; }",
    "resources/libbpf/src/features.c@btf_is_func_proto": "static inline bool btf_is_func_proto ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_FUNC_PROTO ; }",
    "resources/libbpf/src/features.c@btf_is_var": "static inline bool btf_is_var ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_VAR ; }",
    "resources/libbpf/src/features.c@btf_is_datasec": "static inline bool btf_is_datasec ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_DATASEC ; }",
    "resources/libbpf/src/features.c@btf_is_float": "static inline bool btf_is_float ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_FLOAT ; }",
    "resources/libbpf/src/features.c@btf_is_decl_tag": "static inline bool btf_is_decl_tag ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_DECL_TAG ; }",
    "resources/libbpf/src/features.c@btf_is_type_tag": "static inline bool btf_is_type_tag ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_TYPE_TAG ; }",
    "resources/libbpf/src/features.c@btf_is_any_enum": "static inline bool btf_is_any_enum ( const struct btf_type * t ) { return btf_is_enum ( t ) || btf_is_enum64 ( t ) ; }",
    "resources/libbpf/src/features.c@btf_kind_core_compat": "static inline bool btf_kind_core_compat ( const struct btf_type * t1 , const struct btf_type * t2 ) { return btf_kind ( t1 ) == btf_kind ( t2 ) || ( btf_is_any_enum ( t1 ) && btf_is_any_enum ( t2 ) ) ; }",
    "resources/libbpf/src/features.c@btf_int_encoding": "static inline __u8 btf_int_encoding ( const struct btf_type * t ) { return BTF_INT_ENCODING ( * ( __u32 * ) ( t + 1 ) ) ; }",
    "resources/libbpf/src/features.c@btf_int_offset": "static inline __u8 btf_int_offset ( const struct btf_type * t ) { return BTF_INT_OFFSET ( * ( __u32 * ) ( t + 1 ) ) ; }",
    "resources/libbpf/src/features.c@btf_int_bits": "static inline __u8 btf_int_bits ( const struct btf_type * t ) { return BTF_INT_BITS ( * ( __u32 * ) ( t + 1 ) ) ; }",
    "resources/libbpf/src/features.c@btf_array": "static inline struct btf_array * btf_array ( const struct btf_type * t ) { return ( struct btf_array * ) ( t + 1 ) ; }",
    "resources/libbpf/src/features.c@btf_enum": "static inline struct btf_enum * btf_enum ( const struct btf_type * t ) { return ( struct btf_enum * ) ( t + 1 ) ; }",
    "resources/libbpf/src/features.c@btf_enum64": "static inline struct btf_enum64 * btf_enum64 ( const struct btf_type * t ) { return ( struct btf_enum64 * ) ( t + 1 ) ; }",
    "resources/libbpf/src/features.c@btf_enum64_value": "static inline __u64 btf_enum64_value ( const struct btf_enum64 * e ) { /* struct btf_enum64 is introduced in Linux 6.0, which is very\n\t * bleeding-edge. Here we are avoiding relying on struct btf_enum64\n\t * definition coming from kernel UAPI headers to support wider range\n\t * of system-wide kernel headers.\n\t *\n\t * Given this header can be also included from C++ applications, that\n\t * further restricts C tricks we can use (like using compatible\n\t * anonymous struct). So just treat struct btf_enum64 as\n\t * a three-element array of u32 and access second (lo32) and third\n\t * (hi32) elements directly.\n\t *\n\t * For reference, here is a struct btf_enum64 definition:\n\t *\n\t * const struct btf_enum64 {\n\t *\t__u32\tname_off;\n\t *\t__u32\tval_lo32;\n\t *\t__u32\tval_hi32;\n\t * };\n\t */ const __u32 * e64 = ( const __u32 * ) e ; return ( ( __u64 ) e64 [ 2 ] << 32 ) | e64 [ 1 ] ; }",
    "resources/libbpf/src/features.c@btf_members": "static inline struct btf_member * btf_members ( const struct btf_type * t ) { return ( struct btf_member * ) ( t + 1 ) ; }",
    "resources/libbpf/src/features.c@btf_member_bit_offset": "static inline __u32 btf_member_bit_offset ( const struct btf_type * t , __u32 member_idx ) { const struct btf_member * m = btf_members ( t ) + member_idx ; bool kflag = btf_kflag ( t ) ; return kflag ? BTF_MEMBER_BIT_OFFSET ( m -> offset ) : m -> offset ; }",
    "resources/libbpf/src/features.c@btf_member_bitfield_size": "static inline __u32 btf_member_bitfield_size ( const struct btf_type * t , __u32 member_idx ) { const struct btf_member * m = btf_members ( t ) + member_idx ; bool kflag = btf_kflag ( t ) ; return kflag ? BTF_MEMBER_BITFIELD_SIZE ( m -> offset ) : 0 ; }",
    "resources/libbpf/src/features.c@btf_params": "static inline struct btf_param * btf_params ( const struct btf_type * t ) { return ( struct btf_param * ) ( t + 1 ) ; }",
    "resources/libbpf/src/features.c@btf_var": "static inline struct btf_var * btf_var ( const struct btf_type * t ) { return ( struct btf_var * ) ( t + 1 ) ; }",
    "resources/libbpf/src/features.c@btf_var_secinfos": "static inline struct btf_var_secinfo * btf_var_secinfos ( const struct btf_type * t ) { return ( struct btf_var_secinfo * ) ( t + 1 ) ; }",
    "resources/libbpf/src/features.c@btf_decl_tag": "static inline struct btf_decl_tag * btf_decl_tag ( const struct btf_type * t ) { return ( struct btf_decl_tag * ) ( t + 1 ) ; }",
    "resources/libbpf/src/features.c@str_has_sfx": "static inline bool str_has_sfx ( const char * str , const char * sfx ) { size_t str_len = strlen ( str ) ; size_t sfx_len = strlen ( sfx ) ; if ( sfx_len > str_len ) return false ; return strcmp ( str + str_len - sfx_len , sfx ) == 0 ; }",
    "resources/libbpf/src/features.c@libbpf_reallocarray": "static inline void * libbpf_reallocarray ( void * ptr , size_t nmemb , size_t size ) { size_t total ; # if __has_builtin ( __builtin_mul_overflow ) if ( unlikely ( __builtin_mul_overflow ( nmemb , size , & total ) ) ) return NULL ; # else if ( size == 0 || nmemb > ULONG_MAX / size ) return NULL ; total = nmemb * size ; # endif return realloc ( ptr , total ) ; }",
    "resources/libbpf/src/features.c@libbpf_strlcpy": "static inline void libbpf_strlcpy ( char * dst , const char * src , size_t sz ) { size_t i ; if ( sz == 0 ) return ; sz -- ; for ( i = 0 ; i < sz && src [ i ] ; i ++ ) dst [ i ] = src [ i ] ; dst [ i ] = '\\0' ; }",
    "resources/libbpf/src/features.c@btf_func_linkage": "static inline enum btf_func_linkage btf_func_linkage ( const struct btf_type * t ) { return ( enum btf_func_linkage ) ( int ) btf_vlen ( t ) ; }",
    "resources/libbpf/src/features.c@btf_type_info": "static inline __u32 btf_type_info ( int kind , int vlen , int kflag ) { return ( kflag << 31 ) | ( kind << 24 ) | vlen ; }",
    "resources/libbpf/src/features.c@libbpf_is_mem_zeroed": "static inline bool libbpf_is_mem_zeroed ( const char * p , ssize_t len ) { while ( len > 0 ) { if ( * p ) return false ; p ++ ; len -- ; } return true ; }",
    "resources/libbpf/src/features.c@libbpf_validate_opts": "static inline bool libbpf_validate_opts ( const char * opts , size_t opts_sz , size_t user_sz , const char * type_name ) { if ( user_sz < sizeof ( size_t ) ) { pr_warn ( \"%s size (%zu) is too small\\n\" , type_name , user_sz ) ; return false ; } if ( ! libbpf_is_mem_zeroed ( opts + opts_sz , ( ssize_t ) user_sz - opts_sz ) ) { pr_warn ( \"%s has non-zero extra bytes\\n\" , type_name ) ; return false ; } return true ; }",
    "resources/libbpf/src/features.c@libbpf_err": "static inline int libbpf_err ( int ret ) { if ( ret < 0 ) errno = - ret ; return ret ; }",
    "resources/libbpf/src/features.c@libbpf_err_errno": "static inline int libbpf_err_errno ( int ret ) { /* errno is already assumed to be set on error */ return ret < 0 ? - errno : ret ; }",
    "resources/libbpf/src/features.c@libbpf_err_ptr": "static inline void * libbpf_err_ptr ( int err ) { /* set errno on error, this doesn't break anything */ errno = - err ; return NULL ; }",
    "resources/libbpf/src/features.c@libbpf_ptr": "static inline void * libbpf_ptr ( void * ret ) { /* set errno on error, this doesn't break anything */ if ( IS_ERR ( ret ) ) errno = - PTR_ERR ( ret ) ; return IS_ERR ( ret ) ? NULL : ret ; }",
    "resources/libbpf/src/features.c@str_is_empty": "static inline bool str_is_empty ( const char * s ) { return ! s || ! s [ 0 ] ; }",
    "resources/libbpf/src/features.c@is_ldimm64_insn": "static inline bool is_ldimm64_insn ( struct bpf_insn * insn ) { return insn -> code == ( BPF_LD | BPF_IMM | BPF_DW ) ; }",
    "resources/libbpf/src/features.c@dup_good_fd": "static inline int dup_good_fd ( int fd ) { if ( fd < 0 ) return fd ; return fcntl ( fd , F_DUPFD_CLOEXEC , 3 ) ; }",
    "resources/libbpf/src/features.c@ensure_good_fd": "static inline int ensure_good_fd ( int fd ) { int old_fd = fd , saved_errno ; if ( fd < 0 ) return fd ; if ( fd < 3 ) { fd = dup_good_fd ( fd ) ; saved_errno = errno ; close ( old_fd ) ; errno = saved_errno ; if ( fd < 0 ) { pr_warn ( \"failed to dup FD %d to FD > 2: %d\\n\" , old_fd , - saved_errno ) ; errno = saved_errno ; } } return fd ; }",
    "resources/libbpf/src/features.c@sys_dup2": "static inline int sys_dup2 ( int oldfd , int newfd ) { # ifdef __NR_dup2 return syscall ( __NR_dup2 , oldfd , newfd ) ; # else return syscall ( __NR_dup3 , oldfd , newfd , 0 ) ; # endif }",
    "resources/libbpf/src/features.c@reuse_fd": "static inline int reuse_fd ( int fixed_fd , int tmp_fd ) { int err ; err = sys_dup2 ( tmp_fd , fixed_fd ) ; err = err < 0 ? - errno : 0 ; close ( tmp_fd ) ; /* clean up temporary FD */ return err ; }",
    "resources/libbpf/src/features.c@is_pow_of_2": "static inline bool is_pow_of_2 ( size_t x ) { return x && ( x & ( x - 1 ) ) == 0 ; }",
    "resources/libbpf/src/features.c@ptr_to_u64": "static inline __u64 ptr_to_u64 ( const void * ptr ) { return ( __u64 ) ( unsigned long ) ptr ; }",
    "resources/libbpf/src/features.c@probe_fd": "int probe_fd ( int fd ) { if ( fd >= 0 ) close ( fd ) ; return fd >= 0 ; }",
    "resources/libbpf/src/features.c@probe_kern_prog_name": "static int probe_kern_prog_name ( int token_fd ) { const size_t attr_sz = offsetofend ( union bpf_attr , prog_name ) ; struct bpf_insn insns [ ] = { BPF_MOV64_IMM ( BPF_REG_0 , 0 ) , BPF_EXIT_INSN ( ) , } ; union bpf_attr attr ; int ret ; memset ( & attr , 0 , attr_sz ) ; attr . prog_type = BPF_PROG_TYPE_SOCKET_FILTER ; attr . license = ptr_to_u64 ( \"GPL\" ) ; attr . insns = ptr_to_u64 ( insns ) ; attr . insn_cnt = ( __u32 ) ARRAY_SIZE ( insns ) ; attr . prog_token_fd = token_fd ; if ( token_fd ) attr . prog_flags |= BPF_F_TOKEN_FD ; libbpf_strlcpy ( attr . prog_name , \"libbpf_nametest\" , sizeof ( attr . prog_name ) ) ; /* make sure loading with name works */ ret = sys_bpf_prog_load ( & attr , attr_sz , PROG_LOAD_ATTEMPTS ) ; return probe_fd ( ret ) ; }",
    "resources/libbpf/src/features.c@probe_kern_global_data": "static int probe_kern_global_data ( int token_fd ) { char * cp , errmsg [ STRERR_BUFSIZE ] ; struct bpf_insn insns [ ] = { BPF_LD_MAP_VALUE ( BPF_REG_1 , 0 , 16 ) , BPF_ST_MEM ( BPF_DW , BPF_REG_1 , 0 , 42 ) , BPF_MOV64_IMM ( BPF_REG_0 , 0 ) , BPF_EXIT_INSN ( ) , } ; LIBBPF_OPTS ( bpf_map_create_opts , map_opts , . token_fd = token_fd , . map_flags = token_fd ? BPF_F_TOKEN_FD : 0 , ) ; LIBBPF_OPTS ( bpf_prog_load_opts , prog_opts , . token_fd = token_fd , . prog_flags = token_fd ? BPF_F_TOKEN_FD : 0 , ) ; int ret , map , insn_cnt = ARRAY_SIZE ( insns ) ; map = bpf_map_create ( BPF_MAP_TYPE_ARRAY , \"libbpf_global\" , sizeof ( int ) , 32 , 1 , & map_opts ) ; if ( map < 0 ) { ret = - errno ; cp = libbpf_strerror_r ( ret , errmsg , sizeof ( errmsg ) ) ; pr_warn ( \"Error in %s():%s(%d). Couldn't create simple array map.\\n\" , __func__ , cp , - ret ) ; return ret ; } insns [ 0 ] . imm = map ; ret = bpf_prog_load ( BPF_PROG_TYPE_SOCKET_FILTER , NULL , \"GPL\" , insns , insn_cnt , & prog_opts ) ; close ( map ) ; return probe_fd ( ret ) ; }",
    "resources/libbpf/src/features.c@probe_kern_btf": "static int probe_kern_btf ( int token_fd ) { static const char strs [ ] = \"\\0int\" ; __u32 types [ ] = { /* int */ BTF_TYPE_INT_ENC ( 1 , BTF_INT_SIGNED , 0 , 32 , 4 ) , } ; return probe_fd ( libbpf__load_raw_btf ( ( char * ) types , sizeof ( types ) , strs , sizeof ( strs ) , token_fd ) ) ; }",
    "resources/libbpf/src/features.c@probe_kern_btf_func": "static int probe_kern_btf_func ( int token_fd ) { static const char strs [ ] = \"\\0int\\0x\\0a\" ; /* void x(int a) {} */ __u32 types [ ] = { /* int */ BTF_TYPE_INT_ENC ( 1 , BTF_INT_SIGNED , 0 , 32 , 4 ) , /* [1] */ /* FUNC_PROTO */ /* [2] */ BTF_TYPE_ENC ( 0 , BTF_INFO_ENC ( BTF_KIND_FUNC_PROTO , 0 , 1 ) , 0 ) , BTF_PARAM_ENC ( 7 , 1 ) , /* FUNC x */ /* [3] */ BTF_TYPE_ENC ( 5 , BTF_INFO_ENC ( BTF_KIND_FUNC , 0 , 0 ) , 2 ) , } ; return probe_fd ( libbpf__load_raw_btf ( ( char * ) types , sizeof ( types ) , strs , sizeof ( strs ) , token_fd ) ) ; }",
    "resources/libbpf/src/features.c@probe_kern_btf_func_global": "static int probe_kern_btf_func_global ( int token_fd ) { static const char strs [ ] = \"\\0int\\0x\\0a\" ; /* static void x(int a) {} */ __u32 types [ ] = { /* int */ BTF_TYPE_INT_ENC ( 1 , BTF_INT_SIGNED , 0 , 32 , 4 ) , /* [1] */ /* FUNC_PROTO */ /* [2] */ BTF_TYPE_ENC ( 0 , BTF_INFO_ENC ( BTF_KIND_FUNC_PROTO , 0 , 1 ) , 0 ) , BTF_PARAM_ENC ( 7 , 1 ) , /* FUNC x BTF_FUNC_GLOBAL */ /* [3] */ BTF_TYPE_ENC ( 5 , BTF_INFO_ENC ( BTF_KIND_FUNC , 0 , BTF_FUNC_GLOBAL ) , 2 ) , } ; return probe_fd ( libbpf__load_raw_btf ( ( char * ) types , sizeof ( types ) , strs , sizeof ( strs ) , token_fd ) ) ; }",
    "resources/libbpf/src/features.c@probe_kern_btf_datasec": "static int probe_kern_btf_datasec ( int token_fd ) { static const char strs [ ] = \"\\0x\\0.data\" ; /* static int a; */ __u32 types [ ] = { /* int */ BTF_TYPE_INT_ENC ( 0 , BTF_INT_SIGNED , 0 , 32 , 4 ) , /* [1] */ /* VAR x */ /* [2] */ BTF_TYPE_ENC ( 1 , BTF_INFO_ENC ( BTF_KIND_VAR , 0 , 0 ) , 1 ) , BTF_VAR_STATIC , /* DATASEC val */ /* [3] */ BTF_TYPE_ENC ( 3 , BTF_INFO_ENC ( BTF_KIND_DATASEC , 0 , 1 ) , 4 ) , BTF_VAR_SECINFO_ENC ( 2 , 0 , 4 ) , } ; return probe_fd ( libbpf__load_raw_btf ( ( char * ) types , sizeof ( types ) , strs , sizeof ( strs ) , token_fd ) ) ; }",
    "resources/libbpf/src/features.c@probe_kern_btf_qmark_datasec": "static int probe_kern_btf_qmark_datasec ( int token_fd ) { static const char strs [ ] = \"\\0x\\0?.data\" ; /* static int a; */ __u32 types [ ] = { /* int */ BTF_TYPE_INT_ENC ( 0 , BTF_INT_SIGNED , 0 , 32 , 4 ) , /* [1] */ /* VAR x */ /* [2] */ BTF_TYPE_ENC ( 1 , BTF_INFO_ENC ( BTF_KIND_VAR , 0 , 0 ) , 1 ) , BTF_VAR_STATIC , /* DATASEC ?.data */ /* [3] */ BTF_TYPE_ENC ( 3 , BTF_INFO_ENC ( BTF_KIND_DATASEC , 0 , 1 ) , 4 ) , BTF_VAR_SECINFO_ENC ( 2 , 0 , 4 ) , } ; return probe_fd ( libbpf__load_raw_btf ( ( char * ) types , sizeof ( types ) , strs , sizeof ( strs ) , token_fd ) ) ; }",
    "resources/libbpf/src/features.c@probe_kern_btf_float": "static int probe_kern_btf_float ( int token_fd ) { static const char strs [ ] = \"\\0float\" ; __u32 types [ ] = { /* float */ BTF_TYPE_FLOAT_ENC ( 1 , 4 ) , } ; return probe_fd ( libbpf__load_raw_btf ( ( char * ) types , sizeof ( types ) , strs , sizeof ( strs ) , token_fd ) ) ; }",
    "resources/libbpf/src/features.c@probe_kern_btf_decl_tag": "static int probe_kern_btf_decl_tag ( int token_fd ) { static const char strs [ ] = \"\\0tag\" ; __u32 types [ ] = { /* int */ BTF_TYPE_INT_ENC ( 0 , BTF_INT_SIGNED , 0 , 32 , 4 ) , /* [1] */ /* VAR x */ /* [2] */ BTF_TYPE_ENC ( 1 , BTF_INFO_ENC ( BTF_KIND_VAR , 0 , 0 ) , 1 ) , BTF_VAR_STATIC , /* attr */ BTF_TYPE_DECL_TAG_ENC ( 1 , 2 , - 1 ) , } ; return probe_fd ( libbpf__load_raw_btf ( ( char * ) types , sizeof ( types ) , strs , sizeof ( strs ) , token_fd ) ) ; }",
    "resources/libbpf/src/features.c@probe_kern_btf_type_tag": "static int probe_kern_btf_type_tag ( int token_fd ) { static const char strs [ ] = \"\\0tag\" ; __u32 types [ ] = { /* int */ BTF_TYPE_INT_ENC ( 0 , BTF_INT_SIGNED , 0 , 32 , 4 ) , /* [1] */ /* attr */ BTF_TYPE_TYPE_TAG_ENC ( 1 , 1 ) , /* [2] */ /* ptr */ BTF_TYPE_ENC ( 0 , BTF_INFO_ENC ( BTF_KIND_PTR , 0 , 0 ) , 2 ) , /* [3] */ } ; return probe_fd ( libbpf__load_raw_btf ( ( char * ) types , sizeof ( types ) , strs , sizeof ( strs ) , token_fd ) ) ; }",
    "resources/libbpf/src/features.c@probe_kern_array_mmap": "static int probe_kern_array_mmap ( int token_fd ) { LIBBPF_OPTS ( bpf_map_create_opts , opts , . map_flags = BPF_F_MMAPABLE | ( token_fd ? BPF_F_TOKEN_FD : 0 ) , . token_fd = token_fd , ) ; int fd ; fd = bpf_map_create ( BPF_MAP_TYPE_ARRAY , \"libbpf_mmap\" , sizeof ( int ) , sizeof ( int ) , 1 , & opts ) ; return probe_fd ( fd ) ; }",
    "resources/libbpf/src/features.c@probe_kern_exp_attach_type": "static int probe_kern_exp_attach_type ( int token_fd ) { LIBBPF_OPTS ( bpf_prog_load_opts , opts , . expected_attach_type = BPF_CGROUP_INET_SOCK_CREATE , . token_fd = token_fd , . prog_flags = token_fd ? BPF_F_TOKEN_FD : 0 , ) ; struct bpf_insn insns [ ] = { BPF_MOV64_IMM ( BPF_REG_0 , 0 ) , BPF_EXIT_INSN ( ) , } ; int fd , insn_cnt = ARRAY_SIZE ( insns ) ; /* use any valid combination of program type and (optional)\n\t * non-zero expected attach type (i.e., not a BPF_CGROUP_INET_INGRESS)\n\t * to see if kernel supports expected_attach_type field for\n\t * BPF_PROG_LOAD command\n\t */ fd = bpf_prog_load ( BPF_PROG_TYPE_CGROUP_SOCK , NULL , \"GPL\" , insns , insn_cnt , & opts ) ; return probe_fd ( fd ) ; }",
    "resources/libbpf/src/features.c@probe_kern_probe_read_kernel": "static int probe_kern_probe_read_kernel ( int token_fd ) { LIBBPF_OPTS ( bpf_prog_load_opts , opts , . token_fd = token_fd , . prog_flags = token_fd ? BPF_F_TOKEN_FD : 0 , ) ; struct bpf_insn insns [ ] = { BPF_MOV64_REG ( BPF_REG_1 , BPF_REG_10 ) , /* r1 = r10 (fp) */ BPF_ALU64_IMM ( BPF_ADD , BPF_REG_1 , - 8 ) , /* r1 += -8 */ BPF_MOV64_IMM ( BPF_REG_2 , 8 ) , /* r2 = 8 */ BPF_MOV64_IMM ( BPF_REG_3 , 0 ) , /* r3 = 0 */ BPF_RAW_INSN ( BPF_JMP | BPF_CALL , 0 , 0 , 0 , BPF_FUNC_probe_read_kernel ) , BPF_EXIT_INSN ( ) , } ; int fd , insn_cnt = ARRAY_SIZE ( insns ) ; fd = bpf_prog_load ( BPF_PROG_TYPE_TRACEPOINT , NULL , \"GPL\" , insns , insn_cnt , & opts ) ; return probe_fd ( fd ) ; }",
    "resources/libbpf/src/features.c@probe_prog_bind_map": "static int probe_prog_bind_map ( int token_fd ) { char * cp , errmsg [ STRERR_BUFSIZE ] ; struct bpf_insn insns [ ] = { BPF_MOV64_IMM ( BPF_REG_0 , 0 ) , BPF_EXIT_INSN ( ) , } ; LIBBPF_OPTS ( bpf_map_create_opts , map_opts , . token_fd = token_fd , . map_flags = token_fd ? BPF_F_TOKEN_FD : 0 , ) ; LIBBPF_OPTS ( bpf_prog_load_opts , prog_opts , . token_fd = token_fd , . prog_flags = token_fd ? BPF_F_TOKEN_FD : 0 , ) ; int ret , map , prog , insn_cnt = ARRAY_SIZE ( insns ) ; map = bpf_map_create ( BPF_MAP_TYPE_ARRAY , \"libbpf_det_bind\" , sizeof ( int ) , 32 , 1 , & map_opts ) ; if ( map < 0 ) { ret = - errno ; cp = libbpf_strerror_r ( ret , errmsg , sizeof ( errmsg ) ) ; pr_warn ( \"Error in %s():%s(%d). Couldn't create simple array map.\\n\" , __func__ , cp , - ret ) ; return ret ; } prog = bpf_prog_load ( BPF_PROG_TYPE_SOCKET_FILTER , NULL , \"GPL\" , insns , insn_cnt , & prog_opts ) ; if ( prog < 0 ) { close ( map ) ; return 0 ; } ret = bpf_prog_bind_map ( prog , map , NULL ) ; close ( map ) ; close ( prog ) ; return ret >= 0 ; }",
    "resources/libbpf/src/features.c@probe_module_btf": "static int probe_module_btf ( int token_fd ) { static const char strs [ ] = \"\\0int\" ; __u32 types [ ] = { /* int */ BTF_TYPE_INT_ENC ( 1 , BTF_INT_SIGNED , 0 , 32 , 4 ) , } ; struct bpf_btf_info info ; __u32 len = sizeof ( info ) ; char name [ 16 ] ; int fd , err ; fd = libbpf__load_raw_btf ( ( char * ) types , sizeof ( types ) , strs , sizeof ( strs ) , token_fd ) ; if ( fd < 0 ) return 0 ; /* BTF not supported at all */ memset ( & info , 0 , sizeof ( info ) ) ; info . name = ptr_to_u64 ( name ) ; info . name_len = sizeof ( name ) ; /* check that BPF_OBJ_GET_INFO_BY_FD supports specifying name pointer;\n\t * kernel's module BTF support coincides with support for\n\t * name/name_len fields in struct bpf_btf_info.\n\t */ err = bpf_btf_get_info_by_fd ( fd , & info , & len ) ; close ( fd ) ; return ! err ; }",
    "resources/libbpf/src/features.c@probe_perf_link": "static int probe_perf_link ( int token_fd ) { struct bpf_insn insns [ ] = { BPF_MOV64_IMM ( BPF_REG_0 , 0 ) , BPF_EXIT_INSN ( ) , } ; LIBBPF_OPTS ( bpf_prog_load_opts , opts , . token_fd = token_fd , . prog_flags = token_fd ? BPF_F_TOKEN_FD : 0 , ) ; int prog_fd , link_fd , err ; prog_fd = bpf_prog_load ( BPF_PROG_TYPE_TRACEPOINT , NULL , \"GPL\" , insns , ARRAY_SIZE ( insns ) , & opts ) ; if ( prog_fd < 0 ) return - errno ; /* use invalid perf_event FD to get EBADF, if link is supported;\n\t * otherwise EINVAL should be returned\n\t */ link_fd = bpf_link_create ( prog_fd , - 1 , BPF_PERF_EVENT , NULL ) ; err = - errno ; /* close() can clobber errno */ if ( link_fd >= 0 ) close ( link_fd ) ; close ( prog_fd ) ; return link_fd < 0 && err == - EBADF ; }",
    "resources/libbpf/src/features.c@probe_uprobe_multi_link": "static int probe_uprobe_multi_link ( int token_fd ) { LIBBPF_OPTS ( bpf_prog_load_opts , load_opts , . expected_attach_type = BPF_TRACE_UPROBE_MULTI , . token_fd = token_fd , . prog_flags = token_fd ? BPF_F_TOKEN_FD : 0 , ) ; LIBBPF_OPTS ( bpf_link_create_opts , link_opts ) ; struct bpf_insn insns [ ] = { BPF_MOV64_IMM ( BPF_REG_0 , 0 ) , BPF_EXIT_INSN ( ) , } ; int prog_fd , link_fd , err ; unsigned long offset = 0 ; prog_fd = bpf_prog_load ( BPF_PROG_TYPE_KPROBE , NULL , \"GPL\" , insns , ARRAY_SIZE ( insns ) , & load_opts ) ; if ( prog_fd < 0 ) return - errno ; /* Creating uprobe in '/' binary should fail with -EBADF. */ link_opts . uprobe_multi . path = \"/\" ; link_opts . uprobe_multi . offsets = & offset ; link_opts . uprobe_multi . cnt = 1 ; link_fd = bpf_link_create ( prog_fd , - 1 , BPF_TRACE_UPROBE_MULTI , & link_opts ) ; err = - errno ; /* close() can clobber errno */ if ( link_fd >= 0 ) close ( link_fd ) ; close ( prog_fd ) ; return link_fd < 0 && err == - EBADF ; }",
    "resources/libbpf/src/features.c@probe_kern_bpf_cookie": "static int probe_kern_bpf_cookie ( int token_fd ) { struct bpf_insn insns [ ] = { BPF_RAW_INSN ( BPF_JMP | BPF_CALL , 0 , 0 , 0 , BPF_FUNC_get_attach_cookie ) , BPF_EXIT_INSN ( ) , } ; LIBBPF_OPTS ( bpf_prog_load_opts , opts , . token_fd = token_fd , . prog_flags = token_fd ? BPF_F_TOKEN_FD : 0 , ) ; int ret , insn_cnt = ARRAY_SIZE ( insns ) ; ret = bpf_prog_load ( BPF_PROG_TYPE_TRACEPOINT , NULL , \"GPL\" , insns , insn_cnt , & opts ) ; return probe_fd ( ret ) ; }",
    "resources/libbpf/src/features.c@probe_kern_btf_enum64": "static int probe_kern_btf_enum64 ( int token_fd ) { static const char strs [ ] = \"\\0enum64\" ; __u32 types [ ] = { BTF_TYPE_ENC ( 1 , BTF_INFO_ENC ( BTF_KIND_ENUM64 , 0 , 0 ) , 8 ) , } ; return probe_fd ( libbpf__load_raw_btf ( ( char * ) types , sizeof ( types ) , strs , sizeof ( strs ) , token_fd ) ) ; }",
    "resources/libbpf/src/features.c@probe_kern_arg_ctx_tag": "static int probe_kern_arg_ctx_tag ( int token_fd ) { static const char strs [ ] = \"\\0a\\0b\\0arg:ctx\\0\" ; const __u32 types [ ] = { /* [1] INT */ BTF_TYPE_INT_ENC ( 1 /* \"a\" */ , BTF_INT_SIGNED , 0 , 32 , 4 ) , /* [2] PTR -> VOID */ BTF_TYPE_ENC ( 0 , BTF_INFO_ENC ( BTF_KIND_PTR , 0 , 0 ) , 0 ) , /* [3] FUNC_PROTO `int(void *a)` */ BTF_TYPE_ENC ( 0 , BTF_INFO_ENC ( BTF_KIND_FUNC_PROTO , 0 , 1 ) , 1 ) , BTF_PARAM_ENC ( 1 /* \"a\" */ , 2 ) , /* [4] FUNC 'a' -> FUNC_PROTO (main prog) */ BTF_TYPE_ENC ( 1 /* \"a\" */ , BTF_INFO_ENC ( BTF_KIND_FUNC , 0 , BTF_FUNC_GLOBAL ) , 3 ) , /* [5] FUNC_PROTO `int(void *b __arg_ctx)` */ BTF_TYPE_ENC ( 0 , BTF_INFO_ENC ( BTF_KIND_FUNC_PROTO , 0 , 1 ) , 1 ) , BTF_PARAM_ENC ( 3 /* \"b\" */ , 2 ) , /* [6] FUNC 'b' -> FUNC_PROTO (subprog) */ BTF_TYPE_ENC ( 3 /* \"b\" */ , BTF_INFO_ENC ( BTF_KIND_FUNC , 0 , BTF_FUNC_GLOBAL ) , 5 ) , /* [7] DECL_TAG 'arg:ctx' -> func 'b' arg 'b' */ BTF_TYPE_DECL_TAG_ENC ( 5 /* \"arg:ctx\" */ , 6 , 0 ) , } ; const struct bpf_insn insns [ ] = { /* main prog */ BPF_CALL_REL ( + 1 ) , BPF_EXIT_INSN ( ) , /* global subprog */ BPF_EMIT_CALL ( BPF_FUNC_get_func_ip ) , /* needs PTR_TO_CTX */ BPF_EXIT_INSN ( ) , } ; const struct bpf_func_info_min func_infos [ ] = { { 0 , 4 } , /* main prog -> FUNC 'a' */ { 2 , 6 } , /* subprog -> FUNC 'b' */ } ; LIBBPF_OPTS ( bpf_prog_load_opts , opts , . token_fd = token_fd , . prog_flags = token_fd ? BPF_F_TOKEN_FD : 0 , ) ; int prog_fd , btf_fd , insn_cnt = ARRAY_SIZE ( insns ) ; btf_fd = libbpf__load_raw_btf ( ( char * ) types , sizeof ( types ) , strs , sizeof ( strs ) , token_fd ) ; if ( btf_fd < 0 ) return 0 ; opts . prog_btf_fd = btf_fd ; opts . func_info = & func_infos ; opts . func_info_cnt = ARRAY_SIZE ( func_infos ) ; opts . func_info_rec_size = sizeof ( func_infos [ 0 ] ) ; prog_fd = bpf_prog_load ( BPF_PROG_TYPE_KPROBE , \"det_arg_ctx\" , \"GPL\" , insns , insn_cnt , & opts ) ; close ( btf_fd ) ; return probe_fd ( prog_fd ) ; }",
    "resources/libbpf/src/features.c@feat_supported": "",
    "resources/libbpf/src/bpf_prog_linfo.c@btf_kind": "static inline __u16 btf_kind ( const struct btf_type * t ) { return BTF_INFO_KIND ( t -> info ) ; }",
    "resources/libbpf/src/bpf_prog_linfo.c@btf_vlen": "static inline __u16 btf_vlen ( const struct btf_type * t ) { return BTF_INFO_VLEN ( t -> info ) ; }",
    "resources/libbpf/src/bpf_prog_linfo.c@btf_kflag": "static inline bool btf_kflag ( const struct btf_type * t ) { return BTF_INFO_KFLAG ( t -> info ) ; }",
    "resources/libbpf/src/bpf_prog_linfo.c@btf_is_void": "static inline bool btf_is_void ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_UNKN ; }",
    "resources/libbpf/src/bpf_prog_linfo.c@btf_is_int": "static inline bool btf_is_int ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_INT ; }",
    "resources/libbpf/src/bpf_prog_linfo.c@btf_is_ptr": "static inline bool btf_is_ptr ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_PTR ; }",
    "resources/libbpf/src/bpf_prog_linfo.c@btf_is_array": "static inline bool btf_is_array ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_ARRAY ; }",
    "resources/libbpf/src/bpf_prog_linfo.c@btf_is_struct": "static inline bool btf_is_struct ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_STRUCT ; }",
    "resources/libbpf/src/bpf_prog_linfo.c@btf_is_union": "static inline bool btf_is_union ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_UNION ; }",
    "resources/libbpf/src/bpf_prog_linfo.c@btf_is_composite": "static inline bool btf_is_composite ( const struct btf_type * t ) { __u16 kind = btf_kind ( t ) ; return kind == BTF_KIND_STRUCT || kind == BTF_KIND_UNION ; }",
    "resources/libbpf/src/bpf_prog_linfo.c@btf_is_enum": "static inline bool btf_is_enum ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_ENUM ; }",
    "resources/libbpf/src/bpf_prog_linfo.c@btf_is_enum64": "static inline bool btf_is_enum64 ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_ENUM64 ; }",
    "resources/libbpf/src/bpf_prog_linfo.c@btf_is_fwd": "static inline bool btf_is_fwd ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_FWD ; }",
    "resources/libbpf/src/bpf_prog_linfo.c@btf_is_typedef": "static inline bool btf_is_typedef ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_TYPEDEF ; }",
    "resources/libbpf/src/bpf_prog_linfo.c@btf_is_volatile": "static inline bool btf_is_volatile ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_VOLATILE ; }",
    "resources/libbpf/src/bpf_prog_linfo.c@btf_is_const": "static inline bool btf_is_const ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_CONST ; }",
    "resources/libbpf/src/bpf_prog_linfo.c@btf_is_restrict": "static inline bool btf_is_restrict ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_RESTRICT ; }",
    "resources/libbpf/src/bpf_prog_linfo.c@btf_is_mod": "static inline bool btf_is_mod ( const struct btf_type * t ) { __u16 kind = btf_kind ( t ) ; return kind == BTF_KIND_VOLATILE || kind == BTF_KIND_CONST || kind == BTF_KIND_RESTRICT || kind == BTF_KIND_TYPE_TAG ; }",
    "resources/libbpf/src/bpf_prog_linfo.c@btf_is_func": "static inline bool btf_is_func ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_FUNC ; }",
    "resources/libbpf/src/bpf_prog_linfo.c@btf_is_func_proto": "static inline bool btf_is_func_proto ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_FUNC_PROTO ; }",
    "resources/libbpf/src/bpf_prog_linfo.c@btf_is_var": "static inline bool btf_is_var ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_VAR ; }",
    "resources/libbpf/src/bpf_prog_linfo.c@btf_is_datasec": "static inline bool btf_is_datasec ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_DATASEC ; }",
    "resources/libbpf/src/bpf_prog_linfo.c@btf_is_float": "static inline bool btf_is_float ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_FLOAT ; }",
    "resources/libbpf/src/bpf_prog_linfo.c@btf_is_decl_tag": "static inline bool btf_is_decl_tag ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_DECL_TAG ; }",
    "resources/libbpf/src/bpf_prog_linfo.c@btf_is_type_tag": "static inline bool btf_is_type_tag ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_TYPE_TAG ; }",
    "resources/libbpf/src/bpf_prog_linfo.c@btf_is_any_enum": "static inline bool btf_is_any_enum ( const struct btf_type * t ) { return btf_is_enum ( t ) || btf_is_enum64 ( t ) ; }",
    "resources/libbpf/src/bpf_prog_linfo.c@btf_kind_core_compat": "static inline bool btf_kind_core_compat ( const struct btf_type * t1 , const struct btf_type * t2 ) { return btf_kind ( t1 ) == btf_kind ( t2 ) || ( btf_is_any_enum ( t1 ) && btf_is_any_enum ( t2 ) ) ; }",
    "resources/libbpf/src/bpf_prog_linfo.c@btf_int_encoding": "static inline __u8 btf_int_encoding ( const struct btf_type * t ) { return BTF_INT_ENCODING ( * ( __u32 * ) ( t + 1 ) ) ; }",
    "resources/libbpf/src/bpf_prog_linfo.c@btf_int_offset": "static inline __u8 btf_int_offset ( const struct btf_type * t ) { return BTF_INT_OFFSET ( * ( __u32 * ) ( t + 1 ) ) ; }",
    "resources/libbpf/src/bpf_prog_linfo.c@btf_int_bits": "static inline __u8 btf_int_bits ( const struct btf_type * t ) { return BTF_INT_BITS ( * ( __u32 * ) ( t + 1 ) ) ; }",
    "resources/libbpf/src/bpf_prog_linfo.c@btf_array": "static inline struct btf_array * btf_array ( const struct btf_type * t ) { return ( struct btf_array * ) ( t + 1 ) ; }",
    "resources/libbpf/src/bpf_prog_linfo.c@btf_enum": "static inline struct btf_enum * btf_enum ( const struct btf_type * t ) { return ( struct btf_enum * ) ( t + 1 ) ; }",
    "resources/libbpf/src/bpf_prog_linfo.c@btf_enum64": "static inline struct btf_enum64 * btf_enum64 ( const struct btf_type * t ) { return ( struct btf_enum64 * ) ( t + 1 ) ; }",
    "resources/libbpf/src/bpf_prog_linfo.c@btf_enum64_value": "static inline __u64 btf_enum64_value ( const struct btf_enum64 * e ) { /* struct btf_enum64 is introduced in Linux 6.0, which is very\n\t * bleeding-edge. Here we are avoiding relying on struct btf_enum64\n\t * definition coming from kernel UAPI headers to support wider range\n\t * of system-wide kernel headers.\n\t *\n\t * Given this header can be also included from C++ applications, that\n\t * further restricts C tricks we can use (like using compatible\n\t * anonymous struct). So just treat struct btf_enum64 as\n\t * a three-element array of u32 and access second (lo32) and third\n\t * (hi32) elements directly.\n\t *\n\t * For reference, here is a struct btf_enum64 definition:\n\t *\n\t * const struct btf_enum64 {\n\t *\t__u32\tname_off;\n\t *\t__u32\tval_lo32;\n\t *\t__u32\tval_hi32;\n\t * };\n\t */ const __u32 * e64 = ( const __u32 * ) e ; return ( ( __u64 ) e64 [ 2 ] << 32 ) | e64 [ 1 ] ; }",
    "resources/libbpf/src/bpf_prog_linfo.c@btf_members": "static inline struct btf_member * btf_members ( const struct btf_type * t ) { return ( struct btf_member * ) ( t + 1 ) ; }",
    "resources/libbpf/src/bpf_prog_linfo.c@btf_member_bit_offset": "static inline __u32 btf_member_bit_offset ( const struct btf_type * t , __u32 member_idx ) { const struct btf_member * m = btf_members ( t ) + member_idx ; bool kflag = btf_kflag ( t ) ; return kflag ? BTF_MEMBER_BIT_OFFSET ( m -> offset ) : m -> offset ; }",
    "resources/libbpf/src/bpf_prog_linfo.c@btf_member_bitfield_size": "static inline __u32 btf_member_bitfield_size ( const struct btf_type * t , __u32 member_idx ) { const struct btf_member * m = btf_members ( t ) + member_idx ; bool kflag = btf_kflag ( t ) ; return kflag ? BTF_MEMBER_BITFIELD_SIZE ( m -> offset ) : 0 ; }",
    "resources/libbpf/src/bpf_prog_linfo.c@btf_params": "static inline struct btf_param * btf_params ( const struct btf_type * t ) { return ( struct btf_param * ) ( t + 1 ) ; }",
    "resources/libbpf/src/bpf_prog_linfo.c@btf_var": "static inline struct btf_var * btf_var ( const struct btf_type * t ) { return ( struct btf_var * ) ( t + 1 ) ; }",
    "resources/libbpf/src/bpf_prog_linfo.c@btf_var_secinfos": "static inline struct btf_var_secinfo * btf_var_secinfos ( const struct btf_type * t ) { return ( struct btf_var_secinfo * ) ( t + 1 ) ; }",
    "resources/libbpf/src/bpf_prog_linfo.c@btf_decl_tag": "static inline struct btf_decl_tag * btf_decl_tag ( const struct btf_type * t ) { return ( struct btf_decl_tag * ) ( t + 1 ) ; }",
    "resources/libbpf/src/bpf_prog_linfo.c@str_has_sfx": "static inline bool str_has_sfx ( const char * str , const char * sfx ) { size_t str_len = strlen ( str ) ; size_t sfx_len = strlen ( sfx ) ; if ( sfx_len > str_len ) return false ; return strcmp ( str + str_len - sfx_len , sfx ) == 0 ; }",
    "resources/libbpf/src/bpf_prog_linfo.c@libbpf_reallocarray": "static inline void * libbpf_reallocarray ( void * ptr , size_t nmemb , size_t size ) { size_t total ; # if __has_builtin ( __builtin_mul_overflow ) if ( unlikely ( __builtin_mul_overflow ( nmemb , size , & total ) ) ) return NULL ; # else if ( size == 0 || nmemb > ULONG_MAX / size ) return NULL ; total = nmemb * size ; # endif return realloc ( ptr , total ) ; }",
    "resources/libbpf/src/bpf_prog_linfo.c@libbpf_strlcpy": "static inline void libbpf_strlcpy ( char * dst , const char * src , size_t sz ) { size_t i ; if ( sz == 0 ) return ; sz -- ; for ( i = 0 ; i < sz && src [ i ] ; i ++ ) dst [ i ] = src [ i ] ; dst [ i ] = '\\0' ; }",
    "resources/libbpf/src/bpf_prog_linfo.c@btf_func_linkage": "static inline enum btf_func_linkage btf_func_linkage ( const struct btf_type * t ) { return ( enum btf_func_linkage ) ( int ) btf_vlen ( t ) ; }",
    "resources/libbpf/src/bpf_prog_linfo.c@btf_type_info": "static inline __u32 btf_type_info ( int kind , int vlen , int kflag ) { return ( kflag << 31 ) | ( kind << 24 ) | vlen ; }",
    "resources/libbpf/src/bpf_prog_linfo.c@libbpf_is_mem_zeroed": "static inline bool libbpf_is_mem_zeroed ( const char * p , ssize_t len ) { while ( len > 0 ) { if ( * p ) return false ; p ++ ; len -- ; } return true ; }",
    "resources/libbpf/src/bpf_prog_linfo.c@libbpf_validate_opts": "static inline bool libbpf_validate_opts ( const char * opts , size_t opts_sz , size_t user_sz , const char * type_name ) { if ( user_sz < sizeof ( size_t ) ) { pr_warn ( \"%s size (%zu) is too small\\n\" , type_name , user_sz ) ; return false ; } if ( ! libbpf_is_mem_zeroed ( opts + opts_sz , ( ssize_t ) user_sz - opts_sz ) ) { pr_warn ( \"%s has non-zero extra bytes\\n\" , type_name ) ; return false ; } return true ; }",
    "resources/libbpf/src/bpf_prog_linfo.c@libbpf_err": "static inline int libbpf_err ( int ret ) { if ( ret < 0 ) errno = - ret ; return ret ; }",
    "resources/libbpf/src/bpf_prog_linfo.c@libbpf_err_errno": "static inline int libbpf_err_errno ( int ret ) { /* errno is already assumed to be set on error */ return ret < 0 ? - errno : ret ; }",
    "resources/libbpf/src/bpf_prog_linfo.c@libbpf_err_ptr": "static inline void * libbpf_err_ptr ( int err ) { /* set errno on error, this doesn't break anything */ errno = - err ; return NULL ; }",
    "resources/libbpf/src/bpf_prog_linfo.c@libbpf_ptr": "static inline void * libbpf_ptr ( void * ret ) { /* set errno on error, this doesn't break anything */ if ( IS_ERR ( ret ) ) errno = - PTR_ERR ( ret ) ; return IS_ERR ( ret ) ? NULL : ret ; }",
    "resources/libbpf/src/bpf_prog_linfo.c@str_is_empty": "static inline bool str_is_empty ( const char * s ) { return ! s || ! s [ 0 ] ; }",
    "resources/libbpf/src/bpf_prog_linfo.c@is_ldimm64_insn": "static inline bool is_ldimm64_insn ( struct bpf_insn * insn ) { return insn -> code == ( BPF_LD | BPF_IMM | BPF_DW ) ; }",
    "resources/libbpf/src/bpf_prog_linfo.c@dup_good_fd": "static inline int dup_good_fd ( int fd ) { if ( fd < 0 ) return fd ; return fcntl ( fd , F_DUPFD_CLOEXEC , 3 ) ; }",
    "resources/libbpf/src/bpf_prog_linfo.c@ensure_good_fd": "static inline int ensure_good_fd ( int fd ) { int old_fd = fd , saved_errno ; if ( fd < 0 ) return fd ; if ( fd < 3 ) { fd = dup_good_fd ( fd ) ; saved_errno = errno ; close ( old_fd ) ; errno = saved_errno ; if ( fd < 0 ) { pr_warn ( \"failed to dup FD %d to FD > 2: %d\\n\" , old_fd , - saved_errno ) ; errno = saved_errno ; } } return fd ; }",
    "resources/libbpf/src/bpf_prog_linfo.c@sys_dup2": "static inline int sys_dup2 ( int oldfd , int newfd ) { # ifdef __NR_dup2 return syscall ( __NR_dup2 , oldfd , newfd ) ; # else return syscall ( __NR_dup3 , oldfd , newfd , 0 ) ; # endif }",
    "resources/libbpf/src/bpf_prog_linfo.c@reuse_fd": "static inline int reuse_fd ( int fixed_fd , int tmp_fd ) { int err ; err = sys_dup2 ( tmp_fd , fixed_fd ) ; err = err < 0 ? - errno : 0 ; close ( tmp_fd ) ; /* clean up temporary FD */ return err ; }",
    "resources/libbpf/src/bpf_prog_linfo.c@is_pow_of_2": "static inline bool is_pow_of_2 ( size_t x ) { return x && ( x & ( x - 1 ) ) == 0 ; }",
    "resources/libbpf/src/bpf_prog_linfo.c@dissect_jited_func": "static int dissect_jited_func ( struct bpf_prog_linfo * prog_linfo , const __u64 * ksym_func , const __u32 * ksym_len ) { __u32 nr_jited_func , nr_linfo ; const void * raw_jited_linfo ; const __u64 * jited_linfo ; __u64 last_jited_linfo ; /*\n\t * Index to raw_jited_linfo:\n\t *      i: Index for searching the next ksym_func\n\t * prev_i: Index to the last found ksym_func\n\t */ __u32 i , prev_i ; __u32 f ; /* Index to ksym_func */ raw_jited_linfo = prog_linfo -> raw_jited_linfo ; jited_linfo = raw_jited_linfo ; if ( ksym_func [ 0 ] != * jited_linfo ) goto errout ; prog_linfo -> jited_linfo_func_idx [ 0 ] = 0 ; nr_jited_func = prog_linfo -> nr_jited_func ; nr_linfo = prog_linfo -> nr_linfo ; for ( prev_i = 0 , i = 1 , f = 1 ; i < nr_linfo && f < nr_jited_func ; i ++ ) { raw_jited_linfo += prog_linfo -> jited_rec_size ; last_jited_linfo = * jited_linfo ; jited_linfo = raw_jited_linfo ; if ( ksym_func [ f ] == * jited_linfo ) { prog_linfo -> jited_linfo_func_idx [ f ] = i ; /* Sanity check */ if ( last_jited_linfo - ksym_func [ f - 1 ] + 1 > ksym_len [ f - 1 ] ) goto errout ; prog_linfo -> nr_jited_linfo_per_func [ f - 1 ] = i - prev_i ; prev_i = i ; /*\n\t\t\t * The ksym_func[f] is found in jited_linfo.\n\t\t\t * Look for the next one.\n\t\t\t */ f ++ ; } else if ( * jited_linfo <= last_jited_linfo ) { /* Ensure the addr is increasing _within_ a func */ goto errout ; } } if ( f != nr_jited_func ) goto errout ; prog_linfo -> nr_jited_linfo_per_func [ nr_jited_func - 1 ] = nr_linfo - prev_i ; return 0 ; errout : return - EINVAL ; }",
    "resources/libbpf/src/bpf_prog_linfo.c@bpf_prog_linfo__free": "void bpf_prog_linfo__free ( struct bpf_prog_linfo * prog_linfo ) { if ( ! prog_linfo ) return ; free ( prog_linfo -> raw_linfo ) ; free ( prog_linfo -> raw_jited_linfo ) ; free ( prog_linfo -> nr_jited_linfo_per_func ) ; free ( prog_linfo -> jited_linfo_func_idx ) ; free ( prog_linfo ) ; }",
    "resources/libbpf/src/bpf_prog_linfo.c@bpf_prog_linfo__new": "struct bpf_prog_linfo * bpf_prog_linfo__new ( const struct bpf_prog_info * info ) { struct bpf_prog_linfo * prog_linfo ; __u32 nr_linfo , nr_jited_func ; __u64 data_sz ; nr_linfo = info -> nr_line_info ; if ( ! nr_linfo ) return errno = EINVAL , NULL ; /*\n\t * The min size that bpf_prog_linfo has to access for\n\t * searching purpose.\n\t */ if ( info -> line_info_rec_size < offsetof ( struct bpf_line_info , file_name_off ) ) return errno = EINVAL , NULL ; prog_linfo = calloc ( 1 , sizeof ( * prog_linfo ) ) ; if ( ! prog_linfo ) return errno = ENOMEM , NULL ; /* Copy xlated line_info */ prog_linfo -> nr_linfo = nr_linfo ; prog_linfo -> rec_size = info -> line_info_rec_size ; data_sz = ( __u64 ) nr_linfo * prog_linfo -> rec_size ; prog_linfo -> raw_linfo = malloc ( data_sz ) ; if ( ! prog_linfo -> raw_linfo ) goto err_free ; memcpy ( prog_linfo -> raw_linfo , ( void * ) ( long ) info -> line_info , data_sz ) ; nr_jited_func = info -> nr_jited_ksyms ; if ( ! nr_jited_func || ! info -> jited_line_info || info -> nr_jited_line_info != nr_linfo || info -> jited_line_info_rec_size < sizeof ( __u64 ) || info -> nr_jited_func_lens != nr_jited_func || ! info -> jited_ksyms || ! info -> jited_func_lens ) /* Not enough info to provide jited_line_info */ return prog_linfo ; /* Copy jited_line_info */ prog_linfo -> nr_jited_func = nr_jited_func ; prog_linfo -> jited_rec_size = info -> jited_line_info_rec_size ; data_sz = ( __u64 ) nr_linfo * prog_linfo -> jited_rec_size ; prog_linfo -> raw_jited_linfo = malloc ( data_sz ) ; if ( ! prog_linfo -> raw_jited_linfo ) goto err_free ; memcpy ( prog_linfo -> raw_jited_linfo , ( void * ) ( long ) info -> jited_line_info , data_sz ) ; /* Number of jited_line_info per jited func */ prog_linfo -> nr_jited_linfo_per_func = malloc ( nr_jited_func * sizeof ( __u32 ) ) ; if ( ! prog_linfo -> nr_jited_linfo_per_func ) goto err_free ; /*\n\t * For each jited func,\n\t * the start idx to the \"linfo\" and \"jited_linfo\" array,\n\t */ prog_linfo -> jited_linfo_func_idx = malloc ( nr_jited_func * sizeof ( __u32 ) ) ; if ( ! prog_linfo -> jited_linfo_func_idx ) goto err_free ; if ( dissect_jited_func ( prog_linfo , ( __u64 * ) ( long ) info -> jited_ksyms , ( __u32 * ) ( long ) info -> jited_func_lens ) ) goto err_free ; return prog_linfo ; err_free : bpf_prog_linfo__free ( prog_linfo ) ; return errno = EINVAL , NULL ; }",
    "resources/libbpf/src/bpf_prog_linfo.c@bpf_prog_linfo__lfind_addr_func": "const struct bpf_line_info * bpf_prog_linfo__lfind_addr_func ( const struct bpf_prog_linfo * prog_linfo , __u64 addr , __u32 func_idx , __u32 nr_skip ) { __u32 jited_rec_size , rec_size , nr_linfo , start , i ; const void * raw_jited_linfo , * raw_linfo ; const __u64 * jited_linfo ; if ( func_idx >= prog_linfo -> nr_jited_func ) return errno = ENOENT , NULL ; nr_linfo = prog_linfo -> nr_jited_linfo_per_func [ func_idx ] ; if ( nr_skip >= nr_linfo ) return errno = ENOENT , NULL ; start = prog_linfo -> jited_linfo_func_idx [ func_idx ] + nr_skip ; jited_rec_size = prog_linfo -> jited_rec_size ; raw_jited_linfo = prog_linfo -> raw_jited_linfo + ( start * jited_rec_size ) ; jited_linfo = raw_jited_linfo ; if ( addr < * jited_linfo ) return errno = ENOENT , NULL ; nr_linfo -= nr_skip ; rec_size = prog_linfo -> rec_size ; raw_linfo = prog_linfo -> raw_linfo + ( start * rec_size ) ; for ( i = 0 ; i < nr_linfo ; i ++ ) { if ( addr < * jited_linfo ) break ; raw_linfo += rec_size ; raw_jited_linfo += jited_rec_size ; jited_linfo = raw_jited_linfo ; } return raw_linfo - rec_size ; }",
    "resources/libbpf/src/bpf_prog_linfo.c@bpf_prog_linfo__lfind": "const struct bpf_line_info * bpf_prog_linfo__lfind ( const struct bpf_prog_linfo * prog_linfo , __u32 insn_off , __u32 nr_skip ) { const struct bpf_line_info * linfo ; __u32 rec_size , nr_linfo , i ; const void * raw_linfo ; nr_linfo = prog_linfo -> nr_linfo ; if ( nr_skip >= nr_linfo ) return errno = ENOENT , NULL ; rec_size = prog_linfo -> rec_size ; raw_linfo = prog_linfo -> raw_linfo + ( nr_skip * rec_size ) ; linfo = raw_linfo ; if ( insn_off < linfo -> insn_off ) return errno = ENOENT , NULL ; nr_linfo -= nr_skip ; for ( i = 0 ; i < nr_linfo ; i ++ ) { if ( insn_off < linfo -> insn_off ) break ; raw_linfo += rec_size ; linfo = raw_linfo ; } return raw_linfo - rec_size ; }",
    "resources/libbpf/src/ringbuf.c@btf_kind": "static inline __u16 btf_kind ( const struct btf_type * t ) { return BTF_INFO_KIND ( t -> info ) ; }",
    "resources/libbpf/src/ringbuf.c@btf_vlen": "static inline __u16 btf_vlen ( const struct btf_type * t ) { return BTF_INFO_VLEN ( t -> info ) ; }",
    "resources/libbpf/src/ringbuf.c@btf_kflag": "static inline bool btf_kflag ( const struct btf_type * t ) { return BTF_INFO_KFLAG ( t -> info ) ; }",
    "resources/libbpf/src/ringbuf.c@btf_is_void": "static inline bool btf_is_void ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_UNKN ; }",
    "resources/libbpf/src/ringbuf.c@btf_is_int": "static inline bool btf_is_int ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_INT ; }",
    "resources/libbpf/src/ringbuf.c@btf_is_ptr": "static inline bool btf_is_ptr ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_PTR ; }",
    "resources/libbpf/src/ringbuf.c@btf_is_array": "static inline bool btf_is_array ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_ARRAY ; }",
    "resources/libbpf/src/ringbuf.c@btf_is_struct": "static inline bool btf_is_struct ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_STRUCT ; }",
    "resources/libbpf/src/ringbuf.c@btf_is_union": "static inline bool btf_is_union ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_UNION ; }",
    "resources/libbpf/src/ringbuf.c@btf_is_composite": "static inline bool btf_is_composite ( const struct btf_type * t ) { __u16 kind = btf_kind ( t ) ; return kind == BTF_KIND_STRUCT || kind == BTF_KIND_UNION ; }",
    "resources/libbpf/src/ringbuf.c@btf_is_enum": "static inline bool btf_is_enum ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_ENUM ; }",
    "resources/libbpf/src/ringbuf.c@btf_is_enum64": "static inline bool btf_is_enum64 ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_ENUM64 ; }",
    "resources/libbpf/src/ringbuf.c@btf_is_fwd": "static inline bool btf_is_fwd ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_FWD ; }",
    "resources/libbpf/src/ringbuf.c@btf_is_typedef": "static inline bool btf_is_typedef ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_TYPEDEF ; }",
    "resources/libbpf/src/ringbuf.c@btf_is_volatile": "static inline bool btf_is_volatile ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_VOLATILE ; }",
    "resources/libbpf/src/ringbuf.c@btf_is_const": "static inline bool btf_is_const ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_CONST ; }",
    "resources/libbpf/src/ringbuf.c@btf_is_restrict": "static inline bool btf_is_restrict ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_RESTRICT ; }",
    "resources/libbpf/src/ringbuf.c@btf_is_mod": "static inline bool btf_is_mod ( const struct btf_type * t ) { __u16 kind = btf_kind ( t ) ; return kind == BTF_KIND_VOLATILE || kind == BTF_KIND_CONST || kind == BTF_KIND_RESTRICT || kind == BTF_KIND_TYPE_TAG ; }",
    "resources/libbpf/src/ringbuf.c@btf_is_func": "static inline bool btf_is_func ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_FUNC ; }",
    "resources/libbpf/src/ringbuf.c@btf_is_func_proto": "static inline bool btf_is_func_proto ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_FUNC_PROTO ; }",
    "resources/libbpf/src/ringbuf.c@btf_is_var": "static inline bool btf_is_var ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_VAR ; }",
    "resources/libbpf/src/ringbuf.c@btf_is_datasec": "static inline bool btf_is_datasec ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_DATASEC ; }",
    "resources/libbpf/src/ringbuf.c@btf_is_float": "static inline bool btf_is_float ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_FLOAT ; }",
    "resources/libbpf/src/ringbuf.c@btf_is_decl_tag": "static inline bool btf_is_decl_tag ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_DECL_TAG ; }",
    "resources/libbpf/src/ringbuf.c@btf_is_type_tag": "static inline bool btf_is_type_tag ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_TYPE_TAG ; }",
    "resources/libbpf/src/ringbuf.c@btf_is_any_enum": "static inline bool btf_is_any_enum ( const struct btf_type * t ) { return btf_is_enum ( t ) || btf_is_enum64 ( t ) ; }",
    "resources/libbpf/src/ringbuf.c@btf_kind_core_compat": "static inline bool btf_kind_core_compat ( const struct btf_type * t1 , const struct btf_type * t2 ) { return btf_kind ( t1 ) == btf_kind ( t2 ) || ( btf_is_any_enum ( t1 ) && btf_is_any_enum ( t2 ) ) ; }",
    "resources/libbpf/src/ringbuf.c@btf_int_encoding": "static inline __u8 btf_int_encoding ( const struct btf_type * t ) { return BTF_INT_ENCODING ( * ( __u32 * ) ( t + 1 ) ) ; }",
    "resources/libbpf/src/ringbuf.c@btf_int_offset": "static inline __u8 btf_int_offset ( const struct btf_type * t ) { return BTF_INT_OFFSET ( * ( __u32 * ) ( t + 1 ) ) ; }",
    "resources/libbpf/src/ringbuf.c@btf_int_bits": "static inline __u8 btf_int_bits ( const struct btf_type * t ) { return BTF_INT_BITS ( * ( __u32 * ) ( t + 1 ) ) ; }",
    "resources/libbpf/src/ringbuf.c@btf_array": "static inline struct btf_array * btf_array ( const struct btf_type * t ) { return ( struct btf_array * ) ( t + 1 ) ; }",
    "resources/libbpf/src/ringbuf.c@btf_enum": "static inline struct btf_enum * btf_enum ( const struct btf_type * t ) { return ( struct btf_enum * ) ( t + 1 ) ; }",
    "resources/libbpf/src/ringbuf.c@btf_enum64": "static inline struct btf_enum64 * btf_enum64 ( const struct btf_type * t ) { return ( struct btf_enum64 * ) ( t + 1 ) ; }",
    "resources/libbpf/src/ringbuf.c@btf_enum64_value": "static inline __u64 btf_enum64_value ( const struct btf_enum64 * e ) { /* struct btf_enum64 is introduced in Linux 6.0, which is very\n\t * bleeding-edge. Here we are avoiding relying on struct btf_enum64\n\t * definition coming from kernel UAPI headers to support wider range\n\t * of system-wide kernel headers.\n\t *\n\t * Given this header can be also included from C++ applications, that\n\t * further restricts C tricks we can use (like using compatible\n\t * anonymous struct). So just treat struct btf_enum64 as\n\t * a three-element array of u32 and access second (lo32) and third\n\t * (hi32) elements directly.\n\t *\n\t * For reference, here is a struct btf_enum64 definition:\n\t *\n\t * const struct btf_enum64 {\n\t *\t__u32\tname_off;\n\t *\t__u32\tval_lo32;\n\t *\t__u32\tval_hi32;\n\t * };\n\t */ const __u32 * e64 = ( const __u32 * ) e ; return ( ( __u64 ) e64 [ 2 ] << 32 ) | e64 [ 1 ] ; }",
    "resources/libbpf/src/ringbuf.c@btf_members": "static inline struct btf_member * btf_members ( const struct btf_type * t ) { return ( struct btf_member * ) ( t + 1 ) ; }",
    "resources/libbpf/src/ringbuf.c@btf_member_bit_offset": "static inline __u32 btf_member_bit_offset ( const struct btf_type * t , __u32 member_idx ) { const struct btf_member * m = btf_members ( t ) + member_idx ; bool kflag = btf_kflag ( t ) ; return kflag ? BTF_MEMBER_BIT_OFFSET ( m -> offset ) : m -> offset ; }",
    "resources/libbpf/src/ringbuf.c@btf_member_bitfield_size": "static inline __u32 btf_member_bitfield_size ( const struct btf_type * t , __u32 member_idx ) { const struct btf_member * m = btf_members ( t ) + member_idx ; bool kflag = btf_kflag ( t ) ; return kflag ? BTF_MEMBER_BITFIELD_SIZE ( m -> offset ) : 0 ; }",
    "resources/libbpf/src/ringbuf.c@btf_params": "static inline struct btf_param * btf_params ( const struct btf_type * t ) { return ( struct btf_param * ) ( t + 1 ) ; }",
    "resources/libbpf/src/ringbuf.c@btf_var": "static inline struct btf_var * btf_var ( const struct btf_type * t ) { return ( struct btf_var * ) ( t + 1 ) ; }",
    "resources/libbpf/src/ringbuf.c@btf_var_secinfos": "static inline struct btf_var_secinfo * btf_var_secinfos ( const struct btf_type * t ) { return ( struct btf_var_secinfo * ) ( t + 1 ) ; }",
    "resources/libbpf/src/ringbuf.c@btf_decl_tag": "static inline struct btf_decl_tag * btf_decl_tag ( const struct btf_type * t ) { return ( struct btf_decl_tag * ) ( t + 1 ) ; }",
    "resources/libbpf/src/ringbuf.c@str_has_sfx": "static inline bool str_has_sfx ( const char * str , const char * sfx ) { size_t str_len = strlen ( str ) ; size_t sfx_len = strlen ( sfx ) ; if ( sfx_len > str_len ) return false ; return strcmp ( str + str_len - sfx_len , sfx ) == 0 ; }",
    "resources/libbpf/src/ringbuf.c@libbpf_reallocarray": "static inline void * libbpf_reallocarray ( void * ptr , size_t nmemb , size_t size ) { size_t total ; # if __has_builtin ( __builtin_mul_overflow ) if ( unlikely ( __builtin_mul_overflow ( nmemb , size , & total ) ) ) return NULL ; # else if ( size == 0 || nmemb > ULONG_MAX / size ) return NULL ; total = nmemb * size ; # endif return realloc ( ptr , total ) ; }",
    "resources/libbpf/src/ringbuf.c@libbpf_strlcpy": "static inline void libbpf_strlcpy ( char * dst , const char * src , size_t sz ) { size_t i ; if ( sz == 0 ) return ; sz -- ; for ( i = 0 ; i < sz && src [ i ] ; i ++ ) dst [ i ] = src [ i ] ; dst [ i ] = '\\0' ; }",
    "resources/libbpf/src/ringbuf.c@btf_func_linkage": "static inline enum btf_func_linkage btf_func_linkage ( const struct btf_type * t ) { return ( enum btf_func_linkage ) ( int ) btf_vlen ( t ) ; }",
    "resources/libbpf/src/ringbuf.c@btf_type_info": "static inline __u32 btf_type_info ( int kind , int vlen , int kflag ) { return ( kflag << 31 ) | ( kind << 24 ) | vlen ; }",
    "resources/libbpf/src/ringbuf.c@libbpf_is_mem_zeroed": "static inline bool libbpf_is_mem_zeroed ( const char * p , ssize_t len ) { while ( len > 0 ) { if ( * p ) return false ; p ++ ; len -- ; } return true ; }",
    "resources/libbpf/src/ringbuf.c@libbpf_validate_opts": "static inline bool libbpf_validate_opts ( const char * opts , size_t opts_sz , size_t user_sz , const char * type_name ) { if ( user_sz < sizeof ( size_t ) ) { pr_warn ( \"%s size (%zu) is too small\\n\" , type_name , user_sz ) ; return false ; } if ( ! libbpf_is_mem_zeroed ( opts + opts_sz , ( ssize_t ) user_sz - opts_sz ) ) { pr_warn ( \"%s has non-zero extra bytes\\n\" , type_name ) ; return false ; } return true ; }",
    "resources/libbpf/src/ringbuf.c@libbpf_err": "static inline int libbpf_err ( int ret ) { if ( ret < 0 ) errno = - ret ; return ret ; }",
    "resources/libbpf/src/ringbuf.c@libbpf_err_errno": "static inline int libbpf_err_errno ( int ret ) { /* errno is already assumed to be set on error */ return ret < 0 ? - errno : ret ; }",
    "resources/libbpf/src/ringbuf.c@libbpf_err_ptr": "static inline void * libbpf_err_ptr ( int err ) { /* set errno on error, this doesn't break anything */ errno = - err ; return NULL ; }",
    "resources/libbpf/src/ringbuf.c@libbpf_ptr": "static inline void * libbpf_ptr ( void * ret ) { /* set errno on error, this doesn't break anything */ if ( IS_ERR ( ret ) ) errno = - PTR_ERR ( ret ) ; return IS_ERR ( ret ) ? NULL : ret ; }",
    "resources/libbpf/src/ringbuf.c@str_is_empty": "static inline bool str_is_empty ( const char * s ) { return ! s || ! s [ 0 ] ; }",
    "resources/libbpf/src/ringbuf.c@is_ldimm64_insn": "static inline bool is_ldimm64_insn ( struct bpf_insn * insn ) { return insn -> code == ( BPF_LD | BPF_IMM | BPF_DW ) ; }",
    "resources/libbpf/src/ringbuf.c@dup_good_fd": "static inline int dup_good_fd ( int fd ) { if ( fd < 0 ) return fd ; return fcntl ( fd , F_DUPFD_CLOEXEC , 3 ) ; }",
    "resources/libbpf/src/ringbuf.c@ensure_good_fd": "static inline int ensure_good_fd ( int fd ) { int old_fd = fd , saved_errno ; if ( fd < 0 ) return fd ; if ( fd < 3 ) { fd = dup_good_fd ( fd ) ; saved_errno = errno ; close ( old_fd ) ; errno = saved_errno ; if ( fd < 0 ) { pr_warn ( \"failed to dup FD %d to FD > 2: %d\\n\" , old_fd , - saved_errno ) ; errno = saved_errno ; } } return fd ; }",
    "resources/libbpf/src/ringbuf.c@sys_dup2": "static inline int sys_dup2 ( int oldfd , int newfd ) { # ifdef __NR_dup2 return syscall ( __NR_dup2 , oldfd , newfd ) ; # else return syscall ( __NR_dup3 , oldfd , newfd , 0 ) ; # endif }",
    "resources/libbpf/src/ringbuf.c@reuse_fd": "static inline int reuse_fd ( int fixed_fd , int tmp_fd ) { int err ; err = sys_dup2 ( tmp_fd , fixed_fd ) ; err = err < 0 ? - errno : 0 ; close ( tmp_fd ) ; /* clean up temporary FD */ return err ; }",
    "resources/libbpf/src/ringbuf.c@is_pow_of_2": "static inline bool is_pow_of_2 ( size_t x ) { return x && ( x & ( x - 1 ) ) == 0 ; }",
    "resources/libbpf/src/ringbuf.c@ringbuf_free_ring": "static void ringbuf_free_ring ( struct ring_buffer * rb , struct ring * r ) { if ( r -> consumer_pos ) { munmap ( r -> consumer_pos , rb -> page_size ) ; r -> consumer_pos = NULL ; } if ( r -> producer_pos ) { munmap ( r -> producer_pos , rb -> page_size + 2 * ( r -> mask + 1 ) ) ; r -> producer_pos = NULL ; } free ( r ) ; }",
    "resources/libbpf/src/ringbuf.c@ring_buffer__add": "int ring_buffer__add ( struct ring_buffer * rb , int map_fd , ring_buffer_sample_fn sample_cb , void * ctx ) { struct bpf_map_info info ; __u32 len = sizeof ( info ) ; struct epoll_event * e ; struct ring * r ; __u64 mmap_sz ; void * tmp ; int err ; memset ( & info , 0 , sizeof ( info ) ) ; err = bpf_map_get_info_by_fd ( map_fd , & info , & len ) ; if ( err ) { err = - errno ; pr_warn ( \"ringbuf: failed to get map info for fd=%d: %d\\n\" , map_fd , err ) ; return libbpf_err ( err ) ; } if ( info . type != BPF_MAP_TYPE_RINGBUF ) { pr_warn ( \"ringbuf: map fd=%d is not BPF_MAP_TYPE_RINGBUF\\n\" , map_fd ) ; return libbpf_err ( - EINVAL ) ; } tmp = libbpf_reallocarray ( rb -> rings , rb -> ring_cnt + 1 , sizeof ( * rb -> rings ) ) ; if ( ! tmp ) return libbpf_err ( - ENOMEM ) ; rb -> rings = tmp ; tmp = libbpf_reallocarray ( rb -> events , rb -> ring_cnt + 1 , sizeof ( * rb -> events ) ) ; if ( ! tmp ) return libbpf_err ( - ENOMEM ) ; rb -> events = tmp ; r = calloc ( 1 , sizeof ( * r ) ) ; if ( ! r ) return libbpf_err ( - ENOMEM ) ; rb -> rings [ rb -> ring_cnt ] = r ; r -> map_fd = map_fd ; r -> sample_cb = sample_cb ; r -> ctx = ctx ; r -> mask = info . max_entries - 1 ; /* Map writable consumer page */ tmp = mmap ( NULL , rb -> page_size , PROT_READ | PROT_WRITE , MAP_SHARED , map_fd , 0 ) ; if ( tmp == MAP_FAILED ) { err = - errno ; pr_warn ( \"ringbuf: failed to mmap consumer page for map fd=%d: %d\\n\" , map_fd , err ) ; goto err_out ; } r -> consumer_pos = tmp ; /* Map read-only producer page and data pages. We map twice as big\n\t * data size to allow simple reading of samples that wrap around the\n\t * end of a ring buffer. See kernel implementation for details.\n\t */ mmap_sz = rb -> page_size + 2 * ( __u64 ) info . max_entries ; if ( mmap_sz != ( __u64 ) ( size_t ) mmap_sz ) { err = - E2BIG ; pr_warn ( \"ringbuf: ring buffer size (%u) is too big\\n\" , info . max_entries ) ; goto err_out ; } tmp = mmap ( NULL , ( size_t ) mmap_sz , PROT_READ , MAP_SHARED , map_fd , rb -> page_size ) ; if ( tmp == MAP_FAILED ) { err = - errno ; pr_warn ( \"ringbuf: failed to mmap data pages for map fd=%d: %d\\n\" , map_fd , err ) ; goto err_out ; } r -> producer_pos = tmp ; r -> data = tmp + rb -> page_size ; e = & rb -> events [ rb -> ring_cnt ] ; memset ( e , 0 , sizeof ( * e ) ) ; e -> events = EPOLLIN ; e -> data . fd = rb -> ring_cnt ; if ( epoll_ctl ( rb -> epoll_fd , EPOLL_CTL_ADD , map_fd , e ) < 0 ) { err = - errno ; pr_warn ( \"ringbuf: failed to epoll add map fd=%d: %d\\n\" , map_fd , err ) ; goto err_out ; } rb -> ring_cnt ++ ; return 0 ; err_out : ringbuf_free_ring ( rb , r ) ; return libbpf_err ( err ) ; }",
    "resources/libbpf/src/ringbuf.c@ring_buffer__free": "void ring_buffer__free ( struct ring_buffer * rb ) { int i ; if ( ! rb ) return ; for ( i = 0 ; i < rb -> ring_cnt ; ++ i ) ringbuf_free_ring ( rb , rb -> rings [ i ] ) ; if ( rb -> epoll_fd >= 0 ) close ( rb -> epoll_fd ) ; free ( rb -> events ) ; free ( rb -> rings ) ; free ( rb ) ; }",
    "resources/libbpf/src/ringbuf.c@ring_buffer__new": "struct ring_buffer * ring_buffer__new ( int map_fd , ring_buffer_sample_fn sample_cb , void * ctx , const struct ring_buffer_opts * opts ) { struct ring_buffer * rb ; int err ; if ( ! OPTS_VALID ( opts , ring_buffer_opts ) ) return errno = EINVAL , NULL ; rb = calloc ( 1 , sizeof ( * rb ) ) ; if ( ! rb ) return errno = ENOMEM , NULL ; rb -> page_size = getpagesize ( ) ; rb -> epoll_fd = epoll_create1 ( EPOLL_CLOEXEC ) ; if ( rb -> epoll_fd < 0 ) { err = - errno ; pr_warn ( \"ringbuf: failed to create epoll instance: %d\\n\" , err ) ; goto err_out ; } err = ring_buffer__add ( rb , map_fd , sample_cb , ctx ) ; if ( err ) goto err_out ; return rb ; err_out : ring_buffer__free ( rb ) ; return errno = - err , NULL ; }",
    "resources/libbpf/src/ringbuf.c@roundup_len": "static inline int roundup_len ( __u32 len ) { /* clear out top 2 bits (discard and busy, if set) */ len <<= 2 ; len >>= 2 ; /* add length prefix */ len += BPF_RINGBUF_HDR_SZ ; /* round up to 8 byte alignment */ return ( len + 7 ) / 8 * 8 ; }",
    "resources/libbpf/src/ringbuf.c@ringbuf_process_ring": "static int64_t ringbuf_process_ring ( struct ring * r ) { int * len_ptr , len , err ; /* 64-bit to avoid overflow in case of extreme application behavior */ int64_t cnt = 0 ; unsigned long cons_pos , prod_pos ; bool got_new_data ; void * sample ; cons_pos = smp_load_acquire ( r -> consumer_pos ) ; do { got_new_data = false ; prod_pos = smp_load_acquire ( r -> producer_pos ) ; while ( cons_pos < prod_pos ) { len_ptr = r -> data + ( cons_pos & r -> mask ) ; len = smp_load_acquire ( len_ptr ) ; /* sample not committed yet, bail out for now */ if ( len & BPF_RINGBUF_BUSY_BIT ) goto done ; got_new_data = true ; cons_pos += roundup_len ( len ) ; if ( ( len & BPF_RINGBUF_DISCARD_BIT ) == 0 ) { sample = ( void * ) len_ptr + BPF_RINGBUF_HDR_SZ ; err = r -> sample_cb ( r -> ctx , sample , len ) ; if ( err < 0 ) { /* update consumer pos and bail out */ smp_store_release ( r -> consumer_pos , cons_pos ) ; return err ; } cnt ++ ; } smp_store_release ( r -> consumer_pos , cons_pos ) ; } } while ( got_new_data ) ; done : return cnt ; }",
    "resources/libbpf/src/ringbuf.c@ring_buffer__consume": "int ring_buffer__consume ( struct ring_buffer * rb ) { int64_t err , res = 0 ; int i ; for ( i = 0 ; i < rb -> ring_cnt ; i ++ ) { struct ring * ring = rb -> rings [ i ] ; err = ringbuf_process_ring ( ring ) ; if ( err < 0 ) return libbpf_err ( err ) ; res += err ; } if ( res > INT_MAX ) return INT_MAX ; return res ; }",
    "resources/libbpf/src/ringbuf.c@ring_buffer__poll": "int ring_buffer__poll ( struct ring_buffer * rb , int timeout_ms ) { int i , cnt ; int64_t err , res = 0 ; cnt = epoll_wait ( rb -> epoll_fd , rb -> events , rb -> ring_cnt , timeout_ms ) ; if ( cnt < 0 ) return libbpf_err ( - errno ) ; for ( i = 0 ; i < cnt ; i ++ ) { __u32 ring_id = rb -> events [ i ] . data . fd ; struct ring * ring = rb -> rings [ ring_id ] ; err = ringbuf_process_ring ( ring ) ; if ( err < 0 ) return libbpf_err ( err ) ; res += err ; } if ( res > INT_MAX ) return INT_MAX ; return res ; }",
    "resources/libbpf/src/ringbuf.c@ring_buffer__epoll_fd": "int ring_buffer__epoll_fd ( const struct ring_buffer * rb ) { return rb -> epoll_fd ; }",
    "resources/libbpf/src/ringbuf.c@ring_buffer__ring": "struct ring * ring_buffer__ring ( struct ring_buffer * rb , unsigned int idx ) { if ( idx >= rb -> ring_cnt ) return errno = ERANGE , NULL ; return rb -> rings [ idx ] ; }",
    "resources/libbpf/src/ringbuf.c@ring__consumer_pos": "unsigned long ring__consumer_pos ( const struct ring * r ) { /* Synchronizes with smp_store_release() in ringbuf_process_ring(). */ return smp_load_acquire ( r -> consumer_pos ) ; }",
    "resources/libbpf/src/ringbuf.c@ring__producer_pos": "unsigned long ring__producer_pos ( const struct ring * r ) { /* Synchronizes with smp_store_release() in __bpf_ringbuf_reserve() in\n\t * the kernel.\n\t */ return smp_load_acquire ( r -> producer_pos ) ; }",
    "resources/libbpf/src/ringbuf.c@ring__avail_data_size": "size_t ring__avail_data_size ( const struct ring * r ) { unsigned long cons_pos , prod_pos ; cons_pos = ring__consumer_pos ( r ) ; prod_pos = ring__producer_pos ( r ) ; return prod_pos - cons_pos ; }",
    "resources/libbpf/src/ringbuf.c@ring__size": "size_t ring__size ( const struct ring * r ) { return r -> mask + 1 ; }",
    "resources/libbpf/src/ringbuf.c@ring__map_fd": "int ring__map_fd ( const struct ring * r ) { return r -> map_fd ; }",
    "resources/libbpf/src/ringbuf.c@ring__consume": "int ring__consume ( struct ring * r ) { int64_t res ; res = ringbuf_process_ring ( r ) ; if ( res < 0 ) return libbpf_err ( res ) ; return res > INT_MAX ? INT_MAX : res ; }",
    "resources/libbpf/src/ringbuf.c@user_ringbuf_unmap_ring": "static void user_ringbuf_unmap_ring ( struct user_ring_buffer * rb ) { if ( rb -> consumer_pos ) { munmap ( rb -> consumer_pos , rb -> page_size ) ; rb -> consumer_pos = NULL ; } if ( rb -> producer_pos ) { munmap ( rb -> producer_pos , rb -> page_size + 2 * ( rb -> mask + 1 ) ) ; rb -> producer_pos = NULL ; } }",
    "resources/libbpf/src/ringbuf.c@user_ring_buffer__free": "void user_ring_buffer__free ( struct user_ring_buffer * rb ) { if ( ! rb ) return ; user_ringbuf_unmap_ring ( rb ) ; if ( rb -> epoll_fd >= 0 ) close ( rb -> epoll_fd ) ; free ( rb ) ; }",
    "resources/libbpf/src/ringbuf.c@user_ringbuf_map": "static int user_ringbuf_map ( struct user_ring_buffer * rb , int map_fd ) { struct bpf_map_info info ; __u32 len = sizeof ( info ) ; __u64 mmap_sz ; void * tmp ; struct epoll_event * rb_epoll ; int err ; memset ( & info , 0 , sizeof ( info ) ) ; err = bpf_map_get_info_by_fd ( map_fd , & info , & len ) ; if ( err ) { err = - errno ; pr_warn ( \"user ringbuf: failed to get map info for fd=%d: %d\\n\" , map_fd , err ) ; return err ; } if ( info . type != BPF_MAP_TYPE_USER_RINGBUF ) { pr_warn ( \"user ringbuf: map fd=%d is not BPF_MAP_TYPE_USER_RINGBUF\\n\" , map_fd ) ; return - EINVAL ; } rb -> map_fd = map_fd ; rb -> mask = info . max_entries - 1 ; /* Map read-only consumer page */ tmp = mmap ( NULL , rb -> page_size , PROT_READ , MAP_SHARED , map_fd , 0 ) ; if ( tmp == MAP_FAILED ) { err = - errno ; pr_warn ( \"user ringbuf: failed to mmap consumer page for map fd=%d: %d\\n\" , map_fd , err ) ; return err ; } rb -> consumer_pos = tmp ; /* Map read-write the producer page and data pages. We map the data\n\t * region as twice the total size of the ring buffer to allow the\n\t * simple reading and writing of samples that wrap around the end of\n\t * the buffer.  See the kernel implementation for details.\n\t */ mmap_sz = rb -> page_size + 2 * ( __u64 ) info . max_entries ; if ( mmap_sz != ( __u64 ) ( size_t ) mmap_sz ) { pr_warn ( \"user ringbuf: ring buf size (%u) is too big\\n\" , info . max_entries ) ; return - E2BIG ; } tmp = mmap ( NULL , ( size_t ) mmap_sz , PROT_READ | PROT_WRITE , MAP_SHARED , map_fd , rb -> page_size ) ; if ( tmp == MAP_FAILED ) { err = - errno ; pr_warn ( \"user ringbuf: failed to mmap data pages for map fd=%d: %d\\n\" , map_fd , err ) ; return err ; } rb -> producer_pos = tmp ; rb -> data = tmp + rb -> page_size ; rb_epoll = & rb -> event ; rb_epoll -> events = EPOLLOUT ; if ( epoll_ctl ( rb -> epoll_fd , EPOLL_CTL_ADD , map_fd , rb_epoll ) < 0 ) { err = - errno ; pr_warn ( \"user ringbuf: failed to epoll add map fd=%d: %d\\n\" , map_fd , err ) ; return err ; } return 0 ; }",
    "resources/libbpf/src/ringbuf.c@user_ring_buffer__new": "struct user_ring_buffer * user_ring_buffer__new ( int map_fd , const struct user_ring_buffer_opts * opts ) { struct user_ring_buffer * rb ; int err ; if ( ! OPTS_VALID ( opts , user_ring_buffer_opts ) ) return errno = EINVAL , NULL ; rb = calloc ( 1 , sizeof ( * rb ) ) ; if ( ! rb ) return errno = ENOMEM , NULL ; rb -> page_size = getpagesize ( ) ; rb -> epoll_fd = epoll_create1 ( EPOLL_CLOEXEC ) ; if ( rb -> epoll_fd < 0 ) { err = - errno ; pr_warn ( \"user ringbuf: failed to create epoll instance: %d\\n\" , err ) ; goto err_out ; } err = user_ringbuf_map ( rb , map_fd ) ; if ( err ) goto err_out ; return rb ; err_out : user_ring_buffer__free ( rb ) ; return errno = - err , NULL ; }",
    "resources/libbpf/src/ringbuf.c@user_ringbuf_commit": "static void user_ringbuf_commit ( struct user_ring_buffer * rb , void * sample , bool discard ) { __u32 new_len ; struct ringbuf_hdr * hdr ; uintptr_t hdr_offset ; hdr_offset = rb -> mask + 1 + ( sample - rb -> data ) - BPF_RINGBUF_HDR_SZ ; hdr = rb -> data + ( hdr_offset & rb -> mask ) ; new_len = hdr -> len & ~ BPF_RINGBUF_BUSY_BIT ; if ( discard ) new_len |= BPF_RINGBUF_DISCARD_BIT ; /* Synchronizes with smp_load_acquire() in __bpf_user_ringbuf_peek() in\n\t * the kernel.\n\t */ __atomic_exchange_n ( & hdr -> len , new_len , __ATOMIC_ACQ_REL ) ; }",
    "resources/libbpf/src/ringbuf.c@user_ring_buffer__discard": "void user_ring_buffer__discard ( struct user_ring_buffer * rb , void * sample ) { user_ringbuf_commit ( rb , sample , true ) ; }",
    "resources/libbpf/src/ringbuf.c@user_ring_buffer__submit": "void user_ring_buffer__submit ( struct user_ring_buffer * rb , void * sample ) { user_ringbuf_commit ( rb , sample , false ) ; }",
    "resources/libbpf/src/ringbuf.c@user_ring_buffer__reserve": "void * user_ring_buffer__reserve ( struct user_ring_buffer * rb , __u32 size ) { __u32 avail_size , total_size , max_size ; /* 64-bit to avoid overflow in case of extreme application behavior */ __u64 cons_pos , prod_pos ; struct ringbuf_hdr * hdr ; /* The top two bits are used as special flags */ if ( size & ( BPF_RINGBUF_BUSY_BIT | BPF_RINGBUF_DISCARD_BIT ) ) return errno = E2BIG , NULL ; /* Synchronizes with smp_store_release() in __bpf_user_ringbuf_peek() in\n\t * the kernel.\n\t */ cons_pos = smp_load_acquire ( rb -> consumer_pos ) ; /* Synchronizes with smp_store_release() in user_ringbuf_commit() */ prod_pos = smp_load_acquire ( rb -> producer_pos ) ; max_size = rb -> mask + 1 ; avail_size = max_size - ( prod_pos - cons_pos ) ; /* Round up total size to a multiple of 8. */ total_size = ( size + BPF_RINGBUF_HDR_SZ + 7 ) / 8 * 8 ; if ( total_size > max_size ) return errno = E2BIG , NULL ; if ( avail_size < total_size ) return errno = ENOSPC , NULL ; hdr = rb -> data + ( prod_pos & rb -> mask ) ; hdr -> len = size | BPF_RINGBUF_BUSY_BIT ; hdr -> pad = 0 ; /* Synchronizes with smp_load_acquire() in __bpf_user_ringbuf_peek() in\n\t * the kernel.\n\t */ smp_store_release ( rb -> producer_pos , prod_pos + total_size ) ; return ( void * ) rb -> data + ( ( prod_pos + BPF_RINGBUF_HDR_SZ ) & rb -> mask ) ; }",
    "resources/libbpf/src/ringbuf.c@ns_elapsed_timespec": "static __u64 ns_elapsed_timespec ( const struct timespec * start , const struct timespec * end ) { __u64 start_ns , end_ns , ns_per_s = 1000000000 ; start_ns = ( __u64 ) start -> tv_sec * ns_per_s + start -> tv_nsec ; end_ns = ( __u64 ) end -> tv_sec * ns_per_s + end -> tv_nsec ; return end_ns - start_ns ; }",
    "resources/libbpf/src/ringbuf.c@user_ring_buffer__reserve_blocking": "void * user_ring_buffer__reserve_blocking ( struct user_ring_buffer * rb , __u32 size , int timeout_ms ) { void * sample ; int err , ms_remaining = timeout_ms ; struct timespec start ; if ( timeout_ms < 0 && timeout_ms != - 1 ) return errno = EINVAL , NULL ; if ( timeout_ms != - 1 ) { err = clock_gettime ( CLOCK_MONOTONIC , & start ) ; if ( err ) return NULL ; } do { int cnt , ms_elapsed ; struct timespec curr ; __u64 ns_per_ms = 1000000 ; sample = user_ring_buffer__reserve ( rb , size ) ; if ( sample ) return sample ; else if ( errno != ENOSPC ) return NULL ; /* The kernel guarantees at least one event notification\n\t\t * delivery whenever at least one sample is drained from the\n\t\t * ring buffer in an invocation to bpf_ringbuf_drain(). Other\n\t\t * additional events may be delivered at any time, but only one\n\t\t * event is guaranteed per bpf_ringbuf_drain() invocation,\n\t\t * provided that a sample is drained, and the BPF program did\n\t\t * not pass BPF_RB_NO_WAKEUP to bpf_ringbuf_drain(). If\n\t\t * BPF_RB_FORCE_WAKEUP is passed to bpf_ringbuf_drain(), a\n\t\t * wakeup event will be delivered even if no samples are\n\t\t * drained.\n\t\t */ cnt = epoll_wait ( rb -> epoll_fd , & rb -> event , 1 , ms_remaining ) ; if ( cnt < 0 ) return NULL ; if ( timeout_ms == - 1 ) continue ; err = clock_gettime ( CLOCK_MONOTONIC , & curr ) ; if ( err ) return NULL ; ms_elapsed = ns_elapsed_timespec ( & start , & curr ) / ns_per_ms ; ms_remaining = timeout_ms - ms_elapsed ; } while ( ms_remaining > 0 ) ; /* Try one more time to reserve a sample after the specified timeout has elapsed. */ return user_ring_buffer__reserve ( rb , size ) ; }",
    "resources/libbpf/src/btf.c@btf_kind": "static inline __u16 btf_kind ( const struct btf_type * t ) { return BTF_INFO_KIND ( t -> info ) ; }",
    "resources/libbpf/src/btf.c@btf_vlen": "static inline __u16 btf_vlen ( const struct btf_type * t ) { return BTF_INFO_VLEN ( t -> info ) ; }",
    "resources/libbpf/src/btf.c@btf_kflag": "static inline bool btf_kflag ( const struct btf_type * t ) { return BTF_INFO_KFLAG ( t -> info ) ; }",
    "resources/libbpf/src/btf.c@btf_is_void": "static inline bool btf_is_void ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_UNKN ; }",
    "resources/libbpf/src/btf.c@btf_is_int": "static inline bool btf_is_int ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_INT ; }",
    "resources/libbpf/src/btf.c@btf_is_ptr": "static inline bool btf_is_ptr ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_PTR ; }",
    "resources/libbpf/src/btf.c@btf_is_array": "static inline bool btf_is_array ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_ARRAY ; }",
    "resources/libbpf/src/btf.c@btf_is_struct": "static inline bool btf_is_struct ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_STRUCT ; }",
    "resources/libbpf/src/btf.c@btf_is_union": "static inline bool btf_is_union ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_UNION ; }",
    "resources/libbpf/src/btf.c@btf_is_composite": "static inline bool btf_is_composite ( const struct btf_type * t ) { __u16 kind = btf_kind ( t ) ; return kind == BTF_KIND_STRUCT || kind == BTF_KIND_UNION ; }",
    "resources/libbpf/src/btf.c@btf_is_enum": "static inline bool btf_is_enum ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_ENUM ; }",
    "resources/libbpf/src/btf.c@btf_is_enum64": "static inline bool btf_is_enum64 ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_ENUM64 ; }",
    "resources/libbpf/src/btf.c@btf_is_fwd": "static inline bool btf_is_fwd ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_FWD ; }",
    "resources/libbpf/src/btf.c@btf_is_typedef": "static inline bool btf_is_typedef ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_TYPEDEF ; }",
    "resources/libbpf/src/btf.c@btf_is_volatile": "static inline bool btf_is_volatile ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_VOLATILE ; }",
    "resources/libbpf/src/btf.c@btf_is_const": "static inline bool btf_is_const ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_CONST ; }",
    "resources/libbpf/src/btf.c@btf_is_restrict": "static inline bool btf_is_restrict ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_RESTRICT ; }",
    "resources/libbpf/src/btf.c@btf_is_mod": "static inline bool btf_is_mod ( const struct btf_type * t ) { __u16 kind = btf_kind ( t ) ; return kind == BTF_KIND_VOLATILE || kind == BTF_KIND_CONST || kind == BTF_KIND_RESTRICT || kind == BTF_KIND_TYPE_TAG ; }",
    "resources/libbpf/src/btf.c@btf_is_func": "static inline bool btf_is_func ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_FUNC ; }",
    "resources/libbpf/src/btf.c@btf_is_func_proto": "static inline bool btf_is_func_proto ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_FUNC_PROTO ; }",
    "resources/libbpf/src/btf.c@btf_is_var": "static inline bool btf_is_var ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_VAR ; }",
    "resources/libbpf/src/btf.c@btf_is_datasec": "static inline bool btf_is_datasec ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_DATASEC ; }",
    "resources/libbpf/src/btf.c@btf_is_float": "static inline bool btf_is_float ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_FLOAT ; }",
    "resources/libbpf/src/btf.c@btf_is_decl_tag": "static inline bool btf_is_decl_tag ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_DECL_TAG ; }",
    "resources/libbpf/src/btf.c@btf_is_type_tag": "static inline bool btf_is_type_tag ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_TYPE_TAG ; }",
    "resources/libbpf/src/btf.c@btf_is_any_enum": "static inline bool btf_is_any_enum ( const struct btf_type * t ) { return btf_is_enum ( t ) || btf_is_enum64 ( t ) ; }",
    "resources/libbpf/src/btf.c@btf_kind_core_compat": "static inline bool btf_kind_core_compat ( const struct btf_type * t1 , const struct btf_type * t2 ) { return btf_kind ( t1 ) == btf_kind ( t2 ) || ( btf_is_any_enum ( t1 ) && btf_is_any_enum ( t2 ) ) ; }",
    "resources/libbpf/src/btf.c@btf_int_encoding": "static inline __u8 btf_int_encoding ( const struct btf_type * t ) { return BTF_INT_ENCODING ( * ( __u32 * ) ( t + 1 ) ) ; }",
    "resources/libbpf/src/btf.c@btf_int_offset": "static inline __u8 btf_int_offset ( const struct btf_type * t ) { return BTF_INT_OFFSET ( * ( __u32 * ) ( t + 1 ) ) ; }",
    "resources/libbpf/src/btf.c@btf_int_bits": "static inline __u8 btf_int_bits ( const struct btf_type * t ) { return BTF_INT_BITS ( * ( __u32 * ) ( t + 1 ) ) ; }",
    "resources/libbpf/src/btf.c@btf_array": "static inline struct btf_array * btf_array ( const struct btf_type * t ) { return ( struct btf_array * ) ( t + 1 ) ; }",
    "resources/libbpf/src/btf.c@btf_enum": "static inline struct btf_enum * btf_enum ( const struct btf_type * t ) { return ( struct btf_enum * ) ( t + 1 ) ; }",
    "resources/libbpf/src/btf.c@btf_enum64": "static inline struct btf_enum64 * btf_enum64 ( const struct btf_type * t ) { return ( struct btf_enum64 * ) ( t + 1 ) ; }",
    "resources/libbpf/src/btf.c@btf_enum64_value": "static inline __u64 btf_enum64_value ( const struct btf_enum64 * e ) { /* struct btf_enum64 is introduced in Linux 6.0, which is very\n\t * bleeding-edge. Here we are avoiding relying on struct btf_enum64\n\t * definition coming from kernel UAPI headers to support wider range\n\t * of system-wide kernel headers.\n\t *\n\t * Given this header can be also included from C++ applications, that\n\t * further restricts C tricks we can use (like using compatible\n\t * anonymous struct). So just treat struct btf_enum64 as\n\t * a three-element array of u32 and access second (lo32) and third\n\t * (hi32) elements directly.\n\t *\n\t * For reference, here is a struct btf_enum64 definition:\n\t *\n\t * const struct btf_enum64 {\n\t *\t__u32\tname_off;\n\t *\t__u32\tval_lo32;\n\t *\t__u32\tval_hi32;\n\t * };\n\t */ const __u32 * e64 = ( const __u32 * ) e ; return ( ( __u64 ) e64 [ 2 ] << 32 ) | e64 [ 1 ] ; }",
    "resources/libbpf/src/btf.c@btf_members": "static inline struct btf_member * btf_members ( const struct btf_type * t ) { return ( struct btf_member * ) ( t + 1 ) ; }",
    "resources/libbpf/src/btf.c@btf_member_bit_offset": "static inline __u32 btf_member_bit_offset ( const struct btf_type * t , __u32 member_idx ) { const struct btf_member * m = btf_members ( t ) + member_idx ; bool kflag = btf_kflag ( t ) ; return kflag ? BTF_MEMBER_BIT_OFFSET ( m -> offset ) : m -> offset ; }",
    "resources/libbpf/src/btf.c@btf_member_bitfield_size": "static inline __u32 btf_member_bitfield_size ( const struct btf_type * t , __u32 member_idx ) { const struct btf_member * m = btf_members ( t ) + member_idx ; bool kflag = btf_kflag ( t ) ; return kflag ? BTF_MEMBER_BITFIELD_SIZE ( m -> offset ) : 0 ; }",
    "resources/libbpf/src/btf.c@btf_params": "static inline struct btf_param * btf_params ( const struct btf_type * t ) { return ( struct btf_param * ) ( t + 1 ) ; }",
    "resources/libbpf/src/btf.c@btf_var": "static inline struct btf_var * btf_var ( const struct btf_type * t ) { return ( struct btf_var * ) ( t + 1 ) ; }",
    "resources/libbpf/src/btf.c@btf_var_secinfos": "static inline struct btf_var_secinfo * btf_var_secinfos ( const struct btf_type * t ) { return ( struct btf_var_secinfo * ) ( t + 1 ) ; }",
    "resources/libbpf/src/btf.c@btf_decl_tag": "static inline struct btf_decl_tag * btf_decl_tag ( const struct btf_type * t ) { return ( struct btf_decl_tag * ) ( t + 1 ) ; }",
    "resources/libbpf/src/btf.c@str_has_sfx": "static inline bool str_has_sfx ( const char * str , const char * sfx ) { size_t str_len = strlen ( str ) ; size_t sfx_len = strlen ( sfx ) ; if ( sfx_len > str_len ) return false ; return strcmp ( str + str_len - sfx_len , sfx ) == 0 ; }",
    "resources/libbpf/src/btf.c@libbpf_reallocarray": "static inline void * libbpf_reallocarray ( void * ptr , size_t nmemb , size_t size ) { size_t total ; # if __has_builtin ( __builtin_mul_overflow ) if ( unlikely ( __builtin_mul_overflow ( nmemb , size , & total ) ) ) return NULL ; # else if ( size == 0 || nmemb > ULONG_MAX / size ) return NULL ; total = nmemb * size ; # endif return realloc ( ptr , total ) ; }",
    "resources/libbpf/src/btf.c@libbpf_strlcpy": "static inline void libbpf_strlcpy ( char * dst , const char * src , size_t sz ) { size_t i ; if ( sz == 0 ) return ; sz -- ; for ( i = 0 ; i < sz && src [ i ] ; i ++ ) dst [ i ] = src [ i ] ; dst [ i ] = '\\0' ; }",
    "resources/libbpf/src/btf.c@btf_func_linkage": "static inline enum btf_func_linkage btf_func_linkage ( const struct btf_type * t ) { return ( enum btf_func_linkage ) ( int ) btf_vlen ( t ) ; }",
    "resources/libbpf/src/btf.c@btf_type_info": "static inline __u32 btf_type_info ( int kind , int vlen , int kflag ) { return ( kflag << 31 ) | ( kind << 24 ) | vlen ; }",
    "resources/libbpf/src/btf.c@libbpf_is_mem_zeroed": "static inline bool libbpf_is_mem_zeroed ( const char * p , ssize_t len ) { while ( len > 0 ) { if ( * p ) return false ; p ++ ; len -- ; } return true ; }",
    "resources/libbpf/src/btf.c@libbpf_validate_opts": "static inline bool libbpf_validate_opts ( const char * opts , size_t opts_sz , size_t user_sz , const char * type_name ) { if ( user_sz < sizeof ( size_t ) ) { pr_warn ( \"%s size (%zu) is too small\\n\" , type_name , user_sz ) ; return false ; } if ( ! libbpf_is_mem_zeroed ( opts + opts_sz , ( ssize_t ) user_sz - opts_sz ) ) { pr_warn ( \"%s has non-zero extra bytes\\n\" , type_name ) ; return false ; } return true ; }",
    "resources/libbpf/src/btf.c@libbpf_err": "static inline int libbpf_err ( int ret ) { if ( ret < 0 ) errno = - ret ; return ret ; }",
    "resources/libbpf/src/btf.c@libbpf_err_errno": "static inline int libbpf_err_errno ( int ret ) { /* errno is already assumed to be set on error */ return ret < 0 ? - errno : ret ; }",
    "resources/libbpf/src/btf.c@libbpf_err_ptr": "static inline void * libbpf_err_ptr ( int err ) { /* set errno on error, this doesn't break anything */ errno = - err ; return NULL ; }",
    "resources/libbpf/src/btf.c@libbpf_ptr": "static inline void * libbpf_ptr ( void * ret ) { /* set errno on error, this doesn't break anything */ if ( IS_ERR ( ret ) ) errno = - PTR_ERR ( ret ) ; return IS_ERR ( ret ) ? NULL : ret ; }",
    "resources/libbpf/src/btf.c@str_is_empty": "static inline bool str_is_empty ( const char * s ) { return ! s || ! s [ 0 ] ; }",
    "resources/libbpf/src/btf.c@is_ldimm64_insn": "static inline bool is_ldimm64_insn ( struct bpf_insn * insn ) { return insn -> code == ( BPF_LD | BPF_IMM | BPF_DW ) ; }",
    "resources/libbpf/src/btf.c@dup_good_fd": "static inline int dup_good_fd ( int fd ) { if ( fd < 0 ) return fd ; return fcntl ( fd , F_DUPFD_CLOEXEC , 3 ) ; }",
    "resources/libbpf/src/btf.c@ensure_good_fd": "static inline int ensure_good_fd ( int fd ) { int old_fd = fd , saved_errno ; if ( fd < 0 ) return fd ; if ( fd < 3 ) { fd = dup_good_fd ( fd ) ; saved_errno = errno ; close ( old_fd ) ; errno = saved_errno ; if ( fd < 0 ) { pr_warn ( \"failed to dup FD %d to FD > 2: %d\\n\" , old_fd , - saved_errno ) ; errno = saved_errno ; } } return fd ; }",
    "resources/libbpf/src/btf.c@sys_dup2": "static inline int sys_dup2 ( int oldfd , int newfd ) { # ifdef __NR_dup2 return syscall ( __NR_dup2 , oldfd , newfd ) ; # else return syscall ( __NR_dup3 , oldfd , newfd , 0 ) ; # endif }",
    "resources/libbpf/src/btf.c@reuse_fd": "static inline int reuse_fd ( int fixed_fd , int tmp_fd ) { int err ; err = sys_dup2 ( tmp_fd , fixed_fd ) ; err = err < 0 ? - errno : 0 ; close ( tmp_fd ) ; /* clean up temporary FD */ return err ; }",
    "resources/libbpf/src/btf.c@is_pow_of_2": "static inline bool is_pow_of_2 ( size_t x ) { return x && ( x & ( x - 1 ) ) == 0 ; }",
    "resources/libbpf/src/btf.c@hash_bits": "static inline size_t hash_bits ( size_t h , int bits ) { /* shuffle bits and return requested number of upper bits */ if ( bits == 0 ) return 0 ; # if ( __SIZEOF_SIZE_T__ == __SIZEOF_LONG_LONG__ ) /* LP64 case */ return ( h * 11400714819323198485llu ) >> ( __SIZEOF_LONG_LONG__ * 8 - bits ) ; # elif ( __SIZEOF_SIZE_T__ <= __SIZEOF_LONG__ ) return ( h * 2654435769lu ) >> ( __SIZEOF_LONG__ * 8 - bits ) ; # else # error \"Unsupported size_t size\" # endif }",
    "resources/libbpf/src/btf.c@str_hash": "static inline size_t str_hash ( const char * s ) { size_t h = 0 ; while ( * s ) { h = h * 31 + * s ; s ++ ; } return h ; }",
    "resources/libbpf/src/btf.c@ptr_to_u64": "static inline __u64 ptr_to_u64 ( const void * ptr ) { return ( __u64 ) ( unsigned long ) ptr ; }",
    "resources/libbpf/src/btf.c@libbpf_add_mem": "void * libbpf_add_mem ( void * * data , size_t * cap_cnt , size_t elem_sz , size_t cur_cnt , size_t max_cnt , size_t add_cnt ) { size_t new_cnt ; void * new_data ; if ( cur_cnt + add_cnt <= * cap_cnt ) return * data + cur_cnt * elem_sz ; /* requested more than the set limit */ if ( cur_cnt + add_cnt > max_cnt ) return NULL ; new_cnt = * cap_cnt ; new_cnt += new_cnt / 4 ; /* expand by 25% */ if ( new_cnt < 16 ) /* but at least 16 elements */ new_cnt = 16 ; if ( new_cnt > max_cnt ) /* but not exceeding a set limit */ new_cnt = max_cnt ; if ( new_cnt < cur_cnt + add_cnt ) /* also ensure we have enough memory */ new_cnt = cur_cnt + add_cnt ; new_data = libbpf_reallocarray ( * data , new_cnt , elem_sz ) ; if ( ! new_data ) return NULL ; /* zero out newly allocated portion of memory */ memset ( new_data + ( * cap_cnt ) * elem_sz , 0 , ( new_cnt - * cap_cnt ) * elem_sz ) ; * data = new_data ; * cap_cnt = new_cnt ; return new_data + cur_cnt * elem_sz ; }",
    "resources/libbpf/src/btf.c@libbpf_ensure_mem": "int libbpf_ensure_mem ( void * * data , size_t * cap_cnt , size_t elem_sz , size_t need_cnt ) { void * p ; if ( need_cnt <= * cap_cnt ) return 0 ; p = libbpf_add_mem ( data , cap_cnt , elem_sz , * cap_cnt , SIZE_MAX , need_cnt - * cap_cnt ) ; if ( ! p ) return - ENOMEM ; return 0 ; }",
    "resources/libbpf/src/btf.c@btf_add_type_offs_mem": "static void * btf_add_type_offs_mem ( struct btf * btf , size_t add_cnt ) { return libbpf_add_mem ( ( void * * ) & btf -> type_offs , & btf -> type_offs_cap , sizeof ( __u32 ) , btf -> nr_types , BTF_MAX_NR_TYPES , add_cnt ) ; }",
    "resources/libbpf/src/btf.c@btf_add_type_idx_entry": "static int btf_add_type_idx_entry ( struct btf * btf , __u32 type_off ) { __u32 * p ; p = btf_add_type_offs_mem ( btf , 1 ) ; if ( ! p ) return - ENOMEM ; * p = type_off ; return 0 ; }",
    "resources/libbpf/src/btf.c@btf_bswap_hdr": "static void btf_bswap_hdr ( struct btf_header * h ) { h -> magic = bswap_16 ( h -> magic ) ; h -> hdr_len = bswap_32 ( h -> hdr_len ) ; h -> type_off = bswap_32 ( h -> type_off ) ; h -> type_len = bswap_32 ( h -> type_len ) ; h -> str_off = bswap_32 ( h -> str_off ) ; h -> str_len = bswap_32 ( h -> str_len ) ; }",
    "resources/libbpf/src/btf.c@btf_parse_hdr": "static int btf_parse_hdr ( struct btf * btf ) { struct btf_header * hdr = btf -> hdr ; __u32 meta_left ; if ( btf -> raw_size < sizeof ( struct btf_header ) ) { pr_debug ( \"BTF header not found\\n\" ) ; return - EINVAL ; } if ( hdr -> magic == bswap_16 ( BTF_MAGIC ) ) { btf -> swapped_endian = true ; if ( bswap_32 ( hdr -> hdr_len ) != sizeof ( struct btf_header ) ) { pr_warn ( \"Can't load BTF with non-native endianness due to unsupported header length %u\\n\" , bswap_32 ( hdr -> hdr_len ) ) ; return - ENOTSUP ; } btf_bswap_hdr ( hdr ) ; } else if ( hdr -> magic != BTF_MAGIC ) { pr_debug ( \"Invalid BTF magic: %x\\n\" , hdr -> magic ) ; return - EINVAL ; } if ( btf -> raw_size < hdr -> hdr_len ) { pr_debug ( \"BTF header len %u larger than data size %u\\n\" , hdr -> hdr_len , btf -> raw_size ) ; return - EINVAL ; } meta_left = btf -> raw_size - hdr -> hdr_len ; if ( meta_left < ( long long ) hdr -> str_off + hdr -> str_len ) { pr_debug ( \"Invalid BTF total size: %u\\n\" , btf -> raw_size ) ; return - EINVAL ; } if ( ( long long ) hdr -> type_off + hdr -> type_len > hdr -> str_off ) { pr_debug ( \"Invalid BTF data sections layout: type data at %u + %u, strings data at %u + %u\\n\" , hdr -> type_off , hdr -> type_len , hdr -> str_off , hdr -> str_len ) ; return - EINVAL ; } if ( hdr -> type_off % 4 ) { pr_debug ( \"BTF type section is not aligned to 4 bytes\\n\" ) ; return - EINVAL ; } return 0 ; }",
    "resources/libbpf/src/btf.c@btf_parse_str_sec": "static int btf_parse_str_sec ( struct btf * btf ) { const struct btf_header * hdr = btf -> hdr ; const char * start = btf -> strs_data ; const char * end = start + btf -> hdr -> str_len ; if ( btf -> base_btf && hdr -> str_len == 0 ) return 0 ; if ( ! hdr -> str_len || hdr -> str_len - 1 > BTF_MAX_STR_OFFSET || end [ - 1 ] ) { pr_debug ( \"Invalid BTF string section\\n\" ) ; return - EINVAL ; } if ( ! btf -> base_btf && start [ 0 ] ) { pr_debug ( \"Invalid BTF string section\\n\" ) ; return - EINVAL ; } return 0 ; }",
    "resources/libbpf/src/btf.c@btf_type_size": "static int btf_type_size ( const struct btf_type * t ) { const int base_size = sizeof ( struct btf_type ) ; __u16 vlen = btf_vlen ( t ) ; switch ( btf_kind ( t ) ) { case BTF_KIND_FWD : case BTF_KIND_CONST : case BTF_KIND_VOLATILE : case BTF_KIND_RESTRICT : case BTF_KIND_PTR : case BTF_KIND_TYPEDEF : case BTF_KIND_FUNC : case BTF_KIND_FLOAT : case BTF_KIND_TYPE_TAG : return base_size ; case BTF_KIND_INT : return base_size + sizeof ( __u32 ) ; case BTF_KIND_ENUM : return base_size + vlen * sizeof ( struct btf_enum ) ; case BTF_KIND_ENUM64 : return base_size + vlen * sizeof ( struct btf_enum64 ) ; case BTF_KIND_ARRAY : return base_size + sizeof ( struct btf_array ) ; case BTF_KIND_STRUCT : case BTF_KIND_UNION : return base_size + vlen * sizeof ( struct btf_member ) ; case BTF_KIND_FUNC_PROTO : return base_size + vlen * sizeof ( struct btf_param ) ; case BTF_KIND_VAR : return base_size + sizeof ( struct btf_var ) ; case BTF_KIND_DATASEC : return base_size + vlen * sizeof ( struct btf_var_secinfo ) ; case BTF_KIND_DECL_TAG : return base_size + sizeof ( struct btf_decl_tag ) ; default : pr_debug ( \"Unsupported BTF_KIND:%u\\n\" , btf_kind ( t ) ) ; return - EINVAL ; } }",
    "resources/libbpf/src/btf.c@btf_bswap_type_base": "static void btf_bswap_type_base ( struct btf_type * t ) { t -> name_off = bswap_32 ( t -> name_off ) ; t -> info = bswap_32 ( t -> info ) ; t -> type = bswap_32 ( t -> type ) ; }",
    "resources/libbpf/src/btf.c@btf_bswap_type_rest": "static int btf_bswap_type_rest ( struct btf_type * t ) { struct btf_var_secinfo * v ; struct btf_enum64 * e64 ; struct btf_member * m ; struct btf_array * a ; struct btf_param * p ; struct btf_enum * e ; __u16 vlen = btf_vlen ( t ) ; int i ; switch ( btf_kind ( t ) ) { case BTF_KIND_FWD : case BTF_KIND_CONST : case BTF_KIND_VOLATILE : case BTF_KIND_RESTRICT : case BTF_KIND_PTR : case BTF_KIND_TYPEDEF : case BTF_KIND_FUNC : case BTF_KIND_FLOAT : case BTF_KIND_TYPE_TAG : return 0 ; case BTF_KIND_INT : * ( __u32 * ) ( t + 1 ) = bswap_32 ( * ( __u32 * ) ( t + 1 ) ) ; return 0 ; case BTF_KIND_ENUM : for ( i = 0 , e = btf_enum ( t ) ; i < vlen ; i ++ , e ++ ) { e -> name_off = bswap_32 ( e -> name_off ) ; e -> val = bswap_32 ( e -> val ) ; } return 0 ; case BTF_KIND_ENUM64 : for ( i = 0 , e64 = btf_enum64 ( t ) ; i < vlen ; i ++ , e64 ++ ) { e64 -> name_off = bswap_32 ( e64 -> name_off ) ; e64 -> val_lo32 = bswap_32 ( e64 -> val_lo32 ) ; e64 -> val_hi32 = bswap_32 ( e64 -> val_hi32 ) ; } return 0 ; case BTF_KIND_ARRAY : a = btf_array ( t ) ; a -> type = bswap_32 ( a -> type ) ; a -> index_type = bswap_32 ( a -> index_type ) ; a -> nelems = bswap_32 ( a -> nelems ) ; return 0 ; case BTF_KIND_STRUCT : case BTF_KIND_UNION : for ( i = 0 , m = btf_members ( t ) ; i < vlen ; i ++ , m ++ ) { m -> name_off = bswap_32 ( m -> name_off ) ; m -> type = bswap_32 ( m -> type ) ; m -> offset = bswap_32 ( m -> offset ) ; } return 0 ; case BTF_KIND_FUNC_PROTO : for ( i = 0 , p = btf_params ( t ) ; i < vlen ; i ++ , p ++ ) { p -> name_off = bswap_32 ( p -> name_off ) ; p -> type = bswap_32 ( p -> type ) ; } return 0 ; case BTF_KIND_VAR : btf_var ( t ) -> linkage = bswap_32 ( btf_var ( t ) -> linkage ) ; return 0 ; case BTF_KIND_DATASEC : for ( i = 0 , v = btf_var_secinfos ( t ) ; i < vlen ; i ++ , v ++ ) { v -> type = bswap_32 ( v -> type ) ; v -> offset = bswap_32 ( v -> offset ) ; v -> size = bswap_32 ( v -> size ) ; } return 0 ; case BTF_KIND_DECL_TAG : btf_decl_tag ( t ) -> component_idx = bswap_32 ( btf_decl_tag ( t ) -> component_idx ) ; return 0 ; default : pr_debug ( \"Unsupported BTF_KIND:%u\\n\" , btf_kind ( t ) ) ; return - EINVAL ; } }",
    "resources/libbpf/src/btf.c@btf_parse_type_sec": "static int btf_parse_type_sec ( struct btf * btf ) { struct btf_header * hdr = btf -> hdr ; void * next_type = btf -> types_data ; void * end_type = next_type + hdr -> type_len ; int err , type_size ; while ( next_type + sizeof ( struct btf_type ) <= end_type ) { if ( btf -> swapped_endian ) btf_bswap_type_base ( next_type ) ; type_size = btf_type_size ( next_type ) ; if ( type_size < 0 ) return type_size ; if ( next_type + type_size > end_type ) { pr_warn ( \"BTF type [%d] is malformed\\n\" , btf -> start_id + btf -> nr_types ) ; return - EINVAL ; } if ( btf -> swapped_endian && btf_bswap_type_rest ( next_type ) ) return - EINVAL ; err = btf_add_type_idx_entry ( btf , next_type - btf -> types_data ) ; if ( err ) return err ; next_type += type_size ; btf -> nr_types ++ ; } if ( next_type != end_type ) { pr_warn ( \"BTF types data is malformed\\n\" ) ; return - EINVAL ; } return 0 ; }",
    "resources/libbpf/src/btf.c@btf_validate_str": "static int btf_validate_str ( const struct btf * btf , __u32 str_off , const char * what , __u32 type_id ) { const char * s ; s = btf__str_by_offset ( btf , str_off ) ; if ( ! s ) { pr_warn ( \"btf: type [%u]: invalid %s (string offset %u)\\n\" , type_id , what , str_off ) ; return - EINVAL ; } return 0 ; }",
    "resources/libbpf/src/btf.c@btf_validate_id": "static int btf_validate_id ( const struct btf * btf , __u32 id , __u32 ctx_id ) { const struct btf_type * t ; t = btf__type_by_id ( btf , id ) ; if ( ! t ) { pr_warn ( \"btf: type [%u]: invalid referenced type ID %u\\n\" , ctx_id , id ) ; return - EINVAL ; } return 0 ; }",
    "resources/libbpf/src/btf.c@btf_validate_type": "static int btf_validate_type ( const struct btf * btf , const struct btf_type * t , __u32 id ) { __u32 kind = btf_kind ( t ) ; int err , i , n ; err = btf_validate_str ( btf , t -> name_off , \"type name\" , id ) ; if ( err ) return err ; switch ( kind ) { case BTF_KIND_UNKN : case BTF_KIND_INT : case BTF_KIND_FWD : case BTF_KIND_FLOAT : break ; case BTF_KIND_PTR : case BTF_KIND_TYPEDEF : case BTF_KIND_VOLATILE : case BTF_KIND_CONST : case BTF_KIND_RESTRICT : case BTF_KIND_VAR : case BTF_KIND_DECL_TAG : case BTF_KIND_TYPE_TAG : err = btf_validate_id ( btf , t -> type , id ) ; if ( err ) return err ; break ; case BTF_KIND_ARRAY : { const struct btf_array * a = btf_array ( t ) ; err = btf_validate_id ( btf , a -> type , id ) ; err = err ? : btf_validate_id ( btf , a -> index_type , id ) ; if ( err ) return err ; break ; } case BTF_KIND_STRUCT : case BTF_KIND_UNION : { const struct btf_member * m = btf_members ( t ) ; n = btf_vlen ( t ) ; for ( i = 0 ; i < n ; i ++ , m ++ ) { err = btf_validate_str ( btf , m -> name_off , \"field name\" , id ) ; err = err ? : btf_validate_id ( btf , m -> type , id ) ; if ( err ) return err ; } break ; } case BTF_KIND_ENUM : { const struct btf_enum * m = btf_enum ( t ) ; n = btf_vlen ( t ) ; for ( i = 0 ; i < n ; i ++ , m ++ ) { err = btf_validate_str ( btf , m -> name_off , \"enum name\" , id ) ; if ( err ) return err ; } break ; } case BTF_KIND_ENUM64 : { const struct btf_enum64 * m = btf_enum64 ( t ) ; n = btf_vlen ( t ) ; for ( i = 0 ; i < n ; i ++ , m ++ ) { err = btf_validate_str ( btf , m -> name_off , \"enum name\" , id ) ; if ( err ) return err ; } break ; } case BTF_KIND_FUNC : { const struct btf_type * ft ; err = btf_validate_id ( btf , t -> type , id ) ; if ( err ) return err ; ft = btf__type_by_id ( btf , t -> type ) ; if ( btf_kind ( ft ) != BTF_KIND_FUNC_PROTO ) { pr_warn ( \"btf: type [%u]: referenced type [%u] is not FUNC_PROTO\\n\" , id , t -> type ) ; return - EINVAL ; } break ; } case BTF_KIND_FUNC_PROTO : { const struct btf_param * m = btf_params ( t ) ; n = btf_vlen ( t ) ; for ( i = 0 ; i < n ; i ++ , m ++ ) { err = btf_validate_str ( btf , m -> name_off , \"param name\" , id ) ; err = err ? : btf_validate_id ( btf , m -> type , id ) ; if ( err ) return err ; } break ; } case BTF_KIND_DATASEC : { const struct btf_var_secinfo * m = btf_var_secinfos ( t ) ; n = btf_vlen ( t ) ; for ( i = 0 ; i < n ; i ++ , m ++ ) { err = btf_validate_id ( btf , m -> type , id ) ; if ( err ) return err ; } break ; } default : pr_warn ( \"btf: type [%u]: unrecognized kind %u\\n\" , id , kind ) ; return - EINVAL ; } return 0 ; }",
    "resources/libbpf/src/btf.c@btf_sanity_check": "static int btf_sanity_check ( const struct btf * btf ) { const struct btf_type * t ; __u32 i , n = btf__type_cnt ( btf ) ; int err ; for ( i = 1 ; i < n ; i ++ ) { t = btf_type_by_id ( btf , i ) ; err = btf_validate_type ( btf , t , i ) ; if ( err ) return err ; } return 0 ; }",
    "resources/libbpf/src/btf.c@btf__type_cnt": "__u32 btf__type_cnt ( const struct btf * btf ) { return btf -> start_id + btf -> nr_types ; }",
    "resources/libbpf/src/btf.c@btf__base_btf": "const struct btf * btf__base_btf ( const struct btf * btf ) { return btf -> base_btf ; }",
    "resources/libbpf/src/btf.c@btf_type_by_id": "struct btf_type * btf_type_by_id ( const struct btf * btf , __u32 type_id ) { if ( type_id == 0 ) return & btf_void ; if ( type_id < btf -> start_id ) return btf_type_by_id ( btf -> base_btf , type_id ) ; return btf -> types_data + btf -> type_offs [ type_id - btf -> start_id ] ; }",
    "resources/libbpf/src/btf.c@btf__type_by_id": "const struct btf_type * btf__type_by_id ( const struct btf * btf , __u32 type_id ) { if ( type_id >= btf -> start_id + btf -> nr_types ) return errno = EINVAL , NULL ; return btf_type_by_id ( ( struct btf * ) btf , type_id ) ; }",
    "resources/libbpf/src/btf.c@determine_ptr_size": "static int determine_ptr_size ( const struct btf * btf ) { static const char * const long_aliases [ ] = { \"long\" , \"long int\" , \"int long\" , \"unsigned long\" , \"long unsigned\" , \"unsigned long int\" , \"unsigned int long\" , \"long unsigned int\" , \"long int unsigned\" , \"int unsigned long\" , \"int long unsigned\" , } ; const struct btf_type * t ; const char * name ; int i , j , n ; if ( btf -> base_btf && btf -> base_btf -> ptr_sz > 0 ) return btf -> base_btf -> ptr_sz ; n = btf__type_cnt ( btf ) ; for ( i = 1 ; i < n ; i ++ ) { t = btf__type_by_id ( btf , i ) ; if ( ! btf_is_int ( t ) ) continue ; if ( t -> size != 4 && t -> size != 8 ) continue ; name = btf__name_by_offset ( btf , t -> name_off ) ; if ( ! name ) continue ; for ( j = 0 ; j < ARRAY_SIZE ( long_aliases ) ; j ++ ) { if ( strcmp ( name , long_aliases [ j ] ) == 0 ) return t -> size ; } } return - 1 ; }",
    "resources/libbpf/src/btf.c@btf_ptr_sz": "static size_t btf_ptr_sz ( const struct btf * btf ) { if ( ! btf -> ptr_sz ) ( ( struct btf * ) btf ) -> ptr_sz = determine_ptr_size ( btf ) ; return btf -> ptr_sz < 0 ? sizeof ( void * ) : btf -> ptr_sz ; }",
    "resources/libbpf/src/btf.c@btf__pointer_size": "size_t btf__pointer_size ( const struct btf * btf ) { if ( ! btf -> ptr_sz ) ( ( struct btf * ) btf ) -> ptr_sz = determine_ptr_size ( btf ) ; if ( btf -> ptr_sz < 0 ) /* not enough BTF type info to guess */ return 0 ; return btf -> ptr_sz ; }",
    "resources/libbpf/src/btf.c@btf__set_pointer_size": "int btf__set_pointer_size ( struct btf * btf , size_t ptr_sz ) { if ( ptr_sz != 4 && ptr_sz != 8 ) return libbpf_err ( - EINVAL ) ; btf -> ptr_sz = ptr_sz ; return 0 ; }",
    "resources/libbpf/src/btf.c@is_host_big_endian": "static bool is_host_big_endian ( void ) { # if __BYTE_ORDER__ == __ORDER_LITTLE_ENDIAN__ return false ; # elif __BYTE_ORDER__ == __ORDER_BIG_ENDIAN__ return true ; # else # error \"Unrecognized __BYTE_ORDER__\" # endif }",
    "resources/libbpf/src/btf.c@btf__endianness": "enum btf_endianness btf__endianness ( const struct btf * btf ) { if ( is_host_big_endian ( ) ) return btf -> swapped_endian ? BTF_LITTLE_ENDIAN : BTF_BIG_ENDIAN ; else return btf -> swapped_endian ? BTF_BIG_ENDIAN : BTF_LITTLE_ENDIAN ; }",
    "resources/libbpf/src/btf.c@btf__set_endianness": "int btf__set_endianness ( struct btf * btf , enum btf_endianness endian ) { if ( endian != BTF_LITTLE_ENDIAN && endian != BTF_BIG_ENDIAN ) return libbpf_err ( - EINVAL ) ; btf -> swapped_endian = is_host_big_endian ( ) != ( endian == BTF_BIG_ENDIAN ) ; if ( ! btf -> swapped_endian ) { free ( btf -> raw_data_swapped ) ; btf -> raw_data_swapped = NULL ; } return 0 ; }",
    "resources/libbpf/src/btf.c@btf_type_is_void": "static bool btf_type_is_void ( const struct btf_type * t ) { return t == & btf_void || btf_is_fwd ( t ) ; }",
    "resources/libbpf/src/btf.c@btf_type_is_void_or_null": "static bool btf_type_is_void_or_null ( const struct btf_type * t ) { return ! t || btf_type_is_void ( t ) ; }",
    "resources/libbpf/src/btf.c@btf__resolve_size": "__s64 btf__resolve_size ( const struct btf * btf , __u32 type_id ) { const struct btf_array * array ; const struct btf_type * t ; __u32 nelems = 1 ; __s64 size = - 1 ; int i ; t = btf__type_by_id ( btf , type_id ) ; for ( i = 0 ; i < MAX_RESOLVE_DEPTH && ! btf_type_is_void_or_null ( t ) ; i ++ ) { switch ( btf_kind ( t ) ) { case BTF_KIND_INT : case BTF_KIND_STRUCT : case BTF_KIND_UNION : case BTF_KIND_ENUM : case BTF_KIND_ENUM64 : case BTF_KIND_DATASEC : case BTF_KIND_FLOAT : size = t -> size ; goto done ; case BTF_KIND_PTR : size = btf_ptr_sz ( btf ) ; goto done ; case BTF_KIND_TYPEDEF : case BTF_KIND_VOLATILE : case BTF_KIND_CONST : case BTF_KIND_RESTRICT : case BTF_KIND_VAR : case BTF_KIND_DECL_TAG : case BTF_KIND_TYPE_TAG : type_id = t -> type ; break ; case BTF_KIND_ARRAY : array = btf_array ( t ) ; if ( nelems && array -> nelems > UINT32_MAX / nelems ) return libbpf_err ( - E2BIG ) ; nelems *= array -> nelems ; type_id = array -> type ; break ; default : return libbpf_err ( - EINVAL ) ; } t = btf__type_by_id ( btf , type_id ) ; } done : if ( size < 0 ) return libbpf_err ( - EINVAL ) ; if ( nelems && size > UINT32_MAX / nelems ) return libbpf_err ( - E2BIG ) ; return nelems * size ; }",
    "resources/libbpf/src/btf.c@btf__align_of": "int btf__align_of ( const struct btf * btf , __u32 id ) { const struct btf_type * t = btf__type_by_id ( btf , id ) ; __u16 kind = btf_kind ( t ) ; switch ( kind ) { case BTF_KIND_INT : case BTF_KIND_ENUM : case BTF_KIND_ENUM64 : case BTF_KIND_FLOAT : return min ( btf_ptr_sz ( btf ) , ( size_t ) t -> size ) ; case BTF_KIND_PTR : return btf_ptr_sz ( btf ) ; case BTF_KIND_TYPEDEF : case BTF_KIND_VOLATILE : case BTF_KIND_CONST : case BTF_KIND_RESTRICT : case BTF_KIND_TYPE_TAG : return btf__align_of ( btf , t -> type ) ; case BTF_KIND_ARRAY : return btf__align_of ( btf , btf_array ( t ) -> type ) ; case BTF_KIND_STRUCT : case BTF_KIND_UNION : { const struct btf_member * m = btf_members ( t ) ; __u16 vlen = btf_vlen ( t ) ; int i , max_align = 1 , align ; for ( i = 0 ; i < vlen ; i ++ , m ++ ) { align = btf__align_of ( btf , m -> type ) ; if ( align <= 0 ) return libbpf_err ( align ) ; max_align = max ( max_align , align ) ; /* if field offset isn't aligned according to field\n\t\t\t * type's alignment, then struct must be packed\n\t\t\t */ if ( btf_member_bitfield_size ( t , i ) == 0 && ( m -> offset % ( 8 * align ) ) != 0 ) return 1 ; } /* if struct/union size isn't a multiple of its alignment,\n\t\t * then struct must be packed\n\t\t */ if ( ( t -> size % max_align ) != 0 ) return 1 ; return max_align ; } default : pr_warn ( \"unsupported BTF_KIND:%u\\n\" , btf_kind ( t ) ) ; return errno = EINVAL , 0 ; } }",
    "resources/libbpf/src/btf.c@btf__resolve_type": "int btf__resolve_type ( const struct btf * btf , __u32 type_id ) { const struct btf_type * t ; int depth = 0 ; t = btf__type_by_id ( btf , type_id ) ; while ( depth < MAX_RESOLVE_DEPTH && ! btf_type_is_void_or_null ( t ) && ( btf_is_mod ( t ) || btf_is_typedef ( t ) || btf_is_var ( t ) ) ) { type_id = t -> type ; t = btf__type_by_id ( btf , type_id ) ; depth ++ ; } if ( depth == MAX_RESOLVE_DEPTH || btf_type_is_void_or_null ( t ) ) return libbpf_err ( - EINVAL ) ; return type_id ; }",
    "resources/libbpf/src/btf.c@btf__find_by_name": "__s32 btf__find_by_name ( const struct btf * btf , const char * type_name ) { __u32 i , nr_types = btf__type_cnt ( btf ) ; if ( ! strcmp ( type_name , \"void\" ) ) return 0 ; for ( i = 1 ; i < nr_types ; i ++ ) { const struct btf_type * t = btf__type_by_id ( btf , i ) ; const char * name = btf__name_by_offset ( btf , t -> name_off ) ; if ( name && ! strcmp ( type_name , name ) ) return i ; } return libbpf_err ( - ENOENT ) ; }",
    "resources/libbpf/src/btf.c@btf_find_by_name_kind": "static __s32 btf_find_by_name_kind ( const struct btf * btf , int start_id , const char * type_name , __u32 kind ) { __u32 i , nr_types = btf__type_cnt ( btf ) ; if ( kind == BTF_KIND_UNKN || ! strcmp ( type_name , \"void\" ) ) return 0 ; for ( i = start_id ; i < nr_types ; i ++ ) { const struct btf_type * t = btf__type_by_id ( btf , i ) ; const char * name ; if ( btf_kind ( t ) != kind ) continue ; name = btf__name_by_offset ( btf , t -> name_off ) ; if ( name && ! strcmp ( type_name , name ) ) return i ; } return libbpf_err ( - ENOENT ) ; }",
    "resources/libbpf/src/btf.c@btf__find_by_name_kind_own": "__s32 btf__find_by_name_kind_own ( const struct btf * btf , const char * type_name , __u32 kind ) { return btf_find_by_name_kind ( btf , btf -> start_id , type_name , kind ) ; }",
    "resources/libbpf/src/btf.c@btf__find_by_name_kind": "__s32 btf__find_by_name_kind ( const struct btf * btf , const char * type_name , __u32 kind ) { return btf_find_by_name_kind ( btf , 1 , type_name , kind ) ; }",
    "resources/libbpf/src/btf.c@btf_is_modifiable": "static bool btf_is_modifiable ( const struct btf * btf ) { return ( void * ) btf -> hdr != btf -> raw_data ; }",
    "resources/libbpf/src/btf.c@btf__free": "void btf__free ( struct btf * btf ) { if ( IS_ERR_OR_NULL ( btf ) ) return ; if ( btf -> fd >= 0 ) close ( btf -> fd ) ; if ( btf_is_modifiable ( btf ) ) { /* if BTF was modified after loading, it will have a split\n\t\t * in-memory representation for header, types, and strings\n\t\t * sections, so we need to free all of them individually. It\n\t\t * might still have a cached contiguous raw data present,\n\t\t * which will be unconditionally freed below.\n\t\t */ free ( btf -> hdr ) ; free ( btf -> types_data ) ; strset__free ( btf -> strs_set ) ; } free ( btf -> raw_data ) ; free ( btf -> raw_data_swapped ) ; free ( btf -> type_offs ) ; free ( btf ) ; }",
    "resources/libbpf/src/btf.c@btf_new_empty": "static struct btf * btf_new_empty ( struct btf * base_btf ) { struct btf * btf ; btf = calloc ( 1 , sizeof ( * btf ) ) ; if ( ! btf ) return ERR_PTR ( - ENOMEM ) ; btf -> nr_types = 0 ; btf -> start_id = 1 ; btf -> start_str_off = 0 ; btf -> fd = - 1 ; btf -> ptr_sz = sizeof ( void * ) ; btf -> swapped_endian = false ; if ( base_btf ) { btf -> base_btf = base_btf ; btf -> start_id = btf__type_cnt ( base_btf ) ; btf -> start_str_off = base_btf -> hdr -> str_len ; } /* +1 for empty string at offset 0 */ btf -> raw_size = sizeof ( struct btf_header ) + ( base_btf ? 0 : 1 ) ; btf -> raw_data = calloc ( 1 , btf -> raw_size ) ; if ( ! btf -> raw_data ) { free ( btf ) ; return ERR_PTR ( - ENOMEM ) ; } btf -> hdr = btf -> raw_data ; btf -> hdr -> hdr_len = sizeof ( struct btf_header ) ; btf -> hdr -> magic = BTF_MAGIC ; btf -> hdr -> version = BTF_VERSION ; btf -> types_data = btf -> raw_data + btf -> hdr -> hdr_len ; btf -> strs_data = btf -> raw_data + btf -> hdr -> hdr_len ; btf -> hdr -> str_len = base_btf ? 0 : 1 ; /* empty string at offset 0 */ return btf ; }",
    "resources/libbpf/src/btf.c@btf__new_empty": "struct btf * btf__new_empty ( void ) { return libbpf_ptr ( btf_new_empty ( NULL ) ) ; }",
    "resources/libbpf/src/btf.c@btf__new_empty_split": "struct btf * btf__new_empty_split ( struct btf * base_btf ) { return libbpf_ptr ( btf_new_empty ( base_btf ) ) ; }",
    "resources/libbpf/src/btf.c@btf_new": "static struct btf * btf_new ( const void * data , __u32 size , struct btf * base_btf ) { struct btf * btf ; int err ; btf = calloc ( 1 , sizeof ( struct btf ) ) ; if ( ! btf ) return ERR_PTR ( - ENOMEM ) ; btf -> nr_types = 0 ; btf -> start_id = 1 ; btf -> start_str_off = 0 ; btf -> fd = - 1 ; if ( base_btf ) { btf -> base_btf = base_btf ; btf -> start_id = btf__type_cnt ( base_btf ) ; btf -> start_str_off = base_btf -> hdr -> str_len ; } btf -> raw_data = malloc ( size ) ; if ( ! btf -> raw_data ) { err = - ENOMEM ; goto done ; } memcpy ( btf -> raw_data , data , size ) ; btf -> raw_size = size ; btf -> hdr = btf -> raw_data ; err = btf_parse_hdr ( btf ) ; if ( err ) goto done ; btf -> strs_data = btf -> raw_data + btf -> hdr -> hdr_len + btf -> hdr -> str_off ; btf -> types_data = btf -> raw_data + btf -> hdr -> hdr_len + btf -> hdr -> type_off ; err = btf_parse_str_sec ( btf ) ; err = err ? : btf_parse_type_sec ( btf ) ; err = err ? : btf_sanity_check ( btf ) ; if ( err ) goto done ; done : if ( err ) { btf__free ( btf ) ; return ERR_PTR ( err ) ; } return btf ; }",
    "resources/libbpf/src/btf.c@btf__new": "struct btf * btf__new ( const void * data , __u32 size ) { return libbpf_ptr ( btf_new ( data , size , NULL ) ) ; }",
    "resources/libbpf/src/btf.c@btf__new_split": "struct btf * btf__new_split ( const void * data , __u32 size , struct btf * base_btf ) { return libbpf_ptr ( btf_new ( data , size , base_btf ) ) ; }",
    "resources/libbpf/src/btf.c@btf_parse_elf": "static struct btf * btf_parse_elf ( const char * path , struct btf * base_btf , struct btf_ext * * btf_ext ) { Elf_Data * btf_data = NULL , * btf_ext_data = NULL ; int err = 0 , fd = - 1 , idx = 0 ; struct btf * btf = NULL ; Elf_Scn * scn = NULL ; Elf * elf = NULL ; GElf_Ehdr ehdr ; size_t shstrndx ; if ( elf_version ( EV_CURRENT ) == EV_NONE ) { pr_warn ( \"failed to init libelf for %s\\n\" , path ) ; return ERR_PTR ( - LIBBPF_ERRNO__LIBELF ) ; } fd = open ( path , O_RDONLY | O_CLOEXEC ) ; if ( fd < 0 ) { err = - errno ; pr_warn ( \"failed to open %s: %s\\n\" , path , strerror ( errno ) ) ; return ERR_PTR ( err ) ; } err = - LIBBPF_ERRNO__FORMAT ; elf = elf_begin ( fd , ELF_C_READ , NULL ) ; if ( ! elf ) { pr_warn ( \"failed to open %s as ELF file\\n\" , path ) ; goto done ; } if ( ! gelf_getehdr ( elf , & ehdr ) ) { pr_warn ( \"failed to get EHDR from %s\\n\" , path ) ; goto done ; } if ( elf_getshdrstrndx ( elf , & shstrndx ) ) { pr_warn ( \"failed to get section names section index for %s\\n\" , path ) ; goto done ; } if ( ! elf_rawdata ( elf_getscn ( elf , shstrndx ) , NULL ) ) { pr_warn ( \"failed to get e_shstrndx from %s\\n\" , path ) ; goto done ; } while ( ( scn = elf_nextscn ( elf , scn ) ) != NULL ) { GElf_Shdr sh ; char * name ; idx ++ ; if ( gelf_getshdr ( scn , & sh ) != & sh ) { pr_warn ( \"failed to get section(%d) header from %s\\n\" , idx , path ) ; goto done ; } name = elf_strptr ( elf , shstrndx , sh . sh_name ) ; if ( ! name ) { pr_warn ( \"failed to get section(%d) name from %s\\n\" , idx , path ) ; goto done ; } if ( strcmp ( name , BTF_ELF_SEC ) == 0 ) { btf_data = elf_getdata ( scn , 0 ) ; if ( ! btf_data ) { pr_warn ( \"failed to get section(%d, %s) data from %s\\n\" , idx , name , path ) ; goto done ; } continue ; } else if ( btf_ext && strcmp ( name , BTF_EXT_ELF_SEC ) == 0 ) { btf_ext_data = elf_getdata ( scn , 0 ) ; if ( ! btf_ext_data ) { pr_warn ( \"failed to get section(%d, %s) data from %s\\n\" , idx , name , path ) ; goto done ; } continue ; } } if ( ! btf_data ) { pr_warn ( \"failed to find '%s' ELF section in %s\\n\" , BTF_ELF_SEC , path ) ; err = - ENODATA ; goto done ; } btf = btf_new ( btf_data -> d_buf , btf_data -> d_size , base_btf ) ; err = libbpf_get_error ( btf ) ; if ( err ) goto done ; switch ( gelf_getclass ( elf ) ) { case ELFCLASS32 : btf__set_pointer_size ( btf , 4 ) ; break ; case ELFCLASS64 : btf__set_pointer_size ( btf , 8 ) ; break ; default : pr_warn ( \"failed to get ELF class (bitness) for %s\\n\" , path ) ; break ; } if ( btf_ext && btf_ext_data ) { * btf_ext = btf_ext__new ( btf_ext_data -> d_buf , btf_ext_data -> d_size ) ; err = libbpf_get_error ( * btf_ext ) ; if ( err ) goto done ; } else if ( btf_ext ) { * btf_ext = NULL ; } done : if ( elf ) elf_end ( elf ) ; close ( fd ) ; if ( ! err ) return btf ; if ( btf_ext ) btf_ext__free ( * btf_ext ) ; btf__free ( btf ) ; return ERR_PTR ( err ) ; }",
    "resources/libbpf/src/btf.c@btf__parse_elf": "struct btf * btf__parse_elf ( const char * path , struct btf_ext * * btf_ext ) { return libbpf_ptr ( btf_parse_elf ( path , NULL , btf_ext ) ) ; }",
    "resources/libbpf/src/btf.c@btf__parse_elf_split": "struct btf * btf__parse_elf_split ( const char * path , struct btf * base_btf ) { return libbpf_ptr ( btf_parse_elf ( path , base_btf , NULL ) ) ; }",
    "resources/libbpf/src/btf.c@btf_parse_raw": "static struct btf * btf_parse_raw ( const char * path , struct btf * base_btf ) { struct btf * btf = NULL ; void * data = NULL ; FILE * f = NULL ; __u16 magic ; int err = 0 ; long sz ; f = fopen ( path , \"rbe\" ) ; if ( ! f ) { err = - errno ; goto err_out ; } /* check BTF magic */ if ( fread ( & magic , 1 , sizeof ( magic ) , f ) < sizeof ( magic ) ) { err = - EIO ; goto err_out ; } if ( magic != BTF_MAGIC && magic != bswap_16 ( BTF_MAGIC ) ) { /* definitely not a raw BTF */ err = - EPROTO ; goto err_out ; } /* get file size */ if ( fseek ( f , 0 , SEEK_END ) ) { err = - errno ; goto err_out ; } sz = ftell ( f ) ; if ( sz < 0 ) { err = - errno ; goto err_out ; } /* rewind to the start */ if ( fseek ( f , 0 , SEEK_SET ) ) { err = - errno ; goto err_out ; } /* pre-alloc memory and read all of BTF data */ data = malloc ( sz ) ; if ( ! data ) { err = - ENOMEM ; goto err_out ; } if ( fread ( data , 1 , sz , f ) < sz ) { err = - EIO ; goto err_out ; } /* finally parse BTF data */ btf = btf_new ( data , sz , base_btf ) ; err_out : free ( data ) ; if ( f ) fclose ( f ) ; return err ? ERR_PTR ( err ) : btf ; }",
    "resources/libbpf/src/btf.c@btf__parse_raw": "struct btf * btf__parse_raw ( const char * path ) { return libbpf_ptr ( btf_parse_raw ( path , NULL ) ) ; }",
    "resources/libbpf/src/btf.c@btf__parse_raw_split": "struct btf * btf__parse_raw_split ( const char * path , struct btf * base_btf ) { return libbpf_ptr ( btf_parse_raw ( path , base_btf ) ) ; }",
    "resources/libbpf/src/btf.c@btf_parse": "static struct btf * btf_parse ( const char * path , struct btf * base_btf , struct btf_ext * * btf_ext ) { struct btf * btf ; int err ; if ( btf_ext ) * btf_ext = NULL ; btf = btf_parse_raw ( path , base_btf ) ; err = libbpf_get_error ( btf ) ; if ( ! err ) return btf ; if ( err != - EPROTO ) return ERR_PTR ( err ) ; return btf_parse_elf ( path , base_btf , btf_ext ) ; }",
    "resources/libbpf/src/btf.c@btf__parse": "struct btf * btf__parse ( const char * path , struct btf_ext * * btf_ext ) { return libbpf_ptr ( btf_parse ( path , NULL , btf_ext ) ) ; }",
    "resources/libbpf/src/btf.c@btf__parse_split": "struct btf * btf__parse_split ( const char * path , struct btf * base_btf ) { return libbpf_ptr ( btf_parse ( path , base_btf , NULL ) ) ; }",
    "resources/libbpf/src/btf.c@btf_load_into_kernel": "int btf_load_into_kernel ( struct btf * btf , char * log_buf , size_t log_sz , __u32 log_level , int token_fd ) { LIBBPF_OPTS ( bpf_btf_load_opts , opts ) ; __u32 buf_sz = 0 , raw_size ; char * buf = NULL , * tmp ; void * raw_data ; int err = 0 ; if ( btf -> fd >= 0 ) return libbpf_err ( - EEXIST ) ; if ( log_sz && ! log_buf ) return libbpf_err ( - EINVAL ) ; /* cache native raw data representation */ raw_data = btf_get_raw_data ( btf , & raw_size , false ) ; if ( ! raw_data ) { err = - ENOMEM ; goto done ; } btf -> raw_size = raw_size ; btf -> raw_data = raw_data ; retry_load : /* if log_level is 0, we won't provide log_buf/log_size to the kernel,\n\t * initially. Only if BTF loading fails, we bump log_level to 1 and\n\t * retry, using either auto-allocated or custom log_buf. This way\n\t * non-NULL custom log_buf provides a buffer just in case, but hopes\n\t * for successful load and no need for log_buf.\n\t */ if ( log_level ) { /* if caller didn't provide custom log_buf, we'll keep\n\t\t * allocating our own progressively bigger buffers for BTF\n\t\t * verification log\n\t\t */ if ( ! log_buf ) { buf_sz = max ( ( __u32 ) BPF_LOG_BUF_SIZE , buf_sz * 2 ) ; tmp = realloc ( buf , buf_sz ) ; if ( ! tmp ) { err = - ENOMEM ; goto done ; } buf = tmp ; buf [ 0 ] = '\\0' ; } opts . log_buf = log_buf ? log_buf : buf ; opts . log_size = log_buf ? log_sz : buf_sz ; opts . log_level = log_level ; } opts . token_fd = token_fd ; if ( token_fd ) opts . btf_flags |= BPF_F_TOKEN_FD ; btf -> fd = bpf_btf_load ( raw_data , raw_size , & opts ) ; if ( btf -> fd < 0 ) { /* time to turn on verbose mode and try again */ if ( log_level == 0 ) { log_level = 1 ; goto retry_load ; } /* only retry if caller didn't provide custom log_buf, but\n\t\t * make sure we can never overflow buf_sz\n\t\t */ if ( ! log_buf && errno == ENOSPC && buf_sz <= UINT_MAX / 2 ) goto retry_load ; err = - errno ; pr_warn ( \"BTF loading error: %d\\n\" , err ) ; /* don't print out contents of custom log_buf */ if ( ! log_buf && buf [ 0 ] ) pr_warn ( \"-- BEGIN BTF LOAD LOG ---\\n%s\\n-- END BTF LOAD LOG --\\n\" , buf ) ; } done : free ( buf ) ; return libbpf_err ( err ) ; }",
    "resources/libbpf/src/btf.c@btf__load_into_kernel": "int btf__load_into_kernel ( struct btf * btf ) { return btf_load_into_kernel ( btf , NULL , 0 , 0 , 0 ) ; }",
    "resources/libbpf/src/btf.c@btf__fd": "int btf__fd ( const struct btf * btf ) { return btf -> fd ; }",
    "resources/libbpf/src/btf.c@btf__set_fd": "void btf__set_fd ( struct btf * btf , int fd ) { btf -> fd = fd ; }",
    "resources/libbpf/src/btf.c@btf_strs_data": "static const void * btf_strs_data ( const struct btf * btf ) { return btf -> strs_data ? btf -> strs_data : strset__data ( btf -> strs_set ) ; }",
    "resources/libbpf/src/btf.c@btf_get_raw_data": "static void * btf_get_raw_data ( const struct btf * btf , __u32 * size , bool swap_endian ) { struct btf_header * hdr = btf -> hdr ; struct btf_type * t ; void * data , * p ; __u32 data_sz ; int i ; data = swap_endian ? btf -> raw_data_swapped : btf -> raw_data ; if ( data ) { * size = btf -> raw_size ; return data ; } data_sz = hdr -> hdr_len + hdr -> type_len + hdr -> str_len ; data = calloc ( 1 , data_sz ) ; if ( ! data ) return NULL ; p = data ; memcpy ( p , hdr , hdr -> hdr_len ) ; if ( swap_endian ) btf_bswap_hdr ( p ) ; p += hdr -> hdr_len ; memcpy ( p , btf -> types_data , hdr -> type_len ) ; if ( swap_endian ) { for ( i = 0 ; i < btf -> nr_types ; i ++ ) { t = p + btf -> type_offs [ i ] ; /* btf_bswap_type_rest() relies on native t->info, so\n\t\t\t * we swap base type info after we swapped all the\n\t\t\t * additional information\n\t\t\t */ if ( btf_bswap_type_rest ( t ) ) goto err_out ; btf_bswap_type_base ( t ) ; } } p += hdr -> type_len ; memcpy ( p , btf_strs_data ( btf ) , hdr -> str_len ) ; p += hdr -> str_len ; * size = data_sz ; return data ; err_out : free ( data ) ; return NULL ; }",
    "resources/libbpf/src/btf.c@btf__raw_data": "const void * btf__raw_data ( const struct btf * btf_ro , __u32 * size ) { struct btf * btf = ( struct btf * ) btf_ro ; __u32 data_sz ; void * data ; data = btf_get_raw_data ( btf , & data_sz , btf -> swapped_endian ) ; if ( ! data ) return errno = ENOMEM , NULL ; btf -> raw_size = data_sz ; if ( btf -> swapped_endian ) btf -> raw_data_swapped = data ; else btf -> raw_data = data ; * size = data_sz ; return data ; }",
    "resources/libbpf/src/btf.c@btf__str_by_offset": "const char * btf__str_by_offset ( const struct btf * btf , __u32 offset ) { if ( offset < btf -> start_str_off ) return btf__str_by_offset ( btf -> base_btf , offset ) ; else if ( offset - btf -> start_str_off < btf -> hdr -> str_len ) return btf_strs_data ( btf ) + ( offset - btf -> start_str_off ) ; else return errno = EINVAL , NULL ; }",
    "resources/libbpf/src/btf.c@btf__name_by_offset": "const char * btf__name_by_offset ( const struct btf * btf , __u32 offset ) { return btf__str_by_offset ( btf , offset ) ; }",
    "resources/libbpf/src/btf.c@btf_get_from_fd": "struct btf * btf_get_from_fd ( int btf_fd , struct btf * base_btf ) { struct bpf_btf_info btf_info ; __u32 len = sizeof ( btf_info ) ; __u32 last_size ; struct btf * btf ; void * ptr ; int err ; /* we won't know btf_size until we call bpf_btf_get_info_by_fd(). so\n\t * let's start with a sane default - 4KiB here - and resize it only if\n\t * bpf_btf_get_info_by_fd() needs a bigger buffer.\n\t */ last_size = 4096 ; ptr = malloc ( last_size ) ; if ( ! ptr ) return ERR_PTR ( - ENOMEM ) ; memset ( & btf_info , 0 , sizeof ( btf_info ) ) ; btf_info . btf = ptr_to_u64 ( ptr ) ; btf_info . btf_size = last_size ; err = bpf_btf_get_info_by_fd ( btf_fd , & btf_info , & len ) ; if ( ! err && btf_info . btf_size > last_size ) { void * temp_ptr ; last_size = btf_info . btf_size ; temp_ptr = realloc ( ptr , last_size ) ; if ( ! temp_ptr ) { btf = ERR_PTR ( - ENOMEM ) ; goto exit_free ; } ptr = temp_ptr ; len = sizeof ( btf_info ) ; memset ( & btf_info , 0 , sizeof ( btf_info ) ) ; btf_info . btf = ptr_to_u64 ( ptr ) ; btf_info . btf_size = last_size ; err = bpf_btf_get_info_by_fd ( btf_fd , & btf_info , & len ) ; } if ( err || btf_info . btf_size > last_size ) { btf = err ? ERR_PTR ( - errno ) : ERR_PTR ( - E2BIG ) ; goto exit_free ; } btf = btf_new ( ptr , btf_info . btf_size , base_btf ) ; exit_free : free ( ptr ) ; return btf ; }",
    "resources/libbpf/src/btf.c@btf__load_from_kernel_by_id_split": "struct btf * btf__load_from_kernel_by_id_split ( __u32 id , struct btf * base_btf ) { struct btf * btf ; int btf_fd ; btf_fd = bpf_btf_get_fd_by_id ( id ) ; if ( btf_fd < 0 ) return libbpf_err_ptr ( - errno ) ; btf = btf_get_from_fd ( btf_fd , base_btf ) ; close ( btf_fd ) ; return libbpf_ptr ( btf ) ; }",
    "resources/libbpf/src/btf.c@btf__load_from_kernel_by_id": "struct btf * btf__load_from_kernel_by_id ( __u32 id ) { return btf__load_from_kernel_by_id_split ( id , NULL ) ; }",
    "resources/libbpf/src/btf.c@btf_invalidate_raw_data": "static void btf_invalidate_raw_data ( struct btf * btf ) { if ( btf -> raw_data ) { free ( btf -> raw_data ) ; btf -> raw_data = NULL ; } if ( btf -> raw_data_swapped ) { free ( btf -> raw_data_swapped ) ; btf -> raw_data_swapped = NULL ; } }",
    "resources/libbpf/src/btf.c@btf_ensure_modifiable": "static int btf_ensure_modifiable ( struct btf * btf ) { void * hdr , * types ; struct strset * set = NULL ; int err = - ENOMEM ; if ( btf_is_modifiable ( btf ) ) { /* any BTF modification invalidates raw_data */ btf_invalidate_raw_data ( btf ) ; return 0 ; } /* split raw data into three memory regions */ hdr = malloc ( btf -> hdr -> hdr_len ) ; types = malloc ( btf -> hdr -> type_len ) ; if ( ! hdr || ! types ) goto err_out ; memcpy ( hdr , btf -> hdr , btf -> hdr -> hdr_len ) ; memcpy ( types , btf -> types_data , btf -> hdr -> type_len ) ; /* build lookup index for all strings */ set = strset__new ( BTF_MAX_STR_OFFSET , btf -> strs_data , btf -> hdr -> str_len ) ; if ( IS_ERR ( set ) ) { err = PTR_ERR ( set ) ; goto err_out ; } /* only when everything was successful, update internal state */ btf -> hdr = hdr ; btf -> types_data = types ; btf -> types_data_cap = btf -> hdr -> type_len ; btf -> strs_data = NULL ; btf -> strs_set = set ; /* if BTF was created from scratch, all strings are guaranteed to be\n\t * unique and deduplicated\n\t */ if ( btf -> hdr -> str_len == 0 ) btf -> strs_deduped = true ; if ( ! btf -> base_btf && btf -> hdr -> str_len == 1 ) btf -> strs_deduped = true ; /* invalidate raw_data representation */ btf_invalidate_raw_data ( btf ) ; return 0 ; err_out : strset__free ( set ) ; free ( hdr ) ; free ( types ) ; return err ; }",
    "resources/libbpf/src/btf.c@btf__find_str": "int btf__find_str ( struct btf * btf , const char * s ) { int off ; if ( btf -> base_btf ) { off = btf__find_str ( btf -> base_btf , s ) ; if ( off != - ENOENT ) return off ; } /* BTF needs to be in a modifiable state to build string lookup index */ if ( btf_ensure_modifiable ( btf ) ) return libbpf_err ( - ENOMEM ) ; off = strset__find_str ( btf -> strs_set , s ) ; if ( off < 0 ) return libbpf_err ( off ) ; return btf -> start_str_off + off ; }",
    "resources/libbpf/src/btf.c@btf__add_str": "int btf__add_str ( struct btf * btf , const char * s ) { int off ; if ( btf -> base_btf ) { off = btf__find_str ( btf -> base_btf , s ) ; if ( off != - ENOENT ) return off ; } if ( btf_ensure_modifiable ( btf ) ) return libbpf_err ( - ENOMEM ) ; off = strset__add_str ( btf -> strs_set , s ) ; if ( off < 0 ) return libbpf_err ( off ) ; btf -> hdr -> str_len = strset__data_size ( btf -> strs_set ) ; return btf -> start_str_off + off ; }",
    "resources/libbpf/src/btf.c@btf_add_type_mem": "static void * btf_add_type_mem ( struct btf * btf , size_t add_sz ) { return libbpf_add_mem ( & btf -> types_data , & btf -> types_data_cap , 1 , btf -> hdr -> type_len , UINT_MAX , add_sz ) ; }",
    "resources/libbpf/src/btf.c@btf_type_inc_vlen": "static void btf_type_inc_vlen ( struct btf_type * t ) { t -> info = btf_type_info ( btf_kind ( t ) , btf_vlen ( t ) + 1 , btf_kflag ( t ) ) ; }",
    "resources/libbpf/src/btf.c@btf_commit_type": "static int btf_commit_type ( struct btf * btf , int data_sz ) { int err ; err = btf_add_type_idx_entry ( btf , btf -> hdr -> type_len ) ; if ( err ) return libbpf_err ( err ) ; btf -> hdr -> type_len += data_sz ; btf -> hdr -> str_off += data_sz ; btf -> nr_types ++ ; return btf -> start_id + btf -> nr_types - 1 ; }",
    "resources/libbpf/src/btf.c@btf_rewrite_str": "static int btf_rewrite_str ( __u32 * str_off , void * ctx ) { struct btf_pipe * p = ctx ; long mapped_off ; int off , err ; if ( ! * str_off ) /* nothing to do for empty strings */ return 0 ; if ( p -> str_off_map && hashmap__find ( p -> str_off_map , * str_off , & mapped_off ) ) { * str_off = mapped_off ; return 0 ; } off = btf__add_str ( p -> dst , btf__str_by_offset ( p -> src , * str_off ) ) ; if ( off < 0 ) return off ; /* Remember string mapping from src to dst.  It avoids\n\t * performing expensive string comparisons.\n\t */ if ( p -> str_off_map ) { err = hashmap__append ( p -> str_off_map , * str_off , off ) ; if ( err ) return err ; } * str_off = off ; return 0 ; }",
    "resources/libbpf/src/btf.c@btf__add_type": "int btf__add_type ( struct btf * btf , const struct btf * src_btf , const struct btf_type * src_type ) { struct btf_pipe p = { . src = src_btf , . dst = btf } ; struct btf_type * t ; int sz , err ; sz = btf_type_size ( src_type ) ; if ( sz < 0 ) return libbpf_err ( sz ) ; /* deconstruct BTF, if necessary, and invalidate raw_data */ if ( btf_ensure_modifiable ( btf ) ) return libbpf_err ( - ENOMEM ) ; t = btf_add_type_mem ( btf , sz ) ; if ( ! t ) return libbpf_err ( - ENOMEM ) ; memcpy ( t , src_type , sz ) ; err = btf_type_visit_str_offs ( t , btf_rewrite_str , & p ) ; if ( err ) return libbpf_err ( err ) ; return btf_commit_type ( btf , sz ) ; }",
    "resources/libbpf/src/btf.c@btf_rewrite_type_ids": "static int btf_rewrite_type_ids ( __u32 * type_id , void * ctx ) { struct btf * btf = ctx ; if ( ! * type_id ) /* nothing to do for VOID references */ return 0 ; /* we haven't updated btf's type count yet, so\n\t * btf->start_id + btf->nr_types - 1 is the type ID offset we should\n\t * add to all newly added BTF types\n\t */ * type_id += btf -> start_id + btf -> nr_types - 1 ; return 0 ; }",
    "resources/libbpf/src/btf.c@btf__add_btf": "int btf__add_btf ( struct btf * btf , const struct btf * src_btf ) { struct btf_pipe p = { . src = src_btf , . dst = btf } ; int data_sz , sz , cnt , i , err , old_strs_len ; __u32 * off ; void * t ; /* appending split BTF isn't supported yet */ if ( src_btf -> base_btf ) return libbpf_err ( - ENOTSUP ) ; /* deconstruct BTF, if necessary, and invalidate raw_data */ if ( btf_ensure_modifiable ( btf ) ) return libbpf_err ( - ENOMEM ) ; /* remember original strings section size if we have to roll back\n\t * partial strings section changes\n\t */ old_strs_len = btf -> hdr -> str_len ; data_sz = src_btf -> hdr -> type_len ; cnt = btf__type_cnt ( src_btf ) - 1 ; /* pre-allocate enough memory for new types */ t = btf_add_type_mem ( btf , data_sz ) ; if ( ! t ) return libbpf_err ( - ENOMEM ) ; /* pre-allocate enough memory for type offset index for new types */ off = btf_add_type_offs_mem ( btf , cnt ) ; if ( ! off ) return libbpf_err ( - ENOMEM ) ; /* Map the string offsets from src_btf to the offsets from btf to improve performance */ p . str_off_map = hashmap__new ( btf_dedup_identity_hash_fn , btf_dedup_equal_fn , NULL ) ; if ( IS_ERR ( p . str_off_map ) ) return libbpf_err ( - ENOMEM ) ; /* bulk copy types data for all types from src_btf */ memcpy ( t , src_btf -> types_data , data_sz ) ; for ( i = 0 ; i < cnt ; i ++ ) { sz = btf_type_size ( t ) ; if ( sz < 0 ) { /* unlikely, has to be corrupted src_btf */ err = sz ; goto err_out ; } /* fill out type ID to type offset mapping for lookups by type ID */ * off = t - btf -> types_data ; /* add, dedup, and remap strings referenced by this BTF type */ err = btf_type_visit_str_offs ( t , btf_rewrite_str , & p ) ; if ( err ) goto err_out ; /* remap all type IDs referenced from this BTF type */ err = btf_type_visit_type_ids ( t , btf_rewrite_type_ids , btf ) ; if ( err ) goto err_out ; /* go to next type data and type offset index entry */ t += sz ; off ++ ; } /* Up until now any of the copied type data was effectively invisible,\n\t * so if we exited early before this point due to error, BTF would be\n\t * effectively unmodified. There would be extra internal memory\n\t * pre-allocated, but it would not be available for querying.  But now\n\t * that we've copied and rewritten all the data successfully, we can\n\t * update type count and various internal offsets and sizes to\n\t * \"commit\" the changes and made them visible to the outside world.\n\t */ btf -> hdr -> type_len += data_sz ; btf -> hdr -> str_off += data_sz ; btf -> nr_types += cnt ; hashmap__free ( p . str_off_map ) ; /* return type ID of the first added BTF type */ return btf -> start_id + btf -> nr_types - cnt ; err_out : /* zero out preallocated memory as if it was just allocated with\n\t * libbpf_add_mem()\n\t */ memset ( btf -> types_data + btf -> hdr -> type_len , 0 , data_sz ) ; memset ( btf -> strs_data + old_strs_len , 0 , btf -> hdr -> str_len - old_strs_len ) ; /* and now restore original strings section size; types data size\n\t * wasn't modified, so doesn't need restoring, see big comment above\n\t */ btf -> hdr -> str_len = old_strs_len ; hashmap__free ( p . str_off_map ) ; return libbpf_err ( err ) ; }",
    "resources/libbpf/src/btf.c@btf__add_int": "int btf__add_int ( struct btf * btf , const char * name , size_t byte_sz , int encoding ) { struct btf_type * t ; int sz , name_off ; /* non-empty name */ if ( ! name || ! name [ 0 ] ) return libbpf_err ( - EINVAL ) ; /* byte_sz must be power of 2 */ if ( ! byte_sz || ( byte_sz & ( byte_sz - 1 ) ) || byte_sz > 16 ) return libbpf_err ( - EINVAL ) ; if ( encoding & ~ ( BTF_INT_SIGNED | BTF_INT_CHAR | BTF_INT_BOOL ) ) return libbpf_err ( - EINVAL ) ; /* deconstruct BTF, if necessary, and invalidate raw_data */ if ( btf_ensure_modifiable ( btf ) ) return libbpf_err ( - ENOMEM ) ; sz = sizeof ( struct btf_type ) + sizeof ( int ) ; t = btf_add_type_mem ( btf , sz ) ; if ( ! t ) return libbpf_err ( - ENOMEM ) ; /* if something goes wrong later, we might end up with an extra string,\n\t * but that shouldn't be a problem, because BTF can't be constructed\n\t * completely anyway and will most probably be just discarded\n\t */ name_off = btf__add_str ( btf , name ) ; if ( name_off < 0 ) return name_off ; t -> name_off = name_off ; t -> info = btf_type_info ( BTF_KIND_INT , 0 , 0 ) ; t -> size = byte_sz ; /* set INT info, we don't allow setting legacy bit offset/size */ * ( __u32 * ) ( t + 1 ) = ( encoding << 24 ) | ( byte_sz * 8 ) ; return btf_commit_type ( btf , sz ) ; }",
    "resources/libbpf/src/btf.c@btf__add_float": "int btf__add_float ( struct btf * btf , const char * name , size_t byte_sz ) { struct btf_type * t ; int sz , name_off ; /* non-empty name */ if ( ! name || ! name [ 0 ] ) return libbpf_err ( - EINVAL ) ; /* byte_sz must be one of the explicitly allowed values */ if ( byte_sz != 2 && byte_sz != 4 && byte_sz != 8 && byte_sz != 12 && byte_sz != 16 ) return libbpf_err ( - EINVAL ) ; if ( btf_ensure_modifiable ( btf ) ) return libbpf_err ( - ENOMEM ) ; sz = sizeof ( struct btf_type ) ; t = btf_add_type_mem ( btf , sz ) ; if ( ! t ) return libbpf_err ( - ENOMEM ) ; name_off = btf__add_str ( btf , name ) ; if ( name_off < 0 ) return name_off ; t -> name_off = name_off ; t -> info = btf_type_info ( BTF_KIND_FLOAT , 0 , 0 ) ; t -> size = byte_sz ; return btf_commit_type ( btf , sz ) ; }",
    "resources/libbpf/src/btf.c@validate_type_id": "static int validate_type_id ( int id ) { if ( id < 0 || id > BTF_MAX_NR_TYPES ) return - EINVAL ; return 0 ; }",
    "resources/libbpf/src/btf.c@btf_add_ref_kind": "static int btf_add_ref_kind ( struct btf * btf , int kind , const char * name , int ref_type_id ) { struct btf_type * t ; int sz , name_off = 0 ; if ( validate_type_id ( ref_type_id ) ) return libbpf_err ( - EINVAL ) ; if ( btf_ensure_modifiable ( btf ) ) return libbpf_err ( - ENOMEM ) ; sz = sizeof ( struct btf_type ) ; t = btf_add_type_mem ( btf , sz ) ; if ( ! t ) return libbpf_err ( - ENOMEM ) ; if ( name && name [ 0 ] ) { name_off = btf__add_str ( btf , name ) ; if ( name_off < 0 ) return name_off ; } t -> name_off = name_off ; t -> info = btf_type_info ( kind , 0 , 0 ) ; t -> type = ref_type_id ; return btf_commit_type ( btf , sz ) ; }",
    "resources/libbpf/src/btf.c@btf__add_ptr": "int btf__add_ptr ( struct btf * btf , int ref_type_id ) { return btf_add_ref_kind ( btf , BTF_KIND_PTR , NULL , ref_type_id ) ; }",
    "resources/libbpf/src/btf.c@btf__add_array": "int btf__add_array ( struct btf * btf , int index_type_id , int elem_type_id , __u32 nr_elems ) { struct btf_type * t ; struct btf_array * a ; int sz ; if ( validate_type_id ( index_type_id ) || validate_type_id ( elem_type_id ) ) return libbpf_err ( - EINVAL ) ; if ( btf_ensure_modifiable ( btf ) ) return libbpf_err ( - ENOMEM ) ; sz = sizeof ( struct btf_type ) + sizeof ( struct btf_array ) ; t = btf_add_type_mem ( btf , sz ) ; if ( ! t ) return libbpf_err ( - ENOMEM ) ; t -> name_off = 0 ; t -> info = btf_type_info ( BTF_KIND_ARRAY , 0 , 0 ) ; t -> size = 0 ; a = btf_array ( t ) ; a -> type = elem_type_id ; a -> index_type = index_type_id ; a -> nelems = nr_elems ; return btf_commit_type ( btf , sz ) ; }",
    "resources/libbpf/src/btf.c@btf_add_composite": "static int btf_add_composite ( struct btf * btf , int kind , const char * name , __u32 bytes_sz ) { struct btf_type * t ; int sz , name_off = 0 ; if ( btf_ensure_modifiable ( btf ) ) return libbpf_err ( - ENOMEM ) ; sz = sizeof ( struct btf_type ) ; t = btf_add_type_mem ( btf , sz ) ; if ( ! t ) return libbpf_err ( - ENOMEM ) ; if ( name && name [ 0 ] ) { name_off = btf__add_str ( btf , name ) ; if ( name_off < 0 ) return name_off ; } /* start out with vlen=0 and no kflag; this will be adjusted when\n\t * adding each member\n\t */ t -> name_off = name_off ; t -> info = btf_type_info ( kind , 0 , 0 ) ; t -> size = bytes_sz ; return btf_commit_type ( btf , sz ) ; }",
    "resources/libbpf/src/btf.c@btf__add_struct": "int btf__add_struct ( struct btf * btf , const char * name , __u32 byte_sz ) { return btf_add_composite ( btf , BTF_KIND_STRUCT , name , byte_sz ) ; }",
    "resources/libbpf/src/btf.c@btf__add_union": "int btf__add_union ( struct btf * btf , const char * name , __u32 byte_sz ) { return btf_add_composite ( btf , BTF_KIND_UNION , name , byte_sz ) ; }",
    "resources/libbpf/src/btf.c@btf_last_type": "static struct btf_type * btf_last_type ( struct btf * btf ) { return btf_type_by_id ( btf , btf__type_cnt ( btf ) - 1 ) ; }",
    "resources/libbpf/src/btf.c@btf__add_field": "int btf__add_field ( struct btf * btf , const char * name , int type_id , __u32 bit_offset , __u32 bit_size ) { struct btf_type * t ; struct btf_member * m ; bool is_bitfield ; int sz , name_off = 0 ; /* last type should be union/struct */ if ( btf -> nr_types == 0 ) return libbpf_err ( - EINVAL ) ; t = btf_last_type ( btf ) ; if ( ! btf_is_composite ( t ) ) return libbpf_err ( - EINVAL ) ; if ( validate_type_id ( type_id ) ) return libbpf_err ( - EINVAL ) ; /* best-effort bit field offset/size enforcement */ is_bitfield = bit_size || ( bit_offset % 8 != 0 ) ; if ( is_bitfield && ( bit_size == 0 || bit_size > 255 || bit_offset > 0xffffff ) ) return libbpf_err ( - EINVAL ) ; /* only offset 0 is allowed for unions */ if ( btf_is_union ( t ) && bit_offset ) return libbpf_err ( - EINVAL ) ; /* decompose and invalidate raw data */ if ( btf_ensure_modifiable ( btf ) ) return libbpf_err ( - ENOMEM ) ; sz = sizeof ( struct btf_member ) ; m = btf_add_type_mem ( btf , sz ) ; if ( ! m ) return libbpf_err ( - ENOMEM ) ; if ( name && name [ 0 ] ) { name_off = btf__add_str ( btf , name ) ; if ( name_off < 0 ) return name_off ; } m -> name_off = name_off ; m -> type = type_id ; m -> offset = bit_offset | ( bit_size << 24 ) ; /* btf_add_type_mem can invalidate t pointer */ t = btf_last_type ( btf ) ; /* update parent type's vlen and kflag */ t -> info = btf_type_info ( btf_kind ( t ) , btf_vlen ( t ) + 1 , is_bitfield || btf_kflag ( t ) ) ; btf -> hdr -> type_len += sz ; btf -> hdr -> str_off += sz ; return 0 ; }",
    "resources/libbpf/src/btf.c@btf_add_enum_common": "static int btf_add_enum_common ( struct btf * btf , const char * name , __u32 byte_sz , bool is_signed , __u8 kind ) { struct btf_type * t ; int sz , name_off = 0 ; /* byte_sz must be power of 2 */ if ( ! byte_sz || ( byte_sz & ( byte_sz - 1 ) ) || byte_sz > 8 ) return libbpf_err ( - EINVAL ) ; if ( btf_ensure_modifiable ( btf ) ) return libbpf_err ( - ENOMEM ) ; sz = sizeof ( struct btf_type ) ; t = btf_add_type_mem ( btf , sz ) ; if ( ! t ) return libbpf_err ( - ENOMEM ) ; if ( name && name [ 0 ] ) { name_off = btf__add_str ( btf , name ) ; if ( name_off < 0 ) return name_off ; } /* start out with vlen=0; it will be adjusted when adding enum values */ t -> name_off = name_off ; t -> info = btf_type_info ( kind , 0 , is_signed ) ; t -> size = byte_sz ; return btf_commit_type ( btf , sz ) ; }",
    "resources/libbpf/src/btf.c@btf__add_enum": "int btf__add_enum ( struct btf * btf , const char * name , __u32 byte_sz ) { /*\n\t * set the signedness to be unsigned, it will change to signed\n\t * if any later enumerator is negative.\n\t */ return btf_add_enum_common ( btf , name , byte_sz , false , BTF_KIND_ENUM ) ; }",
    "resources/libbpf/src/btf.c@btf__add_enum_value": "int btf__add_enum_value ( struct btf * btf , const char * name , __s64 value ) { struct btf_type * t ; struct btf_enum * v ; int sz , name_off ; /* last type should be BTF_KIND_ENUM */ if ( btf -> nr_types == 0 ) return libbpf_err ( - EINVAL ) ; t = btf_last_type ( btf ) ; if ( ! btf_is_enum ( t ) ) return libbpf_err ( - EINVAL ) ; /* non-empty name */ if ( ! name || ! name [ 0 ] ) return libbpf_err ( - EINVAL ) ; if ( value < INT_MIN || value > UINT_MAX ) return libbpf_err ( - E2BIG ) ; /* decompose and invalidate raw data */ if ( btf_ensure_modifiable ( btf ) ) return libbpf_err ( - ENOMEM ) ; sz = sizeof ( struct btf_enum ) ; v = btf_add_type_mem ( btf , sz ) ; if ( ! v ) return libbpf_err ( - ENOMEM ) ; name_off = btf__add_str ( btf , name ) ; if ( name_off < 0 ) return name_off ; v -> name_off = name_off ; v -> val = value ; /* update parent type's vlen */ t = btf_last_type ( btf ) ; btf_type_inc_vlen ( t ) ; /* if negative value, set signedness to signed */ if ( value < 0 ) t -> info = btf_type_info ( btf_kind ( t ) , btf_vlen ( t ) , true ) ; btf -> hdr -> type_len += sz ; btf -> hdr -> str_off += sz ; return 0 ; }",
    "resources/libbpf/src/btf.c@btf__add_enum64": "int btf__add_enum64 ( struct btf * btf , const char * name , __u32 byte_sz , bool is_signed ) { return btf_add_enum_common ( btf , name , byte_sz , is_signed , BTF_KIND_ENUM64 ) ; }",
    "resources/libbpf/src/btf.c@btf__add_enum64_value": "int btf__add_enum64_value ( struct btf * btf , const char * name , __u64 value ) { struct btf_enum64 * v ; struct btf_type * t ; int sz , name_off ; /* last type should be BTF_KIND_ENUM64 */ if ( btf -> nr_types == 0 ) return libbpf_err ( - EINVAL ) ; t = btf_last_type ( btf ) ; if ( ! btf_is_enum64 ( t ) ) return libbpf_err ( - EINVAL ) ; /* non-empty name */ if ( ! name || ! name [ 0 ] ) return libbpf_err ( - EINVAL ) ; /* decompose and invalidate raw data */ if ( btf_ensure_modifiable ( btf ) ) return libbpf_err ( - ENOMEM ) ; sz = sizeof ( struct btf_enum64 ) ; v = btf_add_type_mem ( btf , sz ) ; if ( ! v ) return libbpf_err ( - ENOMEM ) ; name_off = btf__add_str ( btf , name ) ; if ( name_off < 0 ) return name_off ; v -> name_off = name_off ; v -> val_lo32 = ( __u32 ) value ; v -> val_hi32 = value >> 32 ; /* update parent type's vlen */ t = btf_last_type ( btf ) ; btf_type_inc_vlen ( t ) ; btf -> hdr -> type_len += sz ; btf -> hdr -> str_off += sz ; return 0 ; }",
    "resources/libbpf/src/btf.c@btf__add_fwd": "int btf__add_fwd ( struct btf * btf , const char * name , enum btf_fwd_kind fwd_kind ) { if ( ! name || ! name [ 0 ] ) return libbpf_err ( - EINVAL ) ; switch ( fwd_kind ) { case BTF_FWD_STRUCT : case BTF_FWD_UNION : { struct btf_type * t ; int id ; id = btf_add_ref_kind ( btf , BTF_KIND_FWD , name , 0 ) ; if ( id <= 0 ) return id ; t = btf_type_by_id ( btf , id ) ; t -> info = btf_type_info ( BTF_KIND_FWD , 0 , fwd_kind == BTF_FWD_UNION ) ; return id ; } case BTF_FWD_ENUM : /* enum forward in BTF currently is just an enum with no enum\n\t\t * values; we also assume a standard 4-byte size for it\n\t\t */ return btf__add_enum ( btf , name , sizeof ( int ) ) ; default : return libbpf_err ( - EINVAL ) ; } }",
    "resources/libbpf/src/btf.c@btf__add_typedef": "int btf__add_typedef ( struct btf * btf , const char * name , int ref_type_id ) { if ( ! name || ! name [ 0 ] ) return libbpf_err ( - EINVAL ) ; return btf_add_ref_kind ( btf , BTF_KIND_TYPEDEF , name , ref_type_id ) ; }",
    "resources/libbpf/src/btf.c@btf__add_volatile": "int btf__add_volatile ( struct btf * btf , int ref_type_id ) { return btf_add_ref_kind ( btf , BTF_KIND_VOLATILE , NULL , ref_type_id ) ; }",
    "resources/libbpf/src/btf.c@btf__add_const": "int btf__add_const ( struct btf * btf , int ref_type_id ) { return btf_add_ref_kind ( btf , BTF_KIND_CONST , NULL , ref_type_id ) ; }",
    "resources/libbpf/src/btf.c@btf__add_restrict": "int btf__add_restrict ( struct btf * btf , int ref_type_id ) { return btf_add_ref_kind ( btf , BTF_KIND_RESTRICT , NULL , ref_type_id ) ; }",
    "resources/libbpf/src/btf.c@btf__add_type_tag": "int btf__add_type_tag ( struct btf * btf , const char * value , int ref_type_id ) { if ( ! value || ! value [ 0 ] ) return libbpf_err ( - EINVAL ) ; return btf_add_ref_kind ( btf , BTF_KIND_TYPE_TAG , value , ref_type_id ) ; }",
    "resources/libbpf/src/btf.c@btf__add_func": "int btf__add_func ( struct btf * btf , const char * name , enum btf_func_linkage linkage , int proto_type_id ) { int id ; if ( ! name || ! name [ 0 ] ) return libbpf_err ( - EINVAL ) ; if ( linkage != BTF_FUNC_STATIC && linkage != BTF_FUNC_GLOBAL && linkage != BTF_FUNC_EXTERN ) return libbpf_err ( - EINVAL ) ; id = btf_add_ref_kind ( btf , BTF_KIND_FUNC , name , proto_type_id ) ; if ( id > 0 ) { struct btf_type * t = btf_type_by_id ( btf , id ) ; t -> info = btf_type_info ( BTF_KIND_FUNC , linkage , 0 ) ; } return libbpf_err ( id ) ; }",
    "resources/libbpf/src/btf.c@btf__add_func_proto": "int btf__add_func_proto ( struct btf * btf , int ret_type_id ) { struct btf_type * t ; int sz ; if ( validate_type_id ( ret_type_id ) ) return libbpf_err ( - EINVAL ) ; if ( btf_ensure_modifiable ( btf ) ) return libbpf_err ( - ENOMEM ) ; sz = sizeof ( struct btf_type ) ; t = btf_add_type_mem ( btf , sz ) ; if ( ! t ) return libbpf_err ( - ENOMEM ) ; /* start out with vlen=0; this will be adjusted when adding enum\n\t * values, if necessary\n\t */ t -> name_off = 0 ; t -> info = btf_type_info ( BTF_KIND_FUNC_PROTO , 0 , 0 ) ; t -> type = ret_type_id ; return btf_commit_type ( btf , sz ) ; }",
    "resources/libbpf/src/btf.c@btf__add_func_param": "int btf__add_func_param ( struct btf * btf , const char * name , int type_id ) { struct btf_type * t ; struct btf_param * p ; int sz , name_off = 0 ; if ( validate_type_id ( type_id ) ) return libbpf_err ( - EINVAL ) ; /* last type should be BTF_KIND_FUNC_PROTO */ if ( btf -> nr_types == 0 ) return libbpf_err ( - EINVAL ) ; t = btf_last_type ( btf ) ; if ( ! btf_is_func_proto ( t ) ) return libbpf_err ( - EINVAL ) ; /* decompose and invalidate raw data */ if ( btf_ensure_modifiable ( btf ) ) return libbpf_err ( - ENOMEM ) ; sz = sizeof ( struct btf_param ) ; p = btf_add_type_mem ( btf , sz ) ; if ( ! p ) return libbpf_err ( - ENOMEM ) ; if ( name && name [ 0 ] ) { name_off = btf__add_str ( btf , name ) ; if ( name_off < 0 ) return name_off ; } p -> name_off = name_off ; p -> type = type_id ; /* update parent type's vlen */ t = btf_last_type ( btf ) ; btf_type_inc_vlen ( t ) ; btf -> hdr -> type_len += sz ; btf -> hdr -> str_off += sz ; return 0 ; }",
    "resources/libbpf/src/btf.c@btf__add_var": "int btf__add_var ( struct btf * btf , const char * name , int linkage , int type_id ) { struct btf_type * t ; struct btf_var * v ; int sz , name_off ; /* non-empty name */ if ( ! name || ! name [ 0 ] ) return libbpf_err ( - EINVAL ) ; if ( linkage != BTF_VAR_STATIC && linkage != BTF_VAR_GLOBAL_ALLOCATED && linkage != BTF_VAR_GLOBAL_EXTERN ) return libbpf_err ( - EINVAL ) ; if ( validate_type_id ( type_id ) ) return libbpf_err ( - EINVAL ) ; /* deconstruct BTF, if necessary, and invalidate raw_data */ if ( btf_ensure_modifiable ( btf ) ) return libbpf_err ( - ENOMEM ) ; sz = sizeof ( struct btf_type ) + sizeof ( struct btf_var ) ; t = btf_add_type_mem ( btf , sz ) ; if ( ! t ) return libbpf_err ( - ENOMEM ) ; name_off = btf__add_str ( btf , name ) ; if ( name_off < 0 ) return name_off ; t -> name_off = name_off ; t -> info = btf_type_info ( BTF_KIND_VAR , 0 , 0 ) ; t -> type = type_id ; v = btf_var ( t ) ; v -> linkage = linkage ; return btf_commit_type ( btf , sz ) ; }",
    "resources/libbpf/src/btf.c@btf__add_datasec": "int btf__add_datasec ( struct btf * btf , const char * name , __u32 byte_sz ) { struct btf_type * t ; int sz , name_off ; /* non-empty name */ if ( ! name || ! name [ 0 ] ) return libbpf_err ( - EINVAL ) ; if ( btf_ensure_modifiable ( btf ) ) return libbpf_err ( - ENOMEM ) ; sz = sizeof ( struct btf_type ) ; t = btf_add_type_mem ( btf , sz ) ; if ( ! t ) return libbpf_err ( - ENOMEM ) ; name_off = btf__add_str ( btf , name ) ; if ( name_off < 0 ) return name_off ; /* start with vlen=0, which will be update as var_secinfos are added */ t -> name_off = name_off ; t -> info = btf_type_info ( BTF_KIND_DATASEC , 0 , 0 ) ; t -> size = byte_sz ; return btf_commit_type ( btf , sz ) ; }",
    "resources/libbpf/src/btf.c@btf__add_datasec_var_info": "int btf__add_datasec_var_info ( struct btf * btf , int var_type_id , __u32 offset , __u32 byte_sz ) { struct btf_type * t ; struct btf_var_secinfo * v ; int sz ; /* last type should be BTF_KIND_DATASEC */ if ( btf -> nr_types == 0 ) return libbpf_err ( - EINVAL ) ; t = btf_last_type ( btf ) ; if ( ! btf_is_datasec ( t ) ) return libbpf_err ( - EINVAL ) ; if ( validate_type_id ( var_type_id ) ) return libbpf_err ( - EINVAL ) ; /* decompose and invalidate raw data */ if ( btf_ensure_modifiable ( btf ) ) return libbpf_err ( - ENOMEM ) ; sz = sizeof ( struct btf_var_secinfo ) ; v = btf_add_type_mem ( btf , sz ) ; if ( ! v ) return libbpf_err ( - ENOMEM ) ; v -> type = var_type_id ; v -> offset = offset ; v -> size = byte_sz ; /* update parent type's vlen */ t = btf_last_type ( btf ) ; btf_type_inc_vlen ( t ) ; btf -> hdr -> type_len += sz ; btf -> hdr -> str_off += sz ; return 0 ; }",
    "resources/libbpf/src/btf.c@btf__add_decl_tag": "int btf__add_decl_tag ( struct btf * btf , const char * value , int ref_type_id , int component_idx ) { struct btf_type * t ; int sz , value_off ; if ( ! value || ! value [ 0 ] || component_idx < - 1 ) return libbpf_err ( - EINVAL ) ; if ( validate_type_id ( ref_type_id ) ) return libbpf_err ( - EINVAL ) ; if ( btf_ensure_modifiable ( btf ) ) return libbpf_err ( - ENOMEM ) ; sz = sizeof ( struct btf_type ) + sizeof ( struct btf_decl_tag ) ; t = btf_add_type_mem ( btf , sz ) ; if ( ! t ) return libbpf_err ( - ENOMEM ) ; value_off = btf__add_str ( btf , value ) ; if ( value_off < 0 ) return value_off ; t -> name_off = value_off ; t -> info = btf_type_info ( BTF_KIND_DECL_TAG , 0 , false ) ; t -> type = ref_type_id ; btf_decl_tag ( t ) -> component_idx = component_idx ; return btf_commit_type ( btf , sz ) ; }",
    "resources/libbpf/src/btf.c@btf_ext_setup_info": "static int btf_ext_setup_info ( struct btf_ext * btf_ext , struct btf_ext_sec_setup_param * ext_sec ) { const struct btf_ext_info_sec * sinfo ; struct btf_ext_info * ext_info ; __u32 info_left , record_size ; size_t sec_cnt = 0 ; /* The start of the info sec (including the __u32 record_size). */ void * info ; if ( ext_sec -> len == 0 ) return 0 ; if ( ext_sec -> off & 0x03 ) { pr_debug ( \".BTF.ext %s section is not aligned to 4 bytes\\n\" , ext_sec -> desc ) ; return - EINVAL ; } info = btf_ext -> data + btf_ext -> hdr -> hdr_len + ext_sec -> off ; info_left = ext_sec -> len ; if ( btf_ext -> data + btf_ext -> data_size < info + ext_sec -> len ) { pr_debug ( \"%s section (off:%u len:%u) is beyond the end of the ELF section .BTF.ext\\n\" , ext_sec -> desc , ext_sec -> off , ext_sec -> len ) ; return - EINVAL ; } /* At least a record size */ if ( info_left < sizeof ( __u32 ) ) { pr_debug ( \".BTF.ext %s record size not found\\n\" , ext_sec -> desc ) ; return - EINVAL ; } /* The record size needs to meet the minimum standard */ record_size = * ( __u32 * ) info ; if ( record_size < ext_sec -> min_rec_size || record_size & 0x03 ) { pr_debug ( \"%s section in .BTF.ext has invalid record size %u\\n\" , ext_sec -> desc , record_size ) ; return - EINVAL ; } sinfo = info + sizeof ( __u32 ) ; info_left -= sizeof ( __u32 ) ; /* If no records, return failure now so .BTF.ext won't be used. */ if ( ! info_left ) { pr_debug ( \"%s section in .BTF.ext has no records\" , ext_sec -> desc ) ; return - EINVAL ; } while ( info_left ) { unsigned int sec_hdrlen = sizeof ( struct btf_ext_info_sec ) ; __u64 total_record_size ; __u32 num_records ; if ( info_left < sec_hdrlen ) { pr_debug ( \"%s section header is not found in .BTF.ext\\n\" , ext_sec -> desc ) ; return - EINVAL ; } num_records = sinfo -> num_info ; if ( num_records == 0 ) { pr_debug ( \"%s section has incorrect num_records in .BTF.ext\\n\" , ext_sec -> desc ) ; return - EINVAL ; } total_record_size = sec_hdrlen + ( __u64 ) num_records * record_size ; if ( info_left < total_record_size ) { pr_debug ( \"%s section has incorrect num_records in .BTF.ext\\n\" , ext_sec -> desc ) ; return - EINVAL ; } info_left -= total_record_size ; sinfo = ( void * ) sinfo + total_record_size ; sec_cnt ++ ; } ext_info = ext_sec -> ext_info ; ext_info -> len = ext_sec -> len - sizeof ( __u32 ) ; ext_info -> rec_size = record_size ; ext_info -> info = info + sizeof ( __u32 ) ; ext_info -> sec_cnt = sec_cnt ; return 0 ; }",
    "resources/libbpf/src/btf.c@btf_ext_setup_func_info": "static int btf_ext_setup_func_info ( struct btf_ext * btf_ext ) { struct btf_ext_sec_setup_param param = { . off = btf_ext -> hdr -> func_info_off , . len = btf_ext -> hdr -> func_info_len , . min_rec_size = sizeof ( struct bpf_func_info_min ) , . ext_info = & btf_ext -> func_info , . desc = \"func_info\" } ; return btf_ext_setup_info ( btf_ext , & param ) ; }",
    "resources/libbpf/src/btf.c@btf_ext_setup_line_info": "static int btf_ext_setup_line_info ( struct btf_ext * btf_ext ) { struct btf_ext_sec_setup_param param = { . off = btf_ext -> hdr -> line_info_off , . len = btf_ext -> hdr -> line_info_len , . min_rec_size = sizeof ( struct bpf_line_info_min ) , . ext_info = & btf_ext -> line_info , . desc = \"line_info\" , } ; return btf_ext_setup_info ( btf_ext , & param ) ; }",
    "resources/libbpf/src/btf.c@btf_ext_setup_core_relos": "static int btf_ext_setup_core_relos ( struct btf_ext * btf_ext ) { struct btf_ext_sec_setup_param param = { . off = btf_ext -> hdr -> core_relo_off , . len = btf_ext -> hdr -> core_relo_len , . min_rec_size = sizeof ( struct bpf_core_relo ) , . ext_info = & btf_ext -> core_relo_info , . desc = \"core_relo\" , } ; return btf_ext_setup_info ( btf_ext , & param ) ; }",
    "resources/libbpf/src/btf.c@btf_ext_parse_hdr": "static int btf_ext_parse_hdr ( __u8 * data , __u32 data_size ) { const struct btf_ext_header * hdr = ( struct btf_ext_header * ) data ; if ( data_size < offsetofend ( struct btf_ext_header , hdr_len ) || data_size < hdr -> hdr_len ) { pr_debug ( \"BTF.ext header not found\" ) ; return - EINVAL ; } if ( hdr -> magic == bswap_16 ( BTF_MAGIC ) ) { pr_warn ( \"BTF.ext in non-native endianness is not supported\\n\" ) ; return - ENOTSUP ; } else if ( hdr -> magic != BTF_MAGIC ) { pr_debug ( \"Invalid BTF.ext magic:%x\\n\" , hdr -> magic ) ; return - EINVAL ; } if ( hdr -> version != BTF_VERSION ) { pr_debug ( \"Unsupported BTF.ext version:%u\\n\" , hdr -> version ) ; return - ENOTSUP ; } if ( hdr -> flags ) { pr_debug ( \"Unsupported BTF.ext flags:%x\\n\" , hdr -> flags ) ; return - ENOTSUP ; } if ( data_size == hdr -> hdr_len ) { pr_debug ( \"BTF.ext has no data\\n\" ) ; return - EINVAL ; } return 0 ; }",
    "resources/libbpf/src/btf.c@btf_ext__free": "void btf_ext__free ( struct btf_ext * btf_ext ) { if ( IS_ERR_OR_NULL ( btf_ext ) ) return ; free ( btf_ext -> func_info . sec_idxs ) ; free ( btf_ext -> line_info . sec_idxs ) ; free ( btf_ext -> core_relo_info . sec_idxs ) ; free ( btf_ext -> data ) ; free ( btf_ext ) ; }",
    "resources/libbpf/src/btf.c@btf_ext__new": "struct btf_ext * btf_ext__new ( const __u8 * data , __u32 size ) { struct btf_ext * btf_ext ; int err ; btf_ext = calloc ( 1 , sizeof ( struct btf_ext ) ) ; if ( ! btf_ext ) return libbpf_err_ptr ( - ENOMEM ) ; btf_ext -> data_size = size ; btf_ext -> data = malloc ( size ) ; if ( ! btf_ext -> data ) { err = - ENOMEM ; goto done ; } memcpy ( btf_ext -> data , data , size ) ; err = btf_ext_parse_hdr ( btf_ext -> data , size ) ; if ( err ) goto done ; if ( btf_ext -> hdr -> hdr_len < offsetofend ( struct btf_ext_header , line_info_len ) ) { err = - EINVAL ; goto done ; } err = btf_ext_setup_func_info ( btf_ext ) ; if ( err ) goto done ; err = btf_ext_setup_line_info ( btf_ext ) ; if ( err ) goto done ; if ( btf_ext -> hdr -> hdr_len < offsetofend ( struct btf_ext_header , core_relo_len ) ) goto done ; /* skip core relos parsing */ err = btf_ext_setup_core_relos ( btf_ext ) ; if ( err ) goto done ; done : if ( err ) { btf_ext__free ( btf_ext ) ; return libbpf_err_ptr ( err ) ; } return btf_ext ; }",
    "resources/libbpf/src/btf.c@btf_ext__raw_data": "const void * btf_ext__raw_data ( const struct btf_ext * btf_ext , __u32 * size ) { * size = btf_ext -> data_size ; return btf_ext -> data ; }",
    "resources/libbpf/src/btf.c@btf__dedup": "int btf__dedup ( struct btf * btf , const struct btf_dedup_opts * opts ) { struct btf_dedup * d ; int err ; if ( ! OPTS_VALID ( opts , btf_dedup_opts ) ) return libbpf_err ( - EINVAL ) ; d = btf_dedup_new ( btf , opts ) ; if ( IS_ERR ( d ) ) { pr_debug ( \"btf_dedup_new failed: %ld\" , PTR_ERR ( d ) ) ; return libbpf_err ( - EINVAL ) ; } if ( btf_ensure_modifiable ( btf ) ) { err = - ENOMEM ; goto done ; } err = btf_dedup_prep ( d ) ; if ( err ) { pr_debug ( \"btf_dedup_prep failed:%d\\n\" , err ) ; goto done ; } err = btf_dedup_strings ( d ) ; if ( err < 0 ) { pr_debug ( \"btf_dedup_strings failed:%d\\n\" , err ) ; goto done ; } err = btf_dedup_prim_types ( d ) ; if ( err < 0 ) { pr_debug ( \"btf_dedup_prim_types failed:%d\\n\" , err ) ; goto done ; } err = btf_dedup_struct_types ( d ) ; if ( err < 0 ) { pr_debug ( \"btf_dedup_struct_types failed:%d\\n\" , err ) ; goto done ; } err = btf_dedup_resolve_fwds ( d ) ; if ( err < 0 ) { pr_debug ( \"btf_dedup_resolve_fwds failed:%d\\n\" , err ) ; goto done ; } err = btf_dedup_ref_types ( d ) ; if ( err < 0 ) { pr_debug ( \"btf_dedup_ref_types failed:%d\\n\" , err ) ; goto done ; } err = btf_dedup_compact_types ( d ) ; if ( err < 0 ) { pr_debug ( \"btf_dedup_compact_types failed:%d\\n\" , err ) ; goto done ; } err = btf_dedup_remap_types ( d ) ; if ( err < 0 ) { pr_debug ( \"btf_dedup_remap_types failed:%d\\n\" , err ) ; goto done ; } done : btf_dedup_free ( d ) ; return libbpf_err ( err ) ; }",
    "resources/libbpf/src/btf.c@hash_combine": "static long hash_combine ( long h , long value ) { return h * 31 + value ; }",
    "resources/libbpf/src/btf.c@btf_dedup_table_add": "static int btf_dedup_table_add ( struct btf_dedup * d , long hash , __u32 type_id ) { return hashmap__append ( d -> dedup_table , hash , type_id ) ; }",
    "resources/libbpf/src/btf.c@btf_dedup_hypot_map_add": "static int btf_dedup_hypot_map_add ( struct btf_dedup * d , __u32 from_id , __u32 to_id ) { if ( d -> hypot_cnt == d -> hypot_cap ) { __u32 * new_list ; d -> hypot_cap += max ( ( size_t ) 16 , d -> hypot_cap / 2 ) ; new_list = libbpf_reallocarray ( d -> hypot_list , d -> hypot_cap , sizeof ( __u32 ) ) ; if ( ! new_list ) return - ENOMEM ; d -> hypot_list = new_list ; } d -> hypot_list [ d -> hypot_cnt ++ ] = from_id ; d -> hypot_map [ from_id ] = to_id ; return 0 ; }",
    "resources/libbpf/src/btf.c@btf_dedup_clear_hypot_map": "static void btf_dedup_clear_hypot_map ( struct btf_dedup * d ) { int i ; for ( i = 0 ; i < d -> hypot_cnt ; i ++ ) d -> hypot_map [ d -> hypot_list [ i ] ] = BTF_UNPROCESSED_ID ; d -> hypot_cnt = 0 ; d -> hypot_adjust_canon = false ; }",
    "resources/libbpf/src/btf.c@btf_dedup_free": "static void btf_dedup_free ( struct btf_dedup * d ) { hashmap__free ( d -> dedup_table ) ; d -> dedup_table = NULL ; free ( d -> map ) ; d -> map = NULL ; free ( d -> hypot_map ) ; d -> hypot_map = NULL ; free ( d -> hypot_list ) ; d -> hypot_list = NULL ; free ( d ) ; }",
    "resources/libbpf/src/btf.c@btf_dedup_identity_hash_fn": "static size_t btf_dedup_identity_hash_fn ( long key , void * ctx ) { return key ; }",
    "resources/libbpf/src/btf.c@btf_dedup_collision_hash_fn": "static size_t btf_dedup_collision_hash_fn ( long key , void * ctx ) { return 0 ; }",
    "resources/libbpf/src/btf.c@btf_dedup_equal_fn": "static bool btf_dedup_equal_fn ( long k1 , long k2 , void * ctx ) { return k1 == k2 ; }",
    "resources/libbpf/src/btf.c@btf_dedup_new": "static struct btf_dedup * btf_dedup_new ( struct btf * btf , const struct btf_dedup_opts * opts ) { struct btf_dedup * d = calloc ( 1 , sizeof ( struct btf_dedup ) ) ; hashmap_hash_fn hash_fn = btf_dedup_identity_hash_fn ; int i , err = 0 , type_cnt ; if ( ! d ) return ERR_PTR ( - ENOMEM ) ; if ( OPTS_GET ( opts , force_collisions , false ) ) hash_fn = btf_dedup_collision_hash_fn ; d -> btf = btf ; d -> btf_ext = OPTS_GET ( opts , btf_ext , NULL ) ; d -> dedup_table = hashmap__new ( hash_fn , btf_dedup_equal_fn , NULL ) ; if ( IS_ERR ( d -> dedup_table ) ) { err = PTR_ERR ( d -> dedup_table ) ; d -> dedup_table = NULL ; goto done ; } type_cnt = btf__type_cnt ( btf ) ; d -> map = malloc ( sizeof ( __u32 ) * type_cnt ) ; if ( ! d -> map ) { err = - ENOMEM ; goto done ; } /* special BTF \"void\" type is made canonical immediately */ d -> map [ 0 ] = 0 ; for ( i = 1 ; i < type_cnt ; i ++ ) { struct btf_type * t = btf_type_by_id ( d -> btf , i ) ; /* VAR and DATASEC are never deduped and are self-canonical */ if ( btf_is_var ( t ) || btf_is_datasec ( t ) ) d -> map [ i ] = i ; else d -> map [ i ] = BTF_UNPROCESSED_ID ; } d -> hypot_map = malloc ( sizeof ( __u32 ) * type_cnt ) ; if ( ! d -> hypot_map ) { err = - ENOMEM ; goto done ; } for ( i = 0 ; i < type_cnt ; i ++ ) d -> hypot_map [ i ] = BTF_UNPROCESSED_ID ; done : if ( err ) { btf_dedup_free ( d ) ; return ERR_PTR ( err ) ; } return d ; }",
    "resources/libbpf/src/btf.c@btf_for_each_str_off": "static int btf_for_each_str_off ( struct btf_dedup * d , str_off_visit_fn fn , void * ctx ) { int i , r ; for ( i = 0 ; i < d -> btf -> nr_types ; i ++ ) { struct btf_type * t = btf_type_by_id ( d -> btf , d -> btf -> start_id + i ) ; r = btf_type_visit_str_offs ( t , fn , ctx ) ; if ( r ) return r ; } if ( ! d -> btf_ext ) return 0 ; r = btf_ext_visit_str_offs ( d -> btf_ext , fn , ctx ) ; if ( r ) return r ; return 0 ; }",
    "resources/libbpf/src/btf.c@strs_dedup_remap_str_off": "static int strs_dedup_remap_str_off ( __u32 * str_off_ptr , void * ctx ) { struct btf_dedup * d = ctx ; __u32 str_off = * str_off_ptr ; const char * s ; int off , err ; /* don't touch empty string or string in main BTF */ if ( str_off == 0 || str_off < d -> btf -> start_str_off ) return 0 ; s = btf__str_by_offset ( d -> btf , str_off ) ; if ( d -> btf -> base_btf ) { err = btf__find_str ( d -> btf -> base_btf , s ) ; if ( err >= 0 ) { * str_off_ptr = err ; return 0 ; } if ( err != - ENOENT ) return err ; } off = strset__add_str ( d -> strs_set , s ) ; if ( off < 0 ) return off ; * str_off_ptr = d -> btf -> start_str_off + off ; return 0 ; }",
    "resources/libbpf/src/btf.c@btf_dedup_strings": "static int btf_dedup_strings ( struct btf_dedup * d ) { int err ; if ( d -> btf -> strs_deduped ) return 0 ; d -> strs_set = strset__new ( BTF_MAX_STR_OFFSET , NULL , 0 ) ; if ( IS_ERR ( d -> strs_set ) ) { err = PTR_ERR ( d -> strs_set ) ; goto err_out ; } if ( ! d -> btf -> base_btf ) { /* insert empty string; we won't be looking it up during strings\n\t\t * dedup, but it's good to have it for generic BTF string lookups\n\t\t */ err = strset__add_str ( d -> strs_set , \"\" ) ; if ( err < 0 ) goto err_out ; } /* remap string offsets */ err = btf_for_each_str_off ( d , strs_dedup_remap_str_off , d ) ; if ( err ) goto err_out ; /* replace BTF string data and hash with deduped ones */ strset__free ( d -> btf -> strs_set ) ; d -> btf -> hdr -> str_len = strset__data_size ( d -> strs_set ) ; d -> btf -> strs_set = d -> strs_set ; d -> strs_set = NULL ; d -> btf -> strs_deduped = true ; return 0 ; err_out : strset__free ( d -> strs_set ) ; d -> strs_set = NULL ; return err ; }",
    "resources/libbpf/src/btf.c@btf_hash_common": "static long btf_hash_common ( struct btf_type * t ) { long h ; h = hash_combine ( 0 , t -> name_off ) ; h = hash_combine ( h , t -> info ) ; h = hash_combine ( h , t -> size ) ; return h ; }",
    "resources/libbpf/src/btf.c@btf_equal_common": "static bool btf_equal_common ( struct btf_type * t1 , struct btf_type * t2 ) { return t1 -> name_off == t2 -> name_off && t1 -> info == t2 -> info && t1 -> size == t2 -> size ; }",
    "resources/libbpf/src/btf.c@btf_hash_int_decl_tag": "static long btf_hash_int_decl_tag ( struct btf_type * t ) { __u32 info = * ( __u32 * ) ( t + 1 ) ; long h ; h = btf_hash_common ( t ) ; h = hash_combine ( h , info ) ; return h ; }",
    "resources/libbpf/src/btf.c@btf_equal_int_tag": "static bool btf_equal_int_tag ( struct btf_type * t1 , struct btf_type * t2 ) { __u32 info1 , info2 ; if ( ! btf_equal_common ( t1 , t2 ) ) return false ; info1 = * ( __u32 * ) ( t1 + 1 ) ; info2 = * ( __u32 * ) ( t2 + 1 ) ; return info1 == info2 ; }",
    "resources/libbpf/src/btf.c@btf_hash_enum": "static long btf_hash_enum ( struct btf_type * t ) { long h ; /* don't hash vlen, enum members and size to support enum fwd resolving */ h = hash_combine ( 0 , t -> name_off ) ; return h ; }",
    "resources/libbpf/src/btf.c@btf_equal_enum_members": "static bool btf_equal_enum_members ( struct btf_type * t1 , struct btf_type * t2 ) { const struct btf_enum * m1 , * m2 ; __u16 vlen ; int i ; vlen = btf_vlen ( t1 ) ; m1 = btf_enum ( t1 ) ; m2 = btf_enum ( t2 ) ; for ( i = 0 ; i < vlen ; i ++ ) { if ( m1 -> name_off != m2 -> name_off || m1 -> val != m2 -> val ) return false ; m1 ++ ; m2 ++ ; } return true ; }",
    "resources/libbpf/src/btf.c@btf_equal_enum64_members": "static bool btf_equal_enum64_members ( struct btf_type * t1 , struct btf_type * t2 ) { const struct btf_enum64 * m1 , * m2 ; __u16 vlen ; int i ; vlen = btf_vlen ( t1 ) ; m1 = btf_enum64 ( t1 ) ; m2 = btf_enum64 ( t2 ) ; for ( i = 0 ; i < vlen ; i ++ ) { if ( m1 -> name_off != m2 -> name_off || m1 -> val_lo32 != m2 -> val_lo32 || m1 -> val_hi32 != m2 -> val_hi32 ) return false ; m1 ++ ; m2 ++ ; } return true ; }",
    "resources/libbpf/src/btf.c@btf_equal_enum": "static bool btf_equal_enum ( struct btf_type * t1 , struct btf_type * t2 ) { if ( ! btf_equal_common ( t1 , t2 ) ) return false ; /* t1 & t2 kinds are identical because of btf_equal_common */ if ( btf_kind ( t1 ) == BTF_KIND_ENUM ) return btf_equal_enum_members ( t1 , t2 ) ; else return btf_equal_enum64_members ( t1 , t2 ) ; }",
    "resources/libbpf/src/btf.c@btf_is_enum_fwd": "static inline bool btf_is_enum_fwd ( struct btf_type * t ) { return btf_is_any_enum ( t ) && btf_vlen ( t ) == 0 ; }",
    "resources/libbpf/src/btf.c@btf_compat_enum": "static bool btf_compat_enum ( struct btf_type * t1 , struct btf_type * t2 ) { if ( ! btf_is_enum_fwd ( t1 ) && ! btf_is_enum_fwd ( t2 ) ) return btf_equal_enum ( t1 , t2 ) ; /* At this point either t1 or t2 or both are forward declarations, thus:\n\t * - skip comparing vlen because it is zero for forward declarations;\n\t * - skip comparing size to allow enum forward declarations\n\t *   to be compatible with enum64 full declarations;\n\t * - skip comparing kind for the same reason.\n\t */ return t1 -> name_off == t2 -> name_off && btf_is_any_enum ( t1 ) && btf_is_any_enum ( t2 ) ; }",
    "resources/libbpf/src/btf.c@btf_hash_struct": "static long btf_hash_struct ( struct btf_type * t ) { const struct btf_member * member = btf_members ( t ) ; __u32 vlen = btf_vlen ( t ) ; long h = btf_hash_common ( t ) ; int i ; for ( i = 0 ; i < vlen ; i ++ ) { h = hash_combine ( h , member -> name_off ) ; h = hash_combine ( h , member -> offset ) ; /* no hashing of referenced type ID, it can be unresolved yet */ member ++ ; } return h ; }",
    "resources/libbpf/src/btf.c@btf_shallow_equal_struct": "static bool btf_shallow_equal_struct ( struct btf_type * t1 , struct btf_type * t2 ) { const struct btf_member * m1 , * m2 ; __u16 vlen ; int i ; if ( ! btf_equal_common ( t1 , t2 ) ) return false ; vlen = btf_vlen ( t1 ) ; m1 = btf_members ( t1 ) ; m2 = btf_members ( t2 ) ; for ( i = 0 ; i < vlen ; i ++ ) { if ( m1 -> name_off != m2 -> name_off || m1 -> offset != m2 -> offset ) return false ; m1 ++ ; m2 ++ ; } return true ; }",
    "resources/libbpf/src/btf.c@btf_hash_array": "static long btf_hash_array ( struct btf_type * t ) { const struct btf_array * info = btf_array ( t ) ; long h = btf_hash_common ( t ) ; h = hash_combine ( h , info -> type ) ; h = hash_combine ( h , info -> index_type ) ; h = hash_combine ( h , info -> nelems ) ; return h ; }",
    "resources/libbpf/src/btf.c@btf_equal_array": "static bool btf_equal_array ( struct btf_type * t1 , struct btf_type * t2 ) { const struct btf_array * info1 , * info2 ; if ( ! btf_equal_common ( t1 , t2 ) ) return false ; info1 = btf_array ( t1 ) ; info2 = btf_array ( t2 ) ; return info1 -> type == info2 -> type && info1 -> index_type == info2 -> index_type && info1 -> nelems == info2 -> nelems ; }",
    "resources/libbpf/src/btf.c@btf_compat_array": "static bool btf_compat_array ( struct btf_type * t1 , struct btf_type * t2 ) { if ( ! btf_equal_common ( t1 , t2 ) ) return false ; return btf_array ( t1 ) -> nelems == btf_array ( t2 ) -> nelems ; }",
    "resources/libbpf/src/btf.c@btf_hash_fnproto": "static long btf_hash_fnproto ( struct btf_type * t ) { const struct btf_param * member = btf_params ( t ) ; __u16 vlen = btf_vlen ( t ) ; long h = btf_hash_common ( t ) ; int i ; for ( i = 0 ; i < vlen ; i ++ ) { h = hash_combine ( h , member -> name_off ) ; h = hash_combine ( h , member -> type ) ; member ++ ; } return h ; }",
    "resources/libbpf/src/btf.c@btf_equal_fnproto": "static bool btf_equal_fnproto ( struct btf_type * t1 , struct btf_type * t2 ) { const struct btf_param * m1 , * m2 ; __u16 vlen ; int i ; if ( ! btf_equal_common ( t1 , t2 ) ) return false ; vlen = btf_vlen ( t1 ) ; m1 = btf_params ( t1 ) ; m2 = btf_params ( t2 ) ; for ( i = 0 ; i < vlen ; i ++ ) { if ( m1 -> name_off != m2 -> name_off || m1 -> type != m2 -> type ) return false ; m1 ++ ; m2 ++ ; } return true ; }",
    "resources/libbpf/src/btf.c@btf_compat_fnproto": "static bool btf_compat_fnproto ( struct btf_type * t1 , struct btf_type * t2 ) { const struct btf_param * m1 , * m2 ; __u16 vlen ; int i ; /* skip return type ID */ if ( t1 -> name_off != t2 -> name_off || t1 -> info != t2 -> info ) return false ; vlen = btf_vlen ( t1 ) ; m1 = btf_params ( t1 ) ; m2 = btf_params ( t2 ) ; for ( i = 0 ; i < vlen ; i ++ ) { if ( m1 -> name_off != m2 -> name_off ) return false ; m1 ++ ; m2 ++ ; } return true ; }",
    "resources/libbpf/src/btf.c@btf_dedup_prep": "static int btf_dedup_prep ( struct btf_dedup * d ) { struct btf_type * t ; int type_id ; long h ; if ( ! d -> btf -> base_btf ) return 0 ; for ( type_id = 1 ; type_id < d -> btf -> start_id ; type_id ++ ) { t = btf_type_by_id ( d -> btf , type_id ) ; /* all base BTF types are self-canonical by definition */ d -> map [ type_id ] = type_id ; switch ( btf_kind ( t ) ) { case BTF_KIND_VAR : case BTF_KIND_DATASEC : /* VAR and DATASEC are never hash/deduplicated */ continue ; case BTF_KIND_CONST : case BTF_KIND_VOLATILE : case BTF_KIND_RESTRICT : case BTF_KIND_PTR : case BTF_KIND_FWD : case BTF_KIND_TYPEDEF : case BTF_KIND_FUNC : case BTF_KIND_FLOAT : case BTF_KIND_TYPE_TAG : h = btf_hash_common ( t ) ; break ; case BTF_KIND_INT : case BTF_KIND_DECL_TAG : h = btf_hash_int_decl_tag ( t ) ; break ; case BTF_KIND_ENUM : case BTF_KIND_ENUM64 : h = btf_hash_enum ( t ) ; break ; case BTF_KIND_STRUCT : case BTF_KIND_UNION : h = btf_hash_struct ( t ) ; break ; case BTF_KIND_ARRAY : h = btf_hash_array ( t ) ; break ; case BTF_KIND_FUNC_PROTO : h = btf_hash_fnproto ( t ) ; break ; default : pr_debug ( \"unknown kind %d for type [%d]\\n\" , btf_kind ( t ) , type_id ) ; return - EINVAL ; } if ( btf_dedup_table_add ( d , h , type_id ) ) return - ENOMEM ; } return 0 ; }",
    "resources/libbpf/src/btf.c@btf_dedup_prim_type": "static int btf_dedup_prim_type ( struct btf_dedup * d , __u32 type_id ) { struct btf_type * t = btf_type_by_id ( d -> btf , type_id ) ; struct hashmap_entry * hash_entry ; struct btf_type * cand ; /* if we don't find equivalent type, then we are canonical */ __u32 new_id = type_id ; __u32 cand_id ; long h ; switch ( btf_kind ( t ) ) { case BTF_KIND_CONST : case BTF_KIND_VOLATILE : case BTF_KIND_RESTRICT : case BTF_KIND_PTR : case BTF_KIND_TYPEDEF : case BTF_KIND_ARRAY : case BTF_KIND_STRUCT : case BTF_KIND_UNION : case BTF_KIND_FUNC : case BTF_KIND_FUNC_PROTO : case BTF_KIND_VAR : case BTF_KIND_DATASEC : case BTF_KIND_DECL_TAG : case BTF_KIND_TYPE_TAG : return 0 ; case BTF_KIND_INT : h = btf_hash_int_decl_tag ( t ) ; for_each_dedup_cand ( d , hash_entry , h ) { cand_id = hash_entry -> value ; cand = btf_type_by_id ( d -> btf , cand_id ) ; if ( btf_equal_int_tag ( t , cand ) ) { new_id = cand_id ; break ; } } break ; case BTF_KIND_ENUM : case BTF_KIND_ENUM64 : h = btf_hash_enum ( t ) ; for_each_dedup_cand ( d , hash_entry , h ) { cand_id = hash_entry -> value ; cand = btf_type_by_id ( d -> btf , cand_id ) ; if ( btf_equal_enum ( t , cand ) ) { new_id = cand_id ; break ; } if ( btf_compat_enum ( t , cand ) ) { if ( btf_is_enum_fwd ( t ) ) { /* resolve fwd to full enum */ new_id = cand_id ; break ; } /* resolve canonical enum fwd to full enum */ d -> map [ cand_id ] = type_id ; } } break ; case BTF_KIND_FWD : case BTF_KIND_FLOAT : h = btf_hash_common ( t ) ; for_each_dedup_cand ( d , hash_entry , h ) { cand_id = hash_entry -> value ; cand = btf_type_by_id ( d -> btf , cand_id ) ; if ( btf_equal_common ( t , cand ) ) { new_id = cand_id ; break ; } } break ; default : return - EINVAL ; } d -> map [ type_id ] = new_id ; if ( type_id == new_id && btf_dedup_table_add ( d , h , type_id ) ) return - ENOMEM ; return 0 ; }",
    "resources/libbpf/src/btf.c@btf_dedup_prim_types": "static int btf_dedup_prim_types ( struct btf_dedup * d ) { int i , err ; for ( i = 0 ; i < d -> btf -> nr_types ; i ++ ) { err = btf_dedup_prim_type ( d , d -> btf -> start_id + i ) ; if ( err ) return err ; } return 0 ; }",
    "resources/libbpf/src/btf.c@is_type_mapped": "static inline bool is_type_mapped ( struct btf_dedup * d , uint32_t type_id ) { return d -> map [ type_id ] <= BTF_MAX_NR_TYPES ; }",
    "resources/libbpf/src/btf.c@resolve_type_id": "static inline __u32 resolve_type_id ( struct btf_dedup * d , __u32 type_id ) { while ( is_type_mapped ( d , type_id ) && d -> map [ type_id ] != type_id ) type_id = d -> map [ type_id ] ; return type_id ; }",
    "resources/libbpf/src/btf.c@resolve_fwd_id": "static uint32_t resolve_fwd_id ( struct btf_dedup * d , uint32_t type_id ) { __u32 orig_type_id = type_id ; if ( ! btf_is_fwd ( btf__type_by_id ( d -> btf , type_id ) ) ) return type_id ; while ( is_type_mapped ( d , type_id ) && d -> map [ type_id ] != type_id ) type_id = d -> map [ type_id ] ; if ( ! btf_is_fwd ( btf__type_by_id ( d -> btf , type_id ) ) ) return type_id ; return orig_type_id ; }",
    "resources/libbpf/src/btf.c@btf_fwd_kind": "static inline __u16 btf_fwd_kind ( struct btf_type * t ) { return btf_kflag ( t ) ? BTF_KIND_UNION : BTF_KIND_STRUCT ; }",
    "resources/libbpf/src/btf.c@btf_dedup_identical_arrays": "static bool btf_dedup_identical_arrays ( struct btf_dedup * d , __u32 id1 , __u32 id2 ) { struct btf_type * t1 , * t2 ; t1 = btf_type_by_id ( d -> btf , id1 ) ; t2 = btf_type_by_id ( d -> btf , id2 ) ; if ( ! btf_is_array ( t1 ) || ! btf_is_array ( t2 ) ) return false ; return btf_equal_array ( t1 , t2 ) ; }",
    "resources/libbpf/src/btf.c@btf_dedup_identical_structs": "static bool btf_dedup_identical_structs ( struct btf_dedup * d , __u32 id1 , __u32 id2 ) { const struct btf_member * m1 , * m2 ; struct btf_type * t1 , * t2 ; int n , i ; t1 = btf_type_by_id ( d -> btf , id1 ) ; t2 = btf_type_by_id ( d -> btf , id2 ) ; if ( ! btf_is_composite ( t1 ) || btf_kind ( t1 ) != btf_kind ( t2 ) ) return false ; if ( ! btf_shallow_equal_struct ( t1 , t2 ) ) return false ; m1 = btf_members ( t1 ) ; m2 = btf_members ( t2 ) ; for ( i = 0 , n = btf_vlen ( t1 ) ; i < n ; i ++ , m1 ++ , m2 ++ ) { if ( m1 -> type != m2 -> type && ! btf_dedup_identical_arrays ( d , m1 -> type , m2 -> type ) && ! btf_dedup_identical_structs ( d , m1 -> type , m2 -> type ) ) return false ; } return true ; }",
    "resources/libbpf/src/btf.c@btf_dedup_is_equiv": "static int btf_dedup_is_equiv ( struct btf_dedup * d , __u32 cand_id , __u32 canon_id ) { struct btf_type * cand_type ; struct btf_type * canon_type ; __u32 hypot_type_id ; __u16 cand_kind ; __u16 canon_kind ; int i , eq ; /* if both resolve to the same canonical, they must be equivalent */ if ( resolve_type_id ( d , cand_id ) == resolve_type_id ( d , canon_id ) ) return 1 ; canon_id = resolve_fwd_id ( d , canon_id ) ; hypot_type_id = d -> hypot_map [ canon_id ] ; if ( hypot_type_id <= BTF_MAX_NR_TYPES ) { if ( hypot_type_id == cand_id ) return 1 ; /* In some cases compiler will generate different DWARF types\n\t\t * for *identical* array type definitions and use them for\n\t\t * different fields within the *same* struct. This breaks type\n\t\t * equivalence check, which makes an assumption that candidate\n\t\t * types sub-graph has a consistent and deduped-by-compiler\n\t\t * types within a single CU. So work around that by explicitly\n\t\t * allowing identical array types here.\n\t\t */ if ( btf_dedup_identical_arrays ( d , hypot_type_id , cand_id ) ) return 1 ; /* It turns out that similar situation can happen with\n\t\t * struct/union sometimes, sigh... Handle the case where\n\t\t * structs/unions are exactly the same, down to the referenced\n\t\t * type IDs. Anything more complicated (e.g., if referenced\n\t\t * types are different, but equivalent) is *way more*\n\t\t * complicated and requires a many-to-many equivalence mapping.\n\t\t */ if ( btf_dedup_identical_structs ( d , hypot_type_id , cand_id ) ) return 1 ; return 0 ; } if ( btf_dedup_hypot_map_add ( d , canon_id , cand_id ) ) return - ENOMEM ; cand_type = btf_type_by_id ( d -> btf , cand_id ) ; canon_type = btf_type_by_id ( d -> btf , canon_id ) ; cand_kind = btf_kind ( cand_type ) ; canon_kind = btf_kind ( canon_type ) ; if ( cand_type -> name_off != canon_type -> name_off ) return 0 ; /* FWD <--> STRUCT/UNION equivalence check, if enabled */ if ( ( cand_kind == BTF_KIND_FWD || canon_kind == BTF_KIND_FWD ) && cand_kind != canon_kind ) { __u16 real_kind ; __u16 fwd_kind ; if ( cand_kind == BTF_KIND_FWD ) { real_kind = canon_kind ; fwd_kind = btf_fwd_kind ( cand_type ) ; } else { real_kind = cand_kind ; fwd_kind = btf_fwd_kind ( canon_type ) ; /* we'd need to resolve base FWD to STRUCT/UNION */ if ( fwd_kind == real_kind && canon_id < d -> btf -> start_id ) d -> hypot_adjust_canon = true ; } return fwd_kind == real_kind ; } if ( cand_kind != canon_kind ) return 0 ; switch ( cand_kind ) { case BTF_KIND_INT : return btf_equal_int_tag ( cand_type , canon_type ) ; case BTF_KIND_ENUM : case BTF_KIND_ENUM64 : return btf_compat_enum ( cand_type , canon_type ) ; case BTF_KIND_FWD : case BTF_KIND_FLOAT : return btf_equal_common ( cand_type , canon_type ) ; case BTF_KIND_CONST : case BTF_KIND_VOLATILE : case BTF_KIND_RESTRICT : case BTF_KIND_PTR : case BTF_KIND_TYPEDEF : case BTF_KIND_FUNC : case BTF_KIND_TYPE_TAG : if ( cand_type -> info != canon_type -> info ) return 0 ; return btf_dedup_is_equiv ( d , cand_type -> type , canon_type -> type ) ; case BTF_KIND_ARRAY : { const struct btf_array * cand_arr , * canon_arr ; if ( ! btf_compat_array ( cand_type , canon_type ) ) return 0 ; cand_arr = btf_array ( cand_type ) ; canon_arr = btf_array ( canon_type ) ; eq = btf_dedup_is_equiv ( d , cand_arr -> index_type , canon_arr -> index_type ) ; if ( eq <= 0 ) return eq ; return btf_dedup_is_equiv ( d , cand_arr -> type , canon_arr -> type ) ; } case BTF_KIND_STRUCT : case BTF_KIND_UNION : { const struct btf_member * cand_m , * canon_m ; __u16 vlen ; if ( ! btf_shallow_equal_struct ( cand_type , canon_type ) ) return 0 ; vlen = btf_vlen ( cand_type ) ; cand_m = btf_members ( cand_type ) ; canon_m = btf_members ( canon_type ) ; for ( i = 0 ; i < vlen ; i ++ ) { eq = btf_dedup_is_equiv ( d , cand_m -> type , canon_m -> type ) ; if ( eq <= 0 ) return eq ; cand_m ++ ; canon_m ++ ; } return 1 ; } case BTF_KIND_FUNC_PROTO : { const struct btf_param * cand_p , * canon_p ; __u16 vlen ; if ( ! btf_compat_fnproto ( cand_type , canon_type ) ) return 0 ; eq = btf_dedup_is_equiv ( d , cand_type -> type , canon_type -> type ) ; if ( eq <= 0 ) return eq ; vlen = btf_vlen ( cand_type ) ; cand_p = btf_params ( cand_type ) ; canon_p = btf_params ( canon_type ) ; for ( i = 0 ; i < vlen ; i ++ ) { eq = btf_dedup_is_equiv ( d , cand_p -> type , canon_p -> type ) ; if ( eq <= 0 ) return eq ; cand_p ++ ; canon_p ++ ; } return 1 ; } default : return - EINVAL ; } return 0 ; }",
    "resources/libbpf/src/btf.c@btf_dedup_merge_hypot_map": "static void btf_dedup_merge_hypot_map ( struct btf_dedup * d ) { __u32 canon_type_id , targ_type_id ; __u16 t_kind , c_kind ; __u32 t_id , c_id ; int i ; for ( i = 0 ; i < d -> hypot_cnt ; i ++ ) { canon_type_id = d -> hypot_list [ i ] ; targ_type_id = d -> hypot_map [ canon_type_id ] ; t_id = resolve_type_id ( d , targ_type_id ) ; c_id = resolve_type_id ( d , canon_type_id ) ; t_kind = btf_kind ( btf__type_by_id ( d -> btf , t_id ) ) ; c_kind = btf_kind ( btf__type_by_id ( d -> btf , c_id ) ) ; /*\n\t\t * Resolve FWD into STRUCT/UNION.\n\t\t * It's ok to resolve FWD into STRUCT/UNION that's not yet\n\t\t * mapped to canonical representative (as opposed to\n\t\t * STRUCT/UNION <--> STRUCT/UNION mapping logic below), because\n\t\t * eventually that struct is going to be mapped and all resolved\n\t\t * FWDs will automatically resolve to correct canonical\n\t\t * representative. This will happen before ref type deduping,\n\t\t * which critically depends on stability of these mapping. This\n\t\t * stability is not a requirement for STRUCT/UNION equivalence\n\t\t * checks, though.\n\t\t */ /* if it's the split BTF case, we still need to point base FWD\n\t\t * to STRUCT/UNION in a split BTF, because FWDs from split BTF\n\t\t * will be resolved against base FWD. If we don't point base\n\t\t * canonical FWD to the resolved STRUCT/UNION, then all the\n\t\t * FWDs in split BTF won't be correctly resolved to a proper\n\t\t * STRUCT/UNION.\n\t\t */ if ( t_kind != BTF_KIND_FWD && c_kind == BTF_KIND_FWD ) d -> map [ c_id ] = t_id ; /* if graph equivalence determined that we'd need to adjust\n\t\t * base canonical types, then we need to only point base FWDs\n\t\t * to STRUCTs/UNIONs and do no more modifications. For all\n\t\t * other purposes the type graphs were not equivalent.\n\t\t */ if ( d -> hypot_adjust_canon ) continue ; if ( t_kind == BTF_KIND_FWD && c_kind != BTF_KIND_FWD ) d -> map [ t_id ] = c_id ; if ( ( t_kind == BTF_KIND_STRUCT || t_kind == BTF_KIND_UNION ) && c_kind != BTF_KIND_FWD && is_type_mapped ( d , c_id ) && ! is_type_mapped ( d , t_id ) ) { /*\n\t\t\t * as a perf optimization, we can map struct/union\n\t\t\t * that's part of type graph we just verified for\n\t\t\t * equivalence. We can do that for struct/union that has\n\t\t\t * canonical representative only, though.\n\t\t\t */ d -> map [ t_id ] = c_id ; } } }",
    "resources/libbpf/src/btf.c@btf_dedup_struct_type": "static int btf_dedup_struct_type ( struct btf_dedup * d , __u32 type_id ) { struct btf_type * cand_type , * t ; struct hashmap_entry * hash_entry ; /* if we don't find equivalent type, then we are canonical */ __u32 new_id = type_id ; __u16 kind ; long h ; /* already deduped or is in process of deduping (loop detected) */ if ( d -> map [ type_id ] <= BTF_MAX_NR_TYPES ) return 0 ; t = btf_type_by_id ( d -> btf , type_id ) ; kind = btf_kind ( t ) ; if ( kind != BTF_KIND_STRUCT && kind != BTF_KIND_UNION ) return 0 ; h = btf_hash_struct ( t ) ; for_each_dedup_cand ( d , hash_entry , h ) { __u32 cand_id = hash_entry -> value ; int eq ; /*\n\t\t * Even though btf_dedup_is_equiv() checks for\n\t\t * btf_shallow_equal_struct() internally when checking two\n\t\t * structs (unions) for equivalence, we need to guard here\n\t\t * from picking matching FWD type as a dedup candidate.\n\t\t * This can happen due to hash collision. In such case just\n\t\t * relying on btf_dedup_is_equiv() would lead to potentially\n\t\t * creating a loop (FWD -> STRUCT and STRUCT -> FWD), because\n\t\t * FWD and compatible STRUCT/UNION are considered equivalent.\n\t\t */ cand_type = btf_type_by_id ( d -> btf , cand_id ) ; if ( ! btf_shallow_equal_struct ( t , cand_type ) ) continue ; btf_dedup_clear_hypot_map ( d ) ; eq = btf_dedup_is_equiv ( d , type_id , cand_id ) ; if ( eq < 0 ) return eq ; if ( ! eq ) continue ; btf_dedup_merge_hypot_map ( d ) ; if ( d -> hypot_adjust_canon ) /* not really equivalent */ continue ; new_id = cand_id ; break ; } d -> map [ type_id ] = new_id ; if ( type_id == new_id && btf_dedup_table_add ( d , h , type_id ) ) return - ENOMEM ; return 0 ; }",
    "resources/libbpf/src/btf.c@btf_dedup_struct_types": "static int btf_dedup_struct_types ( struct btf_dedup * d ) { int i , err ; for ( i = 0 ; i < d -> btf -> nr_types ; i ++ ) { err = btf_dedup_struct_type ( d , d -> btf -> start_id + i ) ; if ( err ) return err ; } return 0 ; }",
    "resources/libbpf/src/btf.c@btf_dedup_ref_type": "static int btf_dedup_ref_type ( struct btf_dedup * d , __u32 type_id ) { struct hashmap_entry * hash_entry ; __u32 new_id = type_id , cand_id ; struct btf_type * t , * cand ; /* if we don't find equivalent type, then we are representative type */ int ref_type_id ; long h ; if ( d -> map [ type_id ] == BTF_IN_PROGRESS_ID ) return - ELOOP ; if ( d -> map [ type_id ] <= BTF_MAX_NR_TYPES ) return resolve_type_id ( d , type_id ) ; t = btf_type_by_id ( d -> btf , type_id ) ; d -> map [ type_id ] = BTF_IN_PROGRESS_ID ; switch ( btf_kind ( t ) ) { case BTF_KIND_CONST : case BTF_KIND_VOLATILE : case BTF_KIND_RESTRICT : case BTF_KIND_PTR : case BTF_KIND_TYPEDEF : case BTF_KIND_FUNC : case BTF_KIND_TYPE_TAG : ref_type_id = btf_dedup_ref_type ( d , t -> type ) ; if ( ref_type_id < 0 ) return ref_type_id ; t -> type = ref_type_id ; h = btf_hash_common ( t ) ; for_each_dedup_cand ( d , hash_entry , h ) { cand_id = hash_entry -> value ; cand = btf_type_by_id ( d -> btf , cand_id ) ; if ( btf_equal_common ( t , cand ) ) { new_id = cand_id ; break ; } } break ; case BTF_KIND_DECL_TAG : ref_type_id = btf_dedup_ref_type ( d , t -> type ) ; if ( ref_type_id < 0 ) return ref_type_id ; t -> type = ref_type_id ; h = btf_hash_int_decl_tag ( t ) ; for_each_dedup_cand ( d , hash_entry , h ) { cand_id = hash_entry -> value ; cand = btf_type_by_id ( d -> btf , cand_id ) ; if ( btf_equal_int_tag ( t , cand ) ) { new_id = cand_id ; break ; } } break ; case BTF_KIND_ARRAY : { struct btf_array * info = btf_array ( t ) ; ref_type_id = btf_dedup_ref_type ( d , info -> type ) ; if ( ref_type_id < 0 ) return ref_type_id ; info -> type = ref_type_id ; ref_type_id = btf_dedup_ref_type ( d , info -> index_type ) ; if ( ref_type_id < 0 ) return ref_type_id ; info -> index_type = ref_type_id ; h = btf_hash_array ( t ) ; for_each_dedup_cand ( d , hash_entry , h ) { cand_id = hash_entry -> value ; cand = btf_type_by_id ( d -> btf , cand_id ) ; if ( btf_equal_array ( t , cand ) ) { new_id = cand_id ; break ; } } break ; } case BTF_KIND_FUNC_PROTO : { struct btf_param * param ; __u16 vlen ; int i ; ref_type_id = btf_dedup_ref_type ( d , t -> type ) ; if ( ref_type_id < 0 ) return ref_type_id ; t -> type = ref_type_id ; vlen = btf_vlen ( t ) ; param = btf_params ( t ) ; for ( i = 0 ; i < vlen ; i ++ ) { ref_type_id = btf_dedup_ref_type ( d , param -> type ) ; if ( ref_type_id < 0 ) return ref_type_id ; param -> type = ref_type_id ; param ++ ; } h = btf_hash_fnproto ( t ) ; for_each_dedup_cand ( d , hash_entry , h ) { cand_id = hash_entry -> value ; cand = btf_type_by_id ( d -> btf , cand_id ) ; if ( btf_equal_fnproto ( t , cand ) ) { new_id = cand_id ; break ; } } break ; } default : return - EINVAL ; } d -> map [ type_id ] = new_id ; if ( type_id == new_id && btf_dedup_table_add ( d , h , type_id ) ) return - ENOMEM ; return new_id ; }",
    "resources/libbpf/src/btf.c@btf_dedup_ref_types": "static int btf_dedup_ref_types ( struct btf_dedup * d ) { int i , err ; for ( i = 0 ; i < d -> btf -> nr_types ; i ++ ) { err = btf_dedup_ref_type ( d , d -> btf -> start_id + i ) ; if ( err < 0 ) return err ; } /* we won't need d->dedup_table anymore */ hashmap__free ( d -> dedup_table ) ; d -> dedup_table = NULL ; return 0 ; }",
    "resources/libbpf/src/btf.c@btf_dedup_fill_unique_names_map": "static int btf_dedup_fill_unique_names_map ( struct btf_dedup * d , struct hashmap * names_map ) { __u32 nr_types = btf__type_cnt ( d -> btf ) ; struct btf_type * t ; __u32 type_id ; __u16 kind ; int err ; /*\n\t * Iterate over base and split module ids in order to get all\n\t * available structs in the map.\n\t */ for ( type_id = 1 ; type_id < nr_types ; ++ type_id ) { t = btf_type_by_id ( d -> btf , type_id ) ; kind = btf_kind ( t ) ; if ( kind != BTF_KIND_STRUCT && kind != BTF_KIND_UNION ) continue ; /* Skip non-canonical types */ if ( type_id != d -> map [ type_id ] ) continue ; err = hashmap__add ( names_map , t -> name_off , type_id ) ; if ( err == - EEXIST ) err = hashmap__set ( names_map , t -> name_off , 0 , NULL , NULL ) ; if ( err ) return err ; } return 0 ; }",
    "resources/libbpf/src/btf.c@btf_dedup_resolve_fwd": "static int btf_dedup_resolve_fwd ( struct btf_dedup * d , struct hashmap * names_map , __u32 type_id ) { struct btf_type * t = btf_type_by_id ( d -> btf , type_id ) ; enum btf_fwd_kind fwd_kind = btf_kflag ( t ) ; __u16 cand_kind , kind = btf_kind ( t ) ; struct btf_type * cand_t ; uintptr_t cand_id ; if ( kind != BTF_KIND_FWD ) return 0 ; /* Skip if this FWD already has a mapping */ if ( type_id != d -> map [ type_id ] ) return 0 ; if ( ! hashmap__find ( names_map , t -> name_off , & cand_id ) ) return 0 ; /* Zero is a special value indicating that name is not unique */ if ( ! cand_id ) return 0 ; cand_t = btf_type_by_id ( d -> btf , cand_id ) ; cand_kind = btf_kind ( cand_t ) ; if ( ( cand_kind == BTF_KIND_STRUCT && fwd_kind != BTF_FWD_STRUCT ) || ( cand_kind == BTF_KIND_UNION && fwd_kind != BTF_FWD_UNION ) ) return 0 ; d -> map [ type_id ] = cand_id ; return 0 ; }",
    "resources/libbpf/src/btf.c@btf_dedup_resolve_fwds": "static int btf_dedup_resolve_fwds ( struct btf_dedup * d ) { int i , err ; struct hashmap * names_map ; names_map = hashmap__new ( btf_dedup_identity_hash_fn , btf_dedup_equal_fn , NULL ) ; if ( IS_ERR ( names_map ) ) return PTR_ERR ( names_map ) ; err = btf_dedup_fill_unique_names_map ( d , names_map ) ; if ( err < 0 ) goto exit ; for ( i = 0 ; i < d -> btf -> nr_types ; i ++ ) { err = btf_dedup_resolve_fwd ( d , names_map , d -> btf -> start_id + i ) ; if ( err < 0 ) break ; } exit : hashmap__free ( names_map ) ; return err ; }",
    "resources/libbpf/src/btf.c@btf_dedup_compact_types": "static int btf_dedup_compact_types ( struct btf_dedup * d ) { __u32 * new_offs ; __u32 next_type_id = d -> btf -> start_id ; const struct btf_type * t ; void * p ; int i , id , len ; /* we are going to reuse hypot_map to store compaction remapping */ d -> hypot_map [ 0 ] = 0 ; /* base BTF types are not renumbered */ for ( id = 1 ; id < d -> btf -> start_id ; id ++ ) d -> hypot_map [ id ] = id ; for ( i = 0 , id = d -> btf -> start_id ; i < d -> btf -> nr_types ; i ++ , id ++ ) d -> hypot_map [ id ] = BTF_UNPROCESSED_ID ; p = d -> btf -> types_data ; for ( i = 0 , id = d -> btf -> start_id ; i < d -> btf -> nr_types ; i ++ , id ++ ) { if ( d -> map [ id ] != id ) continue ; t = btf__type_by_id ( d -> btf , id ) ; len = btf_type_size ( t ) ; if ( len < 0 ) return len ; memmove ( p , t , len ) ; d -> hypot_map [ id ] = next_type_id ; d -> btf -> type_offs [ next_type_id - d -> btf -> start_id ] = p - d -> btf -> types_data ; p += len ; next_type_id ++ ; } /* shrink struct btf's internal types index and update btf_header */ d -> btf -> nr_types = next_type_id - d -> btf -> start_id ; d -> btf -> type_offs_cap = d -> btf -> nr_types ; d -> btf -> hdr -> type_len = p - d -> btf -> types_data ; new_offs = libbpf_reallocarray ( d -> btf -> type_offs , d -> btf -> type_offs_cap , sizeof ( * new_offs ) ) ; if ( d -> btf -> type_offs_cap && ! new_offs ) return - ENOMEM ; d -> btf -> type_offs = new_offs ; d -> btf -> hdr -> str_off = d -> btf -> hdr -> type_len ; d -> btf -> raw_size = d -> btf -> hdr -> hdr_len + d -> btf -> hdr -> type_len + d -> btf -> hdr -> str_len ; return 0 ; }",
    "resources/libbpf/src/btf.c@btf_dedup_remap_type_id": "static int btf_dedup_remap_type_id ( __u32 * type_id , void * ctx ) { struct btf_dedup * d = ctx ; __u32 resolved_type_id , new_type_id ; resolved_type_id = resolve_type_id ( d , * type_id ) ; new_type_id = d -> hypot_map [ resolved_type_id ] ; if ( new_type_id > BTF_MAX_NR_TYPES ) return - EINVAL ; * type_id = new_type_id ; return 0 ; }",
    "resources/libbpf/src/btf.c@btf_dedup_remap_types": "static int btf_dedup_remap_types ( struct btf_dedup * d ) { int i , r ; for ( i = 0 ; i < d -> btf -> nr_types ; i ++ ) { struct btf_type * t = btf_type_by_id ( d -> btf , d -> btf -> start_id + i ) ; r = btf_type_visit_type_ids ( t , btf_dedup_remap_type_id , d ) ; if ( r ) return r ; } if ( ! d -> btf_ext ) return 0 ; r = btf_ext_visit_type_ids ( d -> btf_ext , btf_dedup_remap_type_id , d ) ; if ( r ) return r ; return 0 ; }",
    "resources/libbpf/src/btf.c@btf__load_vmlinux_btf": "struct btf * btf__load_vmlinux_btf ( void ) { const char * sysfs_btf_path = \"/sys/kernel/btf/vmlinux\" ; /* fall back locations, trying to find vmlinux on disk */ const char * locations [ ] = { \"/boot/vmlinux-%1$s\" , \"/lib/modules/%1$s/vmlinux-%1$s\" , \"/lib/modules/%1$s/build/vmlinux\" , \"/usr/lib/modules/%1$s/kernel/vmlinux\" , \"/usr/lib/debug/boot/vmlinux-%1$s\" , \"/usr/lib/debug/boot/vmlinux-%1$s.debug\" , \"/usr/lib/debug/lib/modules/%1$s/vmlinux\" , } ; char path [ PATH_MAX + 1 ] ; struct utsname buf ; struct btf * btf ; int i , err ; /* is canonical sysfs location accessible? */ if ( faccessat ( AT_FDCWD , sysfs_btf_path , F_OK , AT_EACCESS ) < 0 ) { pr_warn ( \"kernel BTF is missing at '%s', was CONFIG_DEBUG_INFO_BTF enabled?\\n\" , sysfs_btf_path ) ; } else { btf = btf__parse ( sysfs_btf_path , NULL ) ; if ( ! btf ) { err = - errno ; pr_warn ( \"failed to read kernel BTF from '%s': %d\\n\" , sysfs_btf_path , err ) ; return libbpf_err_ptr ( err ) ; } pr_debug ( \"loaded kernel BTF from '%s'\\n\" , sysfs_btf_path ) ; return btf ; } /* try fallback locations */ uname ( & buf ) ; for ( i = 0 ; i < ARRAY_SIZE ( locations ) ; i ++ ) { snprintf ( path , PATH_MAX , locations [ i ] , buf . release ) ; if ( faccessat ( AT_FDCWD , path , R_OK , AT_EACCESS ) ) continue ; btf = btf__parse ( path , NULL ) ; err = libbpf_get_error ( btf ) ; pr_debug ( \"loading kernel BTF '%s': %d\\n\" , path , err ) ; if ( err ) continue ; return btf ; } pr_warn ( \"failed to find valid kernel BTF\\n\" ) ; return libbpf_err_ptr ( - ESRCH ) ; }",
    "resources/libbpf/src/btf.c@btf__load_module_btf": "struct btf * btf__load_module_btf ( const char * module_name , struct btf * vmlinux_btf ) { char path [ 80 ] ; snprintf ( path , sizeof ( path ) , \"/sys/kernel/btf/%s\" , module_name ) ; return btf__parse_split ( path , vmlinux_btf ) ; }",
    "resources/libbpf/src/btf.c@btf_type_visit_type_ids": "int btf_type_visit_type_ids ( struct btf_type * t , type_id_visit_fn visit , void * ctx ) { int i , n , err ; switch ( btf_kind ( t ) ) { case BTF_KIND_INT : case BTF_KIND_FLOAT : case BTF_KIND_ENUM : case BTF_KIND_ENUM64 : return 0 ; case BTF_KIND_FWD : case BTF_KIND_CONST : case BTF_KIND_VOLATILE : case BTF_KIND_RESTRICT : case BTF_KIND_PTR : case BTF_KIND_TYPEDEF : case BTF_KIND_FUNC : case BTF_KIND_VAR : case BTF_KIND_DECL_TAG : case BTF_KIND_TYPE_TAG : return visit ( & t -> type , ctx ) ; case BTF_KIND_ARRAY : { struct btf_array * a = btf_array ( t ) ; err = visit ( & a -> type , ctx ) ; err = err ? : visit ( & a -> index_type , ctx ) ; return err ; } case BTF_KIND_STRUCT : case BTF_KIND_UNION : { struct btf_member * m = btf_members ( t ) ; for ( i = 0 , n = btf_vlen ( t ) ; i < n ; i ++ , m ++ ) { err = visit ( & m -> type , ctx ) ; if ( err ) return err ; } return 0 ; } case BTF_KIND_FUNC_PROTO : { struct btf_param * m = btf_params ( t ) ; err = visit ( & t -> type , ctx ) ; if ( err ) return err ; for ( i = 0 , n = btf_vlen ( t ) ; i < n ; i ++ , m ++ ) { err = visit ( & m -> type , ctx ) ; if ( err ) return err ; } return 0 ; } case BTF_KIND_DATASEC : { struct btf_var_secinfo * m = btf_var_secinfos ( t ) ; for ( i = 0 , n = btf_vlen ( t ) ; i < n ; i ++ , m ++ ) { err = visit ( & m -> type , ctx ) ; if ( err ) return err ; } return 0 ; } default : return - EINVAL ; } }",
    "resources/libbpf/src/btf.c@btf_type_visit_str_offs": "int btf_type_visit_str_offs ( struct btf_type * t , str_off_visit_fn visit , void * ctx ) { int i , n , err ; err = visit ( & t -> name_off , ctx ) ; if ( err ) return err ; switch ( btf_kind ( t ) ) { case BTF_KIND_STRUCT : case BTF_KIND_UNION : { struct btf_member * m = btf_members ( t ) ; for ( i = 0 , n = btf_vlen ( t ) ; i < n ; i ++ , m ++ ) { err = visit ( & m -> name_off , ctx ) ; if ( err ) return err ; } break ; } case BTF_KIND_ENUM : { struct btf_enum * m = btf_enum ( t ) ; for ( i = 0 , n = btf_vlen ( t ) ; i < n ; i ++ , m ++ ) { err = visit ( & m -> name_off , ctx ) ; if ( err ) return err ; } break ; } case BTF_KIND_ENUM64 : { struct btf_enum64 * m = btf_enum64 ( t ) ; for ( i = 0 , n = btf_vlen ( t ) ; i < n ; i ++ , m ++ ) { err = visit ( & m -> name_off , ctx ) ; if ( err ) return err ; } break ; } case BTF_KIND_FUNC_PROTO : { struct btf_param * m = btf_params ( t ) ; for ( i = 0 , n = btf_vlen ( t ) ; i < n ; i ++ , m ++ ) { err = visit ( & m -> name_off , ctx ) ; if ( err ) return err ; } break ; } default : break ; } return 0 ; }",
    "resources/libbpf/src/btf.c@btf_ext_visit_type_ids": "int btf_ext_visit_type_ids ( struct btf_ext * btf_ext , type_id_visit_fn visit , void * ctx ) { const struct btf_ext_info * seg ; struct btf_ext_info_sec * sec ; int i , err ; seg = & btf_ext -> func_info ; for_each_btf_ext_sec ( seg , sec ) { struct bpf_func_info_min * rec ; for_each_btf_ext_rec ( seg , sec , i , rec ) { err = visit ( & rec -> type_id , ctx ) ; if ( err < 0 ) return err ; } } seg = & btf_ext -> core_relo_info ; for_each_btf_ext_sec ( seg , sec ) { struct bpf_core_relo * rec ; for_each_btf_ext_rec ( seg , sec , i , rec ) { err = visit ( & rec -> type_id , ctx ) ; if ( err < 0 ) return err ; } } return 0 ; }",
    "resources/libbpf/src/btf.c@btf_ext_visit_str_offs": "int btf_ext_visit_str_offs ( struct btf_ext * btf_ext , str_off_visit_fn visit , void * ctx ) { const struct btf_ext_info * seg ; struct btf_ext_info_sec * sec ; int i , err ; seg = & btf_ext -> func_info ; for_each_btf_ext_sec ( seg , sec ) { err = visit ( & sec -> sec_name_off , ctx ) ; if ( err ) return err ; } seg = & btf_ext -> line_info ; for_each_btf_ext_sec ( seg , sec ) { struct bpf_line_info_min * rec ; err = visit ( & sec -> sec_name_off , ctx ) ; if ( err ) return err ; for_each_btf_ext_rec ( seg , sec , i , rec ) { err = visit ( & rec -> file_name_off , ctx ) ; if ( err ) return err ; err = visit ( & rec -> line_off , ctx ) ; if ( err ) return err ; } } seg = & btf_ext -> core_relo_info ; for_each_btf_ext_sec ( seg , sec ) { struct bpf_core_relo * rec ; err = visit ( & sec -> sec_name_off , ctx ) ; if ( err ) return err ; for_each_btf_ext_rec ( seg , sec , i , rec ) { err = visit ( & rec -> access_str_off , ctx ) ; if ( err ) return err ; } } return 0 ; }",
    "resources/libbpf/src/gen_loader.c@btf_kind": "static inline __u16 btf_kind ( const struct btf_type * t ) { return BTF_INFO_KIND ( t -> info ) ; }",
    "resources/libbpf/src/gen_loader.c@btf_vlen": "static inline __u16 btf_vlen ( const struct btf_type * t ) { return BTF_INFO_VLEN ( t -> info ) ; }",
    "resources/libbpf/src/gen_loader.c@btf_kflag": "static inline bool btf_kflag ( const struct btf_type * t ) { return BTF_INFO_KFLAG ( t -> info ) ; }",
    "resources/libbpf/src/gen_loader.c@btf_is_void": "static inline bool btf_is_void ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_UNKN ; }",
    "resources/libbpf/src/gen_loader.c@btf_is_int": "static inline bool btf_is_int ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_INT ; }",
    "resources/libbpf/src/gen_loader.c@btf_is_ptr": "static inline bool btf_is_ptr ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_PTR ; }",
    "resources/libbpf/src/gen_loader.c@btf_is_array": "static inline bool btf_is_array ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_ARRAY ; }",
    "resources/libbpf/src/gen_loader.c@btf_is_struct": "static inline bool btf_is_struct ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_STRUCT ; }",
    "resources/libbpf/src/gen_loader.c@btf_is_union": "static inline bool btf_is_union ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_UNION ; }",
    "resources/libbpf/src/gen_loader.c@btf_is_composite": "static inline bool btf_is_composite ( const struct btf_type * t ) { __u16 kind = btf_kind ( t ) ; return kind == BTF_KIND_STRUCT || kind == BTF_KIND_UNION ; }",
    "resources/libbpf/src/gen_loader.c@btf_is_enum": "static inline bool btf_is_enum ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_ENUM ; }",
    "resources/libbpf/src/gen_loader.c@btf_is_enum64": "static inline bool btf_is_enum64 ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_ENUM64 ; }",
    "resources/libbpf/src/gen_loader.c@btf_is_fwd": "static inline bool btf_is_fwd ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_FWD ; }",
    "resources/libbpf/src/gen_loader.c@btf_is_typedef": "static inline bool btf_is_typedef ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_TYPEDEF ; }",
    "resources/libbpf/src/gen_loader.c@btf_is_volatile": "static inline bool btf_is_volatile ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_VOLATILE ; }",
    "resources/libbpf/src/gen_loader.c@btf_is_const": "static inline bool btf_is_const ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_CONST ; }",
    "resources/libbpf/src/gen_loader.c@btf_is_restrict": "static inline bool btf_is_restrict ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_RESTRICT ; }",
    "resources/libbpf/src/gen_loader.c@btf_is_mod": "static inline bool btf_is_mod ( const struct btf_type * t ) { __u16 kind = btf_kind ( t ) ; return kind == BTF_KIND_VOLATILE || kind == BTF_KIND_CONST || kind == BTF_KIND_RESTRICT || kind == BTF_KIND_TYPE_TAG ; }",
    "resources/libbpf/src/gen_loader.c@btf_is_func": "static inline bool btf_is_func ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_FUNC ; }",
    "resources/libbpf/src/gen_loader.c@btf_is_func_proto": "static inline bool btf_is_func_proto ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_FUNC_PROTO ; }",
    "resources/libbpf/src/gen_loader.c@btf_is_var": "static inline bool btf_is_var ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_VAR ; }",
    "resources/libbpf/src/gen_loader.c@btf_is_datasec": "static inline bool btf_is_datasec ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_DATASEC ; }",
    "resources/libbpf/src/gen_loader.c@btf_is_float": "static inline bool btf_is_float ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_FLOAT ; }",
    "resources/libbpf/src/gen_loader.c@btf_is_decl_tag": "static inline bool btf_is_decl_tag ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_DECL_TAG ; }",
    "resources/libbpf/src/gen_loader.c@btf_is_type_tag": "static inline bool btf_is_type_tag ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_TYPE_TAG ; }",
    "resources/libbpf/src/gen_loader.c@btf_is_any_enum": "static inline bool btf_is_any_enum ( const struct btf_type * t ) { return btf_is_enum ( t ) || btf_is_enum64 ( t ) ; }",
    "resources/libbpf/src/gen_loader.c@btf_kind_core_compat": "static inline bool btf_kind_core_compat ( const struct btf_type * t1 , const struct btf_type * t2 ) { return btf_kind ( t1 ) == btf_kind ( t2 ) || ( btf_is_any_enum ( t1 ) && btf_is_any_enum ( t2 ) ) ; }",
    "resources/libbpf/src/gen_loader.c@btf_int_encoding": "static inline __u8 btf_int_encoding ( const struct btf_type * t ) { return BTF_INT_ENCODING ( * ( __u32 * ) ( t + 1 ) ) ; }",
    "resources/libbpf/src/gen_loader.c@btf_int_offset": "static inline __u8 btf_int_offset ( const struct btf_type * t ) { return BTF_INT_OFFSET ( * ( __u32 * ) ( t + 1 ) ) ; }",
    "resources/libbpf/src/gen_loader.c@btf_int_bits": "static inline __u8 btf_int_bits ( const struct btf_type * t ) { return BTF_INT_BITS ( * ( __u32 * ) ( t + 1 ) ) ; }",
    "resources/libbpf/src/gen_loader.c@btf_array": "static inline struct btf_array * btf_array ( const struct btf_type * t ) { return ( struct btf_array * ) ( t + 1 ) ; }",
    "resources/libbpf/src/gen_loader.c@btf_enum": "static inline struct btf_enum * btf_enum ( const struct btf_type * t ) { return ( struct btf_enum * ) ( t + 1 ) ; }",
    "resources/libbpf/src/gen_loader.c@btf_enum64": "static inline struct btf_enum64 * btf_enum64 ( const struct btf_type * t ) { return ( struct btf_enum64 * ) ( t + 1 ) ; }",
    "resources/libbpf/src/gen_loader.c@btf_enum64_value": "static inline __u64 btf_enum64_value ( const struct btf_enum64 * e ) { /* struct btf_enum64 is introduced in Linux 6.0, which is very\n\t * bleeding-edge. Here we are avoiding relying on struct btf_enum64\n\t * definition coming from kernel UAPI headers to support wider range\n\t * of system-wide kernel headers.\n\t *\n\t * Given this header can be also included from C++ applications, that\n\t * further restricts C tricks we can use (like using compatible\n\t * anonymous struct). So just treat struct btf_enum64 as\n\t * a three-element array of u32 and access second (lo32) and third\n\t * (hi32) elements directly.\n\t *\n\t * For reference, here is a struct btf_enum64 definition:\n\t *\n\t * const struct btf_enum64 {\n\t *\t__u32\tname_off;\n\t *\t__u32\tval_lo32;\n\t *\t__u32\tval_hi32;\n\t * };\n\t */ const __u32 * e64 = ( const __u32 * ) e ; return ( ( __u64 ) e64 [ 2 ] << 32 ) | e64 [ 1 ] ; }",
    "resources/libbpf/src/gen_loader.c@btf_members": "static inline struct btf_member * btf_members ( const struct btf_type * t ) { return ( struct btf_member * ) ( t + 1 ) ; }",
    "resources/libbpf/src/gen_loader.c@btf_member_bit_offset": "static inline __u32 btf_member_bit_offset ( const struct btf_type * t , __u32 member_idx ) { const struct btf_member * m = btf_members ( t ) + member_idx ; bool kflag = btf_kflag ( t ) ; return kflag ? BTF_MEMBER_BIT_OFFSET ( m -> offset ) : m -> offset ; }",
    "resources/libbpf/src/gen_loader.c@btf_member_bitfield_size": "static inline __u32 btf_member_bitfield_size ( const struct btf_type * t , __u32 member_idx ) { const struct btf_member * m = btf_members ( t ) + member_idx ; bool kflag = btf_kflag ( t ) ; return kflag ? BTF_MEMBER_BITFIELD_SIZE ( m -> offset ) : 0 ; }",
    "resources/libbpf/src/gen_loader.c@btf_params": "static inline struct btf_param * btf_params ( const struct btf_type * t ) { return ( struct btf_param * ) ( t + 1 ) ; }",
    "resources/libbpf/src/gen_loader.c@btf_var": "static inline struct btf_var * btf_var ( const struct btf_type * t ) { return ( struct btf_var * ) ( t + 1 ) ; }",
    "resources/libbpf/src/gen_loader.c@btf_var_secinfos": "static inline struct btf_var_secinfo * btf_var_secinfos ( const struct btf_type * t ) { return ( struct btf_var_secinfo * ) ( t + 1 ) ; }",
    "resources/libbpf/src/gen_loader.c@btf_decl_tag": "static inline struct btf_decl_tag * btf_decl_tag ( const struct btf_type * t ) { return ( struct btf_decl_tag * ) ( t + 1 ) ; }",
    "resources/libbpf/src/gen_loader.c@str_has_sfx": "static inline bool str_has_sfx ( const char * str , const char * sfx ) { size_t str_len = strlen ( str ) ; size_t sfx_len = strlen ( sfx ) ; if ( sfx_len > str_len ) return false ; return strcmp ( str + str_len - sfx_len , sfx ) == 0 ; }",
    "resources/libbpf/src/gen_loader.c@libbpf_reallocarray": "static inline void * libbpf_reallocarray ( void * ptr , size_t nmemb , size_t size ) { size_t total ; # if __has_builtin ( __builtin_mul_overflow ) if ( unlikely ( __builtin_mul_overflow ( nmemb , size , & total ) ) ) return NULL ; # else if ( size == 0 || nmemb > ULONG_MAX / size ) return NULL ; total = nmemb * size ; # endif return realloc ( ptr , total ) ; }",
    "resources/libbpf/src/gen_loader.c@libbpf_strlcpy": "static inline void libbpf_strlcpy ( char * dst , const char * src , size_t sz ) { size_t i ; if ( sz == 0 ) return ; sz -- ; for ( i = 0 ; i < sz && src [ i ] ; i ++ ) dst [ i ] = src [ i ] ; dst [ i ] = '\\0' ; }",
    "resources/libbpf/src/gen_loader.c@btf_func_linkage": "static inline enum btf_func_linkage btf_func_linkage ( const struct btf_type * t ) { return ( enum btf_func_linkage ) ( int ) btf_vlen ( t ) ; }",
    "resources/libbpf/src/gen_loader.c@btf_type_info": "static inline __u32 btf_type_info ( int kind , int vlen , int kflag ) { return ( kflag << 31 ) | ( kind << 24 ) | vlen ; }",
    "resources/libbpf/src/gen_loader.c@libbpf_is_mem_zeroed": "static inline bool libbpf_is_mem_zeroed ( const char * p , ssize_t len ) { while ( len > 0 ) { if ( * p ) return false ; p ++ ; len -- ; } return true ; }",
    "resources/libbpf/src/gen_loader.c@libbpf_validate_opts": "static inline bool libbpf_validate_opts ( const char * opts , size_t opts_sz , size_t user_sz , const char * type_name ) { if ( user_sz < sizeof ( size_t ) ) { pr_warn ( \"%s size (%zu) is too small\\n\" , type_name , user_sz ) ; return false ; } if ( ! libbpf_is_mem_zeroed ( opts + opts_sz , ( ssize_t ) user_sz - opts_sz ) ) { pr_warn ( \"%s has non-zero extra bytes\\n\" , type_name ) ; return false ; } return true ; }",
    "resources/libbpf/src/gen_loader.c@libbpf_err": "static inline int libbpf_err ( int ret ) { if ( ret < 0 ) errno = - ret ; return ret ; }",
    "resources/libbpf/src/gen_loader.c@libbpf_err_errno": "static inline int libbpf_err_errno ( int ret ) { /* errno is already assumed to be set on error */ return ret < 0 ? - errno : ret ; }",
    "resources/libbpf/src/gen_loader.c@libbpf_err_ptr": "static inline void * libbpf_err_ptr ( int err ) { /* set errno on error, this doesn't break anything */ errno = - err ; return NULL ; }",
    "resources/libbpf/src/gen_loader.c@libbpf_ptr": "static inline void * libbpf_ptr ( void * ret ) { /* set errno on error, this doesn't break anything */ if ( IS_ERR ( ret ) ) errno = - PTR_ERR ( ret ) ; return IS_ERR ( ret ) ? NULL : ret ; }",
    "resources/libbpf/src/gen_loader.c@str_is_empty": "static inline bool str_is_empty ( const char * s ) { return ! s || ! s [ 0 ] ; }",
    "resources/libbpf/src/gen_loader.c@is_ldimm64_insn": "static inline bool is_ldimm64_insn ( struct bpf_insn * insn ) { return insn -> code == ( BPF_LD | BPF_IMM | BPF_DW ) ; }",
    "resources/libbpf/src/gen_loader.c@dup_good_fd": "static inline int dup_good_fd ( int fd ) { if ( fd < 0 ) return fd ; return fcntl ( fd , F_DUPFD_CLOEXEC , 3 ) ; }",
    "resources/libbpf/src/gen_loader.c@ensure_good_fd": "static inline int ensure_good_fd ( int fd ) { int old_fd = fd , saved_errno ; if ( fd < 0 ) return fd ; if ( fd < 3 ) { fd = dup_good_fd ( fd ) ; saved_errno = errno ; close ( old_fd ) ; errno = saved_errno ; if ( fd < 0 ) { pr_warn ( \"failed to dup FD %d to FD > 2: %d\\n\" , old_fd , - saved_errno ) ; errno = saved_errno ; } } return fd ; }",
    "resources/libbpf/src/gen_loader.c@sys_dup2": "static inline int sys_dup2 ( int oldfd , int newfd ) { # ifdef __NR_dup2 return syscall ( __NR_dup2 , oldfd , newfd ) ; # else return syscall ( __NR_dup3 , oldfd , newfd , 0 ) ; # endif }",
    "resources/libbpf/src/gen_loader.c@reuse_fd": "static inline int reuse_fd ( int fixed_fd , int tmp_fd ) { int err ; err = sys_dup2 ( tmp_fd , fixed_fd ) ; err = err < 0 ? - errno : 0 ; close ( tmp_fd ) ; /* clean up temporary FD */ return err ; }",
    "resources/libbpf/src/gen_loader.c@is_pow_of_2": "static inline bool is_pow_of_2 ( size_t x ) { return x && ( x & ( x - 1 ) ) == 0 ; }",
    "resources/libbpf/src/gen_loader.c@hash_bits": "static inline size_t hash_bits ( size_t h , int bits ) { /* shuffle bits and return requested number of upper bits */ if ( bits == 0 ) return 0 ; # if ( __SIZEOF_SIZE_T__ == __SIZEOF_LONG_LONG__ ) /* LP64 case */ return ( h * 11400714819323198485llu ) >> ( __SIZEOF_LONG_LONG__ * 8 - bits ) ; # elif ( __SIZEOF_SIZE_T__ <= __SIZEOF_LONG__ ) return ( h * 2654435769lu ) >> ( __SIZEOF_LONG__ * 8 - bits ) ; # else # error \"Unsupported size_t size\" # endif }",
    "resources/libbpf/src/gen_loader.c@str_hash": "static inline size_t str_hash ( const char * s ) { size_t h = 0 ; while ( * s ) { h = h * 31 + * s ; s ++ ; } return h ; }",
    "resources/libbpf/src/gen_loader.c@skel_sys_bpf": "static inline int skel_sys_bpf ( enum bpf_cmd cmd , union bpf_attr * attr , unsigned int size ) { # ifdef __KERNEL__ return kern_sys_bpf ( cmd , attr , size ) ; # else return syscall ( __NR_bpf , cmd , attr , size ) ; # endif }",
    "resources/libbpf/src/gen_loader.c@skel_alloc": "static inline void * skel_alloc ( size_t size ) { return calloc ( 1 , size ) ; }",
    "resources/libbpf/src/gen_loader.c@skel_free": "static inline void skel_free ( void * p ) { free ( p ) ; }",
    "resources/libbpf/src/gen_loader.c@skel_free_map_data": "static inline void skel_free_map_data ( void * p , __u64 addr , size_t sz ) { munmap ( p , sz ) ; }",
    "resources/libbpf/src/gen_loader.c@skel_prep_map_data": "static inline void * skel_prep_map_data ( const void * val , size_t mmap_sz , size_t val_sz ) { void * addr ; addr = mmap ( NULL , mmap_sz , PROT_READ | PROT_WRITE , MAP_SHARED | MAP_ANONYMOUS , - 1 , 0 ) ; if ( addr == ( void * ) - 1 ) return NULL ; memcpy ( addr , val , val_sz ) ; return addr ; }",
    "resources/libbpf/src/gen_loader.c@skel_finalize_map_data": "static inline void * skel_finalize_map_data ( __u64 * init_val , size_t mmap_sz , int flags , int fd ) { void * addr ; addr = mmap ( ( void * ) ( long ) * init_val , mmap_sz , flags , MAP_SHARED | MAP_FIXED , fd , 0 ) ; if ( addr == ( void * ) - 1 ) return NULL ; return addr ; }",
    "resources/libbpf/src/gen_loader.c@skel_closenz": "static inline int skel_closenz ( int fd ) { if ( fd > 0 ) return close ( fd ) ; return - EINVAL ; }",
    "resources/libbpf/src/gen_loader.c@skel_map_create": "static inline int skel_map_create ( enum bpf_map_type map_type , const char * map_name , __u32 key_size , __u32 value_size , __u32 max_entries ) { const size_t attr_sz = offsetofend ( union bpf_attr , map_extra ) ; union bpf_attr attr ; memset ( & attr , 0 , attr_sz ) ; attr . map_type = map_type ; strncpy ( attr . map_name , map_name , sizeof ( attr . map_name ) ) ; attr . key_size = key_size ; attr . value_size = value_size ; attr . max_entries = max_entries ; return skel_sys_bpf ( BPF_MAP_CREATE , & attr , attr_sz ) ; }",
    "resources/libbpf/src/gen_loader.c@skel_map_update_elem": "static inline int skel_map_update_elem ( int fd , const void * key , const void * value , __u64 flags ) { const size_t attr_sz = offsetofend ( union bpf_attr , flags ) ; union bpf_attr attr ; memset ( & attr , 0 , attr_sz ) ; attr . map_fd = fd ; attr . key = ( long ) key ; attr . value = ( long ) value ; attr . flags = flags ; return skel_sys_bpf ( BPF_MAP_UPDATE_ELEM , & attr , attr_sz ) ; }",
    "resources/libbpf/src/gen_loader.c@skel_map_delete_elem": "static inline int skel_map_delete_elem ( int fd , const void * key ) { const size_t attr_sz = offsetofend ( union bpf_attr , flags ) ; union bpf_attr attr ; memset ( & attr , 0 , attr_sz ) ; attr . map_fd = fd ; attr . key = ( long ) key ; return skel_sys_bpf ( BPF_MAP_DELETE_ELEM , & attr , attr_sz ) ; }",
    "resources/libbpf/src/gen_loader.c@skel_map_get_fd_by_id": "static inline int skel_map_get_fd_by_id ( __u32 id ) { const size_t attr_sz = offsetofend ( union bpf_attr , flags ) ; union bpf_attr attr ; memset ( & attr , 0 , attr_sz ) ; attr . map_id = id ; return skel_sys_bpf ( BPF_MAP_GET_FD_BY_ID , & attr , attr_sz ) ; }",
    "resources/libbpf/src/gen_loader.c@skel_raw_tracepoint_open": "static inline int skel_raw_tracepoint_open ( const char * name , int prog_fd ) { const size_t attr_sz = offsetofend ( union bpf_attr , raw_tracepoint . prog_fd ) ; union bpf_attr attr ; memset ( & attr , 0 , attr_sz ) ; attr . raw_tracepoint . name = ( long ) name ; attr . raw_tracepoint . prog_fd = prog_fd ; return skel_sys_bpf ( BPF_RAW_TRACEPOINT_OPEN , & attr , attr_sz ) ; }",
    "resources/libbpf/src/gen_loader.c@skel_link_create": "static inline int skel_link_create ( int prog_fd , int target_fd , enum bpf_attach_type attach_type ) { const size_t attr_sz = offsetofend ( union bpf_attr , link_create . iter_info_len ) ; union bpf_attr attr ; memset ( & attr , 0 , attr_sz ) ; attr . link_create . prog_fd = prog_fd ; attr . link_create . target_fd = target_fd ; attr . link_create . attach_type = attach_type ; return skel_sys_bpf ( BPF_LINK_CREATE , & attr , attr_sz ) ; }",
    "resources/libbpf/src/gen_loader.c@bpf_load_and_run": "static inline int bpf_load_and_run ( struct bpf_load_and_run_opts * opts ) { const size_t prog_load_attr_sz = offsetofend ( union bpf_attr , fd_array ) ; const size_t test_run_attr_sz = offsetofend ( union bpf_attr , test ) ; int map_fd = - 1 , prog_fd = - 1 , key = 0 , err ; union bpf_attr attr ; err = map_fd = skel_map_create ( BPF_MAP_TYPE_ARRAY , \"__loader.map\" , 4 , opts -> data_sz , 1 ) ; if ( map_fd < 0 ) { opts -> errstr = \"failed to create loader map\" ; set_err ; goto out ; } err = skel_map_update_elem ( map_fd , & key , opts -> data , 0 ) ; if ( err < 0 ) { opts -> errstr = \"failed to update loader map\" ; set_err ; goto out ; } memset ( & attr , 0 , prog_load_attr_sz ) ; attr . prog_type = BPF_PROG_TYPE_SYSCALL ; attr . insns = ( long ) opts -> insns ; attr . insn_cnt = opts -> insns_sz / sizeof ( struct bpf_insn ) ; attr . license = ( long ) \"Dual BSD/GPL\" ; memcpy ( attr . prog_name , \"__loader.prog\" , sizeof ( \"__loader.prog\" ) ) ; attr . fd_array = ( long ) & map_fd ; attr . log_level = opts -> ctx -> log_level ; attr . log_size = opts -> ctx -> log_size ; attr . log_buf = opts -> ctx -> log_buf ; attr . prog_flags = BPF_F_SLEEPABLE ; err = prog_fd = skel_sys_bpf ( BPF_PROG_LOAD , & attr , prog_load_attr_sz ) ; if ( prog_fd < 0 ) { opts -> errstr = \"failed to load loader prog\" ; set_err ; goto out ; } memset ( & attr , 0 , test_run_attr_sz ) ; attr . test . prog_fd = prog_fd ; attr . test . ctx_in = ( long ) opts -> ctx ; attr . test . ctx_size_in = opts -> ctx -> sz ; err = skel_sys_bpf ( BPF_PROG_RUN , & attr , test_run_attr_sz ) ; if ( err < 0 || ( int ) attr . test . retval < 0 ) { opts -> errstr = \"failed to execute loader prog\" ; if ( err < 0 ) { set_err ; } else { err = ( int ) attr . test . retval ; # ifndef __KERNEL__ errno = - err ; # endif } goto out ; } err = 0 ; out : if ( map_fd >= 0 ) close ( map_fd ) ; if ( prog_fd >= 0 ) close ( prog_fd ) ; return err ; }",
    "resources/libbpf/src/gen_loader.c@blob_fd_array_off": "static int blob_fd_array_off ( struct bpf_gen * gen , int index ) { return gen -> fd_array + index * sizeof ( int ) ; }",
    "resources/libbpf/src/gen_loader.c@realloc_insn_buf": "static int realloc_insn_buf ( struct bpf_gen * gen , __u32 size ) { size_t off = gen -> insn_cur - gen -> insn_start ; void * insn_start ; if ( gen -> error ) return gen -> error ; if ( size > INT32_MAX || off + size > INT32_MAX ) { gen -> error = - ERANGE ; return - ERANGE ; } insn_start = realloc ( gen -> insn_start , off + size ) ; if ( ! insn_start ) { gen -> error = - ENOMEM ; free ( gen -> insn_start ) ; gen -> insn_start = NULL ; return - ENOMEM ; } gen -> insn_start = insn_start ; gen -> insn_cur = insn_start + off ; return 0 ; }",
    "resources/libbpf/src/gen_loader.c@realloc_data_buf": "static int realloc_data_buf ( struct bpf_gen * gen , __u32 size ) { size_t off = gen -> data_cur - gen -> data_start ; void * data_start ; if ( gen -> error ) return gen -> error ; if ( size > INT32_MAX || off + size > INT32_MAX ) { gen -> error = - ERANGE ; return - ERANGE ; } data_start = realloc ( gen -> data_start , off + size ) ; if ( ! data_start ) { gen -> error = - ENOMEM ; free ( gen -> data_start ) ; gen -> data_start = NULL ; return - ENOMEM ; } gen -> data_start = data_start ; gen -> data_cur = data_start + off ; return 0 ; }",
    "resources/libbpf/src/gen_loader.c@emit": "static void emit ( struct bpf_gen * gen , struct bpf_insn insn ) { if ( realloc_insn_buf ( gen , sizeof ( insn ) ) ) return ; memcpy ( gen -> insn_cur , & insn , sizeof ( insn ) ) ; gen -> insn_cur += sizeof ( insn ) ; }",
    "resources/libbpf/src/gen_loader.c@emit2": "static void emit2 ( struct bpf_gen * gen , struct bpf_insn insn1 , struct bpf_insn insn2 ) { emit ( gen , insn1 ) ; emit ( gen , insn2 ) ; }",
    "resources/libbpf/src/gen_loader.c@bpf_gen__init": "void bpf_gen__init ( struct bpf_gen * gen , int log_level , int nr_progs , int nr_maps ) { size_t stack_sz = sizeof ( struct loader_stack ) , nr_progs_sz ; int i ; gen -> fd_array = add_data ( gen , NULL , MAX_FD_ARRAY_SZ * sizeof ( int ) ) ; gen -> log_level = log_level ; /* save ctx pointer into R6 */ emit ( gen , BPF_MOV64_REG ( BPF_REG_6 , BPF_REG_1 ) ) ; /* bzero stack */ emit ( gen , BPF_MOV64_REG ( BPF_REG_1 , BPF_REG_10 ) ) ; emit ( gen , BPF_ALU64_IMM ( BPF_ADD , BPF_REG_1 , - stack_sz ) ) ; emit ( gen , BPF_MOV64_IMM ( BPF_REG_2 , stack_sz ) ) ; emit ( gen , BPF_MOV64_IMM ( BPF_REG_3 , 0 ) ) ; emit ( gen , BPF_EMIT_CALL ( BPF_FUNC_probe_read_kernel ) ) ; /* amount of stack actually used, only used to calculate iterations, not stack offset */ nr_progs_sz = offsetof ( struct loader_stack , prog_fd [ nr_progs ] ) ; /* jump over cleanup code */ emit ( gen , BPF_JMP_IMM ( BPF_JA , 0 , 0 , /* size of cleanup code below (including map fd cleanup) */ ( nr_progs_sz / 4 ) * 3 + 2 + /* 6 insns for emit_sys_close_blob,\n\t\t\t       * 6 insns for debug_regs in emit_sys_close_blob\n\t\t\t       */ nr_maps * ( 6 + ( gen -> log_level ? 6 : 0 ) ) ) ) ; /* remember the label where all error branches will jump to */ gen -> cleanup_label = gen -> insn_cur - gen -> insn_start ; /* emit cleanup code: close all temp FDs */ for ( i = 0 ; i < nr_progs_sz ; i += 4 ) { emit ( gen , BPF_LDX_MEM ( BPF_W , BPF_REG_1 , BPF_REG_10 , - stack_sz + i ) ) ; emit ( gen , BPF_JMP_IMM ( BPF_JSLE , BPF_REG_1 , 0 , 1 ) ) ; emit ( gen , BPF_EMIT_CALL ( BPF_FUNC_sys_close ) ) ; } for ( i = 0 ; i < nr_maps ; i ++ ) emit_sys_close_blob ( gen , blob_fd_array_off ( gen , i ) ) ; /* R7 contains the error code from sys_bpf. Copy it into R0 and exit. */ emit ( gen , BPF_MOV64_REG ( BPF_REG_0 , BPF_REG_7 ) ) ; emit ( gen , BPF_EXIT_INSN ( ) ) ; }",
    "resources/libbpf/src/gen_loader.c@add_data": "static int add_data ( struct bpf_gen * gen , const void * data , __u32 size ) { __u32 size8 = roundup ( size , 8 ) ; __u64 zero = 0 ; void * prev ; if ( realloc_data_buf ( gen , size8 ) ) return 0 ; prev = gen -> data_cur ; if ( data ) { memcpy ( gen -> data_cur , data , size ) ; memcpy ( gen -> data_cur + size , & zero , size8 - size ) ; } else { memset ( gen -> data_cur , 0 , size8 ) ; } gen -> data_cur += size8 ; return prev - gen -> data_start ; }",
    "resources/libbpf/src/gen_loader.c@add_map_fd": "static int add_map_fd ( struct bpf_gen * gen ) { if ( gen -> nr_maps == MAX_USED_MAPS ) { pr_warn ( \"Total maps exceeds %d\\n\" , MAX_USED_MAPS ) ; gen -> error = - E2BIG ; return 0 ; } return gen -> nr_maps ++ ; }",
    "resources/libbpf/src/gen_loader.c@add_kfunc_btf_fd": "static int add_kfunc_btf_fd ( struct bpf_gen * gen ) { int cur ; if ( gen -> nr_fd_array == MAX_KFUNC_DESCS ) { cur = add_data ( gen , NULL , sizeof ( int ) ) ; return ( cur - gen -> fd_array ) / sizeof ( int ) ; } return MAX_USED_MAPS + gen -> nr_fd_array ++ ; }",
    "resources/libbpf/src/gen_loader.c@insn_bytes_to_bpf_size": "static int insn_bytes_to_bpf_size ( __u32 sz ) { switch ( sz ) { case 8 : return BPF_DW ; case 4 : return BPF_W ; case 2 : return BPF_H ; case 1 : return BPF_B ; default : return - 1 ; } }",
    "resources/libbpf/src/gen_loader.c@emit_rel_store": "static void emit_rel_store ( struct bpf_gen * gen , int off , int data ) { emit2 ( gen , BPF_LD_IMM64_RAW_FULL ( BPF_REG_0 , BPF_PSEUDO_MAP_IDX_VALUE , 0 , 0 , 0 , data ) ) ; emit2 ( gen , BPF_LD_IMM64_RAW_FULL ( BPF_REG_1 , BPF_PSEUDO_MAP_IDX_VALUE , 0 , 0 , 0 , off ) ) ; emit ( gen , BPF_STX_MEM ( BPF_DW , BPF_REG_1 , BPF_REG_0 , 0 ) ) ; }",
    "resources/libbpf/src/gen_loader.c@move_blob2blob": "static void move_blob2blob ( struct bpf_gen * gen , int off , int size , int blob_off ) { emit2 ( gen , BPF_LD_IMM64_RAW_FULL ( BPF_REG_2 , BPF_PSEUDO_MAP_IDX_VALUE , 0 , 0 , 0 , blob_off ) ) ; emit ( gen , BPF_LDX_MEM ( insn_bytes_to_bpf_size ( size ) , BPF_REG_0 , BPF_REG_2 , 0 ) ) ; emit2 ( gen , BPF_LD_IMM64_RAW_FULL ( BPF_REG_1 , BPF_PSEUDO_MAP_IDX_VALUE , 0 , 0 , 0 , off ) ) ; emit ( gen , BPF_STX_MEM ( insn_bytes_to_bpf_size ( size ) , BPF_REG_1 , BPF_REG_0 , 0 ) ) ; }",
    "resources/libbpf/src/gen_loader.c@move_blob2ctx": "static void move_blob2ctx ( struct bpf_gen * gen , int ctx_off , int size , int blob_off ) { emit2 ( gen , BPF_LD_IMM64_RAW_FULL ( BPF_REG_1 , BPF_PSEUDO_MAP_IDX_VALUE , 0 , 0 , 0 , blob_off ) ) ; emit ( gen , BPF_LDX_MEM ( insn_bytes_to_bpf_size ( size ) , BPF_REG_0 , BPF_REG_1 , 0 ) ) ; emit ( gen , BPF_STX_MEM ( insn_bytes_to_bpf_size ( size ) , BPF_REG_6 , BPF_REG_0 , ctx_off ) ) ; }",
    "resources/libbpf/src/gen_loader.c@move_ctx2blob": "static void move_ctx2blob ( struct bpf_gen * gen , int off , int size , int ctx_off , bool check_non_zero ) { emit ( gen , BPF_LDX_MEM ( insn_bytes_to_bpf_size ( size ) , BPF_REG_0 , BPF_REG_6 , ctx_off ) ) ; if ( check_non_zero ) /* If value in ctx is zero don't update the blob.\n\t\t * For example: when ctx->map.max_entries == 0, keep default max_entries from bpf.c\n\t\t */ emit ( gen , BPF_JMP_IMM ( BPF_JEQ , BPF_REG_0 , 0 , 3 ) ) ; emit2 ( gen , BPF_LD_IMM64_RAW_FULL ( BPF_REG_1 , BPF_PSEUDO_MAP_IDX_VALUE , 0 , 0 , 0 , off ) ) ; emit ( gen , BPF_STX_MEM ( insn_bytes_to_bpf_size ( size ) , BPF_REG_1 , BPF_REG_0 , 0 ) ) ; }",
    "resources/libbpf/src/gen_loader.c@move_stack2blob": "static void move_stack2blob ( struct bpf_gen * gen , int off , int size , int stack_off ) { emit ( gen , BPF_LDX_MEM ( insn_bytes_to_bpf_size ( size ) , BPF_REG_0 , BPF_REG_10 , stack_off ) ) ; emit2 ( gen , BPF_LD_IMM64_RAW_FULL ( BPF_REG_1 , BPF_PSEUDO_MAP_IDX_VALUE , 0 , 0 , 0 , off ) ) ; emit ( gen , BPF_STX_MEM ( insn_bytes_to_bpf_size ( size ) , BPF_REG_1 , BPF_REG_0 , 0 ) ) ; }",
    "resources/libbpf/src/gen_loader.c@move_stack2ctx": "static void move_stack2ctx ( struct bpf_gen * gen , int ctx_off , int size , int stack_off ) { emit ( gen , BPF_LDX_MEM ( insn_bytes_to_bpf_size ( size ) , BPF_REG_0 , BPF_REG_10 , stack_off ) ) ; emit ( gen , BPF_STX_MEM ( insn_bytes_to_bpf_size ( size ) , BPF_REG_6 , BPF_REG_0 , ctx_off ) ) ; }",
    "resources/libbpf/src/gen_loader.c@emit_sys_bpf": "static void emit_sys_bpf ( struct bpf_gen * gen , int cmd , int attr , int attr_size ) { emit ( gen , BPF_MOV64_IMM ( BPF_REG_1 , cmd ) ) ; emit2 ( gen , BPF_LD_IMM64_RAW_FULL ( BPF_REG_2 , BPF_PSEUDO_MAP_IDX_VALUE , 0 , 0 , 0 , attr ) ) ; emit ( gen , BPF_MOV64_IMM ( BPF_REG_3 , attr_size ) ) ; emit ( gen , BPF_EMIT_CALL ( BPF_FUNC_sys_bpf ) ) ; /* remember the result in R7 */ emit ( gen , BPF_MOV64_REG ( BPF_REG_7 , BPF_REG_0 ) ) ; }",
    "resources/libbpf/src/gen_loader.c@is_simm16": "static bool is_simm16 ( __s64 value ) { return value == ( __s64 ) ( __s16 ) value ; }",
    "resources/libbpf/src/gen_loader.c@emit_check_err": "static void emit_check_err ( struct bpf_gen * gen ) { __s64 off = - ( gen -> insn_cur - gen -> insn_start - gen -> cleanup_label ) / 8 - 1 ; /* R7 contains result of last sys_bpf command.\n\t * if (R7 < 0) goto cleanup;\n\t */ if ( is_simm16 ( off ) ) { emit ( gen , BPF_JMP_IMM ( BPF_JSLT , BPF_REG_7 , 0 , off ) ) ; } else { gen -> error = - ERANGE ; emit ( gen , BPF_JMP_IMM ( BPF_JA , 0 , 0 , - 1 ) ) ; } }",
    "resources/libbpf/src/gen_loader.c@emit_debug": "static void emit_debug ( struct bpf_gen * gen , int reg1 , int reg2 , const char * fmt , va_list args ) { char buf [ 1024 ] ; int addr , len , ret ; if ( ! gen -> log_level ) return ; ret = vsnprintf ( buf , sizeof ( buf ) , fmt , args ) ; if ( ret < 1024 - 7 && reg1 >= 0 && reg2 < 0 ) /* The special case to accommodate common debug_ret():\n\t\t * to avoid specifying BPF_REG_7 and adding \" r=%%d\" to\n\t\t * prints explicitly.\n\t\t */ strcat ( buf , \" r=%d\" ) ; len = strlen ( buf ) + 1 ; addr = add_data ( gen , buf , len ) ; emit2 ( gen , BPF_LD_IMM64_RAW_FULL ( BPF_REG_1 , BPF_PSEUDO_MAP_IDX_VALUE , 0 , 0 , 0 , addr ) ) ; emit ( gen , BPF_MOV64_IMM ( BPF_REG_2 , len ) ) ; if ( reg1 >= 0 ) emit ( gen , BPF_MOV64_REG ( BPF_REG_3 , reg1 ) ) ; if ( reg2 >= 0 ) emit ( gen , BPF_MOV64_REG ( BPF_REG_4 , reg2 ) ) ; emit ( gen , BPF_EMIT_CALL ( BPF_FUNC_trace_printk ) ) ; }",
    "resources/libbpf/src/gen_loader.c@debug_regs": "static void debug_regs ( struct bpf_gen * gen , int reg1 , int reg2 , const char * fmt , ... ) { va_list args ; va_start ( args , fmt ) ; emit_debug ( gen , reg1 , reg2 , fmt , args ) ; va_end ( args ) ; }",
    "resources/libbpf/src/gen_loader.c@debug_ret": "static void debug_ret ( struct bpf_gen * gen , const char * fmt , ... ) { va_list args ; va_start ( args , fmt ) ; emit_debug ( gen , BPF_REG_7 , - 1 , fmt , args ) ; va_end ( args ) ; }",
    "resources/libbpf/src/gen_loader.c@__emit_sys_close": "static void __emit_sys_close ( struct bpf_gen * gen ) { emit ( gen , BPF_JMP_IMM ( BPF_JSLE , BPF_REG_1 , 0 , /* 2 is the number of the following insns\n\t\t\t       * * 6 is additional insns in debug_regs\n\t\t\t       */ 2 + ( gen -> log_level ? 6 : 0 ) ) ) ; emit ( gen , BPF_MOV64_REG ( BPF_REG_9 , BPF_REG_1 ) ) ; emit ( gen , BPF_EMIT_CALL ( BPF_FUNC_sys_close ) ) ; debug_regs ( gen , BPF_REG_9 , BPF_REG_0 , \"close(%%d) = %%d\" ) ; }",
    "resources/libbpf/src/gen_loader.c@emit_sys_close_stack": "static void emit_sys_close_stack ( struct bpf_gen * gen , int stack_off ) { emit ( gen , BPF_LDX_MEM ( BPF_W , BPF_REG_1 , BPF_REG_10 , stack_off ) ) ; __emit_sys_close ( gen ) ; }",
    "resources/libbpf/src/gen_loader.c@emit_sys_close_blob": "static void emit_sys_close_blob ( struct bpf_gen * gen , int blob_off ) { emit2 ( gen , BPF_LD_IMM64_RAW_FULL ( BPF_REG_0 , BPF_PSEUDO_MAP_IDX_VALUE , 0 , 0 , 0 , blob_off ) ) ; emit ( gen , BPF_LDX_MEM ( BPF_W , BPF_REG_1 , BPF_REG_0 , 0 ) ) ; __emit_sys_close ( gen ) ; }",
    "resources/libbpf/src/gen_loader.c@bpf_gen__finish": "int bpf_gen__finish ( struct bpf_gen * gen , int nr_progs , int nr_maps ) { int i ; if ( nr_progs < gen -> nr_progs || nr_maps != gen -> nr_maps ) { pr_warn ( \"nr_progs %d/%d nr_maps %d/%d mismatch\\n\" , nr_progs , gen -> nr_progs , nr_maps , gen -> nr_maps ) ; gen -> error = - EFAULT ; return gen -> error ; } emit_sys_close_stack ( gen , stack_off ( btf_fd ) ) ; for ( i = 0 ; i < gen -> nr_progs ; i ++ ) move_stack2ctx ( gen , sizeof ( struct bpf_loader_ctx ) + sizeof ( struct bpf_map_desc ) * gen -> nr_maps + sizeof ( struct bpf_prog_desc ) * i + offsetof ( struct bpf_prog_desc , prog_fd ) , 4 , stack_off ( prog_fd [ i ] ) ) ; for ( i = 0 ; i < gen -> nr_maps ; i ++ ) move_blob2ctx ( gen , sizeof ( struct bpf_loader_ctx ) + sizeof ( struct bpf_map_desc ) * i + offsetof ( struct bpf_map_desc , map_fd ) , 4 , blob_fd_array_off ( gen , i ) ) ; emit ( gen , BPF_MOV64_IMM ( BPF_REG_0 , 0 ) ) ; emit ( gen , BPF_EXIT_INSN ( ) ) ; pr_debug ( \"gen: finish %d\\n\" , gen -> error ) ; if ( ! gen -> error ) { struct gen_loader_opts * opts = gen -> opts ; opts -> insns = gen -> insn_start ; opts -> insns_sz = gen -> insn_cur - gen -> insn_start ; opts -> data = gen -> data_start ; opts -> data_sz = gen -> data_cur - gen -> data_start ; } return gen -> error ; }",
    "resources/libbpf/src/gen_loader.c@bpf_gen__free": "void bpf_gen__free ( struct bpf_gen * gen ) { if ( ! gen ) return ; free ( gen -> data_start ) ; free ( gen -> insn_start ) ; free ( gen ) ; }",
    "resources/libbpf/src/gen_loader.c@bpf_gen__load_btf": "void bpf_gen__load_btf ( struct bpf_gen * gen , const void * btf_raw_data , __u32 btf_raw_size ) { int attr_size = offsetofend ( union bpf_attr , btf_log_level ) ; int btf_data , btf_load_attr ; union bpf_attr attr ; memset ( & attr , 0 , attr_size ) ; pr_debug ( \"gen: load_btf: size %d\\n\" , btf_raw_size ) ; btf_data = add_data ( gen , btf_raw_data , btf_raw_size ) ; attr . btf_size = btf_raw_size ; btf_load_attr = add_data ( gen , & attr , attr_size ) ; /* populate union bpf_attr with user provided log details */ move_ctx2blob ( gen , attr_field ( btf_load_attr , btf_log_level ) , 4 , offsetof ( struct bpf_loader_ctx , log_level ) , false ) ; move_ctx2blob ( gen , attr_field ( btf_load_attr , btf_log_size ) , 4 , offsetof ( struct bpf_loader_ctx , log_size ) , false ) ; move_ctx2blob ( gen , attr_field ( btf_load_attr , btf_log_buf ) , 8 , offsetof ( struct bpf_loader_ctx , log_buf ) , false ) ; /* populate union bpf_attr with a pointer to the BTF data */ emit_rel_store ( gen , attr_field ( btf_load_attr , btf ) , btf_data ) ; /* emit BTF_LOAD command */ emit_sys_bpf ( gen , BPF_BTF_LOAD , btf_load_attr , attr_size ) ; debug_ret ( gen , \"btf_load size %d\" , btf_raw_size ) ; emit_check_err ( gen ) ; /* remember btf_fd in the stack, if successful */ emit ( gen , BPF_STX_MEM ( BPF_W , BPF_REG_10 , BPF_REG_7 , stack_off ( btf_fd ) ) ) ; }",
    "resources/libbpf/src/gen_loader.c@bpf_gen__map_create": "void bpf_gen__map_create ( struct bpf_gen * gen , enum bpf_map_type map_type , const char * map_name , __u32 key_size , __u32 value_size , __u32 max_entries , struct bpf_map_create_opts * map_attr , int map_idx ) { int attr_size = offsetofend ( union bpf_attr , map_extra ) ; bool close_inner_map_fd = false ; int map_create_attr , idx ; union bpf_attr attr ; memset ( & attr , 0 , attr_size ) ; attr . map_type = map_type ; attr . key_size = key_size ; attr . value_size = value_size ; attr . map_flags = map_attr -> map_flags ; attr . map_extra = map_attr -> map_extra ; if ( map_name ) libbpf_strlcpy ( attr . map_name , map_name , sizeof ( attr . map_name ) ) ; attr . numa_node = map_attr -> numa_node ; attr . map_ifindex = map_attr -> map_ifindex ; attr . max_entries = max_entries ; attr . btf_key_type_id = map_attr -> btf_key_type_id ; attr . btf_value_type_id = map_attr -> btf_value_type_id ; pr_debug ( \"gen: map_create: %s idx %d type %d value_type_id %d\\n\" , attr . map_name , map_idx , map_type , attr . btf_value_type_id ) ; map_create_attr = add_data ( gen , & attr , attr_size ) ; if ( attr . btf_value_type_id ) /* populate union bpf_attr with btf_fd saved in the stack earlier */ move_stack2blob ( gen , attr_field ( map_create_attr , btf_fd ) , 4 , stack_off ( btf_fd ) ) ; switch ( attr . map_type ) { case BPF_MAP_TYPE_ARRAY_OF_MAPS : case BPF_MAP_TYPE_HASH_OF_MAPS : move_stack2blob ( gen , attr_field ( map_create_attr , inner_map_fd ) , 4 , stack_off ( inner_map_fd ) ) ; close_inner_map_fd = true ; break ; default : break ; } /* conditionally update max_entries */ if ( map_idx >= 0 ) move_ctx2blob ( gen , attr_field ( map_create_attr , max_entries ) , 4 , sizeof ( struct bpf_loader_ctx ) + sizeof ( struct bpf_map_desc ) * map_idx + offsetof ( struct bpf_map_desc , max_entries ) , true /* check that max_entries != 0 */ ) ; /* emit MAP_CREATE command */ emit_sys_bpf ( gen , BPF_MAP_CREATE , map_create_attr , attr_size ) ; debug_ret ( gen , \"map_create %s idx %d type %d value_size %d value_btf_id %d\" , attr . map_name , map_idx , map_type , value_size , attr . btf_value_type_id ) ; emit_check_err ( gen ) ; /* remember map_fd in the stack, if successful */ if ( map_idx < 0 ) { /* This bpf_gen__map_create() function is called with map_idx >= 0\n\t\t * for all maps that libbpf loading logic tracks.\n\t\t * It's called with -1 to create an inner map.\n\t\t */ emit ( gen , BPF_STX_MEM ( BPF_W , BPF_REG_10 , BPF_REG_7 , stack_off ( inner_map_fd ) ) ) ; } else if ( map_idx != gen -> nr_maps ) { gen -> error = - EDOM ; /* internal bug */ return ; } else { /* add_map_fd does gen->nr_maps++ */ idx = add_map_fd ( gen ) ; emit2 ( gen , BPF_LD_IMM64_RAW_FULL ( BPF_REG_1 , BPF_PSEUDO_MAP_IDX_VALUE , 0 , 0 , 0 , blob_fd_array_off ( gen , idx ) ) ) ; emit ( gen , BPF_STX_MEM ( BPF_W , BPF_REG_1 , BPF_REG_7 , 0 ) ) ; } if ( close_inner_map_fd ) emit_sys_close_stack ( gen , stack_off ( inner_map_fd ) ) ; }",
    "resources/libbpf/src/gen_loader.c@bpf_gen__record_attach_target": "void bpf_gen__record_attach_target ( struct bpf_gen * gen , const char * attach_name , enum bpf_attach_type type ) { const char * prefix ; int kind , ret ; btf_get_kernel_prefix_kind ( type , & prefix , & kind ) ; gen -> attach_kind = kind ; ret = snprintf ( gen -> attach_target , sizeof ( gen -> attach_target ) , \"%s%s\" , prefix , attach_name ) ; if ( ret >= sizeof ( gen -> attach_target ) ) gen -> error = - ENOSPC ; }",
    "resources/libbpf/src/gen_loader.c@emit_find_attach_target": "static void emit_find_attach_target ( struct bpf_gen * gen ) { int name , len = strlen ( gen -> attach_target ) + 1 ; pr_debug ( \"gen: find_attach_tgt %s %d\\n\" , gen -> attach_target , gen -> attach_kind ) ; name = add_data ( gen , gen -> attach_target , len ) ; emit2 ( gen , BPF_LD_IMM64_RAW_FULL ( BPF_REG_1 , BPF_PSEUDO_MAP_IDX_VALUE , 0 , 0 , 0 , name ) ) ; emit ( gen , BPF_MOV64_IMM ( BPF_REG_2 , len ) ) ; emit ( gen , BPF_MOV64_IMM ( BPF_REG_3 , gen -> attach_kind ) ) ; emit ( gen , BPF_MOV64_IMM ( BPF_REG_4 , 0 ) ) ; emit ( gen , BPF_EMIT_CALL ( BPF_FUNC_btf_find_by_name_kind ) ) ; emit ( gen , BPF_MOV64_REG ( BPF_REG_7 , BPF_REG_0 ) ) ; debug_ret ( gen , \"find_by_name_kind(%s,%d)\" , gen -> attach_target , gen -> attach_kind ) ; emit_check_err ( gen ) ; /* if successful, btf_id is in lower 32-bit of R7 and\n\t * btf_obj_fd is in upper 32-bit\n\t */ }",
    "resources/libbpf/src/gen_loader.c@bpf_gen__record_extern": "void bpf_gen__record_extern ( struct bpf_gen * gen , const char * name , bool is_weak , bool is_typeless , bool is_ld64 , int kind , int insn_idx ) { struct ksym_relo_desc * relo ; relo = libbpf_reallocarray ( gen -> relos , gen -> relo_cnt + 1 , sizeof ( * relo ) ) ; if ( ! relo ) { gen -> error = - ENOMEM ; return ; } gen -> relos = relo ; relo += gen -> relo_cnt ; relo -> name = name ; relo -> is_weak = is_weak ; relo -> is_typeless = is_typeless ; relo -> is_ld64 = is_ld64 ; relo -> kind = kind ; relo -> insn_idx = insn_idx ; gen -> relo_cnt ++ ; }",
    "resources/libbpf/src/gen_loader.c@get_ksym_desc": "static struct ksym_desc * get_ksym_desc ( struct bpf_gen * gen , struct ksym_relo_desc * relo ) { struct ksym_desc * kdesc ; int i ; for ( i = 0 ; i < gen -> nr_ksyms ; i ++ ) { kdesc = & gen -> ksyms [ i ] ; if ( kdesc -> kind == relo -> kind && kdesc -> is_ld64 == relo -> is_ld64 && ! strcmp ( kdesc -> name , relo -> name ) ) { kdesc -> ref ++ ; return kdesc ; } } kdesc = libbpf_reallocarray ( gen -> ksyms , gen -> nr_ksyms + 1 , sizeof ( * kdesc ) ) ; if ( ! kdesc ) { gen -> error = - ENOMEM ; return NULL ; } gen -> ksyms = kdesc ; kdesc = & gen -> ksyms [ gen -> nr_ksyms ++ ] ; kdesc -> name = relo -> name ; kdesc -> kind = relo -> kind ; kdesc -> ref = 1 ; kdesc -> off = 0 ; kdesc -> insn = 0 ; kdesc -> is_ld64 = relo -> is_ld64 ; return kdesc ; }",
    "resources/libbpf/src/gen_loader.c@emit_bpf_find_by_name_kind": "static void emit_bpf_find_by_name_kind ( struct bpf_gen * gen , struct ksym_relo_desc * relo ) { int name_off , len = strlen ( relo -> name ) + 1 ; name_off = add_data ( gen , relo -> name , len ) ; emit2 ( gen , BPF_LD_IMM64_RAW_FULL ( BPF_REG_1 , BPF_PSEUDO_MAP_IDX_VALUE , 0 , 0 , 0 , name_off ) ) ; emit ( gen , BPF_MOV64_IMM ( BPF_REG_2 , len ) ) ; emit ( gen , BPF_MOV64_IMM ( BPF_REG_3 , relo -> kind ) ) ; emit ( gen , BPF_MOV64_IMM ( BPF_REG_4 , 0 ) ) ; emit ( gen , BPF_EMIT_CALL ( BPF_FUNC_btf_find_by_name_kind ) ) ; emit ( gen , BPF_MOV64_REG ( BPF_REG_7 , BPF_REG_0 ) ) ; debug_ret ( gen , \"find_by_name_kind(%s,%d)\" , relo -> name , relo -> kind ) ; }",
    "resources/libbpf/src/gen_loader.c@emit_bpf_kallsyms_lookup_name": "static void emit_bpf_kallsyms_lookup_name ( struct bpf_gen * gen , struct ksym_relo_desc * relo ) { int name_off , len = strlen ( relo -> name ) + 1 , res_off ; name_off = add_data ( gen , relo -> name , len ) ; res_off = add_data ( gen , NULL , 8 ) ; /* res is u64 */ emit2 ( gen , BPF_LD_IMM64_RAW_FULL ( BPF_REG_1 , BPF_PSEUDO_MAP_IDX_VALUE , 0 , 0 , 0 , name_off ) ) ; emit ( gen , BPF_MOV64_IMM ( BPF_REG_2 , len ) ) ; emit ( gen , BPF_MOV64_IMM ( BPF_REG_3 , 0 ) ) ; emit2 ( gen , BPF_LD_IMM64_RAW_FULL ( BPF_REG_4 , BPF_PSEUDO_MAP_IDX_VALUE , 0 , 0 , 0 , res_off ) ) ; emit ( gen , BPF_MOV64_REG ( BPF_REG_7 , BPF_REG_4 ) ) ; emit ( gen , BPF_EMIT_CALL ( BPF_FUNC_kallsyms_lookup_name ) ) ; emit ( gen , BPF_LDX_MEM ( BPF_DW , BPF_REG_9 , BPF_REG_7 , 0 ) ) ; emit ( gen , BPF_MOV64_REG ( BPF_REG_7 , BPF_REG_0 ) ) ; debug_ret ( gen , \"kallsyms_lookup_name(%s,%d)\" , relo -> name , relo -> kind ) ; }",
    "resources/libbpf/src/gen_loader.c@emit_relo_kfunc_btf": "static void emit_relo_kfunc_btf ( struct bpf_gen * gen , struct ksym_relo_desc * relo , int insn ) { struct ksym_desc * kdesc ; int btf_fd_idx ; kdesc = get_ksym_desc ( gen , relo ) ; if ( ! kdesc ) return ; /* try to copy from existing bpf_insn */ if ( kdesc -> ref > 1 ) { move_blob2blob ( gen , insn + offsetof ( struct bpf_insn , imm ) , 4 , kdesc -> insn + offsetof ( struct bpf_insn , imm ) ) ; move_blob2blob ( gen , insn + offsetof ( struct bpf_insn , off ) , 2 , kdesc -> insn + offsetof ( struct bpf_insn , off ) ) ; goto log ; } /* remember insn offset, so we can copy BTF ID and FD later */ kdesc -> insn = insn ; emit_bpf_find_by_name_kind ( gen , relo ) ; if ( ! relo -> is_weak ) emit_check_err ( gen ) ; /* get index in fd_array to store BTF FD at */ btf_fd_idx = add_kfunc_btf_fd ( gen ) ; if ( btf_fd_idx > INT16_MAX ) { pr_warn ( \"BTF fd off %d for kfunc %s exceeds INT16_MAX, cannot process relocation\\n\" , btf_fd_idx , relo -> name ) ; gen -> error = - E2BIG ; return ; } kdesc -> off = btf_fd_idx ; /* jump to success case */ emit ( gen , BPF_JMP_IMM ( BPF_JSGE , BPF_REG_7 , 0 , 3 ) ) ; /* set value for imm, off as 0 */ emit ( gen , BPF_ST_MEM ( BPF_W , BPF_REG_8 , offsetof ( struct bpf_insn , imm ) , 0 ) ) ; emit ( gen , BPF_ST_MEM ( BPF_H , BPF_REG_8 , offsetof ( struct bpf_insn , off ) , 0 ) ) ; /* skip success case for ret < 0 */ emit ( gen , BPF_JMP_IMM ( BPF_JA , 0 , 0 , 10 ) ) ; /* store btf_id into insn[insn_idx].imm */ emit ( gen , BPF_STX_MEM ( BPF_W , BPF_REG_8 , BPF_REG_7 , offsetof ( struct bpf_insn , imm ) ) ) ; /* obtain fd in BPF_REG_9 */ emit ( gen , BPF_MOV64_REG ( BPF_REG_9 , BPF_REG_7 ) ) ; emit ( gen , BPF_ALU64_IMM ( BPF_RSH , BPF_REG_9 , 32 ) ) ; /* load fd_array slot pointer */ emit2 ( gen , BPF_LD_IMM64_RAW_FULL ( BPF_REG_0 , BPF_PSEUDO_MAP_IDX_VALUE , 0 , 0 , 0 , blob_fd_array_off ( gen , btf_fd_idx ) ) ) ; /* store BTF fd in slot, 0 for vmlinux */ emit ( gen , BPF_STX_MEM ( BPF_W , BPF_REG_0 , BPF_REG_9 , 0 ) ) ; /* jump to insn[insn_idx].off store if fd denotes module BTF */ emit ( gen , BPF_JMP_IMM ( BPF_JNE , BPF_REG_9 , 0 , 2 ) ) ; /* set the default value for off */ emit ( gen , BPF_ST_MEM ( BPF_H , BPF_REG_8 , offsetof ( struct bpf_insn , off ) , 0 ) ) ; /* skip BTF fd store for vmlinux BTF */ emit ( gen , BPF_JMP_IMM ( BPF_JA , 0 , 0 , 1 ) ) ; /* store index into insn[insn_idx].off */ emit ( gen , BPF_ST_MEM ( BPF_H , BPF_REG_8 , offsetof ( struct bpf_insn , off ) , btf_fd_idx ) ) ; log : if ( ! gen -> log_level ) return ; emit ( gen , BPF_LDX_MEM ( BPF_W , BPF_REG_7 , BPF_REG_8 , offsetof ( struct bpf_insn , imm ) ) ) ; emit ( gen , BPF_LDX_MEM ( BPF_H , BPF_REG_9 , BPF_REG_8 , offsetof ( struct bpf_insn , off ) ) ) ; debug_regs ( gen , BPF_REG_7 , BPF_REG_9 , \" func (%s:count=%d): imm: %%d, off: %%d\" , relo -> name , kdesc -> ref ) ; emit2 ( gen , BPF_LD_IMM64_RAW_FULL ( BPF_REG_0 , BPF_PSEUDO_MAP_IDX_VALUE , 0 , 0 , 0 , blob_fd_array_off ( gen , kdesc -> off ) ) ) ; emit ( gen , BPF_LDX_MEM ( BPF_W , BPF_REG_9 , BPF_REG_0 , 0 ) ) ; debug_regs ( gen , BPF_REG_9 , - 1 , \" func (%s:count=%d): btf_fd\" , relo -> name , kdesc -> ref ) ; }",
    "resources/libbpf/src/gen_loader.c@emit_ksym_relo_log": "static void emit_ksym_relo_log ( struct bpf_gen * gen , struct ksym_relo_desc * relo , int ref ) { if ( ! gen -> log_level ) return ; emit ( gen , BPF_LDX_MEM ( BPF_W , BPF_REG_7 , BPF_REG_8 , offsetof ( struct bpf_insn , imm ) ) ) ; emit ( gen , BPF_LDX_MEM ( BPF_H , BPF_REG_9 , BPF_REG_8 , sizeof ( struct bpf_insn ) + offsetof ( struct bpf_insn , imm ) ) ) ; debug_regs ( gen , BPF_REG_7 , BPF_REG_9 , \" var t=%d w=%d (%s:count=%d): imm[0]: %%d, imm[1]: %%d\" , relo -> is_typeless , relo -> is_weak , relo -> name , ref ) ; emit ( gen , BPF_LDX_MEM ( BPF_B , BPF_REG_9 , BPF_REG_8 , offsetofend ( struct bpf_insn , code ) ) ) ; debug_regs ( gen , BPF_REG_9 , - 1 , \" var t=%d w=%d (%s:count=%d): insn.reg\" , relo -> is_typeless , relo -> is_weak , relo -> name , ref ) ; }",
    "resources/libbpf/src/gen_loader.c@emit_relo_ksym_typeless": "static void emit_relo_ksym_typeless ( struct bpf_gen * gen , struct ksym_relo_desc * relo , int insn ) { struct ksym_desc * kdesc ; kdesc = get_ksym_desc ( gen , relo ) ; if ( ! kdesc ) return ; /* try to copy from existing ldimm64 insn */ if ( kdesc -> ref > 1 ) { move_blob2blob ( gen , insn + offsetof ( struct bpf_insn , imm ) , 4 , kdesc -> insn + offsetof ( struct bpf_insn , imm ) ) ; move_blob2blob ( gen , insn + sizeof ( struct bpf_insn ) + offsetof ( struct bpf_insn , imm ) , 4 , kdesc -> insn + sizeof ( struct bpf_insn ) + offsetof ( struct bpf_insn , imm ) ) ; goto log ; } /* remember insn offset, so we can copy ksym addr later */ kdesc -> insn = insn ; /* skip typeless ksym_desc in fd closing loop in cleanup_relos */ kdesc -> typeless = true ; emit_bpf_kallsyms_lookup_name ( gen , relo ) ; emit ( gen , BPF_JMP_IMM ( BPF_JEQ , BPF_REG_7 , - ENOENT , 1 ) ) ; emit_check_err ( gen ) ; /* store lower half of addr into insn[insn_idx].imm */ emit ( gen , BPF_STX_MEM ( BPF_W , BPF_REG_8 , BPF_REG_9 , offsetof ( struct bpf_insn , imm ) ) ) ; /* store upper half of addr into insn[insn_idx + 1].imm */ emit ( gen , BPF_ALU64_IMM ( BPF_RSH , BPF_REG_9 , 32 ) ) ; emit ( gen , BPF_STX_MEM ( BPF_W , BPF_REG_8 , BPF_REG_9 , sizeof ( struct bpf_insn ) + offsetof ( struct bpf_insn , imm ) ) ) ; log : emit_ksym_relo_log ( gen , relo , kdesc -> ref ) ; }",
    "resources/libbpf/src/gen_loader.c@src_reg_mask": "static __u32 src_reg_mask ( void ) { # if defined ( __LITTLE_ENDIAN_BITFIELD ) return 0x0f ; /* src_reg,dst_reg,... */ # elif defined ( __BIG_ENDIAN_BITFIELD ) return 0xf0 ; /* dst_reg,src_reg,... */ # else # error \"Unsupported bit endianness, cannot proceed\" # endif }",
    "resources/libbpf/src/gen_loader.c@emit_relo_ksym_btf": "static void emit_relo_ksym_btf ( struct bpf_gen * gen , struct ksym_relo_desc * relo , int insn ) { struct ksym_desc * kdesc ; __u32 reg_mask ; kdesc = get_ksym_desc ( gen , relo ) ; if ( ! kdesc ) return ; /* try to copy from existing ldimm64 insn */ if ( kdesc -> ref > 1 ) { move_blob2blob ( gen , insn + sizeof ( struct bpf_insn ) + offsetof ( struct bpf_insn , imm ) , 4 , kdesc -> insn + sizeof ( struct bpf_insn ) + offsetof ( struct bpf_insn , imm ) ) ; move_blob2blob ( gen , insn + offsetof ( struct bpf_insn , imm ) , 4 , kdesc -> insn + offsetof ( struct bpf_insn , imm ) ) ; /* jump over src_reg adjustment if imm (btf_id) is not 0, reuse BPF_REG_0 from move_blob2blob\n\t\t * If btf_id is zero, clear BPF_PSEUDO_BTF_ID flag in src_reg of ld_imm64 insn\n\t\t */ emit ( gen , BPF_JMP_IMM ( BPF_JNE , BPF_REG_0 , 0 , 3 ) ) ; goto clear_src_reg ; } /* remember insn offset, so we can copy BTF ID and FD later */ kdesc -> insn = insn ; emit_bpf_find_by_name_kind ( gen , relo ) ; if ( ! relo -> is_weak ) emit_check_err ( gen ) ; /* jump to success case */ emit ( gen , BPF_JMP_IMM ( BPF_JSGE , BPF_REG_7 , 0 , 3 ) ) ; /* set values for insn[insn_idx].imm, insn[insn_idx + 1].imm as 0 */ emit ( gen , BPF_ST_MEM ( BPF_W , BPF_REG_8 , offsetof ( struct bpf_insn , imm ) , 0 ) ) ; emit ( gen , BPF_ST_MEM ( BPF_W , BPF_REG_8 , sizeof ( struct bpf_insn ) + offsetof ( struct bpf_insn , imm ) , 0 ) ) ; /* skip success case for ret < 0 */ emit ( gen , BPF_JMP_IMM ( BPF_JA , 0 , 0 , 4 ) ) ; /* store btf_id into insn[insn_idx].imm */ emit ( gen , BPF_STX_MEM ( BPF_W , BPF_REG_8 , BPF_REG_7 , offsetof ( struct bpf_insn , imm ) ) ) ; /* store btf_obj_fd into insn[insn_idx + 1].imm */ emit ( gen , BPF_ALU64_IMM ( BPF_RSH , BPF_REG_7 , 32 ) ) ; emit ( gen , BPF_STX_MEM ( BPF_W , BPF_REG_8 , BPF_REG_7 , sizeof ( struct bpf_insn ) + offsetof ( struct bpf_insn , imm ) ) ) ; /* skip src_reg adjustment */ emit ( gen , BPF_JMP_IMM ( BPF_JA , 0 , 0 , 3 ) ) ; clear_src_reg : /* clear bpf_object__relocate_data's src_reg assignment, otherwise we get a verifier failure */ reg_mask = src_reg_mask ( ) ; emit ( gen , BPF_LDX_MEM ( BPF_B , BPF_REG_9 , BPF_REG_8 , offsetofend ( struct bpf_insn , code ) ) ) ; emit ( gen , BPF_ALU32_IMM ( BPF_AND , BPF_REG_9 , reg_mask ) ) ; emit ( gen , BPF_STX_MEM ( BPF_B , BPF_REG_8 , BPF_REG_9 , offsetofend ( struct bpf_insn , code ) ) ) ; emit_ksym_relo_log ( gen , relo , kdesc -> ref ) ; }",
    "resources/libbpf/src/gen_loader.c@bpf_gen__record_relo_core": "void bpf_gen__record_relo_core ( struct bpf_gen * gen , const struct bpf_core_relo * core_relo ) { struct bpf_core_relo * relos ; relos = libbpf_reallocarray ( gen -> core_relos , gen -> core_relo_cnt + 1 , sizeof ( * relos ) ) ; if ( ! relos ) { gen -> error = - ENOMEM ; return ; } gen -> core_relos = relos ; relos += gen -> core_relo_cnt ; memcpy ( relos , core_relo , sizeof ( * relos ) ) ; gen -> core_relo_cnt ++ ; }",
    "resources/libbpf/src/gen_loader.c@emit_relo": "static void emit_relo ( struct bpf_gen * gen , struct ksym_relo_desc * relo , int insns ) { int insn ; pr_debug ( \"gen: emit_relo (%d): %s at %d %s\\n\" , relo -> kind , relo -> name , relo -> insn_idx , relo -> is_ld64 ? \"ld64\" : \"call\" ) ; insn = insns + sizeof ( struct bpf_insn ) * relo -> insn_idx ; emit2 ( gen , BPF_LD_IMM64_RAW_FULL ( BPF_REG_8 , BPF_PSEUDO_MAP_IDX_VALUE , 0 , 0 , 0 , insn ) ) ; if ( relo -> is_ld64 ) { if ( relo -> is_typeless ) emit_relo_ksym_typeless ( gen , relo , insn ) ; else emit_relo_ksym_btf ( gen , relo , insn ) ; } else { emit_relo_kfunc_btf ( gen , relo , insn ) ; } }",
    "resources/libbpf/src/gen_loader.c@emit_relos": "static void emit_relos ( struct bpf_gen * gen , int insns ) { int i ; for ( i = 0 ; i < gen -> relo_cnt ; i ++ ) emit_relo ( gen , gen -> relos + i , insns ) ; }",
    "resources/libbpf/src/gen_loader.c@cleanup_core_relo": "static void cleanup_core_relo ( struct bpf_gen * gen ) { if ( ! gen -> core_relo_cnt ) return ; free ( gen -> core_relos ) ; gen -> core_relo_cnt = 0 ; gen -> core_relos = NULL ; }",
    "resources/libbpf/src/gen_loader.c@cleanup_relos": "static void cleanup_relos ( struct bpf_gen * gen , int insns ) { struct ksym_desc * kdesc ; int i , insn ; for ( i = 0 ; i < gen -> nr_ksyms ; i ++ ) { kdesc = & gen -> ksyms [ i ] ; /* only close fds for typed ksyms and kfuncs */ if ( kdesc -> is_ld64 && ! kdesc -> typeless ) { /* close fd recorded in insn[insn_idx + 1].imm */ insn = kdesc -> insn ; insn += sizeof ( struct bpf_insn ) + offsetof ( struct bpf_insn , imm ) ; emit_sys_close_blob ( gen , insn ) ; } else if ( ! kdesc -> is_ld64 ) { emit_sys_close_blob ( gen , blob_fd_array_off ( gen , kdesc -> off ) ) ; if ( kdesc -> off < MAX_FD_ARRAY_SZ ) gen -> nr_fd_array -- ; } } if ( gen -> nr_ksyms ) { free ( gen -> ksyms ) ; gen -> nr_ksyms = 0 ; gen -> ksyms = NULL ; } if ( gen -> relo_cnt ) { free ( gen -> relos ) ; gen -> relo_cnt = 0 ; gen -> relos = NULL ; } cleanup_core_relo ( gen ) ; }",
    "resources/libbpf/src/gen_loader.c@bpf_gen__prog_load": "void bpf_gen__prog_load ( struct bpf_gen * gen , enum bpf_prog_type prog_type , const char * prog_name , const char * license , struct bpf_insn * insns , size_t insn_cnt , struct bpf_prog_load_opts * load_attr , int prog_idx ) { int prog_load_attr , license_off , insns_off , func_info , line_info , core_relos ; int attr_size = offsetofend ( union bpf_attr , core_relo_rec_size ) ; union bpf_attr attr ; memset ( & attr , 0 , attr_size ) ; pr_debug ( \"gen: prog_load: type %d insns_cnt %zd progi_idx %d\\n\" , prog_type , insn_cnt , prog_idx ) ; /* add license string to blob of bytes */ license_off = add_data ( gen , license , strlen ( license ) + 1 ) ; /* add insns to blob of bytes */ insns_off = add_data ( gen , insns , insn_cnt * sizeof ( struct bpf_insn ) ) ; attr . prog_type = prog_type ; attr . expected_attach_type = load_attr -> expected_attach_type ; attr . attach_btf_id = load_attr -> attach_btf_id ; attr . prog_ifindex = load_attr -> prog_ifindex ; attr . kern_version = 0 ; attr . insn_cnt = ( __u32 ) insn_cnt ; attr . prog_flags = load_attr -> prog_flags ; attr . func_info_rec_size = load_attr -> func_info_rec_size ; attr . func_info_cnt = load_attr -> func_info_cnt ; func_info = add_data ( gen , load_attr -> func_info , attr . func_info_cnt * attr . func_info_rec_size ) ; attr . line_info_rec_size = load_attr -> line_info_rec_size ; attr . line_info_cnt = load_attr -> line_info_cnt ; line_info = add_data ( gen , load_attr -> line_info , attr . line_info_cnt * attr . line_info_rec_size ) ; attr . core_relo_rec_size = sizeof ( struct bpf_core_relo ) ; attr . core_relo_cnt = gen -> core_relo_cnt ; core_relos = add_data ( gen , gen -> core_relos , attr . core_relo_cnt * attr . core_relo_rec_size ) ; libbpf_strlcpy ( attr . prog_name , prog_name , sizeof ( attr . prog_name ) ) ; prog_load_attr = add_data ( gen , & attr , attr_size ) ; /* populate union bpf_attr with a pointer to license */ emit_rel_store ( gen , attr_field ( prog_load_attr , license ) , license_off ) ; /* populate union bpf_attr with a pointer to instructions */ emit_rel_store ( gen , attr_field ( prog_load_attr , insns ) , insns_off ) ; /* populate union bpf_attr with a pointer to func_info */ emit_rel_store ( gen , attr_field ( prog_load_attr , func_info ) , func_info ) ; /* populate union bpf_attr with a pointer to line_info */ emit_rel_store ( gen , attr_field ( prog_load_attr , line_info ) , line_info ) ; /* populate union bpf_attr with a pointer to core_relos */ emit_rel_store ( gen , attr_field ( prog_load_attr , core_relos ) , core_relos ) ; /* populate union bpf_attr fd_array with a pointer to data where map_fds are saved */ emit_rel_store ( gen , attr_field ( prog_load_attr , fd_array ) , gen -> fd_array ) ; /* populate union bpf_attr with user provided log details */ move_ctx2blob ( gen , attr_field ( prog_load_attr , log_level ) , 4 , offsetof ( struct bpf_loader_ctx , log_level ) , false ) ; move_ctx2blob ( gen , attr_field ( prog_load_attr , log_size ) , 4 , offsetof ( struct bpf_loader_ctx , log_size ) , false ) ; move_ctx2blob ( gen , attr_field ( prog_load_attr , log_buf ) , 8 , offsetof ( struct bpf_loader_ctx , log_buf ) , false ) ; /* populate union bpf_attr with btf_fd saved in the stack earlier */ move_stack2blob ( gen , attr_field ( prog_load_attr , prog_btf_fd ) , 4 , stack_off ( btf_fd ) ) ; if ( gen -> attach_kind ) { emit_find_attach_target ( gen ) ; /* populate union bpf_attr with btf_id and btf_obj_fd found by helper */ emit2 ( gen , BPF_LD_IMM64_RAW_FULL ( BPF_REG_0 , BPF_PSEUDO_MAP_IDX_VALUE , 0 , 0 , 0 , prog_load_attr ) ) ; emit ( gen , BPF_STX_MEM ( BPF_W , BPF_REG_0 , BPF_REG_7 , offsetof ( union bpf_attr , attach_btf_id ) ) ) ; emit ( gen , BPF_ALU64_IMM ( BPF_RSH , BPF_REG_7 , 32 ) ) ; emit ( gen , BPF_STX_MEM ( BPF_W , BPF_REG_0 , BPF_REG_7 , offsetof ( union bpf_attr , attach_btf_obj_fd ) ) ) ; } emit_relos ( gen , insns_off ) ; /* emit PROG_LOAD command */ emit_sys_bpf ( gen , BPF_PROG_LOAD , prog_load_attr , attr_size ) ; debug_ret ( gen , \"prog_load %s insn_cnt %d\" , attr . prog_name , attr . insn_cnt ) ; /* successful or not, close btf module FDs used in extern ksyms and attach_btf_obj_fd */ cleanup_relos ( gen , insns_off ) ; if ( gen -> attach_kind ) { emit_sys_close_blob ( gen , attr_field ( prog_load_attr , attach_btf_obj_fd ) ) ; gen -> attach_kind = 0 ; } emit_check_err ( gen ) ; /* remember prog_fd in the stack, if successful */ emit ( gen , BPF_STX_MEM ( BPF_W , BPF_REG_10 , BPF_REG_7 , stack_off ( prog_fd [ gen -> nr_progs ] ) ) ) ; gen -> nr_progs ++ ; }",
    "resources/libbpf/src/gen_loader.c@bpf_gen__map_update_elem": "void bpf_gen__map_update_elem ( struct bpf_gen * gen , int map_idx , void * pvalue , __u32 value_size ) { int attr_size = offsetofend ( union bpf_attr , flags ) ; int map_update_attr , value , key ; union bpf_attr attr ; int zero = 0 ; memset ( & attr , 0 , attr_size ) ; pr_debug ( \"gen: map_update_elem: idx %d\\n\" , map_idx ) ; value = add_data ( gen , pvalue , value_size ) ; key = add_data ( gen , & zero , sizeof ( zero ) ) ; /* if (map_desc[map_idx].initial_value) {\n\t *    if (ctx->flags & BPF_SKEL_KERNEL)\n\t *        bpf_probe_read_kernel(value, value_size, initial_value);\n\t *    else\n\t *        bpf_copy_from_user(value, value_size, initial_value);\n\t * }\n\t */ emit ( gen , BPF_LDX_MEM ( BPF_DW , BPF_REG_3 , BPF_REG_6 , sizeof ( struct bpf_loader_ctx ) + sizeof ( struct bpf_map_desc ) * map_idx + offsetof ( struct bpf_map_desc , initial_value ) ) ) ; emit ( gen , BPF_JMP_IMM ( BPF_JEQ , BPF_REG_3 , 0 , 8 ) ) ; emit2 ( gen , BPF_LD_IMM64_RAW_FULL ( BPF_REG_1 , BPF_PSEUDO_MAP_IDX_VALUE , 0 , 0 , 0 , value ) ) ; emit ( gen , BPF_MOV64_IMM ( BPF_REG_2 , value_size ) ) ; emit ( gen , BPF_LDX_MEM ( BPF_W , BPF_REG_0 , BPF_REG_6 , offsetof ( struct bpf_loader_ctx , flags ) ) ) ; emit ( gen , BPF_JMP_IMM ( BPF_JSET , BPF_REG_0 , BPF_SKEL_KERNEL , 2 ) ) ; emit ( gen , BPF_EMIT_CALL ( BPF_FUNC_copy_from_user ) ) ; emit ( gen , BPF_JMP_IMM ( BPF_JA , 0 , 0 , 1 ) ) ; emit ( gen , BPF_EMIT_CALL ( BPF_FUNC_probe_read_kernel ) ) ; map_update_attr = add_data ( gen , & attr , attr_size ) ; move_blob2blob ( gen , attr_field ( map_update_attr , map_fd ) , 4 , blob_fd_array_off ( gen , map_idx ) ) ; emit_rel_store ( gen , attr_field ( map_update_attr , key ) , key ) ; emit_rel_store ( gen , attr_field ( map_update_attr , value ) , value ) ; /* emit MAP_UPDATE_ELEM command */ emit_sys_bpf ( gen , BPF_MAP_UPDATE_ELEM , map_update_attr , attr_size ) ; debug_ret ( gen , \"update_elem idx %d value_size %d\" , map_idx , value_size ) ; emit_check_err ( gen ) ; }",
    "resources/libbpf/src/gen_loader.c@bpf_gen__populate_outer_map": "void bpf_gen__populate_outer_map ( struct bpf_gen * gen , int outer_map_idx , int slot , int inner_map_idx ) { int attr_size = offsetofend ( union bpf_attr , flags ) ; int map_update_attr , key ; union bpf_attr attr ; memset ( & attr , 0 , attr_size ) ; pr_debug ( \"gen: populate_outer_map: outer %d key %d inner %d\\n\" , outer_map_idx , slot , inner_map_idx ) ; key = add_data ( gen , & slot , sizeof ( slot ) ) ; map_update_attr = add_data ( gen , & attr , attr_size ) ; move_blob2blob ( gen , attr_field ( map_update_attr , map_fd ) , 4 , blob_fd_array_off ( gen , outer_map_idx ) ) ; emit_rel_store ( gen , attr_field ( map_update_attr , key ) , key ) ; emit_rel_store ( gen , attr_field ( map_update_attr , value ) , blob_fd_array_off ( gen , inner_map_idx ) ) ; /* emit MAP_UPDATE_ELEM command */ emit_sys_bpf ( gen , BPF_MAP_UPDATE_ELEM , map_update_attr , attr_size ) ; debug_ret ( gen , \"populate_outer_map outer %d key %d inner %d\" , outer_map_idx , slot , inner_map_idx ) ; emit_check_err ( gen ) ; }",
    "resources/libbpf/src/gen_loader.c@bpf_gen__map_freeze": "void bpf_gen__map_freeze ( struct bpf_gen * gen , int map_idx ) { int attr_size = offsetofend ( union bpf_attr , map_fd ) ; int map_freeze_attr ; union bpf_attr attr ; memset ( & attr , 0 , attr_size ) ; pr_debug ( \"gen: map_freeze: idx %d\\n\" , map_idx ) ; map_freeze_attr = add_data ( gen , & attr , attr_size ) ; move_blob2blob ( gen , attr_field ( map_freeze_attr , map_fd ) , 4 , blob_fd_array_off ( gen , map_idx ) ) ; /* emit MAP_FREEZE command */ emit_sys_bpf ( gen , BPF_MAP_FREEZE , map_freeze_attr , attr_size ) ; debug_ret ( gen , \"map_freeze\" ) ; emit_check_err ( gen ) ; }",
    "resources/libbpf/src/bpf.c@btf_kind": "static inline __u16 btf_kind ( const struct btf_type * t ) { return BTF_INFO_KIND ( t -> info ) ; }",
    "resources/libbpf/src/bpf.c@btf_vlen": "static inline __u16 btf_vlen ( const struct btf_type * t ) { return BTF_INFO_VLEN ( t -> info ) ; }",
    "resources/libbpf/src/bpf.c@btf_kflag": "static inline bool btf_kflag ( const struct btf_type * t ) { return BTF_INFO_KFLAG ( t -> info ) ; }",
    "resources/libbpf/src/bpf.c@btf_is_void": "static inline bool btf_is_void ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_UNKN ; }",
    "resources/libbpf/src/bpf.c@btf_is_int": "static inline bool btf_is_int ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_INT ; }",
    "resources/libbpf/src/bpf.c@btf_is_ptr": "static inline bool btf_is_ptr ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_PTR ; }",
    "resources/libbpf/src/bpf.c@btf_is_array": "static inline bool btf_is_array ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_ARRAY ; }",
    "resources/libbpf/src/bpf.c@btf_is_struct": "static inline bool btf_is_struct ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_STRUCT ; }",
    "resources/libbpf/src/bpf.c@btf_is_union": "static inline bool btf_is_union ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_UNION ; }",
    "resources/libbpf/src/bpf.c@btf_is_composite": "static inline bool btf_is_composite ( const struct btf_type * t ) { __u16 kind = btf_kind ( t ) ; return kind == BTF_KIND_STRUCT || kind == BTF_KIND_UNION ; }",
    "resources/libbpf/src/bpf.c@btf_is_enum": "static inline bool btf_is_enum ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_ENUM ; }",
    "resources/libbpf/src/bpf.c@btf_is_enum64": "static inline bool btf_is_enum64 ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_ENUM64 ; }",
    "resources/libbpf/src/bpf.c@btf_is_fwd": "static inline bool btf_is_fwd ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_FWD ; }",
    "resources/libbpf/src/bpf.c@btf_is_typedef": "static inline bool btf_is_typedef ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_TYPEDEF ; }",
    "resources/libbpf/src/bpf.c@btf_is_volatile": "static inline bool btf_is_volatile ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_VOLATILE ; }",
    "resources/libbpf/src/bpf.c@btf_is_const": "static inline bool btf_is_const ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_CONST ; }",
    "resources/libbpf/src/bpf.c@btf_is_restrict": "static inline bool btf_is_restrict ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_RESTRICT ; }",
    "resources/libbpf/src/bpf.c@btf_is_mod": "static inline bool btf_is_mod ( const struct btf_type * t ) { __u16 kind = btf_kind ( t ) ; return kind == BTF_KIND_VOLATILE || kind == BTF_KIND_CONST || kind == BTF_KIND_RESTRICT || kind == BTF_KIND_TYPE_TAG ; }",
    "resources/libbpf/src/bpf.c@btf_is_func": "static inline bool btf_is_func ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_FUNC ; }",
    "resources/libbpf/src/bpf.c@btf_is_func_proto": "static inline bool btf_is_func_proto ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_FUNC_PROTO ; }",
    "resources/libbpf/src/bpf.c@btf_is_var": "static inline bool btf_is_var ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_VAR ; }",
    "resources/libbpf/src/bpf.c@btf_is_datasec": "static inline bool btf_is_datasec ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_DATASEC ; }",
    "resources/libbpf/src/bpf.c@btf_is_float": "static inline bool btf_is_float ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_FLOAT ; }",
    "resources/libbpf/src/bpf.c@btf_is_decl_tag": "static inline bool btf_is_decl_tag ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_DECL_TAG ; }",
    "resources/libbpf/src/bpf.c@btf_is_type_tag": "static inline bool btf_is_type_tag ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_TYPE_TAG ; }",
    "resources/libbpf/src/bpf.c@btf_is_any_enum": "static inline bool btf_is_any_enum ( const struct btf_type * t ) { return btf_is_enum ( t ) || btf_is_enum64 ( t ) ; }",
    "resources/libbpf/src/bpf.c@btf_kind_core_compat": "static inline bool btf_kind_core_compat ( const struct btf_type * t1 , const struct btf_type * t2 ) { return btf_kind ( t1 ) == btf_kind ( t2 ) || ( btf_is_any_enum ( t1 ) && btf_is_any_enum ( t2 ) ) ; }",
    "resources/libbpf/src/bpf.c@btf_int_encoding": "static inline __u8 btf_int_encoding ( const struct btf_type * t ) { return BTF_INT_ENCODING ( * ( __u32 * ) ( t + 1 ) ) ; }",
    "resources/libbpf/src/bpf.c@btf_int_offset": "static inline __u8 btf_int_offset ( const struct btf_type * t ) { return BTF_INT_OFFSET ( * ( __u32 * ) ( t + 1 ) ) ; }",
    "resources/libbpf/src/bpf.c@btf_int_bits": "static inline __u8 btf_int_bits ( const struct btf_type * t ) { return BTF_INT_BITS ( * ( __u32 * ) ( t + 1 ) ) ; }",
    "resources/libbpf/src/bpf.c@btf_array": "static inline struct btf_array * btf_array ( const struct btf_type * t ) { return ( struct btf_array * ) ( t + 1 ) ; }",
    "resources/libbpf/src/bpf.c@btf_enum": "static inline struct btf_enum * btf_enum ( const struct btf_type * t ) { return ( struct btf_enum * ) ( t + 1 ) ; }",
    "resources/libbpf/src/bpf.c@btf_enum64": "static inline struct btf_enum64 * btf_enum64 ( const struct btf_type * t ) { return ( struct btf_enum64 * ) ( t + 1 ) ; }",
    "resources/libbpf/src/bpf.c@btf_enum64_value": "static inline __u64 btf_enum64_value ( const struct btf_enum64 * e ) { /* struct btf_enum64 is introduced in Linux 6.0, which is very\n\t * bleeding-edge. Here we are avoiding relying on struct btf_enum64\n\t * definition coming from kernel UAPI headers to support wider range\n\t * of system-wide kernel headers.\n\t *\n\t * Given this header can be also included from C++ applications, that\n\t * further restricts C tricks we can use (like using compatible\n\t * anonymous struct). So just treat struct btf_enum64 as\n\t * a three-element array of u32 and access second (lo32) and third\n\t * (hi32) elements directly.\n\t *\n\t * For reference, here is a struct btf_enum64 definition:\n\t *\n\t * const struct btf_enum64 {\n\t *\t__u32\tname_off;\n\t *\t__u32\tval_lo32;\n\t *\t__u32\tval_hi32;\n\t * };\n\t */ const __u32 * e64 = ( const __u32 * ) e ; return ( ( __u64 ) e64 [ 2 ] << 32 ) | e64 [ 1 ] ; }",
    "resources/libbpf/src/bpf.c@btf_members": "static inline struct btf_member * btf_members ( const struct btf_type * t ) { return ( struct btf_member * ) ( t + 1 ) ; }",
    "resources/libbpf/src/bpf.c@btf_member_bit_offset": "static inline __u32 btf_member_bit_offset ( const struct btf_type * t , __u32 member_idx ) { const struct btf_member * m = btf_members ( t ) + member_idx ; bool kflag = btf_kflag ( t ) ; return kflag ? BTF_MEMBER_BIT_OFFSET ( m -> offset ) : m -> offset ; }",
    "resources/libbpf/src/bpf.c@btf_member_bitfield_size": "static inline __u32 btf_member_bitfield_size ( const struct btf_type * t , __u32 member_idx ) { const struct btf_member * m = btf_members ( t ) + member_idx ; bool kflag = btf_kflag ( t ) ; return kflag ? BTF_MEMBER_BITFIELD_SIZE ( m -> offset ) : 0 ; }",
    "resources/libbpf/src/bpf.c@btf_params": "static inline struct btf_param * btf_params ( const struct btf_type * t ) { return ( struct btf_param * ) ( t + 1 ) ; }",
    "resources/libbpf/src/bpf.c@btf_var": "static inline struct btf_var * btf_var ( const struct btf_type * t ) { return ( struct btf_var * ) ( t + 1 ) ; }",
    "resources/libbpf/src/bpf.c@btf_var_secinfos": "static inline struct btf_var_secinfo * btf_var_secinfos ( const struct btf_type * t ) { return ( struct btf_var_secinfo * ) ( t + 1 ) ; }",
    "resources/libbpf/src/bpf.c@btf_decl_tag": "static inline struct btf_decl_tag * btf_decl_tag ( const struct btf_type * t ) { return ( struct btf_decl_tag * ) ( t + 1 ) ; }",
    "resources/libbpf/src/bpf.c@str_has_sfx": "static inline bool str_has_sfx ( const char * str , const char * sfx ) { size_t str_len = strlen ( str ) ; size_t sfx_len = strlen ( sfx ) ; if ( sfx_len > str_len ) return false ; return strcmp ( str + str_len - sfx_len , sfx ) == 0 ; }",
    "resources/libbpf/src/bpf.c@libbpf_reallocarray": "static inline void * libbpf_reallocarray ( void * ptr , size_t nmemb , size_t size ) { size_t total ; # if __has_builtin ( __builtin_mul_overflow ) if ( unlikely ( __builtin_mul_overflow ( nmemb , size , & total ) ) ) return NULL ; # else if ( size == 0 || nmemb > ULONG_MAX / size ) return NULL ; total = nmemb * size ; # endif return realloc ( ptr , total ) ; }",
    "resources/libbpf/src/bpf.c@libbpf_strlcpy": "static inline void libbpf_strlcpy ( char * dst , const char * src , size_t sz ) { size_t i ; if ( sz == 0 ) return ; sz -- ; for ( i = 0 ; i < sz && src [ i ] ; i ++ ) dst [ i ] = src [ i ] ; dst [ i ] = '\\0' ; }",
    "resources/libbpf/src/bpf.c@btf_func_linkage": "static inline enum btf_func_linkage btf_func_linkage ( const struct btf_type * t ) { return ( enum btf_func_linkage ) ( int ) btf_vlen ( t ) ; }",
    "resources/libbpf/src/bpf.c@btf_type_info": "static inline __u32 btf_type_info ( int kind , int vlen , int kflag ) { return ( kflag << 31 ) | ( kind << 24 ) | vlen ; }",
    "resources/libbpf/src/bpf.c@libbpf_is_mem_zeroed": "static inline bool libbpf_is_mem_zeroed ( const char * p , ssize_t len ) { while ( len > 0 ) { if ( * p ) return false ; p ++ ; len -- ; } return true ; }",
    "resources/libbpf/src/bpf.c@libbpf_validate_opts": "static inline bool libbpf_validate_opts ( const char * opts , size_t opts_sz , size_t user_sz , const char * type_name ) { if ( user_sz < sizeof ( size_t ) ) { pr_warn ( \"%s size (%zu) is too small\\n\" , type_name , user_sz ) ; return false ; } if ( ! libbpf_is_mem_zeroed ( opts + opts_sz , ( ssize_t ) user_sz - opts_sz ) ) { pr_warn ( \"%s has non-zero extra bytes\\n\" , type_name ) ; return false ; } return true ; }",
    "resources/libbpf/src/bpf.c@libbpf_err": "static inline int libbpf_err ( int ret ) { if ( ret < 0 ) errno = - ret ; return ret ; }",
    "resources/libbpf/src/bpf.c@libbpf_err_errno": "static inline int libbpf_err_errno ( int ret ) { /* errno is already assumed to be set on error */ return ret < 0 ? - errno : ret ; }",
    "resources/libbpf/src/bpf.c@libbpf_err_ptr": "static inline void * libbpf_err_ptr ( int err ) { /* set errno on error, this doesn't break anything */ errno = - err ; return NULL ; }",
    "resources/libbpf/src/bpf.c@libbpf_ptr": "static inline void * libbpf_ptr ( void * ret ) { /* set errno on error, this doesn't break anything */ if ( IS_ERR ( ret ) ) errno = - PTR_ERR ( ret ) ; return IS_ERR ( ret ) ? NULL : ret ; }",
    "resources/libbpf/src/bpf.c@str_is_empty": "static inline bool str_is_empty ( const char * s ) { return ! s || ! s [ 0 ] ; }",
    "resources/libbpf/src/bpf.c@is_ldimm64_insn": "static inline bool is_ldimm64_insn ( struct bpf_insn * insn ) { return insn -> code == ( BPF_LD | BPF_IMM | BPF_DW ) ; }",
    "resources/libbpf/src/bpf.c@dup_good_fd": "static inline int dup_good_fd ( int fd ) { if ( fd < 0 ) return fd ; return fcntl ( fd , F_DUPFD_CLOEXEC , 3 ) ; }",
    "resources/libbpf/src/bpf.c@ensure_good_fd": "static inline int ensure_good_fd ( int fd ) { int old_fd = fd , saved_errno ; if ( fd < 0 ) return fd ; if ( fd < 3 ) { fd = dup_good_fd ( fd ) ; saved_errno = errno ; close ( old_fd ) ; errno = saved_errno ; if ( fd < 0 ) { pr_warn ( \"failed to dup FD %d to FD > 2: %d\\n\" , old_fd , - saved_errno ) ; errno = saved_errno ; } } return fd ; }",
    "resources/libbpf/src/bpf.c@sys_dup2": "static inline int sys_dup2 ( int oldfd , int newfd ) { # ifdef __NR_dup2 return syscall ( __NR_dup2 , oldfd , newfd ) ; # else return syscall ( __NR_dup3 , oldfd , newfd , 0 ) ; # endif }",
    "resources/libbpf/src/bpf.c@reuse_fd": "static inline int reuse_fd ( int fixed_fd , int tmp_fd ) { int err ; err = sys_dup2 ( tmp_fd , fixed_fd ) ; err = err < 0 ? - errno : 0 ; close ( tmp_fd ) ; /* clean up temporary FD */ return err ; }",
    "resources/libbpf/src/bpf.c@is_pow_of_2": "static inline bool is_pow_of_2 ( size_t x ) { return x && ( x & ( x - 1 ) ) == 0 ; }",
    "resources/libbpf/src/bpf.c@ptr_to_u64": "static inline __u64 ptr_to_u64 ( const void * ptr ) { return ( __u64 ) ( unsigned long ) ptr ; }",
    "resources/libbpf/src/bpf.c@sys_bpf": "static inline int sys_bpf ( enum bpf_cmd cmd , union bpf_attr * attr , unsigned int size ) { return syscall ( __NR_bpf , cmd , attr , size ) ; }",
    "resources/libbpf/src/bpf.c@sys_bpf_fd": "static inline int sys_bpf_fd ( enum bpf_cmd cmd , union bpf_attr * attr , unsigned int size ) { int fd ; fd = sys_bpf ( cmd , attr , size ) ; return ensure_good_fd ( fd ) ; }",
    "resources/libbpf/src/bpf.c@sys_bpf_prog_load": "int sys_bpf_prog_load ( union bpf_attr * attr , unsigned int size , int attempts ) { int fd ; do { fd = sys_bpf_fd ( BPF_PROG_LOAD , attr , size ) ; } while ( fd < 0 && errno == EAGAIN && -- attempts > 0 ) ; return fd ; }",
    "resources/libbpf/src/bpf.c@probe_memcg_account": "int probe_memcg_account ( int token_fd ) { const size_t attr_sz = offsetofend ( union bpf_attr , attach_btf_obj_fd ) ; struct bpf_insn insns [ ] = { BPF_EMIT_CALL ( BPF_FUNC_ktime_get_coarse_ns ) , BPF_EXIT_INSN ( ) , } ; size_t insn_cnt = ARRAY_SIZE ( insns ) ; union bpf_attr attr ; int prog_fd ; /* attempt loading freplace trying to use custom BTF */ memset ( & attr , 0 , attr_sz ) ; attr . prog_type = BPF_PROG_TYPE_SOCKET_FILTER ; attr . insns = ptr_to_u64 ( insns ) ; attr . insn_cnt = insn_cnt ; attr . license = ptr_to_u64 ( \"GPL\" ) ; attr . prog_token_fd = token_fd ; if ( token_fd ) attr . prog_flags |= BPF_F_TOKEN_FD ; prog_fd = sys_bpf_fd ( BPF_PROG_LOAD , & attr , attr_sz ) ; if ( prog_fd >= 0 ) { close ( prog_fd ) ; return 1 ; } return 0 ; }",
    "resources/libbpf/src/bpf.c@libbpf_set_memlock_rlim": "int libbpf_set_memlock_rlim ( size_t memlock_bytes ) { if ( memlock_bumped ) return libbpf_err ( - EBUSY ) ; memlock_rlim = memlock_bytes ; return 0 ; }",
    "resources/libbpf/src/bpf.c@bump_rlimit_memlock": "int bump_rlimit_memlock ( void ) { struct rlimit rlim ; /* if kernel supports memcg-based accounting, skip bumping RLIMIT_MEMLOCK */ if ( memlock_bumped || feat_supported ( NULL , FEAT_MEMCG_ACCOUNT ) ) return 0 ; memlock_bumped = true ; /* zero memlock_rlim_max disables auto-bumping RLIMIT_MEMLOCK */ if ( memlock_rlim == 0 ) return 0 ; rlim . rlim_cur = rlim . rlim_max = memlock_rlim ; if ( setrlimit ( RLIMIT_MEMLOCK , & rlim ) ) return - errno ; return 0 ; }",
    "resources/libbpf/src/bpf.c@bpf_map_create": "int bpf_map_create ( enum bpf_map_type map_type , const char * map_name , __u32 key_size , __u32 value_size , __u32 max_entries , const struct bpf_map_create_opts * opts ) { const size_t attr_sz = offsetofend ( union bpf_attr , map_token_fd ) ; union bpf_attr attr ; int fd ; bump_rlimit_memlock ( ) ; memset ( & attr , 0 , attr_sz ) ; if ( ! OPTS_VALID ( opts , bpf_map_create_opts ) ) return libbpf_err ( - EINVAL ) ; attr . map_type = map_type ; if ( map_name && feat_supported ( NULL , FEAT_PROG_NAME ) ) libbpf_strlcpy ( attr . map_name , map_name , sizeof ( attr . map_name ) ) ; attr . key_size = key_size ; attr . value_size = value_size ; attr . max_entries = max_entries ; attr . btf_fd = OPTS_GET ( opts , btf_fd , 0 ) ; attr . btf_key_type_id = OPTS_GET ( opts , btf_key_type_id , 0 ) ; attr . btf_value_type_id = OPTS_GET ( opts , btf_value_type_id , 0 ) ; attr . btf_vmlinux_value_type_id = OPTS_GET ( opts , btf_vmlinux_value_type_id , 0 ) ; attr . value_type_btf_obj_fd = OPTS_GET ( opts , value_type_btf_obj_fd , 0 ) ; attr . inner_map_fd = OPTS_GET ( opts , inner_map_fd , 0 ) ; attr . map_flags = OPTS_GET ( opts , map_flags , 0 ) ; attr . map_extra = OPTS_GET ( opts , map_extra , 0 ) ; attr . numa_node = OPTS_GET ( opts , numa_node , 0 ) ; attr . map_ifindex = OPTS_GET ( opts , map_ifindex , 0 ) ; attr . map_token_fd = OPTS_GET ( opts , token_fd , 0 ) ; fd = sys_bpf_fd ( BPF_MAP_CREATE , & attr , attr_sz ) ; return libbpf_err_errno ( fd ) ; }",
    "resources/libbpf/src/bpf.c@alloc_zero_tailing_info": "static void * alloc_zero_tailing_info ( const void * orecord , __u32 cnt , __u32 actual_rec_size , __u32 expected_rec_size ) { __u64 info_len = ( __u64 ) actual_rec_size * cnt ; void * info , * nrecord ; int i ; info = malloc ( info_len ) ; if ( ! info ) return NULL ; /* zero out bytes kernel does not understand */ nrecord = info ; for ( i = 0 ; i < cnt ; i ++ ) { memcpy ( nrecord , orecord , expected_rec_size ) ; memset ( nrecord + expected_rec_size , 0 , actual_rec_size - expected_rec_size ) ; orecord += actual_rec_size ; nrecord += actual_rec_size ; } return info ; }",
    "resources/libbpf/src/bpf.c@bpf_prog_load": "int bpf_prog_load ( enum bpf_prog_type prog_type , const char * prog_name , const char * license , const struct bpf_insn * insns , size_t insn_cnt , struct bpf_prog_load_opts * opts ) { const size_t attr_sz = offsetofend ( union bpf_attr , prog_token_fd ) ; void * finfo = NULL , * linfo = NULL ; const char * func_info , * line_info ; __u32 log_size , log_level , attach_prog_fd , attach_btf_obj_fd ; __u32 func_info_rec_size , line_info_rec_size ; int fd , attempts ; union bpf_attr attr ; char * log_buf ; bump_rlimit_memlock ( ) ; if ( ! OPTS_VALID ( opts , bpf_prog_load_opts ) ) return libbpf_err ( - EINVAL ) ; attempts = OPTS_GET ( opts , attempts , 0 ) ; if ( attempts < 0 ) return libbpf_err ( - EINVAL ) ; if ( attempts == 0 ) attempts = PROG_LOAD_ATTEMPTS ; memset ( & attr , 0 , attr_sz ) ; attr . prog_type = prog_type ; attr . expected_attach_type = OPTS_GET ( opts , expected_attach_type , 0 ) ; attr . prog_btf_fd = OPTS_GET ( opts , prog_btf_fd , 0 ) ; attr . prog_flags = OPTS_GET ( opts , prog_flags , 0 ) ; attr . prog_ifindex = OPTS_GET ( opts , prog_ifindex , 0 ) ; attr . kern_version = OPTS_GET ( opts , kern_version , 0 ) ; attr . prog_token_fd = OPTS_GET ( opts , token_fd , 0 ) ; if ( prog_name && feat_supported ( NULL , FEAT_PROG_NAME ) ) libbpf_strlcpy ( attr . prog_name , prog_name , sizeof ( attr . prog_name ) ) ; attr . license = ptr_to_u64 ( license ) ; if ( insn_cnt > UINT_MAX ) return libbpf_err ( - E2BIG ) ; attr . insns = ptr_to_u64 ( insns ) ; attr . insn_cnt = ( __u32 ) insn_cnt ; attach_prog_fd = OPTS_GET ( opts , attach_prog_fd , 0 ) ; attach_btf_obj_fd = OPTS_GET ( opts , attach_btf_obj_fd , 0 ) ; if ( attach_prog_fd && attach_btf_obj_fd ) return libbpf_err ( - EINVAL ) ; attr . attach_btf_id = OPTS_GET ( opts , attach_btf_id , 0 ) ; if ( attach_prog_fd ) attr . attach_prog_fd = attach_prog_fd ; else attr . attach_btf_obj_fd = attach_btf_obj_fd ; log_buf = OPTS_GET ( opts , log_buf , NULL ) ; log_size = OPTS_GET ( opts , log_size , 0 ) ; log_level = OPTS_GET ( opts , log_level , 0 ) ; if ( ! ! log_buf != ! ! log_size ) return libbpf_err ( - EINVAL ) ; func_info_rec_size = OPTS_GET ( opts , func_info_rec_size , 0 ) ; func_info = OPTS_GET ( opts , func_info , NULL ) ; attr . func_info_rec_size = func_info_rec_size ; attr . func_info = ptr_to_u64 ( func_info ) ; attr . func_info_cnt = OPTS_GET ( opts , func_info_cnt , 0 ) ; line_info_rec_size = OPTS_GET ( opts , line_info_rec_size , 0 ) ; line_info = OPTS_GET ( opts , line_info , NULL ) ; attr . line_info_rec_size = line_info_rec_size ; attr . line_info = ptr_to_u64 ( line_info ) ; attr . line_info_cnt = OPTS_GET ( opts , line_info_cnt , 0 ) ; attr . fd_array = ptr_to_u64 ( OPTS_GET ( opts , fd_array , NULL ) ) ; if ( log_level ) { attr . log_buf = ptr_to_u64 ( log_buf ) ; attr . log_size = log_size ; attr . log_level = log_level ; } fd = sys_bpf_prog_load ( & attr , attr_sz , attempts ) ; OPTS_SET ( opts , log_true_size , attr . log_true_size ) ; if ( fd >= 0 ) return fd ; /* After bpf_prog_load, the kernel may modify certain attributes\n\t * to give user space a hint how to deal with loading failure.\n\t * Check to see whether we can make some changes and load again.\n\t */ while ( errno == E2BIG && ( ! finfo || ! linfo ) ) { if ( ! finfo && attr . func_info_cnt && attr . func_info_rec_size < func_info_rec_size ) { /* try with corrected func info records */ finfo = alloc_zero_tailing_info ( func_info , attr . func_info_cnt , func_info_rec_size , attr . func_info_rec_size ) ; if ( ! finfo ) { errno = E2BIG ; goto done ; } attr . func_info = ptr_to_u64 ( finfo ) ; attr . func_info_rec_size = func_info_rec_size ; } else if ( ! linfo && attr . line_info_cnt && attr . line_info_rec_size < line_info_rec_size ) { linfo = alloc_zero_tailing_info ( line_info , attr . line_info_cnt , line_info_rec_size , attr . line_info_rec_size ) ; if ( ! linfo ) { errno = E2BIG ; goto done ; } attr . line_info = ptr_to_u64 ( linfo ) ; attr . line_info_rec_size = line_info_rec_size ; } else { break ; } fd = sys_bpf_prog_load ( & attr , attr_sz , attempts ) ; OPTS_SET ( opts , log_true_size , attr . log_true_size ) ; if ( fd >= 0 ) goto done ; } if ( log_level == 0 && log_buf ) { /* log_level == 0 with non-NULL log_buf requires retrying on error\n\t\t * with log_level == 1 and log_buf/log_buf_size set, to get details of\n\t\t * failure\n\t\t */ attr . log_buf = ptr_to_u64 ( log_buf ) ; attr . log_size = log_size ; attr . log_level = 1 ; fd = sys_bpf_prog_load ( & attr , attr_sz , attempts ) ; OPTS_SET ( opts , log_true_size , attr . log_true_size ) ; } done : /* free() doesn't affect errno, so we don't need to restore it */ free ( finfo ) ; free ( linfo ) ; return libbpf_err_errno ( fd ) ; }",
    "resources/libbpf/src/bpf.c@bpf_map_update_elem": "int bpf_map_update_elem ( int fd , const void * key , const void * value , __u64 flags ) { const size_t attr_sz = offsetofend ( union bpf_attr , flags ) ; union bpf_attr attr ; int ret ; memset ( & attr , 0 , attr_sz ) ; attr . map_fd = fd ; attr . key = ptr_to_u64 ( key ) ; attr . value = ptr_to_u64 ( value ) ; attr . flags = flags ; ret = sys_bpf ( BPF_MAP_UPDATE_ELEM , & attr , attr_sz ) ; return libbpf_err_errno ( ret ) ; }",
    "resources/libbpf/src/bpf.c@bpf_map_lookup_elem": "int bpf_map_lookup_elem ( int fd , const void * key , void * value ) { const size_t attr_sz = offsetofend ( union bpf_attr , flags ) ; union bpf_attr attr ; int ret ; memset ( & attr , 0 , attr_sz ) ; attr . map_fd = fd ; attr . key = ptr_to_u64 ( key ) ; attr . value = ptr_to_u64 ( value ) ; ret = sys_bpf ( BPF_MAP_LOOKUP_ELEM , & attr , attr_sz ) ; return libbpf_err_errno ( ret ) ; }",
    "resources/libbpf/src/bpf.c@bpf_map_lookup_elem_flags": "int bpf_map_lookup_elem_flags ( int fd , const void * key , void * value , __u64 flags ) { const size_t attr_sz = offsetofend ( union bpf_attr , flags ) ; union bpf_attr attr ; int ret ; memset ( & attr , 0 , attr_sz ) ; attr . map_fd = fd ; attr . key = ptr_to_u64 ( key ) ; attr . value = ptr_to_u64 ( value ) ; attr . flags = flags ; ret = sys_bpf ( BPF_MAP_LOOKUP_ELEM , & attr , attr_sz ) ; return libbpf_err_errno ( ret ) ; }",
    "resources/libbpf/src/bpf.c@bpf_map_lookup_and_delete_elem": "int bpf_map_lookup_and_delete_elem ( int fd , const void * key , void * value ) { const size_t attr_sz = offsetofend ( union bpf_attr , flags ) ; union bpf_attr attr ; int ret ; memset ( & attr , 0 , attr_sz ) ; attr . map_fd = fd ; attr . key = ptr_to_u64 ( key ) ; attr . value = ptr_to_u64 ( value ) ; ret = sys_bpf ( BPF_MAP_LOOKUP_AND_DELETE_ELEM , & attr , attr_sz ) ; return libbpf_err_errno ( ret ) ; }",
    "resources/libbpf/src/bpf.c@bpf_map_lookup_and_delete_elem_flags": "int bpf_map_lookup_and_delete_elem_flags ( int fd , const void * key , void * value , __u64 flags ) { const size_t attr_sz = offsetofend ( union bpf_attr , flags ) ; union bpf_attr attr ; int ret ; memset ( & attr , 0 , attr_sz ) ; attr . map_fd = fd ; attr . key = ptr_to_u64 ( key ) ; attr . value = ptr_to_u64 ( value ) ; attr . flags = flags ; ret = sys_bpf ( BPF_MAP_LOOKUP_AND_DELETE_ELEM , & attr , attr_sz ) ; return libbpf_err_errno ( ret ) ; }",
    "resources/libbpf/src/bpf.c@bpf_map_delete_elem": "int bpf_map_delete_elem ( int fd , const void * key ) { const size_t attr_sz = offsetofend ( union bpf_attr , flags ) ; union bpf_attr attr ; int ret ; memset ( & attr , 0 , attr_sz ) ; attr . map_fd = fd ; attr . key = ptr_to_u64 ( key ) ; ret = sys_bpf ( BPF_MAP_DELETE_ELEM , & attr , attr_sz ) ; return libbpf_err_errno ( ret ) ; }",
    "resources/libbpf/src/bpf.c@bpf_map_delete_elem_flags": "int bpf_map_delete_elem_flags ( int fd , const void * key , __u64 flags ) { const size_t attr_sz = offsetofend ( union bpf_attr , flags ) ; union bpf_attr attr ; int ret ; memset ( & attr , 0 , attr_sz ) ; attr . map_fd = fd ; attr . key = ptr_to_u64 ( key ) ; attr . flags = flags ; ret = sys_bpf ( BPF_MAP_DELETE_ELEM , & attr , attr_sz ) ; return libbpf_err_errno ( ret ) ; }",
    "resources/libbpf/src/bpf.c@bpf_map_get_next_key": "int bpf_map_get_next_key ( int fd , const void * key , void * next_key ) { const size_t attr_sz = offsetofend ( union bpf_attr , next_key ) ; union bpf_attr attr ; int ret ; memset ( & attr , 0 , attr_sz ) ; attr . map_fd = fd ; attr . key = ptr_to_u64 ( key ) ; attr . next_key = ptr_to_u64 ( next_key ) ; ret = sys_bpf ( BPF_MAP_GET_NEXT_KEY , & attr , attr_sz ) ; return libbpf_err_errno ( ret ) ; }",
    "resources/libbpf/src/bpf.c@bpf_map_freeze": "int bpf_map_freeze ( int fd ) { const size_t attr_sz = offsetofend ( union bpf_attr , map_fd ) ; union bpf_attr attr ; int ret ; memset ( & attr , 0 , attr_sz ) ; attr . map_fd = fd ; ret = sys_bpf ( BPF_MAP_FREEZE , & attr , attr_sz ) ; return libbpf_err_errno ( ret ) ; }",
    "resources/libbpf/src/bpf.c@bpf_map_batch_common": "static int bpf_map_batch_common ( int cmd , int fd , void * in_batch , void * out_batch , void * keys , void * values , __u32 * count , const struct bpf_map_batch_opts * opts ) { const size_t attr_sz = offsetofend ( union bpf_attr , batch ) ; union bpf_attr attr ; int ret ; if ( ! OPTS_VALID ( opts , bpf_map_batch_opts ) ) return libbpf_err ( - EINVAL ) ; memset ( & attr , 0 , attr_sz ) ; attr . batch . map_fd = fd ; attr . batch . in_batch = ptr_to_u64 ( in_batch ) ; attr . batch . out_batch = ptr_to_u64 ( out_batch ) ; attr . batch . keys = ptr_to_u64 ( keys ) ; attr . batch . values = ptr_to_u64 ( values ) ; attr . batch . count = * count ; attr . batch . elem_flags = OPTS_GET ( opts , elem_flags , 0 ) ; attr . batch . flags = OPTS_GET ( opts , flags , 0 ) ; ret = sys_bpf ( cmd , & attr , attr_sz ) ; * count = attr . batch . count ; return libbpf_err_errno ( ret ) ; }",
    "resources/libbpf/src/bpf.c@bpf_map_delete_batch": "int bpf_map_delete_batch ( int fd , const void * keys , __u32 * count , const struct bpf_map_batch_opts * opts ) { return bpf_map_batch_common ( BPF_MAP_DELETE_BATCH , fd , NULL , NULL , ( void * ) keys , NULL , count , opts ) ; }",
    "resources/libbpf/src/bpf.c@bpf_map_lookup_batch": "int bpf_map_lookup_batch ( int fd , void * in_batch , void * out_batch , void * keys , void * values , __u32 * count , const struct bpf_map_batch_opts * opts ) { return bpf_map_batch_common ( BPF_MAP_LOOKUP_BATCH , fd , in_batch , out_batch , keys , values , count , opts ) ; }",
    "resources/libbpf/src/bpf.c@bpf_map_lookup_and_delete_batch": "int bpf_map_lookup_and_delete_batch ( int fd , void * in_batch , void * out_batch , void * keys , void * values , __u32 * count , const struct bpf_map_batch_opts * opts ) { return bpf_map_batch_common ( BPF_MAP_LOOKUP_AND_DELETE_BATCH , fd , in_batch , out_batch , keys , values , count , opts ) ; }",
    "resources/libbpf/src/bpf.c@bpf_map_update_batch": "int bpf_map_update_batch ( int fd , const void * keys , const void * values , __u32 * count , const struct bpf_map_batch_opts * opts ) { return bpf_map_batch_common ( BPF_MAP_UPDATE_BATCH , fd , NULL , NULL , ( void * ) keys , ( void * ) values , count , opts ) ; }",
    "resources/libbpf/src/bpf.c@bpf_obj_pin_opts": "int bpf_obj_pin_opts ( int fd , const char * pathname , const struct bpf_obj_pin_opts * opts ) { const size_t attr_sz = offsetofend ( union bpf_attr , path_fd ) ; union bpf_attr attr ; int ret ; if ( ! OPTS_VALID ( opts , bpf_obj_pin_opts ) ) return libbpf_err ( - EINVAL ) ; memset ( & attr , 0 , attr_sz ) ; attr . path_fd = OPTS_GET ( opts , path_fd , 0 ) ; attr . pathname = ptr_to_u64 ( ( void * ) pathname ) ; attr . file_flags = OPTS_GET ( opts , file_flags , 0 ) ; attr . bpf_fd = fd ; ret = sys_bpf ( BPF_OBJ_PIN , & attr , attr_sz ) ; return libbpf_err_errno ( ret ) ; }",
    "resources/libbpf/src/bpf.c@bpf_obj_pin": "int bpf_obj_pin ( int fd , const char * pathname ) { return bpf_obj_pin_opts ( fd , pathname , NULL ) ; }",
    "resources/libbpf/src/bpf.c@bpf_obj_get": "int bpf_obj_get ( const char * pathname ) { return bpf_obj_get_opts ( pathname , NULL ) ; }",
    "resources/libbpf/src/bpf.c@bpf_obj_get_opts": "int bpf_obj_get_opts ( const char * pathname , const struct bpf_obj_get_opts * opts ) { const size_t attr_sz = offsetofend ( union bpf_attr , path_fd ) ; union bpf_attr attr ; int fd ; if ( ! OPTS_VALID ( opts , bpf_obj_get_opts ) ) return libbpf_err ( - EINVAL ) ; memset ( & attr , 0 , attr_sz ) ; attr . path_fd = OPTS_GET ( opts , path_fd , 0 ) ; attr . pathname = ptr_to_u64 ( ( void * ) pathname ) ; attr . file_flags = OPTS_GET ( opts , file_flags , 0 ) ; fd = sys_bpf_fd ( BPF_OBJ_GET , & attr , attr_sz ) ; return libbpf_err_errno ( fd ) ; }",
    "resources/libbpf/src/bpf.c@bpf_prog_attach": "int bpf_prog_attach ( int prog_fd , int target_fd , enum bpf_attach_type type , unsigned int flags ) { DECLARE_LIBBPF_OPTS ( bpf_prog_attach_opts , opts , . flags = flags , ) ; return bpf_prog_attach_opts ( prog_fd , target_fd , type , & opts ) ; }",
    "resources/libbpf/src/bpf.c@bpf_prog_attach_opts": "int bpf_prog_attach_opts ( int prog_fd , int target , enum bpf_attach_type type , const struct bpf_prog_attach_opts * opts ) { const size_t attr_sz = offsetofend ( union bpf_attr , expected_revision ) ; __u32 relative_id , flags ; int ret , relative_fd ; union bpf_attr attr ; if ( ! OPTS_VALID ( opts , bpf_prog_attach_opts ) ) return libbpf_err ( - EINVAL ) ; relative_id = OPTS_GET ( opts , relative_id , 0 ) ; relative_fd = OPTS_GET ( opts , relative_fd , 0 ) ; flags = OPTS_GET ( opts , flags , 0 ) ; /* validate we don't have unexpected combinations of non-zero fields */ if ( relative_fd && relative_id ) return libbpf_err ( - EINVAL ) ; memset ( & attr , 0 , attr_sz ) ; attr . target_fd = target ; attr . attach_bpf_fd = prog_fd ; attr . attach_type = type ; attr . replace_bpf_fd = OPTS_GET ( opts , replace_fd , 0 ) ; attr . expected_revision = OPTS_GET ( opts , expected_revision , 0 ) ; if ( relative_id ) { attr . attach_flags = flags | BPF_F_ID ; attr . relative_id = relative_id ; } else { attr . attach_flags = flags ; attr . relative_fd = relative_fd ; } ret = sys_bpf ( BPF_PROG_ATTACH , & attr , attr_sz ) ; return libbpf_err_errno ( ret ) ; }",
    "resources/libbpf/src/bpf.c@bpf_prog_detach_opts": "int bpf_prog_detach_opts ( int prog_fd , int target , enum bpf_attach_type type , const struct bpf_prog_detach_opts * opts ) { const size_t attr_sz = offsetofend ( union bpf_attr , expected_revision ) ; __u32 relative_id , flags ; int ret , relative_fd ; union bpf_attr attr ; if ( ! OPTS_VALID ( opts , bpf_prog_detach_opts ) ) return libbpf_err ( - EINVAL ) ; relative_id = OPTS_GET ( opts , relative_id , 0 ) ; relative_fd = OPTS_GET ( opts , relative_fd , 0 ) ; flags = OPTS_GET ( opts , flags , 0 ) ; /* validate we don't have unexpected combinations of non-zero fields */ if ( relative_fd && relative_id ) return libbpf_err ( - EINVAL ) ; memset ( & attr , 0 , attr_sz ) ; attr . target_fd = target ; attr . attach_bpf_fd = prog_fd ; attr . attach_type = type ; attr . expected_revision = OPTS_GET ( opts , expected_revision , 0 ) ; if ( relative_id ) { attr . attach_flags = flags | BPF_F_ID ; attr . relative_id = relative_id ; } else { attr . attach_flags = flags ; attr . relative_fd = relative_fd ; } ret = sys_bpf ( BPF_PROG_DETACH , & attr , attr_sz ) ; return libbpf_err_errno ( ret ) ; }",
    "resources/libbpf/src/bpf.c@bpf_prog_detach": "int bpf_prog_detach ( int target_fd , enum bpf_attach_type type ) { return bpf_prog_detach_opts ( 0 , target_fd , type , NULL ) ; }",
    "resources/libbpf/src/bpf.c@bpf_prog_detach2": "int bpf_prog_detach2 ( int prog_fd , int target_fd , enum bpf_attach_type type ) { return bpf_prog_detach_opts ( prog_fd , target_fd , type , NULL ) ; }",
    "resources/libbpf/src/bpf.c@bpf_link_create": "int bpf_link_create ( int prog_fd , int target_fd , enum bpf_attach_type attach_type , const struct bpf_link_create_opts * opts ) { const size_t attr_sz = offsetofend ( union bpf_attr , link_create ) ; __u32 target_btf_id , iter_info_len , relative_id ; int fd , err , relative_fd ; union bpf_attr attr ; if ( ! OPTS_VALID ( opts , bpf_link_create_opts ) ) return libbpf_err ( - EINVAL ) ; iter_info_len = OPTS_GET ( opts , iter_info_len , 0 ) ; target_btf_id = OPTS_GET ( opts , target_btf_id , 0 ) ; /* validate we don't have unexpected combinations of non-zero fields */ if ( iter_info_len || target_btf_id ) { if ( iter_info_len && target_btf_id ) return libbpf_err ( - EINVAL ) ; if ( ! OPTS_ZEROED ( opts , target_btf_id ) ) return libbpf_err ( - EINVAL ) ; } memset ( & attr , 0 , attr_sz ) ; attr . link_create . prog_fd = prog_fd ; attr . link_create . target_fd = target_fd ; attr . link_create . attach_type = attach_type ; attr . link_create . flags = OPTS_GET ( opts , flags , 0 ) ; if ( target_btf_id ) { attr . link_create . target_btf_id = target_btf_id ; goto proceed ; } switch ( attach_type ) { case BPF_TRACE_ITER : attr . link_create . iter_info = ptr_to_u64 ( OPTS_GET ( opts , iter_info , ( void * ) 0 ) ) ; attr . link_create . iter_info_len = iter_info_len ; break ; case BPF_PERF_EVENT : attr . link_create . perf_event . bpf_cookie = OPTS_GET ( opts , perf_event . bpf_cookie , 0 ) ; if ( ! OPTS_ZEROED ( opts , perf_event ) ) return libbpf_err ( - EINVAL ) ; break ; case BPF_TRACE_KPROBE_MULTI : attr . link_create . kprobe_multi . flags = OPTS_GET ( opts , kprobe_multi . flags , 0 ) ; attr . link_create . kprobe_multi . cnt = OPTS_GET ( opts , kprobe_multi . cnt , 0 ) ; attr . link_create . kprobe_multi . syms = ptr_to_u64 ( OPTS_GET ( opts , kprobe_multi . syms , 0 ) ) ; attr . link_create . kprobe_multi . addrs = ptr_to_u64 ( OPTS_GET ( opts , kprobe_multi . addrs , 0 ) ) ; attr . link_create . kprobe_multi . cookies = ptr_to_u64 ( OPTS_GET ( opts , kprobe_multi . cookies , 0 ) ) ; if ( ! OPTS_ZEROED ( opts , kprobe_multi ) ) return libbpf_err ( - EINVAL ) ; break ; case BPF_TRACE_UPROBE_MULTI : attr . link_create . uprobe_multi . flags = OPTS_GET ( opts , uprobe_multi . flags , 0 ) ; attr . link_create . uprobe_multi . cnt = OPTS_GET ( opts , uprobe_multi . cnt , 0 ) ; attr . link_create . uprobe_multi . path = ptr_to_u64 ( OPTS_GET ( opts , uprobe_multi . path , 0 ) ) ; attr . link_create . uprobe_multi . offsets = ptr_to_u64 ( OPTS_GET ( opts , uprobe_multi . offsets , 0 ) ) ; attr . link_create . uprobe_multi . ref_ctr_offsets = ptr_to_u64 ( OPTS_GET ( opts , uprobe_multi . ref_ctr_offsets , 0 ) ) ; attr . link_create . uprobe_multi . cookies = ptr_to_u64 ( OPTS_GET ( opts , uprobe_multi . cookies , 0 ) ) ; attr . link_create . uprobe_multi . pid = OPTS_GET ( opts , uprobe_multi . pid , 0 ) ; if ( ! OPTS_ZEROED ( opts , uprobe_multi ) ) return libbpf_err ( - EINVAL ) ; break ; case BPF_TRACE_RAW_TP : case BPF_TRACE_FENTRY : case BPF_TRACE_FEXIT : case BPF_MODIFY_RETURN : case BPF_LSM_MAC : attr . link_create . tracing . cookie = OPTS_GET ( opts , tracing . cookie , 0 ) ; if ( ! OPTS_ZEROED ( opts , tracing ) ) return libbpf_err ( - EINVAL ) ; break ; case BPF_NETFILTER : attr . link_create . netfilter . pf = OPTS_GET ( opts , netfilter . pf , 0 ) ; attr . link_create . netfilter . hooknum = OPTS_GET ( opts , netfilter . hooknum , 0 ) ; attr . link_create . netfilter . priority = OPTS_GET ( opts , netfilter . priority , 0 ) ; attr . link_create . netfilter . flags = OPTS_GET ( opts , netfilter . flags , 0 ) ; if ( ! OPTS_ZEROED ( opts , netfilter ) ) return libbpf_err ( - EINVAL ) ; break ; case BPF_TCX_INGRESS : case BPF_TCX_EGRESS : relative_fd = OPTS_GET ( opts , tcx . relative_fd , 0 ) ; relative_id = OPTS_GET ( opts , tcx . relative_id , 0 ) ; if ( relative_fd && relative_id ) return libbpf_err ( - EINVAL ) ; if ( relative_id ) { attr . link_create . tcx . relative_id = relative_id ; attr . link_create . flags |= BPF_F_ID ; } else { attr . link_create . tcx . relative_fd = relative_fd ; } attr . link_create . tcx . expected_revision = OPTS_GET ( opts , tcx . expected_revision , 0 ) ; if ( ! OPTS_ZEROED ( opts , tcx ) ) return libbpf_err ( - EINVAL ) ; break ; case BPF_NETKIT_PRIMARY : case BPF_NETKIT_PEER : relative_fd = OPTS_GET ( opts , netkit . relative_fd , 0 ) ; relative_id = OPTS_GET ( opts , netkit . relative_id , 0 ) ; if ( relative_fd && relative_id ) return libbpf_err ( - EINVAL ) ; if ( relative_id ) { attr . link_create . netkit . relative_id = relative_id ; attr . link_create . flags |= BPF_F_ID ; } else { attr . link_create . netkit . relative_fd = relative_fd ; } attr . link_create . netkit . expected_revision = OPTS_GET ( opts , netkit . expected_revision , 0 ) ; if ( ! OPTS_ZEROED ( opts , netkit ) ) return libbpf_err ( - EINVAL ) ; break ; default : if ( ! OPTS_ZEROED ( opts , flags ) ) return libbpf_err ( - EINVAL ) ; break ; } proceed : fd = sys_bpf_fd ( BPF_LINK_CREATE , & attr , attr_sz ) ; if ( fd >= 0 ) return fd ; /* we'll get EINVAL if LINK_CREATE doesn't support attaching fentry\n\t * and other similar programs\n\t */ err = - errno ; if ( err != - EINVAL ) return libbpf_err ( err ) ; /* if user used features not supported by\n\t * BPF_RAW_TRACEPOINT_OPEN command, then just give up immediately\n\t */ if ( attr . link_create . target_fd || attr . link_create . target_btf_id ) return libbpf_err ( err ) ; if ( ! OPTS_ZEROED ( opts , sz ) ) return libbpf_err ( err ) ; /* otherwise, for few select kinds of programs that can be\n\t * attached using BPF_RAW_TRACEPOINT_OPEN command, try that as\n\t * a fallback for older kernels\n\t */ switch ( attach_type ) { case BPF_TRACE_RAW_TP : case BPF_LSM_MAC : case BPF_TRACE_FENTRY : case BPF_TRACE_FEXIT : case BPF_MODIFY_RETURN : return bpf_raw_tracepoint_open ( NULL , prog_fd ) ; default : return libbpf_err ( err ) ; } }",
    "resources/libbpf/src/bpf.c@bpf_link_detach": "int bpf_link_detach ( int link_fd ) { const size_t attr_sz = offsetofend ( union bpf_attr , link_detach ) ; union bpf_attr attr ; int ret ; memset ( & attr , 0 , attr_sz ) ; attr . link_detach . link_fd = link_fd ; ret = sys_bpf ( BPF_LINK_DETACH , & attr , attr_sz ) ; return libbpf_err_errno ( ret ) ; }",
    "resources/libbpf/src/bpf.c@bpf_link_update": "int bpf_link_update ( int link_fd , int new_prog_fd , const struct bpf_link_update_opts * opts ) { const size_t attr_sz = offsetofend ( union bpf_attr , link_update ) ; union bpf_attr attr ; int ret ; if ( ! OPTS_VALID ( opts , bpf_link_update_opts ) ) return libbpf_err ( - EINVAL ) ; if ( OPTS_GET ( opts , old_prog_fd , 0 ) && OPTS_GET ( opts , old_map_fd , 0 ) ) return libbpf_err ( - EINVAL ) ; memset ( & attr , 0 , attr_sz ) ; attr . link_update . link_fd = link_fd ; attr . link_update . new_prog_fd = new_prog_fd ; attr . link_update . flags = OPTS_GET ( opts , flags , 0 ) ; if ( OPTS_GET ( opts , old_prog_fd , 0 ) ) attr . link_update . old_prog_fd = OPTS_GET ( opts , old_prog_fd , 0 ) ; else if ( OPTS_GET ( opts , old_map_fd , 0 ) ) attr . link_update . old_map_fd = OPTS_GET ( opts , old_map_fd , 0 ) ; ret = sys_bpf ( BPF_LINK_UPDATE , & attr , attr_sz ) ; return libbpf_err_errno ( ret ) ; }",
    "resources/libbpf/src/bpf.c@bpf_iter_create": "int bpf_iter_create ( int link_fd ) { const size_t attr_sz = offsetofend ( union bpf_attr , iter_create ) ; union bpf_attr attr ; int fd ; memset ( & attr , 0 , attr_sz ) ; attr . iter_create . link_fd = link_fd ; fd = sys_bpf_fd ( BPF_ITER_CREATE , & attr , attr_sz ) ; return libbpf_err_errno ( fd ) ; }",
    "resources/libbpf/src/bpf.c@bpf_prog_query_opts": "int bpf_prog_query_opts ( int target , enum bpf_attach_type type , struct bpf_prog_query_opts * opts ) { const size_t attr_sz = offsetofend ( union bpf_attr , query ) ; union bpf_attr attr ; int ret ; if ( ! OPTS_VALID ( opts , bpf_prog_query_opts ) ) return libbpf_err ( - EINVAL ) ; memset ( & attr , 0 , attr_sz ) ; attr . query . target_fd = target ; attr . query . attach_type = type ; attr . query . query_flags = OPTS_GET ( opts , query_flags , 0 ) ; attr . query . count = OPTS_GET ( opts , count , 0 ) ; attr . query . prog_ids = ptr_to_u64 ( OPTS_GET ( opts , prog_ids , NULL ) ) ; attr . query . link_ids = ptr_to_u64 ( OPTS_GET ( opts , link_ids , NULL ) ) ; attr . query . prog_attach_flags = ptr_to_u64 ( OPTS_GET ( opts , prog_attach_flags , NULL ) ) ; attr . query . link_attach_flags = ptr_to_u64 ( OPTS_GET ( opts , link_attach_flags , NULL ) ) ; ret = sys_bpf ( BPF_PROG_QUERY , & attr , attr_sz ) ; OPTS_SET ( opts , attach_flags , attr . query . attach_flags ) ; OPTS_SET ( opts , revision , attr . query . revision ) ; OPTS_SET ( opts , count , attr . query . count ) ; return libbpf_err_errno ( ret ) ; }",
    "resources/libbpf/src/bpf.c@bpf_prog_query": "int bpf_prog_query ( int target_fd , enum bpf_attach_type type , __u32 query_flags , __u32 * attach_flags , __u32 * prog_ids , __u32 * prog_cnt ) { LIBBPF_OPTS ( bpf_prog_query_opts , opts ) ; int ret ; opts . query_flags = query_flags ; opts . prog_ids = prog_ids ; opts . prog_cnt = * prog_cnt ; ret = bpf_prog_query_opts ( target_fd , type , & opts ) ; if ( attach_flags ) * attach_flags = opts . attach_flags ; * prog_cnt = opts . prog_cnt ; return libbpf_err_errno ( ret ) ; }",
    "resources/libbpf/src/bpf.c@bpf_prog_test_run_opts": "int bpf_prog_test_run_opts ( int prog_fd , struct bpf_test_run_opts * opts ) { const size_t attr_sz = offsetofend ( union bpf_attr , test ) ; union bpf_attr attr ; int ret ; if ( ! OPTS_VALID ( opts , bpf_test_run_opts ) ) return libbpf_err ( - EINVAL ) ; memset ( & attr , 0 , attr_sz ) ; attr . test . prog_fd = prog_fd ; attr . test . batch_size = OPTS_GET ( opts , batch_size , 0 ) ; attr . test . cpu = OPTS_GET ( opts , cpu , 0 ) ; attr . test . flags = OPTS_GET ( opts , flags , 0 ) ; attr . test . repeat = OPTS_GET ( opts , repeat , 0 ) ; attr . test . duration = OPTS_GET ( opts , duration , 0 ) ; attr . test . ctx_size_in = OPTS_GET ( opts , ctx_size_in , 0 ) ; attr . test . ctx_size_out = OPTS_GET ( opts , ctx_size_out , 0 ) ; attr . test . data_size_in = OPTS_GET ( opts , data_size_in , 0 ) ; attr . test . data_size_out = OPTS_GET ( opts , data_size_out , 0 ) ; attr . test . ctx_in = ptr_to_u64 ( OPTS_GET ( opts , ctx_in , NULL ) ) ; attr . test . ctx_out = ptr_to_u64 ( OPTS_GET ( opts , ctx_out , NULL ) ) ; attr . test . data_in = ptr_to_u64 ( OPTS_GET ( opts , data_in , NULL ) ) ; attr . test . data_out = ptr_to_u64 ( OPTS_GET ( opts , data_out , NULL ) ) ; ret = sys_bpf ( BPF_PROG_TEST_RUN , & attr , attr_sz ) ; OPTS_SET ( opts , data_size_out , attr . test . data_size_out ) ; OPTS_SET ( opts , ctx_size_out , attr . test . ctx_size_out ) ; OPTS_SET ( opts , duration , attr . test . duration ) ; OPTS_SET ( opts , retval , attr . test . retval ) ; return libbpf_err_errno ( ret ) ; }",
    "resources/libbpf/src/bpf.c@bpf_obj_get_next_id": "static int bpf_obj_get_next_id ( __u32 start_id , __u32 * next_id , int cmd ) { const size_t attr_sz = offsetofend ( union bpf_attr , open_flags ) ; union bpf_attr attr ; int err ; memset ( & attr , 0 , attr_sz ) ; attr . start_id = start_id ; err = sys_bpf ( cmd , & attr , attr_sz ) ; if ( ! err ) * next_id = attr . next_id ; return libbpf_err_errno ( err ) ; }",
    "resources/libbpf/src/bpf.c@bpf_prog_get_next_id": "int bpf_prog_get_next_id ( __u32 start_id , __u32 * next_id ) { return bpf_obj_get_next_id ( start_id , next_id , BPF_PROG_GET_NEXT_ID ) ; }",
    "resources/libbpf/src/bpf.c@bpf_map_get_next_id": "int bpf_map_get_next_id ( __u32 start_id , __u32 * next_id ) { return bpf_obj_get_next_id ( start_id , next_id , BPF_MAP_GET_NEXT_ID ) ; }",
    "resources/libbpf/src/bpf.c@bpf_btf_get_next_id": "int bpf_btf_get_next_id ( __u32 start_id , __u32 * next_id ) { return bpf_obj_get_next_id ( start_id , next_id , BPF_BTF_GET_NEXT_ID ) ; }",
    "resources/libbpf/src/bpf.c@bpf_link_get_next_id": "int bpf_link_get_next_id ( __u32 start_id , __u32 * next_id ) { return bpf_obj_get_next_id ( start_id , next_id , BPF_LINK_GET_NEXT_ID ) ; }",
    "resources/libbpf/src/bpf.c@bpf_prog_get_fd_by_id_opts": "int bpf_prog_get_fd_by_id_opts ( __u32 id , const struct bpf_get_fd_by_id_opts * opts ) { const size_t attr_sz = offsetofend ( union bpf_attr , open_flags ) ; union bpf_attr attr ; int fd ; if ( ! OPTS_VALID ( opts , bpf_get_fd_by_id_opts ) ) return libbpf_err ( - EINVAL ) ; memset ( & attr , 0 , attr_sz ) ; attr . prog_id = id ; attr . open_flags = OPTS_GET ( opts , open_flags , 0 ) ; fd = sys_bpf_fd ( BPF_PROG_GET_FD_BY_ID , & attr , attr_sz ) ; return libbpf_err_errno ( fd ) ; }",
    "resources/libbpf/src/bpf.c@bpf_prog_get_fd_by_id": "int bpf_prog_get_fd_by_id ( __u32 id ) { return bpf_prog_get_fd_by_id_opts ( id , NULL ) ; }",
    "resources/libbpf/src/bpf.c@bpf_map_get_fd_by_id_opts": "int bpf_map_get_fd_by_id_opts ( __u32 id , const struct bpf_get_fd_by_id_opts * opts ) { const size_t attr_sz = offsetofend ( union bpf_attr , open_flags ) ; union bpf_attr attr ; int fd ; if ( ! OPTS_VALID ( opts , bpf_get_fd_by_id_opts ) ) return libbpf_err ( - EINVAL ) ; memset ( & attr , 0 , attr_sz ) ; attr . map_id = id ; attr . open_flags = OPTS_GET ( opts , open_flags , 0 ) ; fd = sys_bpf_fd ( BPF_MAP_GET_FD_BY_ID , & attr , attr_sz ) ; return libbpf_err_errno ( fd ) ; }",
    "resources/libbpf/src/bpf.c@bpf_map_get_fd_by_id": "int bpf_map_get_fd_by_id ( __u32 id ) { return bpf_map_get_fd_by_id_opts ( id , NULL ) ; }",
    "resources/libbpf/src/bpf.c@bpf_btf_get_fd_by_id_opts": "int bpf_btf_get_fd_by_id_opts ( __u32 id , const struct bpf_get_fd_by_id_opts * opts ) { const size_t attr_sz = offsetofend ( union bpf_attr , open_flags ) ; union bpf_attr attr ; int fd ; if ( ! OPTS_VALID ( opts , bpf_get_fd_by_id_opts ) ) return libbpf_err ( - EINVAL ) ; memset ( & attr , 0 , attr_sz ) ; attr . btf_id = id ; attr . open_flags = OPTS_GET ( opts , open_flags , 0 ) ; fd = sys_bpf_fd ( BPF_BTF_GET_FD_BY_ID , & attr , attr_sz ) ; return libbpf_err_errno ( fd ) ; }",
    "resources/libbpf/src/bpf.c@bpf_btf_get_fd_by_id": "int bpf_btf_get_fd_by_id ( __u32 id ) { return bpf_btf_get_fd_by_id_opts ( id , NULL ) ; }",
    "resources/libbpf/src/bpf.c@bpf_link_get_fd_by_id_opts": "int bpf_link_get_fd_by_id_opts ( __u32 id , const struct bpf_get_fd_by_id_opts * opts ) { const size_t attr_sz = offsetofend ( union bpf_attr , open_flags ) ; union bpf_attr attr ; int fd ; if ( ! OPTS_VALID ( opts , bpf_get_fd_by_id_opts ) ) return libbpf_err ( - EINVAL ) ; memset ( & attr , 0 , attr_sz ) ; attr . link_id = id ; attr . open_flags = OPTS_GET ( opts , open_flags , 0 ) ; fd = sys_bpf_fd ( BPF_LINK_GET_FD_BY_ID , & attr , attr_sz ) ; return libbpf_err_errno ( fd ) ; }",
    "resources/libbpf/src/bpf.c@bpf_link_get_fd_by_id": "int bpf_link_get_fd_by_id ( __u32 id ) { return bpf_link_get_fd_by_id_opts ( id , NULL ) ; }",
    "resources/libbpf/src/bpf.c@bpf_obj_get_info_by_fd": "int bpf_obj_get_info_by_fd ( int bpf_fd , void * info , __u32 * info_len ) { const size_t attr_sz = offsetofend ( union bpf_attr , info ) ; union bpf_attr attr ; int err ; memset ( & attr , 0 , attr_sz ) ; attr . info . bpf_fd = bpf_fd ; attr . info . info_len = * info_len ; attr . info . info = ptr_to_u64 ( info ) ; err = sys_bpf ( BPF_OBJ_GET_INFO_BY_FD , & attr , attr_sz ) ; if ( ! err ) * info_len = attr . info . info_len ; return libbpf_err_errno ( err ) ; }",
    "resources/libbpf/src/bpf.c@bpf_prog_get_info_by_fd": "int bpf_prog_get_info_by_fd ( int prog_fd , struct bpf_prog_info * info , __u32 * info_len ) { return bpf_obj_get_info_by_fd ( prog_fd , info , info_len ) ; }",
    "resources/libbpf/src/bpf.c@bpf_map_get_info_by_fd": "int bpf_map_get_info_by_fd ( int map_fd , struct bpf_map_info * info , __u32 * info_len ) { return bpf_obj_get_info_by_fd ( map_fd , info , info_len ) ; }",
    "resources/libbpf/src/bpf.c@bpf_btf_get_info_by_fd": "int bpf_btf_get_info_by_fd ( int btf_fd , struct bpf_btf_info * info , __u32 * info_len ) { return bpf_obj_get_info_by_fd ( btf_fd , info , info_len ) ; }",
    "resources/libbpf/src/bpf.c@bpf_link_get_info_by_fd": "int bpf_link_get_info_by_fd ( int link_fd , struct bpf_link_info * info , __u32 * info_len ) { return bpf_obj_get_info_by_fd ( link_fd , info , info_len ) ; }",
    "resources/libbpf/src/bpf.c@bpf_raw_tracepoint_open_opts": "int bpf_raw_tracepoint_open_opts ( int prog_fd , struct bpf_raw_tp_opts * opts ) { const size_t attr_sz = offsetofend ( union bpf_attr , raw_tracepoint ) ; union bpf_attr attr ; int fd ; if ( ! OPTS_VALID ( opts , bpf_raw_tp_opts ) ) return libbpf_err ( - EINVAL ) ; memset ( & attr , 0 , attr_sz ) ; attr . raw_tracepoint . prog_fd = prog_fd ; attr . raw_tracepoint . name = ptr_to_u64 ( OPTS_GET ( opts , tp_name , NULL ) ) ; attr . raw_tracepoint . cookie = OPTS_GET ( opts , cookie , 0 ) ; fd = sys_bpf_fd ( BPF_RAW_TRACEPOINT_OPEN , & attr , attr_sz ) ; return libbpf_err_errno ( fd ) ; }",
    "resources/libbpf/src/bpf.c@bpf_raw_tracepoint_open": "int bpf_raw_tracepoint_open ( const char * name , int prog_fd ) { LIBBPF_OPTS ( bpf_raw_tp_opts , opts , . tp_name = name ) ; return bpf_raw_tracepoint_open_opts ( prog_fd , & opts ) ; }",
    "resources/libbpf/src/bpf.c@bpf_btf_load": "int bpf_btf_load ( const void * btf_data , size_t btf_size , struct bpf_btf_load_opts * opts ) { const size_t attr_sz = offsetofend ( union bpf_attr , btf_token_fd ) ; union bpf_attr attr ; char * log_buf ; size_t log_size ; __u32 log_level ; int fd ; bump_rlimit_memlock ( ) ; memset ( & attr , 0 , attr_sz ) ; if ( ! OPTS_VALID ( opts , bpf_btf_load_opts ) ) return libbpf_err ( - EINVAL ) ; log_buf = OPTS_GET ( opts , log_buf , NULL ) ; log_size = OPTS_GET ( opts , log_size , 0 ) ; log_level = OPTS_GET ( opts , log_level , 0 ) ; if ( log_size > UINT_MAX ) return libbpf_err ( - EINVAL ) ; if ( log_size && ! log_buf ) return libbpf_err ( - EINVAL ) ; attr . btf = ptr_to_u64 ( btf_data ) ; attr . btf_size = btf_size ; attr . btf_flags = OPTS_GET ( opts , btf_flags , 0 ) ; attr . btf_token_fd = OPTS_GET ( opts , token_fd , 0 ) ; /* log_level == 0 and log_buf != NULL means \"try loading without\n\t * log_buf, but retry with log_buf and log_level=1 on error\", which is\n\t * consistent across low-level and high-level BTF and program loading\n\t * APIs within libbpf and provides a sensible behavior in practice\n\t */ if ( log_level ) { attr . btf_log_buf = ptr_to_u64 ( log_buf ) ; attr . btf_log_size = ( __u32 ) log_size ; attr . btf_log_level = log_level ; } fd = sys_bpf_fd ( BPF_BTF_LOAD , & attr , attr_sz ) ; if ( fd < 0 && log_buf && log_level == 0 ) { attr . btf_log_buf = ptr_to_u64 ( log_buf ) ; attr . btf_log_size = ( __u32 ) log_size ; attr . btf_log_level = 1 ; fd = sys_bpf_fd ( BPF_BTF_LOAD , & attr , attr_sz ) ; } OPTS_SET ( opts , log_true_size , attr . btf_log_true_size ) ; return libbpf_err_errno ( fd ) ; }",
    "resources/libbpf/src/bpf.c@bpf_task_fd_query": "int bpf_task_fd_query ( int pid , int fd , __u32 flags , char * buf , __u32 * buf_len , __u32 * prog_id , __u32 * fd_type , __u64 * probe_offset , __u64 * probe_addr ) { const size_t attr_sz = offsetofend ( union bpf_attr , task_fd_query ) ; union bpf_attr attr ; int err ; memset ( & attr , 0 , attr_sz ) ; attr . task_fd_query . pid = pid ; attr . task_fd_query . fd = fd ; attr . task_fd_query . flags = flags ; attr . task_fd_query . buf = ptr_to_u64 ( buf ) ; attr . task_fd_query . buf_len = * buf_len ; err = sys_bpf ( BPF_TASK_FD_QUERY , & attr , attr_sz ) ; * buf_len = attr . task_fd_query . buf_len ; * prog_id = attr . task_fd_query . prog_id ; * fd_type = attr . task_fd_query . fd_type ; * probe_offset = attr . task_fd_query . probe_offset ; * probe_addr = attr . task_fd_query . probe_addr ; return libbpf_err_errno ( err ) ; }",
    "resources/libbpf/src/bpf.c@bpf_enable_stats": "int bpf_enable_stats ( enum bpf_stats_type type ) { const size_t attr_sz = offsetofend ( union bpf_attr , enable_stats ) ; union bpf_attr attr ; int fd ; memset ( & attr , 0 , attr_sz ) ; attr . enable_stats . type = type ; fd = sys_bpf_fd ( BPF_ENABLE_STATS , & attr , attr_sz ) ; return libbpf_err_errno ( fd ) ; }",
    "resources/libbpf/src/bpf.c@bpf_prog_bind_map": "int bpf_prog_bind_map ( int prog_fd , int map_fd , const struct bpf_prog_bind_opts * opts ) { const size_t attr_sz = offsetofend ( union bpf_attr , prog_bind_map ) ; union bpf_attr attr ; int ret ; if ( ! OPTS_VALID ( opts , bpf_prog_bind_opts ) ) return libbpf_err ( - EINVAL ) ; memset ( & attr , 0 , attr_sz ) ; attr . prog_bind_map . prog_fd = prog_fd ; attr . prog_bind_map . map_fd = map_fd ; attr . prog_bind_map . flags = OPTS_GET ( opts , flags , 0 ) ; ret = sys_bpf ( BPF_PROG_BIND_MAP , & attr , attr_sz ) ; return libbpf_err_errno ( ret ) ; }",
    "resources/libbpf/src/bpf.c@bpf_token_create": "int bpf_token_create ( int bpffs_fd , struct bpf_token_create_opts * opts ) { const size_t attr_sz = offsetofend ( union bpf_attr , token_create ) ; union bpf_attr attr ; int fd ; if ( ! OPTS_VALID ( opts , bpf_token_create_opts ) ) return libbpf_err ( - EINVAL ) ; memset ( & attr , 0 , attr_sz ) ; attr . token_create . bpffs_fd = bpffs_fd ; attr . token_create . flags = OPTS_GET ( opts , flags , 0 ) ; fd = sys_bpf_fd ( BPF_TOKEN_CREATE , & attr , attr_sz ) ; return libbpf_err_errno ( fd ) ; }",
    "resources/libbpf/src/netlink.c@btf_kind": "static inline __u16 btf_kind ( const struct btf_type * t ) { return BTF_INFO_KIND ( t -> info ) ; }",
    "resources/libbpf/src/netlink.c@btf_vlen": "static inline __u16 btf_vlen ( const struct btf_type * t ) { return BTF_INFO_VLEN ( t -> info ) ; }",
    "resources/libbpf/src/netlink.c@btf_kflag": "static inline bool btf_kflag ( const struct btf_type * t ) { return BTF_INFO_KFLAG ( t -> info ) ; }",
    "resources/libbpf/src/netlink.c@btf_is_void": "static inline bool btf_is_void ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_UNKN ; }",
    "resources/libbpf/src/netlink.c@btf_is_int": "static inline bool btf_is_int ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_INT ; }",
    "resources/libbpf/src/netlink.c@btf_is_ptr": "static inline bool btf_is_ptr ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_PTR ; }",
    "resources/libbpf/src/netlink.c@btf_is_array": "static inline bool btf_is_array ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_ARRAY ; }",
    "resources/libbpf/src/netlink.c@btf_is_struct": "static inline bool btf_is_struct ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_STRUCT ; }",
    "resources/libbpf/src/netlink.c@btf_is_union": "static inline bool btf_is_union ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_UNION ; }",
    "resources/libbpf/src/netlink.c@btf_is_composite": "static inline bool btf_is_composite ( const struct btf_type * t ) { __u16 kind = btf_kind ( t ) ; return kind == BTF_KIND_STRUCT || kind == BTF_KIND_UNION ; }",
    "resources/libbpf/src/netlink.c@btf_is_enum": "static inline bool btf_is_enum ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_ENUM ; }",
    "resources/libbpf/src/netlink.c@btf_is_enum64": "static inline bool btf_is_enum64 ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_ENUM64 ; }",
    "resources/libbpf/src/netlink.c@btf_is_fwd": "static inline bool btf_is_fwd ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_FWD ; }",
    "resources/libbpf/src/netlink.c@btf_is_typedef": "static inline bool btf_is_typedef ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_TYPEDEF ; }",
    "resources/libbpf/src/netlink.c@btf_is_volatile": "static inline bool btf_is_volatile ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_VOLATILE ; }",
    "resources/libbpf/src/netlink.c@btf_is_const": "static inline bool btf_is_const ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_CONST ; }",
    "resources/libbpf/src/netlink.c@btf_is_restrict": "static inline bool btf_is_restrict ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_RESTRICT ; }",
    "resources/libbpf/src/netlink.c@btf_is_mod": "static inline bool btf_is_mod ( const struct btf_type * t ) { __u16 kind = btf_kind ( t ) ; return kind == BTF_KIND_VOLATILE || kind == BTF_KIND_CONST || kind == BTF_KIND_RESTRICT || kind == BTF_KIND_TYPE_TAG ; }",
    "resources/libbpf/src/netlink.c@btf_is_func": "static inline bool btf_is_func ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_FUNC ; }",
    "resources/libbpf/src/netlink.c@btf_is_func_proto": "static inline bool btf_is_func_proto ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_FUNC_PROTO ; }",
    "resources/libbpf/src/netlink.c@btf_is_var": "static inline bool btf_is_var ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_VAR ; }",
    "resources/libbpf/src/netlink.c@btf_is_datasec": "static inline bool btf_is_datasec ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_DATASEC ; }",
    "resources/libbpf/src/netlink.c@btf_is_float": "static inline bool btf_is_float ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_FLOAT ; }",
    "resources/libbpf/src/netlink.c@btf_is_decl_tag": "static inline bool btf_is_decl_tag ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_DECL_TAG ; }",
    "resources/libbpf/src/netlink.c@btf_is_type_tag": "static inline bool btf_is_type_tag ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_TYPE_TAG ; }",
    "resources/libbpf/src/netlink.c@btf_is_any_enum": "static inline bool btf_is_any_enum ( const struct btf_type * t ) { return btf_is_enum ( t ) || btf_is_enum64 ( t ) ; }",
    "resources/libbpf/src/netlink.c@btf_kind_core_compat": "static inline bool btf_kind_core_compat ( const struct btf_type * t1 , const struct btf_type * t2 ) { return btf_kind ( t1 ) == btf_kind ( t2 ) || ( btf_is_any_enum ( t1 ) && btf_is_any_enum ( t2 ) ) ; }",
    "resources/libbpf/src/netlink.c@btf_int_encoding": "static inline __u8 btf_int_encoding ( const struct btf_type * t ) { return BTF_INT_ENCODING ( * ( __u32 * ) ( t + 1 ) ) ; }",
    "resources/libbpf/src/netlink.c@btf_int_offset": "static inline __u8 btf_int_offset ( const struct btf_type * t ) { return BTF_INT_OFFSET ( * ( __u32 * ) ( t + 1 ) ) ; }",
    "resources/libbpf/src/netlink.c@btf_int_bits": "static inline __u8 btf_int_bits ( const struct btf_type * t ) { return BTF_INT_BITS ( * ( __u32 * ) ( t + 1 ) ) ; }",
    "resources/libbpf/src/netlink.c@btf_array": "static inline struct btf_array * btf_array ( const struct btf_type * t ) { return ( struct btf_array * ) ( t + 1 ) ; }",
    "resources/libbpf/src/netlink.c@btf_enum": "static inline struct btf_enum * btf_enum ( const struct btf_type * t ) { return ( struct btf_enum * ) ( t + 1 ) ; }",
    "resources/libbpf/src/netlink.c@btf_enum64": "static inline struct btf_enum64 * btf_enum64 ( const struct btf_type * t ) { return ( struct btf_enum64 * ) ( t + 1 ) ; }",
    "resources/libbpf/src/netlink.c@btf_enum64_value": "static inline __u64 btf_enum64_value ( const struct btf_enum64 * e ) { /* struct btf_enum64 is introduced in Linux 6.0, which is very\n\t * bleeding-edge. Here we are avoiding relying on struct btf_enum64\n\t * definition coming from kernel UAPI headers to support wider range\n\t * of system-wide kernel headers.\n\t *\n\t * Given this header can be also included from C++ applications, that\n\t * further restricts C tricks we can use (like using compatible\n\t * anonymous struct). So just treat struct btf_enum64 as\n\t * a three-element array of u32 and access second (lo32) and third\n\t * (hi32) elements directly.\n\t *\n\t * For reference, here is a struct btf_enum64 definition:\n\t *\n\t * const struct btf_enum64 {\n\t *\t__u32\tname_off;\n\t *\t__u32\tval_lo32;\n\t *\t__u32\tval_hi32;\n\t * };\n\t */ const __u32 * e64 = ( const __u32 * ) e ; return ( ( __u64 ) e64 [ 2 ] << 32 ) | e64 [ 1 ] ; }",
    "resources/libbpf/src/netlink.c@btf_members": "static inline struct btf_member * btf_members ( const struct btf_type * t ) { return ( struct btf_member * ) ( t + 1 ) ; }",
    "resources/libbpf/src/netlink.c@btf_member_bit_offset": "static inline __u32 btf_member_bit_offset ( const struct btf_type * t , __u32 member_idx ) { const struct btf_member * m = btf_members ( t ) + member_idx ; bool kflag = btf_kflag ( t ) ; return kflag ? BTF_MEMBER_BIT_OFFSET ( m -> offset ) : m -> offset ; }",
    "resources/libbpf/src/netlink.c@btf_member_bitfield_size": "static inline __u32 btf_member_bitfield_size ( const struct btf_type * t , __u32 member_idx ) { const struct btf_member * m = btf_members ( t ) + member_idx ; bool kflag = btf_kflag ( t ) ; return kflag ? BTF_MEMBER_BITFIELD_SIZE ( m -> offset ) : 0 ; }",
    "resources/libbpf/src/netlink.c@btf_params": "static inline struct btf_param * btf_params ( const struct btf_type * t ) { return ( struct btf_param * ) ( t + 1 ) ; }",
    "resources/libbpf/src/netlink.c@btf_var": "static inline struct btf_var * btf_var ( const struct btf_type * t ) { return ( struct btf_var * ) ( t + 1 ) ; }",
    "resources/libbpf/src/netlink.c@btf_var_secinfos": "static inline struct btf_var_secinfo * btf_var_secinfos ( const struct btf_type * t ) { return ( struct btf_var_secinfo * ) ( t + 1 ) ; }",
    "resources/libbpf/src/netlink.c@btf_decl_tag": "static inline struct btf_decl_tag * btf_decl_tag ( const struct btf_type * t ) { return ( struct btf_decl_tag * ) ( t + 1 ) ; }",
    "resources/libbpf/src/netlink.c@str_has_sfx": "static inline bool str_has_sfx ( const char * str , const char * sfx ) { size_t str_len = strlen ( str ) ; size_t sfx_len = strlen ( sfx ) ; if ( sfx_len > str_len ) return false ; return strcmp ( str + str_len - sfx_len , sfx ) == 0 ; }",
    "resources/libbpf/src/netlink.c@libbpf_reallocarray": "static inline void * libbpf_reallocarray ( void * ptr , size_t nmemb , size_t size ) { size_t total ; # if __has_builtin ( __builtin_mul_overflow ) if ( unlikely ( __builtin_mul_overflow ( nmemb , size , & total ) ) ) return NULL ; # else if ( size == 0 || nmemb > ULONG_MAX / size ) return NULL ; total = nmemb * size ; # endif return realloc ( ptr , total ) ; }",
    "resources/libbpf/src/netlink.c@libbpf_strlcpy": "static inline void libbpf_strlcpy ( char * dst , const char * src , size_t sz ) { size_t i ; if ( sz == 0 ) return ; sz -- ; for ( i = 0 ; i < sz && src [ i ] ; i ++ ) dst [ i ] = src [ i ] ; dst [ i ] = '\\0' ; }",
    "resources/libbpf/src/netlink.c@btf_func_linkage": "static inline enum btf_func_linkage btf_func_linkage ( const struct btf_type * t ) { return ( enum btf_func_linkage ) ( int ) btf_vlen ( t ) ; }",
    "resources/libbpf/src/netlink.c@btf_type_info": "static inline __u32 btf_type_info ( int kind , int vlen , int kflag ) { return ( kflag << 31 ) | ( kind << 24 ) | vlen ; }",
    "resources/libbpf/src/netlink.c@libbpf_is_mem_zeroed": "static inline bool libbpf_is_mem_zeroed ( const char * p , ssize_t len ) { while ( len > 0 ) { if ( * p ) return false ; p ++ ; len -- ; } return true ; }",
    "resources/libbpf/src/netlink.c@libbpf_validate_opts": "static inline bool libbpf_validate_opts ( const char * opts , size_t opts_sz , size_t user_sz , const char * type_name ) { if ( user_sz < sizeof ( size_t ) ) { pr_warn ( \"%s size (%zu) is too small\\n\" , type_name , user_sz ) ; return false ; } if ( ! libbpf_is_mem_zeroed ( opts + opts_sz , ( ssize_t ) user_sz - opts_sz ) ) { pr_warn ( \"%s has non-zero extra bytes\\n\" , type_name ) ; return false ; } return true ; }",
    "resources/libbpf/src/netlink.c@libbpf_err": "static inline int libbpf_err ( int ret ) { if ( ret < 0 ) errno = - ret ; return ret ; }",
    "resources/libbpf/src/netlink.c@libbpf_err_errno": "static inline int libbpf_err_errno ( int ret ) { /* errno is already assumed to be set on error */ return ret < 0 ? - errno : ret ; }",
    "resources/libbpf/src/netlink.c@libbpf_err_ptr": "static inline void * libbpf_err_ptr ( int err ) { /* set errno on error, this doesn't break anything */ errno = - err ; return NULL ; }",
    "resources/libbpf/src/netlink.c@libbpf_ptr": "static inline void * libbpf_ptr ( void * ret ) { /* set errno on error, this doesn't break anything */ if ( IS_ERR ( ret ) ) errno = - PTR_ERR ( ret ) ; return IS_ERR ( ret ) ? NULL : ret ; }",
    "resources/libbpf/src/netlink.c@str_is_empty": "static inline bool str_is_empty ( const char * s ) { return ! s || ! s [ 0 ] ; }",
    "resources/libbpf/src/netlink.c@is_ldimm64_insn": "static inline bool is_ldimm64_insn ( struct bpf_insn * insn ) { return insn -> code == ( BPF_LD | BPF_IMM | BPF_DW ) ; }",
    "resources/libbpf/src/netlink.c@dup_good_fd": "static inline int dup_good_fd ( int fd ) { if ( fd < 0 ) return fd ; return fcntl ( fd , F_DUPFD_CLOEXEC , 3 ) ; }",
    "resources/libbpf/src/netlink.c@ensure_good_fd": "static inline int ensure_good_fd ( int fd ) { int old_fd = fd , saved_errno ; if ( fd < 0 ) return fd ; if ( fd < 3 ) { fd = dup_good_fd ( fd ) ; saved_errno = errno ; close ( old_fd ) ; errno = saved_errno ; if ( fd < 0 ) { pr_warn ( \"failed to dup FD %d to FD > 2: %d\\n\" , old_fd , - saved_errno ) ; errno = saved_errno ; } } return fd ; }",
    "resources/libbpf/src/netlink.c@sys_dup2": "static inline int sys_dup2 ( int oldfd , int newfd ) { # ifdef __NR_dup2 return syscall ( __NR_dup2 , oldfd , newfd ) ; # else return syscall ( __NR_dup3 , oldfd , newfd , 0 ) ; # endif }",
    "resources/libbpf/src/netlink.c@reuse_fd": "static inline int reuse_fd ( int fixed_fd , int tmp_fd ) { int err ; err = sys_dup2 ( tmp_fd , fixed_fd ) ; err = err < 0 ? - errno : 0 ; close ( tmp_fd ) ; /* clean up temporary FD */ return err ; }",
    "resources/libbpf/src/netlink.c@is_pow_of_2": "static inline bool is_pow_of_2 ( size_t x ) { return x && ( x & ( x - 1 ) ) == 0 ; }",
    "resources/libbpf/src/netlink.c@libbpf_nla_data": "static inline void * libbpf_nla_data ( const struct nlattr * nla ) { return ( void * ) nla + NLA_HDRLEN ; }",
    "resources/libbpf/src/netlink.c@libbpf_nla_getattr_u8": "static inline uint8_t libbpf_nla_getattr_u8 ( const struct nlattr * nla ) { return * ( uint8_t * ) libbpf_nla_data ( nla ) ; }",
    "resources/libbpf/src/netlink.c@libbpf_nla_getattr_u16": "static inline uint16_t libbpf_nla_getattr_u16 ( const struct nlattr * nla ) { return * ( uint16_t * ) libbpf_nla_data ( nla ) ; }",
    "resources/libbpf/src/netlink.c@libbpf_nla_getattr_u32": "static inline uint32_t libbpf_nla_getattr_u32 ( const struct nlattr * nla ) { return * ( uint32_t * ) libbpf_nla_data ( nla ) ; }",
    "resources/libbpf/src/netlink.c@libbpf_nla_getattr_u64": "static inline uint64_t libbpf_nla_getattr_u64 ( const struct nlattr * nla ) { return * ( uint64_t * ) libbpf_nla_data ( nla ) ; }",
    "resources/libbpf/src/netlink.c@libbpf_nla_getattr_str": "static inline const char * libbpf_nla_getattr_str ( const struct nlattr * nla ) { return ( const char * ) libbpf_nla_data ( nla ) ; }",
    "resources/libbpf/src/netlink.c@libbpf_nla_len": "static inline int libbpf_nla_len ( const struct nlattr * nla ) { return nla -> nla_len - NLA_HDRLEN ; }",
    "resources/libbpf/src/netlink.c@nla_data": "static inline struct nlattr * nla_data ( struct nlattr * nla ) { return ( struct nlattr * ) ( ( void * ) nla + NLA_HDRLEN ) ; }",
    "resources/libbpf/src/netlink.c@req_tail": "static inline struct nlattr * req_tail ( struct libbpf_nla_req * req ) { return ( struct nlattr * ) ( ( void * ) req + NLMSG_ALIGN ( req -> nh . nlmsg_len ) ) ; }",
    "resources/libbpf/src/netlink.c@nlattr_add": "static inline int nlattr_add ( struct libbpf_nla_req * req , int type , const void * data , int len ) { struct nlattr * nla ; if ( NLMSG_ALIGN ( req -> nh . nlmsg_len ) + NLA_ALIGN ( NLA_HDRLEN + len ) > sizeof ( * req ) ) return - EMSGSIZE ; if ( ! ! data != ! ! len ) return - EINVAL ; nla = req_tail ( req ) ; nla -> nla_type = type ; nla -> nla_len = NLA_HDRLEN + len ; if ( data ) memcpy ( nla_data ( nla ) , data , len ) ; req -> nh . nlmsg_len = NLMSG_ALIGN ( req -> nh . nlmsg_len ) + NLA_ALIGN ( nla -> nla_len ) ; return 0 ; }",
    "resources/libbpf/src/netlink.c@nlattr_begin_nested": "static inline struct nlattr * nlattr_begin_nested ( struct libbpf_nla_req * req , int type ) { struct nlattr * tail ; tail = req_tail ( req ) ; if ( nlattr_add ( req , type | NLA_F_NESTED , NULL , 0 ) ) return NULL ; return tail ; }",
    "resources/libbpf/src/netlink.c@nlattr_end_nested": "static inline void nlattr_end_nested ( struct libbpf_nla_req * req , struct nlattr * tail ) { tail -> nla_len = ( void * ) req_tail ( req ) - ( void * ) tail ; }",
    "resources/libbpf/src/netlink.c@libbpf_netlink_open": "static int libbpf_netlink_open ( __u32 * nl_pid , int proto ) { struct sockaddr_nl sa ; socklen_t addrlen ; int one = 1 , ret ; int sock ; memset ( & sa , 0 , sizeof ( sa ) ) ; sa . nl_family = AF_NETLINK ; sock = socket ( AF_NETLINK , SOCK_RAW | SOCK_CLOEXEC , proto ) ; if ( sock < 0 ) return - errno ; if ( setsockopt ( sock , SOL_NETLINK , NETLINK_EXT_ACK , & one , sizeof ( one ) ) < 0 ) { pr_warn ( \"Netlink error reporting not supported\\n\" ) ; } if ( bind ( sock , ( struct sockaddr * ) & sa , sizeof ( sa ) ) < 0 ) { ret = - errno ; goto cleanup ; } addrlen = sizeof ( sa ) ; if ( getsockname ( sock , ( struct sockaddr * ) & sa , & addrlen ) < 0 ) { ret = - errno ; goto cleanup ; } if ( addrlen != sizeof ( sa ) ) { ret = - LIBBPF_ERRNO__INTERNAL ; goto cleanup ; } * nl_pid = sa . nl_pid ; return sock ; cleanup : close ( sock ) ; return ret ; }",
    "resources/libbpf/src/netlink.c@libbpf_netlink_close": "static void libbpf_netlink_close ( int sock ) { close ( sock ) ; }",
    "resources/libbpf/src/netlink.c@netlink_recvmsg": "static int netlink_recvmsg ( int sock , struct msghdr * mhdr , int flags ) { int len ; do { len = recvmsg ( sock , mhdr , flags ) ; } while ( len < 0 && ( errno == EINTR || errno == EAGAIN ) ) ; if ( len < 0 ) return - errno ; return len ; }",
    "resources/libbpf/src/netlink.c@alloc_iov": "static int alloc_iov ( struct iovec * iov , int len ) { void * nbuf ; nbuf = realloc ( iov -> iov_base , len ) ; if ( ! nbuf ) return - ENOMEM ; iov -> iov_base = nbuf ; iov -> iov_len = len ; return 0 ; }",
    "resources/libbpf/src/netlink.c@libbpf_netlink_recv": "static int libbpf_netlink_recv ( int sock , __u32 nl_pid , int seq , __dump_nlmsg_t _fn , libbpf_dump_nlmsg_t fn , void * cookie ) { struct iovec iov = { } ; struct msghdr mhdr = { . msg_iov = & iov , . msg_iovlen = 1 , } ; bool multipart = true ; struct nlmsgerr * err ; struct nlmsghdr * nh ; int len , ret ; ret = alloc_iov ( & iov , 4096 ) ; if ( ret ) goto done ; while ( multipart ) { start : multipart = false ; len = netlink_recvmsg ( sock , & mhdr , MSG_PEEK | MSG_TRUNC ) ; if ( len < 0 ) { ret = len ; goto done ; } if ( len > iov . iov_len ) { ret = alloc_iov ( & iov , len ) ; if ( ret ) goto done ; } len = netlink_recvmsg ( sock , & mhdr , 0 ) ; if ( len < 0 ) { ret = len ; goto done ; } if ( len == 0 ) break ; for ( nh = ( struct nlmsghdr * ) iov . iov_base ; NLMSG_OK ( nh , len ) ; nh = NLMSG_NEXT ( nh , len ) ) { if ( nh -> nlmsg_pid != nl_pid ) { ret = - LIBBPF_ERRNO__WRNGPID ; goto done ; } if ( nh -> nlmsg_seq != seq ) { ret = - LIBBPF_ERRNO__INVSEQ ; goto done ; } if ( nh -> nlmsg_flags & NLM_F_MULTI ) multipart = true ; switch ( nh -> nlmsg_type ) { case NLMSG_ERROR : err = ( struct nlmsgerr * ) NLMSG_DATA ( nh ) ; if ( ! err -> error ) continue ; ret = err -> error ; libbpf_nla_dump_errormsg ( nh ) ; goto done ; case NLMSG_DONE : ret = 0 ; goto done ; default : break ; } if ( _fn ) { ret = _fn ( nh , fn , cookie ) ; switch ( ret ) { case NL_CONT : break ; case NL_NEXT : goto start ; case NL_DONE : ret = 0 ; goto done ; default : goto done ; } } } } ret = 0 ; done : free ( iov . iov_base ) ; return ret ; }",
    "resources/libbpf/src/netlink.c@libbpf_netlink_send_recv": "static int libbpf_netlink_send_recv ( struct libbpf_nla_req * req , int proto , __dump_nlmsg_t parse_msg , libbpf_dump_nlmsg_t parse_attr , void * cookie ) { __u32 nl_pid = 0 ; int sock , ret ; sock = libbpf_netlink_open ( & nl_pid , proto ) ; if ( sock < 0 ) return sock ; req -> nh . nlmsg_pid = 0 ; req -> nh . nlmsg_seq = time ( NULL ) ; if ( send ( sock , req , req -> nh . nlmsg_len , 0 ) < 0 ) { ret = - errno ; goto out ; } ret = libbpf_netlink_recv ( sock , nl_pid , req -> nh . nlmsg_seq , parse_msg , parse_attr , cookie ) ; out : libbpf_netlink_close ( sock ) ; return ret ; }",
    "resources/libbpf/src/netlink.c@parse_genl_family_id": "static int parse_genl_family_id ( struct nlmsghdr * nh , libbpf_dump_nlmsg_t fn , void * cookie ) { struct genlmsghdr * gnl = NLMSG_DATA ( nh ) ; struct nlattr * na = ( struct nlattr * ) ( ( void * ) gnl + GENL_HDRLEN ) ; struct nlattr * tb [ CTRL_ATTR_FAMILY_ID + 1 ] ; __u16 * id = cookie ; libbpf_nla_parse ( tb , CTRL_ATTR_FAMILY_ID , na , NLMSG_PAYLOAD ( nh , sizeof ( * gnl ) ) , NULL ) ; if ( ! tb [ CTRL_ATTR_FAMILY_ID ] ) return NL_CONT ; * id = libbpf_nla_getattr_u16 ( tb [ CTRL_ATTR_FAMILY_ID ] ) ; return NL_DONE ; }",
    "resources/libbpf/src/netlink.c@libbpf_netlink_resolve_genl_family_id": "static int libbpf_netlink_resolve_genl_family_id ( const char * name , __u16 len , __u16 * id ) { struct libbpf_nla_req req = { . nh . nlmsg_len = NLMSG_LENGTH ( GENL_HDRLEN ) , . nh . nlmsg_type = GENL_ID_CTRL , . nh . nlmsg_flags = NLM_F_REQUEST , . gnl . cmd = CTRL_CMD_GETFAMILY , . gnl . version = 2 , } ; int err ; err = nlattr_add ( & req , CTRL_ATTR_FAMILY_NAME , name , len ) ; if ( err < 0 ) return err ; return libbpf_netlink_send_recv ( & req , NETLINK_GENERIC , parse_genl_family_id , NULL , id ) ; }",
    "resources/libbpf/src/netlink.c@__bpf_set_link_xdp_fd_replace": "static int __bpf_set_link_xdp_fd_replace ( int ifindex , int fd , int old_fd , __u32 flags ) { struct nlattr * nla ; int ret ; struct libbpf_nla_req req ; memset ( & req , 0 , sizeof ( req ) ) ; req . nh . nlmsg_len = NLMSG_LENGTH ( sizeof ( struct ifinfomsg ) ) ; req . nh . nlmsg_flags = NLM_F_REQUEST | NLM_F_ACK ; req . nh . nlmsg_type = RTM_SETLINK ; req . ifinfo . ifi_family = AF_UNSPEC ; req . ifinfo . ifi_index = ifindex ; nla = nlattr_begin_nested ( & req , IFLA_XDP ) ; if ( ! nla ) return - EMSGSIZE ; ret = nlattr_add ( & req , IFLA_XDP_FD , & fd , sizeof ( fd ) ) ; if ( ret < 0 ) return ret ; if ( flags ) { ret = nlattr_add ( & req , IFLA_XDP_FLAGS , & flags , sizeof ( flags ) ) ; if ( ret < 0 ) return ret ; } if ( flags & XDP_FLAGS_REPLACE ) { ret = nlattr_add ( & req , IFLA_XDP_EXPECTED_FD , & old_fd , sizeof ( old_fd ) ) ; if ( ret < 0 ) return ret ; } nlattr_end_nested ( & req , nla ) ; return libbpf_netlink_send_recv ( & req , NETLINK_ROUTE , NULL , NULL , NULL ) ; }",
    "resources/libbpf/src/netlink.c@bpf_xdp_attach": "int bpf_xdp_attach ( int ifindex , int prog_fd , __u32 flags , const struct bpf_xdp_attach_opts * opts ) { int old_prog_fd , err ; if ( ! OPTS_VALID ( opts , bpf_xdp_attach_opts ) ) return libbpf_err ( - EINVAL ) ; old_prog_fd = OPTS_GET ( opts , old_prog_fd , 0 ) ; if ( old_prog_fd ) flags |= XDP_FLAGS_REPLACE ; else old_prog_fd = - 1 ; err = __bpf_set_link_xdp_fd_replace ( ifindex , prog_fd , old_prog_fd , flags ) ; return libbpf_err ( err ) ; }",
    "resources/libbpf/src/netlink.c@bpf_xdp_detach": "int bpf_xdp_detach ( int ifindex , __u32 flags , const struct bpf_xdp_attach_opts * opts ) { return bpf_xdp_attach ( ifindex , - 1 , flags , opts ) ; }",
    "resources/libbpf/src/netlink.c@__dump_link_nlmsg": "static int __dump_link_nlmsg ( struct nlmsghdr * nlh , libbpf_dump_nlmsg_t dump_link_nlmsg , void * cookie ) { struct nlattr * tb [ IFLA_MAX + 1 ] , * attr ; struct ifinfomsg * ifi = NLMSG_DATA ( nlh ) ; int len ; len = nlh -> nlmsg_len - NLMSG_LENGTH ( sizeof ( * ifi ) ) ; attr = ( struct nlattr * ) ( ( void * ) ifi + NLMSG_ALIGN ( sizeof ( * ifi ) ) ) ; if ( libbpf_nla_parse ( tb , IFLA_MAX , attr , len , NULL ) != 0 ) return - LIBBPF_ERRNO__NLPARSE ; return dump_link_nlmsg ( cookie , ifi , tb ) ; }",
    "resources/libbpf/src/netlink.c@get_xdp_info": "static int get_xdp_info ( void * cookie , void * msg , struct nlattr * * tb ) { struct nlattr * xdp_tb [ IFLA_XDP_MAX + 1 ] ; struct xdp_id_md * xdp_id = cookie ; struct ifinfomsg * ifinfo = msg ; int ret ; if ( xdp_id -> ifindex && xdp_id -> ifindex != ifinfo -> ifi_index ) return 0 ; if ( ! tb [ IFLA_XDP ] ) return 0 ; ret = libbpf_nla_parse_nested ( xdp_tb , IFLA_XDP_MAX , tb [ IFLA_XDP ] , NULL ) ; if ( ret ) return ret ; if ( ! xdp_tb [ IFLA_XDP_ATTACHED ] ) return 0 ; xdp_id -> info . attach_mode = libbpf_nla_getattr_u8 ( xdp_tb [ IFLA_XDP_ATTACHED ] ) ; if ( xdp_id -> info . attach_mode == XDP_ATTACHED_NONE ) return 0 ; if ( xdp_tb [ IFLA_XDP_PROG_ID ] ) xdp_id -> info . prog_id = libbpf_nla_getattr_u32 ( xdp_tb [ IFLA_XDP_PROG_ID ] ) ; if ( xdp_tb [ IFLA_XDP_SKB_PROG_ID ] ) xdp_id -> info . skb_prog_id = libbpf_nla_getattr_u32 ( xdp_tb [ IFLA_XDP_SKB_PROG_ID ] ) ; if ( xdp_tb [ IFLA_XDP_DRV_PROG_ID ] ) xdp_id -> info . drv_prog_id = libbpf_nla_getattr_u32 ( xdp_tb [ IFLA_XDP_DRV_PROG_ID ] ) ; if ( xdp_tb [ IFLA_XDP_HW_PROG_ID ] ) xdp_id -> info . hw_prog_id = libbpf_nla_getattr_u32 ( xdp_tb [ IFLA_XDP_HW_PROG_ID ] ) ; return 0 ; }",
    "resources/libbpf/src/netlink.c@parse_xdp_features": "static int parse_xdp_features ( struct nlmsghdr * nh , libbpf_dump_nlmsg_t fn , void * cookie ) { struct genlmsghdr * gnl = NLMSG_DATA ( nh ) ; struct nlattr * na = ( struct nlattr * ) ( ( void * ) gnl + GENL_HDRLEN ) ; struct nlattr * tb [ NETDEV_CMD_MAX + 1 ] ; struct xdp_features_md * md = cookie ; __u32 ifindex ; libbpf_nla_parse ( tb , NETDEV_CMD_MAX , na , NLMSG_PAYLOAD ( nh , sizeof ( * gnl ) ) , NULL ) ; if ( ! tb [ NETDEV_A_DEV_IFINDEX ] || ! tb [ NETDEV_A_DEV_XDP_FEATURES ] ) return NL_CONT ; ifindex = libbpf_nla_getattr_u32 ( tb [ NETDEV_A_DEV_IFINDEX ] ) ; if ( ifindex != md -> ifindex ) return NL_CONT ; md -> flags = libbpf_nla_getattr_u64 ( tb [ NETDEV_A_DEV_XDP_FEATURES ] ) ; if ( tb [ NETDEV_A_DEV_XDP_ZC_MAX_SEGS ] ) md -> xdp_zc_max_segs = libbpf_nla_getattr_u32 ( tb [ NETDEV_A_DEV_XDP_ZC_MAX_SEGS ] ) ; return NL_DONE ; }",
    "resources/libbpf/src/netlink.c@bpf_xdp_query": "int bpf_xdp_query ( int ifindex , int xdp_flags , struct bpf_xdp_query_opts * opts ) { struct libbpf_nla_req req = { . nh . nlmsg_len = NLMSG_LENGTH ( sizeof ( struct ifinfomsg ) ) , . nh . nlmsg_type = RTM_GETLINK , . nh . nlmsg_flags = NLM_F_DUMP | NLM_F_REQUEST , . ifinfo . ifi_family = AF_PACKET , } ; struct xdp_id_md xdp_id = { } ; struct xdp_features_md md = { . ifindex = ifindex , } ; __u16 id ; int err ; if ( ! OPTS_VALID ( opts , bpf_xdp_query_opts ) ) return libbpf_err ( - EINVAL ) ; if ( xdp_flags & ~ XDP_FLAGS_MASK ) return libbpf_err ( - EINVAL ) ; /* Check whether the single {HW,DRV,SKB} mode is set */ xdp_flags &= XDP_FLAGS_SKB_MODE | XDP_FLAGS_DRV_MODE | XDP_FLAGS_HW_MODE ; if ( xdp_flags & ( xdp_flags - 1 ) ) return libbpf_err ( - EINVAL ) ; xdp_id . ifindex = ifindex ; xdp_id . flags = xdp_flags ; err = libbpf_netlink_send_recv ( & req , NETLINK_ROUTE , __dump_link_nlmsg , get_xdp_info , & xdp_id ) ; if ( err ) return libbpf_err ( err ) ; OPTS_SET ( opts , prog_id , xdp_id . info . prog_id ) ; OPTS_SET ( opts , drv_prog_id , xdp_id . info . drv_prog_id ) ; OPTS_SET ( opts , hw_prog_id , xdp_id . info . hw_prog_id ) ; OPTS_SET ( opts , skb_prog_id , xdp_id . info . skb_prog_id ) ; OPTS_SET ( opts , attach_mode , xdp_id . info . attach_mode ) ; if ( ! OPTS_HAS ( opts , feature_flags ) ) return 0 ; err = libbpf_netlink_resolve_genl_family_id ( \"netdev\" , sizeof ( \"netdev\" ) , & id ) ; if ( err < 0 ) { if ( err == - ENOENT ) { opts -> feature_flags = 0 ; goto skip_feature_flags ; } return libbpf_err ( err ) ; } memset ( & req , 0 , sizeof ( req ) ) ; req . nh . nlmsg_len = NLMSG_LENGTH ( GENL_HDRLEN ) ; req . nh . nlmsg_flags = NLM_F_REQUEST ; req . nh . nlmsg_type = id ; req . gnl . cmd = NETDEV_CMD_DEV_GET ; req . gnl . version = 2 ; err = nlattr_add ( & req , NETDEV_A_DEV_IFINDEX , & ifindex , sizeof ( ifindex ) ) ; if ( err < 0 ) return libbpf_err ( err ) ; err = libbpf_netlink_send_recv ( & req , NETLINK_GENERIC , parse_xdp_features , NULL , & md ) ; if ( err ) return libbpf_err ( err ) ; OPTS_SET ( opts , feature_flags , md . flags ) ; OPTS_SET ( opts , xdp_zc_max_segs , md . xdp_zc_max_segs ) ; skip_feature_flags : return 0 ; }",
    "resources/libbpf/src/netlink.c@bpf_xdp_query_id": "int bpf_xdp_query_id ( int ifindex , int flags , __u32 * prog_id ) { LIBBPF_OPTS ( bpf_xdp_query_opts , opts ) ; int ret ; ret = bpf_xdp_query ( ifindex , flags , & opts ) ; if ( ret ) return libbpf_err ( ret ) ; flags &= XDP_FLAGS_MODES ; if ( opts . attach_mode != XDP_ATTACHED_MULTI && ! flags ) * prog_id = opts . prog_id ; else if ( flags & XDP_FLAGS_DRV_MODE ) * prog_id = opts . drv_prog_id ; else if ( flags & XDP_FLAGS_HW_MODE ) * prog_id = opts . hw_prog_id ; else if ( flags & XDP_FLAGS_SKB_MODE ) * prog_id = opts . skb_prog_id ; else * prog_id = 0 ; return 0 ; }",
    "resources/libbpf/src/netlink.c@clsact_config": "static int clsact_config ( struct libbpf_nla_req * req ) { req -> tc . tcm_parent = TC_H_CLSACT ; req -> tc . tcm_handle = TC_H_MAKE ( TC_H_CLSACT , 0 ) ; return nlattr_add ( req , TCA_KIND , \"clsact\" , sizeof ( \"clsact\" ) ) ; }",
    "resources/libbpf/src/netlink.c@attach_point_to_config": "static int attach_point_to_config ( struct bpf_tc_hook * hook , qdisc_config_t * config ) { switch ( OPTS_GET ( hook , attach_point , 0 ) ) { case BPF_TC_INGRESS : case BPF_TC_EGRESS : case BPF_TC_INGRESS | BPF_TC_EGRESS : if ( OPTS_GET ( hook , parent , 0 ) ) return - EINVAL ; * config = & clsact_config ; return 0 ; case BPF_TC_CUSTOM : return - EOPNOTSUPP ; default : return - EINVAL ; } }",
    "resources/libbpf/src/netlink.c@tc_get_tcm_parent": "static int tc_get_tcm_parent ( enum bpf_tc_attach_point attach_point , __u32 * parent ) { switch ( attach_point ) { case BPF_TC_INGRESS : case BPF_TC_EGRESS : if ( * parent ) return - EINVAL ; * parent = TC_H_MAKE ( TC_H_CLSACT , attach_point == BPF_TC_INGRESS ? TC_H_MIN_INGRESS : TC_H_MIN_EGRESS ) ; break ; case BPF_TC_CUSTOM : if ( ! * parent ) return - EINVAL ; break ; default : return - EINVAL ; } return 0 ; }",
    "resources/libbpf/src/netlink.c@tc_qdisc_modify": "static int tc_qdisc_modify ( struct bpf_tc_hook * hook , int cmd , int flags ) { qdisc_config_t config ; int ret ; struct libbpf_nla_req req ; ret = attach_point_to_config ( hook , & config ) ; if ( ret < 0 ) return ret ; memset ( & req , 0 , sizeof ( req ) ) ; req . nh . nlmsg_len = NLMSG_LENGTH ( sizeof ( struct tcmsg ) ) ; req . nh . nlmsg_flags = NLM_F_REQUEST | NLM_F_ACK | flags ; req . nh . nlmsg_type = cmd ; req . tc . tcm_family = AF_UNSPEC ; req . tc . tcm_ifindex = OPTS_GET ( hook , ifindex , 0 ) ; ret = config ( & req ) ; if ( ret < 0 ) return ret ; return libbpf_netlink_send_recv ( & req , NETLINK_ROUTE , NULL , NULL , NULL ) ; }",
    "resources/libbpf/src/netlink.c@tc_qdisc_create_excl": "static int tc_qdisc_create_excl ( struct bpf_tc_hook * hook ) { return tc_qdisc_modify ( hook , RTM_NEWQDISC , NLM_F_CREATE | NLM_F_EXCL ) ; }",
    "resources/libbpf/src/netlink.c@tc_qdisc_delete": "static int tc_qdisc_delete ( struct bpf_tc_hook * hook ) { return tc_qdisc_modify ( hook , RTM_DELQDISC , 0 ) ; }",
    "resources/libbpf/src/netlink.c@bpf_tc_hook_create": "int bpf_tc_hook_create ( struct bpf_tc_hook * hook ) { int ret ; if ( ! hook || ! OPTS_VALID ( hook , bpf_tc_hook ) || OPTS_GET ( hook , ifindex , 0 ) <= 0 ) return libbpf_err ( - EINVAL ) ; ret = tc_qdisc_create_excl ( hook ) ; return libbpf_err ( ret ) ; }",
    "resources/libbpf/src/netlink.c@bpf_tc_hook_destroy": "int bpf_tc_hook_destroy ( struct bpf_tc_hook * hook ) { if ( ! hook || ! OPTS_VALID ( hook , bpf_tc_hook ) || OPTS_GET ( hook , ifindex , 0 ) <= 0 ) return libbpf_err ( - EINVAL ) ; switch ( OPTS_GET ( hook , attach_point , 0 ) ) { case BPF_TC_INGRESS : case BPF_TC_EGRESS : return libbpf_err ( __bpf_tc_detach ( hook , NULL , true ) ) ; case BPF_TC_INGRESS | BPF_TC_EGRESS : return libbpf_err ( tc_qdisc_delete ( hook ) ) ; case BPF_TC_CUSTOM : return libbpf_err ( - EOPNOTSUPP ) ; default : return libbpf_err ( - EINVAL ) ; } }",
    "resources/libbpf/src/netlink.c@__get_tc_info": "static int __get_tc_info ( void * cookie , struct tcmsg * tc , struct nlattr * * tb , bool unicast ) { struct nlattr * tbb [ TCA_BPF_MAX + 1 ] ; struct bpf_cb_ctx * info = cookie ; if ( ! info || ! info -> opts ) return - EINVAL ; if ( unicast && info -> processed ) return - EINVAL ; if ( ! tb [ TCA_OPTIONS ] ) return NL_CONT ; libbpf_nla_parse_nested ( tbb , TCA_BPF_MAX , tb [ TCA_OPTIONS ] , NULL ) ; if ( ! tbb [ TCA_BPF_ID ] ) return - EINVAL ; OPTS_SET ( info -> opts , prog_id , libbpf_nla_getattr_u32 ( tbb [ TCA_BPF_ID ] ) ) ; OPTS_SET ( info -> opts , handle , tc -> tcm_handle ) ; OPTS_SET ( info -> opts , priority , TC_H_MAJ ( tc -> tcm_info ) >> 16 ) ; info -> processed = true ; return unicast ? NL_NEXT : NL_DONE ; }",
    "resources/libbpf/src/netlink.c@get_tc_info": "static int get_tc_info ( struct nlmsghdr * nh , libbpf_dump_nlmsg_t fn , void * cookie ) { struct tcmsg * tc = NLMSG_DATA ( nh ) ; struct nlattr * tb [ TCA_MAX + 1 ] ; libbpf_nla_parse ( tb , TCA_MAX , ( struct nlattr * ) ( ( void * ) tc + NLMSG_ALIGN ( sizeof ( * tc ) ) ) , NLMSG_PAYLOAD ( nh , sizeof ( * tc ) ) , NULL ) ; if ( ! tb [ TCA_KIND ] ) return NL_CONT ; return __get_tc_info ( cookie , tc , tb , nh -> nlmsg_flags & NLM_F_ECHO ) ; }",
    "resources/libbpf/src/netlink.c@tc_add_fd_and_name": "static int tc_add_fd_and_name ( struct libbpf_nla_req * req , int fd ) { struct bpf_prog_info info ; __u32 info_len = sizeof ( info ) ; char name [ 256 ] ; int len , ret ; memset ( & info , 0 , info_len ) ; ret = bpf_prog_get_info_by_fd ( fd , & info , & info_len ) ; if ( ret < 0 ) return ret ; ret = nlattr_add ( req , TCA_BPF_FD , & fd , sizeof ( fd ) ) ; if ( ret < 0 ) return ret ; len = snprintf ( name , sizeof ( name ) , \"%s:[%u]\" , info . name , info . id ) ; if ( len < 0 ) return - errno ; if ( len >= sizeof ( name ) ) return - ENAMETOOLONG ; return nlattr_add ( req , TCA_BPF_NAME , name , len + 1 ) ; }",
    "resources/libbpf/src/netlink.c@bpf_tc_attach": "int bpf_tc_attach ( const struct bpf_tc_hook * hook , struct bpf_tc_opts * opts ) { __u32 protocol , bpf_flags , handle , priority , parent , prog_id , flags ; int ret , ifindex , attach_point , prog_fd ; struct bpf_cb_ctx info = { } ; struct libbpf_nla_req req ; struct nlattr * nla ; if ( ! hook || ! opts || ! OPTS_VALID ( hook , bpf_tc_hook ) || ! OPTS_VALID ( opts , bpf_tc_opts ) ) return libbpf_err ( - EINVAL ) ; ifindex = OPTS_GET ( hook , ifindex , 0 ) ; parent = OPTS_GET ( hook , parent , 0 ) ; attach_point = OPTS_GET ( hook , attach_point , 0 ) ; handle = OPTS_GET ( opts , handle , 0 ) ; priority = OPTS_GET ( opts , priority , 0 ) ; prog_fd = OPTS_GET ( opts , prog_fd , 0 ) ; prog_id = OPTS_GET ( opts , prog_id , 0 ) ; flags = OPTS_GET ( opts , flags , 0 ) ; if ( ifindex <= 0 || ! prog_fd || prog_id ) return libbpf_err ( - EINVAL ) ; if ( priority > UINT16_MAX ) return libbpf_err ( - EINVAL ) ; if ( flags & ~ BPF_TC_F_REPLACE ) return libbpf_err ( - EINVAL ) ; flags = ( flags & BPF_TC_F_REPLACE ) ? NLM_F_REPLACE : NLM_F_EXCL ; protocol = ETH_P_ALL ; memset ( & req , 0 , sizeof ( req ) ) ; req . nh . nlmsg_len = NLMSG_LENGTH ( sizeof ( struct tcmsg ) ) ; req . nh . nlmsg_flags = NLM_F_REQUEST | NLM_F_ACK | NLM_F_CREATE | NLM_F_ECHO | flags ; req . nh . nlmsg_type = RTM_NEWTFILTER ; req . tc . tcm_family = AF_UNSPEC ; req . tc . tcm_ifindex = ifindex ; req . tc . tcm_handle = handle ; req . tc . tcm_info = TC_H_MAKE ( priority << 16 , htons ( protocol ) ) ; ret = tc_get_tcm_parent ( attach_point , & parent ) ; if ( ret < 0 ) return libbpf_err ( ret ) ; req . tc . tcm_parent = parent ; ret = nlattr_add ( & req , TCA_KIND , \"bpf\" , sizeof ( \"bpf\" ) ) ; if ( ret < 0 ) return libbpf_err ( ret ) ; nla = nlattr_begin_nested ( & req , TCA_OPTIONS ) ; if ( ! nla ) return libbpf_err ( - EMSGSIZE ) ; ret = tc_add_fd_and_name ( & req , prog_fd ) ; if ( ret < 0 ) return libbpf_err ( ret ) ; bpf_flags = TCA_BPF_FLAG_ACT_DIRECT ; ret = nlattr_add ( & req , TCA_BPF_FLAGS , & bpf_flags , sizeof ( bpf_flags ) ) ; if ( ret < 0 ) return libbpf_err ( ret ) ; nlattr_end_nested ( & req , nla ) ; info . opts = opts ; ret = libbpf_netlink_send_recv ( & req , NETLINK_ROUTE , get_tc_info , NULL , & info ) ; if ( ret < 0 ) return libbpf_err ( ret ) ; if ( ! info . processed ) return libbpf_err ( - ENOENT ) ; return ret ; }",
    "resources/libbpf/src/netlink.c@__bpf_tc_detach": "static int __bpf_tc_detach ( const struct bpf_tc_hook * hook , const struct bpf_tc_opts * opts , const bool flush ) { __u32 protocol = 0 , handle , priority , parent , prog_id , flags ; int ret , ifindex , attach_point , prog_fd ; struct libbpf_nla_req req ; if ( ! hook || ! OPTS_VALID ( hook , bpf_tc_hook ) || ! OPTS_VALID ( opts , bpf_tc_opts ) ) return - EINVAL ; ifindex = OPTS_GET ( hook , ifindex , 0 ) ; parent = OPTS_GET ( hook , parent , 0 ) ; attach_point = OPTS_GET ( hook , attach_point , 0 ) ; handle = OPTS_GET ( opts , handle , 0 ) ; priority = OPTS_GET ( opts , priority , 0 ) ; prog_fd = OPTS_GET ( opts , prog_fd , 0 ) ; prog_id = OPTS_GET ( opts , prog_id , 0 ) ; flags = OPTS_GET ( opts , flags , 0 ) ; if ( ifindex <= 0 || flags || prog_fd || prog_id ) return - EINVAL ; if ( priority > UINT16_MAX ) return - EINVAL ; if ( ! flush ) { if ( ! handle || ! priority ) return - EINVAL ; protocol = ETH_P_ALL ; } else { if ( handle || priority ) return - EINVAL ; } memset ( & req , 0 , sizeof ( req ) ) ; req . nh . nlmsg_len = NLMSG_LENGTH ( sizeof ( struct tcmsg ) ) ; req . nh . nlmsg_flags = NLM_F_REQUEST | NLM_F_ACK ; req . nh . nlmsg_type = RTM_DELTFILTER ; req . tc . tcm_family = AF_UNSPEC ; req . tc . tcm_ifindex = ifindex ; if ( ! flush ) { req . tc . tcm_handle = handle ; req . tc . tcm_info = TC_H_MAKE ( priority << 16 , htons ( protocol ) ) ; } ret = tc_get_tcm_parent ( attach_point , & parent ) ; if ( ret < 0 ) return ret ; req . tc . tcm_parent = parent ; if ( ! flush ) { ret = nlattr_add ( & req , TCA_KIND , \"bpf\" , sizeof ( \"bpf\" ) ) ; if ( ret < 0 ) return ret ; } return libbpf_netlink_send_recv ( & req , NETLINK_ROUTE , NULL , NULL , NULL ) ; }",
    "resources/libbpf/src/netlink.c@bpf_tc_detach": "int bpf_tc_detach ( const struct bpf_tc_hook * hook , const struct bpf_tc_opts * opts ) { int ret ; if ( ! opts ) return libbpf_err ( - EINVAL ) ; ret = __bpf_tc_detach ( hook , opts , false ) ; return libbpf_err ( ret ) ; }",
    "resources/libbpf/src/netlink.c@bpf_tc_query": "int bpf_tc_query ( const struct bpf_tc_hook * hook , struct bpf_tc_opts * opts ) { __u32 protocol , handle , priority , parent , prog_id , flags ; int ret , ifindex , attach_point , prog_fd ; struct bpf_cb_ctx info = { } ; struct libbpf_nla_req req ; if ( ! hook || ! opts || ! OPTS_VALID ( hook , bpf_tc_hook ) || ! OPTS_VALID ( opts , bpf_tc_opts ) ) return libbpf_err ( - EINVAL ) ; ifindex = OPTS_GET ( hook , ifindex , 0 ) ; parent = OPTS_GET ( hook , parent , 0 ) ; attach_point = OPTS_GET ( hook , attach_point , 0 ) ; handle = OPTS_GET ( opts , handle , 0 ) ; priority = OPTS_GET ( opts , priority , 0 ) ; prog_fd = OPTS_GET ( opts , prog_fd , 0 ) ; prog_id = OPTS_GET ( opts , prog_id , 0 ) ; flags = OPTS_GET ( opts , flags , 0 ) ; if ( ifindex <= 0 || flags || prog_fd || prog_id || ! handle || ! priority ) return libbpf_err ( - EINVAL ) ; if ( priority > UINT16_MAX ) return libbpf_err ( - EINVAL ) ; protocol = ETH_P_ALL ; memset ( & req , 0 , sizeof ( req ) ) ; req . nh . nlmsg_len = NLMSG_LENGTH ( sizeof ( struct tcmsg ) ) ; req . nh . nlmsg_flags = NLM_F_REQUEST ; req . nh . nlmsg_type = RTM_GETTFILTER ; req . tc . tcm_family = AF_UNSPEC ; req . tc . tcm_ifindex = ifindex ; req . tc . tcm_handle = handle ; req . tc . tcm_info = TC_H_MAKE ( priority << 16 , htons ( protocol ) ) ; ret = tc_get_tcm_parent ( attach_point , & parent ) ; if ( ret < 0 ) return libbpf_err ( ret ) ; req . tc . tcm_parent = parent ; ret = nlattr_add ( & req , TCA_KIND , \"bpf\" , sizeof ( \"bpf\" ) ) ; if ( ret < 0 ) return libbpf_err ( ret ) ; info . opts = opts ; ret = libbpf_netlink_send_recv ( & req , NETLINK_ROUTE , get_tc_info , NULL , & info ) ; if ( ret < 0 ) return libbpf_err ( ret ) ; if ( ! info . processed ) return libbpf_err ( - ENOENT ) ; return ret ; }",
    "resources/libbpf/src/elf.c@btf_kind": "static inline __u16 btf_kind ( const struct btf_type * t ) { return BTF_INFO_KIND ( t -> info ) ; }",
    "resources/libbpf/src/elf.c@btf_vlen": "static inline __u16 btf_vlen ( const struct btf_type * t ) { return BTF_INFO_VLEN ( t -> info ) ; }",
    "resources/libbpf/src/elf.c@btf_kflag": "static inline bool btf_kflag ( const struct btf_type * t ) { return BTF_INFO_KFLAG ( t -> info ) ; }",
    "resources/libbpf/src/elf.c@btf_is_void": "static inline bool btf_is_void ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_UNKN ; }",
    "resources/libbpf/src/elf.c@btf_is_int": "static inline bool btf_is_int ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_INT ; }",
    "resources/libbpf/src/elf.c@btf_is_ptr": "static inline bool btf_is_ptr ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_PTR ; }",
    "resources/libbpf/src/elf.c@btf_is_array": "static inline bool btf_is_array ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_ARRAY ; }",
    "resources/libbpf/src/elf.c@btf_is_struct": "static inline bool btf_is_struct ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_STRUCT ; }",
    "resources/libbpf/src/elf.c@btf_is_union": "static inline bool btf_is_union ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_UNION ; }",
    "resources/libbpf/src/elf.c@btf_is_composite": "static inline bool btf_is_composite ( const struct btf_type * t ) { __u16 kind = btf_kind ( t ) ; return kind == BTF_KIND_STRUCT || kind == BTF_KIND_UNION ; }",
    "resources/libbpf/src/elf.c@btf_is_enum": "static inline bool btf_is_enum ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_ENUM ; }",
    "resources/libbpf/src/elf.c@btf_is_enum64": "static inline bool btf_is_enum64 ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_ENUM64 ; }",
    "resources/libbpf/src/elf.c@btf_is_fwd": "static inline bool btf_is_fwd ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_FWD ; }",
    "resources/libbpf/src/elf.c@btf_is_typedef": "static inline bool btf_is_typedef ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_TYPEDEF ; }",
    "resources/libbpf/src/elf.c@btf_is_volatile": "static inline bool btf_is_volatile ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_VOLATILE ; }",
    "resources/libbpf/src/elf.c@btf_is_const": "static inline bool btf_is_const ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_CONST ; }",
    "resources/libbpf/src/elf.c@btf_is_restrict": "static inline bool btf_is_restrict ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_RESTRICT ; }",
    "resources/libbpf/src/elf.c@btf_is_mod": "static inline bool btf_is_mod ( const struct btf_type * t ) { __u16 kind = btf_kind ( t ) ; return kind == BTF_KIND_VOLATILE || kind == BTF_KIND_CONST || kind == BTF_KIND_RESTRICT || kind == BTF_KIND_TYPE_TAG ; }",
    "resources/libbpf/src/elf.c@btf_is_func": "static inline bool btf_is_func ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_FUNC ; }",
    "resources/libbpf/src/elf.c@btf_is_func_proto": "static inline bool btf_is_func_proto ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_FUNC_PROTO ; }",
    "resources/libbpf/src/elf.c@btf_is_var": "static inline bool btf_is_var ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_VAR ; }",
    "resources/libbpf/src/elf.c@btf_is_datasec": "static inline bool btf_is_datasec ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_DATASEC ; }",
    "resources/libbpf/src/elf.c@btf_is_float": "static inline bool btf_is_float ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_FLOAT ; }",
    "resources/libbpf/src/elf.c@btf_is_decl_tag": "static inline bool btf_is_decl_tag ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_DECL_TAG ; }",
    "resources/libbpf/src/elf.c@btf_is_type_tag": "static inline bool btf_is_type_tag ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_TYPE_TAG ; }",
    "resources/libbpf/src/elf.c@btf_is_any_enum": "static inline bool btf_is_any_enum ( const struct btf_type * t ) { return btf_is_enum ( t ) || btf_is_enum64 ( t ) ; }",
    "resources/libbpf/src/elf.c@btf_kind_core_compat": "static inline bool btf_kind_core_compat ( const struct btf_type * t1 , const struct btf_type * t2 ) { return btf_kind ( t1 ) == btf_kind ( t2 ) || ( btf_is_any_enum ( t1 ) && btf_is_any_enum ( t2 ) ) ; }",
    "resources/libbpf/src/elf.c@btf_int_encoding": "static inline __u8 btf_int_encoding ( const struct btf_type * t ) { return BTF_INT_ENCODING ( * ( __u32 * ) ( t + 1 ) ) ; }",
    "resources/libbpf/src/elf.c@btf_int_offset": "static inline __u8 btf_int_offset ( const struct btf_type * t ) { return BTF_INT_OFFSET ( * ( __u32 * ) ( t + 1 ) ) ; }",
    "resources/libbpf/src/elf.c@btf_int_bits": "static inline __u8 btf_int_bits ( const struct btf_type * t ) { return BTF_INT_BITS ( * ( __u32 * ) ( t + 1 ) ) ; }",
    "resources/libbpf/src/elf.c@btf_array": "static inline struct btf_array * btf_array ( const struct btf_type * t ) { return ( struct btf_array * ) ( t + 1 ) ; }",
    "resources/libbpf/src/elf.c@btf_enum": "static inline struct btf_enum * btf_enum ( const struct btf_type * t ) { return ( struct btf_enum * ) ( t + 1 ) ; }",
    "resources/libbpf/src/elf.c@btf_enum64": "static inline struct btf_enum64 * btf_enum64 ( const struct btf_type * t ) { return ( struct btf_enum64 * ) ( t + 1 ) ; }",
    "resources/libbpf/src/elf.c@btf_enum64_value": "static inline __u64 btf_enum64_value ( const struct btf_enum64 * e ) { /* struct btf_enum64 is introduced in Linux 6.0, which is very\n\t * bleeding-edge. Here we are avoiding relying on struct btf_enum64\n\t * definition coming from kernel UAPI headers to support wider range\n\t * of system-wide kernel headers.\n\t *\n\t * Given this header can be also included from C++ applications, that\n\t * further restricts C tricks we can use (like using compatible\n\t * anonymous struct). So just treat struct btf_enum64 as\n\t * a three-element array of u32 and access second (lo32) and third\n\t * (hi32) elements directly.\n\t *\n\t * For reference, here is a struct btf_enum64 definition:\n\t *\n\t * const struct btf_enum64 {\n\t *\t__u32\tname_off;\n\t *\t__u32\tval_lo32;\n\t *\t__u32\tval_hi32;\n\t * };\n\t */ const __u32 * e64 = ( const __u32 * ) e ; return ( ( __u64 ) e64 [ 2 ] << 32 ) | e64 [ 1 ] ; }",
    "resources/libbpf/src/elf.c@btf_members": "static inline struct btf_member * btf_members ( const struct btf_type * t ) { return ( struct btf_member * ) ( t + 1 ) ; }",
    "resources/libbpf/src/elf.c@btf_member_bit_offset": "static inline __u32 btf_member_bit_offset ( const struct btf_type * t , __u32 member_idx ) { const struct btf_member * m = btf_members ( t ) + member_idx ; bool kflag = btf_kflag ( t ) ; return kflag ? BTF_MEMBER_BIT_OFFSET ( m -> offset ) : m -> offset ; }",
    "resources/libbpf/src/elf.c@btf_member_bitfield_size": "static inline __u32 btf_member_bitfield_size ( const struct btf_type * t , __u32 member_idx ) { const struct btf_member * m = btf_members ( t ) + member_idx ; bool kflag = btf_kflag ( t ) ; return kflag ? BTF_MEMBER_BITFIELD_SIZE ( m -> offset ) : 0 ; }",
    "resources/libbpf/src/elf.c@btf_params": "static inline struct btf_param * btf_params ( const struct btf_type * t ) { return ( struct btf_param * ) ( t + 1 ) ; }",
    "resources/libbpf/src/elf.c@btf_var": "static inline struct btf_var * btf_var ( const struct btf_type * t ) { return ( struct btf_var * ) ( t + 1 ) ; }",
    "resources/libbpf/src/elf.c@btf_var_secinfos": "static inline struct btf_var_secinfo * btf_var_secinfos ( const struct btf_type * t ) { return ( struct btf_var_secinfo * ) ( t + 1 ) ; }",
    "resources/libbpf/src/elf.c@btf_decl_tag": "static inline struct btf_decl_tag * btf_decl_tag ( const struct btf_type * t ) { return ( struct btf_decl_tag * ) ( t + 1 ) ; }",
    "resources/libbpf/src/elf.c@str_has_sfx": "static inline bool str_has_sfx ( const char * str , const char * sfx ) { size_t str_len = strlen ( str ) ; size_t sfx_len = strlen ( sfx ) ; if ( sfx_len > str_len ) return false ; return strcmp ( str + str_len - sfx_len , sfx ) == 0 ; }",
    "resources/libbpf/src/elf.c@libbpf_reallocarray": "static inline void * libbpf_reallocarray ( void * ptr , size_t nmemb , size_t size ) { size_t total ; # if __has_builtin ( __builtin_mul_overflow ) if ( unlikely ( __builtin_mul_overflow ( nmemb , size , & total ) ) ) return NULL ; # else if ( size == 0 || nmemb > ULONG_MAX / size ) return NULL ; total = nmemb * size ; # endif return realloc ( ptr , total ) ; }",
    "resources/libbpf/src/elf.c@libbpf_strlcpy": "static inline void libbpf_strlcpy ( char * dst , const char * src , size_t sz ) { size_t i ; if ( sz == 0 ) return ; sz -- ; for ( i = 0 ; i < sz && src [ i ] ; i ++ ) dst [ i ] = src [ i ] ; dst [ i ] = '\\0' ; }",
    "resources/libbpf/src/elf.c@btf_func_linkage": "static inline enum btf_func_linkage btf_func_linkage ( const struct btf_type * t ) { return ( enum btf_func_linkage ) ( int ) btf_vlen ( t ) ; }",
    "resources/libbpf/src/elf.c@btf_type_info": "static inline __u32 btf_type_info ( int kind , int vlen , int kflag ) { return ( kflag << 31 ) | ( kind << 24 ) | vlen ; }",
    "resources/libbpf/src/elf.c@libbpf_is_mem_zeroed": "static inline bool libbpf_is_mem_zeroed ( const char * p , ssize_t len ) { while ( len > 0 ) { if ( * p ) return false ; p ++ ; len -- ; } return true ; }",
    "resources/libbpf/src/elf.c@libbpf_validate_opts": "static inline bool libbpf_validate_opts ( const char * opts , size_t opts_sz , size_t user_sz , const char * type_name ) { if ( user_sz < sizeof ( size_t ) ) { pr_warn ( \"%s size (%zu) is too small\\n\" , type_name , user_sz ) ; return false ; } if ( ! libbpf_is_mem_zeroed ( opts + opts_sz , ( ssize_t ) user_sz - opts_sz ) ) { pr_warn ( \"%s has non-zero extra bytes\\n\" , type_name ) ; return false ; } return true ; }",
    "resources/libbpf/src/elf.c@libbpf_err": "static inline int libbpf_err ( int ret ) { if ( ret < 0 ) errno = - ret ; return ret ; }",
    "resources/libbpf/src/elf.c@libbpf_err_errno": "static inline int libbpf_err_errno ( int ret ) { /* errno is already assumed to be set on error */ return ret < 0 ? - errno : ret ; }",
    "resources/libbpf/src/elf.c@libbpf_err_ptr": "static inline void * libbpf_err_ptr ( int err ) { /* set errno on error, this doesn't break anything */ errno = - err ; return NULL ; }",
    "resources/libbpf/src/elf.c@libbpf_ptr": "static inline void * libbpf_ptr ( void * ret ) { /* set errno on error, this doesn't break anything */ if ( IS_ERR ( ret ) ) errno = - PTR_ERR ( ret ) ; return IS_ERR ( ret ) ? NULL : ret ; }",
    "resources/libbpf/src/elf.c@str_is_empty": "static inline bool str_is_empty ( const char * s ) { return ! s || ! s [ 0 ] ; }",
    "resources/libbpf/src/elf.c@is_ldimm64_insn": "static inline bool is_ldimm64_insn ( struct bpf_insn * insn ) { return insn -> code == ( BPF_LD | BPF_IMM | BPF_DW ) ; }",
    "resources/libbpf/src/elf.c@dup_good_fd": "static inline int dup_good_fd ( int fd ) { if ( fd < 0 ) return fd ; return fcntl ( fd , F_DUPFD_CLOEXEC , 3 ) ; }",
    "resources/libbpf/src/elf.c@ensure_good_fd": "static inline int ensure_good_fd ( int fd ) { int old_fd = fd , saved_errno ; if ( fd < 0 ) return fd ; if ( fd < 3 ) { fd = dup_good_fd ( fd ) ; saved_errno = errno ; close ( old_fd ) ; errno = saved_errno ; if ( fd < 0 ) { pr_warn ( \"failed to dup FD %d to FD > 2: %d\\n\" , old_fd , - saved_errno ) ; errno = saved_errno ; } } return fd ; }",
    "resources/libbpf/src/elf.c@sys_dup2": "static inline int sys_dup2 ( int oldfd , int newfd ) { # ifdef __NR_dup2 return syscall ( __NR_dup2 , oldfd , newfd ) ; # else return syscall ( __NR_dup3 , oldfd , newfd , 0 ) ; # endif }",
    "resources/libbpf/src/elf.c@reuse_fd": "static inline int reuse_fd ( int fixed_fd , int tmp_fd ) { int err ; err = sys_dup2 ( tmp_fd , fixed_fd ) ; err = err < 0 ? - errno : 0 ; close ( tmp_fd ) ; /* clean up temporary FD */ return err ; }",
    "resources/libbpf/src/elf.c@is_pow_of_2": "static inline bool is_pow_of_2 ( size_t x ) { return x && ( x & ( x - 1 ) ) == 0 ; }",
    "resources/libbpf/src/elf.c@elf_open": "int elf_open ( const char * binary_path , struct elf_fd * elf_fd ) { char errmsg [ STRERR_BUFSIZE ] ; int fd , ret ; Elf * elf ; if ( elf_version ( EV_CURRENT ) == EV_NONE ) { pr_warn ( \"elf: failed to init libelf for %s\\n\" , binary_path ) ; return - LIBBPF_ERRNO__LIBELF ; } fd = open ( binary_path , O_RDONLY | O_CLOEXEC ) ; if ( fd < 0 ) { ret = - errno ; pr_warn ( \"elf: failed to open %s: %s\\n\" , binary_path , libbpf_strerror_r ( ret , errmsg , sizeof ( errmsg ) ) ) ; return ret ; } elf = elf_begin ( fd , ELF_C_READ_MMAP , NULL ) ; if ( ! elf ) { pr_warn ( \"elf: could not read elf from %s: %s\\n\" , binary_path , elf_errmsg ( - 1 ) ) ; close ( fd ) ; return - LIBBPF_ERRNO__FORMAT ; } elf_fd -> fd = fd ; elf_fd -> elf = elf ; return 0 ; }",
    "resources/libbpf/src/elf.c@elf_close": "void elf_close ( struct elf_fd * elf_fd ) { if ( ! elf_fd ) return ; elf_end ( elf_fd -> elf ) ; close ( elf_fd -> fd ) ; }",
    "resources/libbpf/src/elf.c@elf_find_next_scn_by_type": "static Elf_Scn * elf_find_next_scn_by_type ( Elf * elf , int sh_type , Elf_Scn * scn ) { while ( ( scn = elf_nextscn ( elf , scn ) ) != NULL ) { GElf_Shdr sh ; if ( ! gelf_getshdr ( scn , & sh ) ) continue ; if ( sh . sh_type == sh_type ) return scn ; } return NULL ; }",
    "resources/libbpf/src/elf.c@elf_sym_iter_new": "static int elf_sym_iter_new ( struct elf_sym_iter * iter , Elf * elf , const char * binary_path , int sh_type , int st_type ) { Elf_Scn * scn = NULL ; GElf_Ehdr ehdr ; GElf_Shdr sh ; memset ( iter , 0 , sizeof ( * iter ) ) ; if ( ! gelf_getehdr ( elf , & ehdr ) ) { pr_warn ( \"elf: failed to get ehdr from %s: %s\\n\" , binary_path , elf_errmsg ( - 1 ) ) ; return - EINVAL ; } scn = elf_find_next_scn_by_type ( elf , sh_type , NULL ) ; if ( ! scn ) { pr_debug ( \"elf: failed to find symbol table ELF sections in '%s'\\n\" , binary_path ) ; return - ENOENT ; } if ( ! gelf_getshdr ( scn , & sh ) ) return - EINVAL ; iter -> strtabidx = sh . sh_link ; iter -> syms = elf_getdata ( scn , 0 ) ; if ( ! iter -> syms ) { pr_warn ( \"elf: failed to get symbols for symtab section in '%s': %s\\n\" , binary_path , elf_errmsg ( - 1 ) ) ; return - EINVAL ; } iter -> nr_syms = iter -> syms -> d_size / sh . sh_entsize ; iter -> elf = elf ; iter -> st_type = st_type ; /* Version symbol table is meaningful to dynsym only */ if ( sh_type != SHT_DYNSYM ) return 0 ; scn = elf_find_next_scn_by_type ( elf , SHT_GNU_versym , NULL ) ; if ( ! scn ) return 0 ; iter -> versyms = elf_getdata ( scn , 0 ) ; scn = elf_find_next_scn_by_type ( elf , SHT_GNU_verdef , NULL ) ; if ( ! scn ) return 0 ; iter -> verdefs = elf_getdata ( scn , 0 ) ; if ( ! iter -> verdefs || ! gelf_getshdr ( scn , & sh ) ) { pr_warn ( \"elf: failed to get verdef ELF section in '%s'\\n\" , binary_path ) ; return - EINVAL ; } iter -> verdef_strtabidx = sh . sh_link ; return 0 ; }",
    "resources/libbpf/src/elf.c@elf_sym_iter_next": "static struct elf_sym * elf_sym_iter_next ( struct elf_sym_iter * iter ) { struct elf_sym * ret = & iter -> sym ; GElf_Sym * sym = & ret -> sym ; const char * name = NULL ; GElf_Versym versym ; Elf_Scn * sym_scn ; size_t idx ; for ( idx = iter -> next_sym_idx ; idx < iter -> nr_syms ; idx ++ ) { if ( ! gelf_getsym ( iter -> syms , idx , sym ) ) continue ; if ( GELF_ST_TYPE ( sym -> st_info ) != iter -> st_type ) continue ; name = elf_strptr ( iter -> elf , iter -> strtabidx , sym -> st_name ) ; if ( ! name ) continue ; sym_scn = elf_getscn ( iter -> elf , sym -> st_shndx ) ; if ( ! sym_scn ) continue ; if ( ! gelf_getshdr ( sym_scn , & ret -> sh ) ) continue ; iter -> next_sym_idx = idx + 1 ; ret -> name = name ; ret -> ver = 0 ; ret -> hidden = false ; if ( iter -> versyms ) { if ( ! gelf_getversym ( iter -> versyms , idx , & versym ) ) continue ; ret -> ver = versym & VERSYM_VERSION ; ret -> hidden = versym & VERSYM_HIDDEN ; } return ret ; } return NULL ; }",
    "resources/libbpf/src/elf.c@elf_get_vername": "static const char * elf_get_vername ( struct elf_sym_iter * iter , int ver ) { GElf_Verdaux verdaux ; GElf_Verdef verdef ; int offset ; if ( ! iter -> verdefs ) return NULL ; offset = 0 ; while ( gelf_getverdef ( iter -> verdefs , offset , & verdef ) ) { if ( verdef . vd_ndx != ver ) { if ( ! verdef . vd_next ) break ; offset += verdef . vd_next ; continue ; } if ( ! gelf_getverdaux ( iter -> verdefs , offset + verdef . vd_aux , & verdaux ) ) break ; return elf_strptr ( iter -> elf , iter -> verdef_strtabidx , verdaux . vda_name ) ; } return NULL ; }",
    "resources/libbpf/src/elf.c@symbol_match": "static bool symbol_match ( struct elf_sym_iter * iter , int sh_type , struct elf_sym * sym , const char * name , size_t name_len , const char * lib_ver ) { const char * ver_name ; /* Symbols are in forms of func, func@LIB_VER or func@@LIB_VER\n\t * make sure the func part matches the user specified name\n\t */ if ( strncmp ( sym -> name , name , name_len ) != 0 ) return false ; /* ...but we don't want a search for \"foo\" to match 'foo2\" also, so any\n\t * additional characters in sname should be of the form \"@@LIB\".\n\t */ if ( sym -> name [ name_len ] != '\\0' && sym -> name [ name_len ] != '@' ) return false ; /* If user does not specify symbol version, then we got a match */ if ( ! lib_ver ) return true ; /* If user specifies symbol version, for dynamic symbols,\n\t * get version name from ELF verdef section for comparison.\n\t */ if ( sh_type == SHT_DYNSYM ) { ver_name = elf_get_vername ( iter , sym -> ver ) ; if ( ! ver_name ) return false ; return strcmp ( ver_name , lib_ver ) == 0 ; } /* For normal symbols, it is already in form of func@LIB_VER */ return strcmp ( sym -> name , name ) == 0 ; }",
    "resources/libbpf/src/elf.c@elf_sym_offset": "static unsigned long elf_sym_offset ( struct elf_sym * sym ) { return sym -> sym . st_value - sym -> sh . sh_addr + sym -> sh . sh_offset ; }",
    "resources/libbpf/src/elf.c@elf_find_func_offset": "long elf_find_func_offset ( Elf * elf , const char * binary_path , const char * name ) { int i , sh_types [ 2 ] = { SHT_DYNSYM , SHT_SYMTAB } ; const char * at_symbol , * lib_ver ; bool is_shared_lib ; long ret = - ENOENT ; size_t name_len ; GElf_Ehdr ehdr ; if ( ! gelf_getehdr ( elf , & ehdr ) ) { pr_warn ( \"elf: failed to get ehdr from %s: %s\\n\" , binary_path , elf_errmsg ( - 1 ) ) ; ret = - LIBBPF_ERRNO__FORMAT ; goto out ; } /* for shared lib case, we do not need to calculate relative offset */ is_shared_lib = ehdr . e_type == ET_DYN ; /* Does name specify \"@@LIB_VER\" or \"@LIB_VER\" ? */ at_symbol = strchr ( name , '@' ) ; if ( at_symbol ) { name_len = at_symbol - name ; /* skip second @ if it's @@LIB_VER case */ if ( at_symbol [ 1 ] == '@' ) at_symbol ++ ; lib_ver = at_symbol + 1 ; } else { name_len = strlen ( name ) ; lib_ver = NULL ; } /* Search SHT_DYNSYM, SHT_SYMTAB for symbol. This search order is used because if\n\t * a binary is stripped, it may only have SHT_DYNSYM, and a fully-statically\n\t * linked binary may not have SHT_DYMSYM, so absence of a section should not be\n\t * reported as a warning/error.\n\t */ for ( i = 0 ; i < ARRAY_SIZE ( sh_types ) ; i ++ ) { struct elf_sym_iter iter ; struct elf_sym * sym ; int last_bind = - 1 ; int cur_bind ; ret = elf_sym_iter_new ( & iter , elf , binary_path , sh_types [ i ] , STT_FUNC ) ; if ( ret == - ENOENT ) continue ; if ( ret ) goto out ; while ( ( sym = elf_sym_iter_next ( & iter ) ) ) { if ( ! symbol_match ( & iter , sh_types [ i ] , sym , name , name_len , lib_ver ) ) continue ; cur_bind = GELF_ST_BIND ( sym -> sym . st_info ) ; if ( ret > 0 ) { /* handle multiple matches */ if ( elf_sym_offset ( sym ) == ret ) { /* same offset, no problem */ continue ; } else if ( last_bind != STB_WEAK && cur_bind != STB_WEAK ) { /* Only accept one non-weak bind. */ pr_warn ( \"elf: ambiguous match for '%s', '%s' in '%s'\\n\" , sym -> name , name , binary_path ) ; ret = - LIBBPF_ERRNO__FORMAT ; goto out ; } else if ( cur_bind == STB_WEAK ) { /* already have a non-weak bind, and\n\t\t\t\t\t * this is a weak bind, so ignore.\n\t\t\t\t\t */ continue ; } } ret = elf_sym_offset ( sym ) ; last_bind = cur_bind ; } if ( ret > 0 ) break ; } if ( ret > 0 ) { pr_debug ( \"elf: symbol address match for '%s' in '%s': 0x%lx\\n\" , name , binary_path , ret ) ; } else { if ( ret == 0 ) { pr_warn ( \"elf: '%s' is 0 in symtab for '%s': %s\\n\" , name , binary_path , is_shared_lib ? \"should not be 0 in a shared library\" : \"try using shared library path instead\" ) ; ret = - ENOENT ; } else { pr_warn ( \"elf: failed to find symbol '%s' in '%s'\\n\" , name , binary_path ) ; } } out : return ret ; }",
    "resources/libbpf/src/elf.c@elf_find_func_offset_from_file": "long elf_find_func_offset_from_file ( const char * binary_path , const char * name ) { struct elf_fd elf_fd ; long ret = - ENOENT ; ret = elf_open ( binary_path , & elf_fd ) ; if ( ret ) return ret ; ret = elf_find_func_offset ( elf_fd . elf , binary_path , name ) ; elf_close ( & elf_fd ) ; return ret ; }",
    "resources/libbpf/src/elf.c@symbol_cmp": "static int symbol_cmp ( const void * a , const void * b ) { const struct symbol * sym_a = a ; const struct symbol * sym_b = b ; return strcmp ( sym_a -> name , sym_b -> name ) ; }",
    "resources/libbpf/src/elf.c@elf_resolve_syms_offsets": "int elf_resolve_syms_offsets ( const char * binary_path , int cnt , const char * * syms , unsigned long * * poffsets , int st_type ) { int sh_types [ 2 ] = { SHT_DYNSYM , SHT_SYMTAB } ; int err = 0 , i , cnt_done = 0 ; unsigned long * offsets ; struct symbol * symbols ; struct elf_fd elf_fd ; err = elf_open ( binary_path , & elf_fd ) ; if ( err ) return err ; offsets = calloc ( cnt , sizeof ( * offsets ) ) ; symbols = calloc ( cnt , sizeof ( * symbols ) ) ; if ( ! offsets || ! symbols ) { err = - ENOMEM ; goto out ; } for ( i = 0 ; i < cnt ; i ++ ) { symbols [ i ] . name = syms [ i ] ; symbols [ i ] . idx = i ; } qsort ( symbols , cnt , sizeof ( * symbols ) , symbol_cmp ) ; for ( i = 0 ; i < ARRAY_SIZE ( sh_types ) ; i ++ ) { struct elf_sym_iter iter ; struct elf_sym * sym ; err = elf_sym_iter_new ( & iter , elf_fd . elf , binary_path , sh_types [ i ] , st_type ) ; if ( err == - ENOENT ) continue ; if ( err ) goto out ; while ( ( sym = elf_sym_iter_next ( & iter ) ) ) { unsigned long sym_offset = elf_sym_offset ( sym ) ; int bind = GELF_ST_BIND ( sym -> sym . st_info ) ; struct symbol * found , tmp = { . name = sym -> name , } ; unsigned long * offset ; found = bsearch ( & tmp , symbols , cnt , sizeof ( * symbols ) , symbol_cmp ) ; if ( ! found ) continue ; offset = & offsets [ found -> idx ] ; if ( * offset > 0 ) { /* same offset, no problem */ if ( * offset == sym_offset ) continue ; /* handle multiple matches */ if ( found -> bind != STB_WEAK && bind != STB_WEAK ) { /* Only accept one non-weak bind. */ pr_warn ( \"elf: ambiguous match found '%s@%lu' in '%s' previous offset %lu\\n\" , sym -> name , sym_offset , binary_path , * offset ) ; err = - ESRCH ; goto out ; } else if ( bind == STB_WEAK ) { /* already have a non-weak bind, and\n\t\t\t\t\t * this is a weak bind, so ignore.\n\t\t\t\t\t */ continue ; } } else { cnt_done ++ ; } * offset = sym_offset ; found -> bind = bind ; } } if ( cnt != cnt_done ) { err = - ENOENT ; goto out ; } * poffsets = offsets ; out : free ( symbols ) ; if ( err ) free ( offsets ) ; elf_close ( & elf_fd ) ; return err ; }",
    "resources/libbpf/src/elf.c@elf_resolve_pattern_offsets": "int elf_resolve_pattern_offsets ( const char * binary_path , const char * pattern , unsigned long * * poffsets , size_t * pcnt ) { int sh_types [ 2 ] = { SHT_SYMTAB , SHT_DYNSYM } ; unsigned long * offsets = NULL ; size_t cap = 0 , cnt = 0 ; struct elf_fd elf_fd ; int err = 0 , i ; err = elf_open ( binary_path , & elf_fd ) ; if ( err ) return err ; for ( i = 0 ; i < ARRAY_SIZE ( sh_types ) ; i ++ ) { struct elf_sym_iter iter ; struct elf_sym * sym ; err = elf_sym_iter_new ( & iter , elf_fd . elf , binary_path , sh_types [ i ] , STT_FUNC ) ; if ( err == - ENOENT ) continue ; if ( err ) goto out ; while ( ( sym = elf_sym_iter_next ( & iter ) ) ) { if ( ! glob_match ( sym -> name , pattern ) ) continue ; err = libbpf_ensure_mem ( ( void * * ) & offsets , & cap , sizeof ( * offsets ) , cnt + 1 ) ; if ( err ) goto out ; offsets [ cnt ++ ] = elf_sym_offset ( sym ) ; } /* If we found anything in the first symbol section,\n\t\t * do not search others to avoid duplicates.\n\t\t */ if ( cnt ) break ; } if ( cnt ) { * poffsets = offsets ; * pcnt = cnt ; } else { err = - ENOENT ; } out : if ( err ) free ( offsets ) ; elf_close ( & elf_fd ) ; return err ; }",
    "resources/libbpf/src/hashmap.c@hash_bits": "static inline size_t hash_bits ( size_t h , int bits ) { /* shuffle bits and return requested number of upper bits */ if ( bits == 0 ) return 0 ; # if ( __SIZEOF_SIZE_T__ == __SIZEOF_LONG_LONG__ ) /* LP64 case */ return ( h * 11400714819323198485llu ) >> ( __SIZEOF_LONG_LONG__ * 8 - bits ) ; # elif ( __SIZEOF_SIZE_T__ <= __SIZEOF_LONG__ ) return ( h * 2654435769lu ) >> ( __SIZEOF_LONG__ * 8 - bits ) ; # else # error \"Unsupported size_t size\" # endif }",
    "resources/libbpf/src/hashmap.c@str_hash": "static inline size_t str_hash ( const char * s ) { size_t h = 0 ; while ( * s ) { h = h * 31 + * s ; s ++ ; } return h ; }",
    "resources/libbpf/src/hashmap.c@hashmap_add_entry": "static void hashmap_add_entry ( struct hashmap_entry * * pprev , struct hashmap_entry * entry ) { entry -> next = * pprev ; * pprev = entry ; }",
    "resources/libbpf/src/hashmap.c@hashmap_del_entry": "static void hashmap_del_entry ( struct hashmap_entry * * pprev , struct hashmap_entry * entry ) { * pprev = entry -> next ; entry -> next = NULL ; }",
    "resources/libbpf/src/hashmap.c@hashmap__init": "void hashmap__init ( struct hashmap * map , hashmap_hash_fn hash_fn , hashmap_equal_fn equal_fn , void * ctx ) { map -> hash_fn = hash_fn ; map -> equal_fn = equal_fn ; map -> ctx = ctx ; map -> buckets = NULL ; map -> cap = 0 ; map -> cap_bits = 0 ; map -> sz = 0 ; }",
    "resources/libbpf/src/hashmap.c@hashmap__new": "struct hashmap * hashmap__new ( hashmap_hash_fn hash_fn , hashmap_equal_fn equal_fn , void * ctx ) { struct hashmap * map = malloc ( sizeof ( struct hashmap ) ) ; if ( ! map ) return ERR_PTR ( - ENOMEM ) ; hashmap__init ( map , hash_fn , equal_fn , ctx ) ; return map ; }",
    "resources/libbpf/src/hashmap.c@hashmap__clear": "void hashmap__clear ( struct hashmap * map ) { struct hashmap_entry * cur , * tmp ; size_t bkt ; hashmap__for_each_entry_safe ( map , cur , tmp , bkt ) { free ( cur ) ; } free ( map -> buckets ) ; map -> buckets = NULL ; map -> cap = map -> cap_bits = map -> sz = 0 ; }",
    "resources/libbpf/src/hashmap.c@hashmap__free": "void hashmap__free ( struct hashmap * map ) { if ( IS_ERR_OR_NULL ( map ) ) return ; hashmap__clear ( map ) ; free ( map ) ; }",
    "resources/libbpf/src/hashmap.c@hashmap__size": "size_t hashmap__size ( const struct hashmap * map ) { return map -> sz ; }",
    "resources/libbpf/src/hashmap.c@hashmap__capacity": "size_t hashmap__capacity ( const struct hashmap * map ) { return map -> cap ; }",
    "resources/libbpf/src/hashmap.c@hashmap_needs_to_grow": "static bool hashmap_needs_to_grow ( struct hashmap * map ) { /* grow if empty or more than 75% filled */ return ( map -> cap == 0 ) || ( ( map -> sz + 1 ) * 4 / 3 > map -> cap ) ; }",
    "resources/libbpf/src/hashmap.c@hashmap_grow": "static int hashmap_grow ( struct hashmap * map ) { struct hashmap_entry * * new_buckets ; struct hashmap_entry * cur , * tmp ; size_t new_cap_bits , new_cap ; size_t h , bkt ; new_cap_bits = map -> cap_bits + 1 ; if ( new_cap_bits < HASHMAP_MIN_CAP_BITS ) new_cap_bits = HASHMAP_MIN_CAP_BITS ; new_cap = 1UL << new_cap_bits ; new_buckets = calloc ( new_cap , sizeof ( new_buckets [ 0 ] ) ) ; if ( ! new_buckets ) return - ENOMEM ; hashmap__for_each_entry_safe ( map , cur , tmp , bkt ) { h = hash_bits ( map -> hash_fn ( cur -> key , map -> ctx ) , new_cap_bits ) ; hashmap_add_entry ( & new_buckets [ h ] , cur ) ; } map -> cap = new_cap ; map -> cap_bits = new_cap_bits ; free ( map -> buckets ) ; map -> buckets = new_buckets ; return 0 ; }",
    "resources/libbpf/src/hashmap.c@hashmap_find_entry": "static bool hashmap_find_entry ( const struct hashmap * map , const long key , size_t hash , struct hashmap_entry * * * pprev , struct hashmap_entry * * entry ) { struct hashmap_entry * cur , * * prev_ptr ; if ( ! map -> buckets ) return false ; for ( prev_ptr = & map -> buckets [ hash ] , cur = * prev_ptr ; cur ; prev_ptr = & cur -> next , cur = cur -> next ) { if ( map -> equal_fn ( cur -> key , key , map -> ctx ) ) { if ( pprev ) * pprev = prev_ptr ; * entry = cur ; return true ; } } return false ; }",
    "resources/libbpf/src/hashmap.c@hashmap_insert": "int hashmap_insert ( struct hashmap * map , long key , long value , enum hashmap_insert_strategy strategy , long * old_key , long * old_value ) { struct hashmap_entry * entry ; size_t h ; int err ; if ( old_key ) * old_key = 0 ; if ( old_value ) * old_value = 0 ; h = hash_bits ( map -> hash_fn ( key , map -> ctx ) , map -> cap_bits ) ; if ( strategy != HASHMAP_APPEND && hashmap_find_entry ( map , key , h , NULL , & entry ) ) { if ( old_key ) * old_key = entry -> key ; if ( old_value ) * old_value = entry -> value ; if ( strategy == HASHMAP_SET || strategy == HASHMAP_UPDATE ) { entry -> key = key ; entry -> value = value ; return 0 ; } else if ( strategy == HASHMAP_ADD ) { return - EEXIST ; } } if ( strategy == HASHMAP_UPDATE ) return - ENOENT ; if ( hashmap_needs_to_grow ( map ) ) { err = hashmap_grow ( map ) ; if ( err ) return err ; h = hash_bits ( map -> hash_fn ( key , map -> ctx ) , map -> cap_bits ) ; } entry = malloc ( sizeof ( struct hashmap_entry ) ) ; if ( ! entry ) return - ENOMEM ; entry -> key = key ; entry -> value = value ; hashmap_add_entry ( & map -> buckets [ h ] , entry ) ; map -> sz ++ ; return 0 ; }",
    "resources/libbpf/src/hashmap.c@hashmap_find": "",
    "resources/libbpf/src/hashmap.c@hashmap_delete": "",
    "resources/libbpf/src/libbpf.c@btf_kind": "static inline __u16 btf_kind ( const struct btf_type * t ) { return BTF_INFO_KIND ( t -> info ) ; }",
    "resources/libbpf/src/libbpf.c@btf_vlen": "static inline __u16 btf_vlen ( const struct btf_type * t ) { return BTF_INFO_VLEN ( t -> info ) ; }",
    "resources/libbpf/src/libbpf.c@btf_kflag": "static inline bool btf_kflag ( const struct btf_type * t ) { return BTF_INFO_KFLAG ( t -> info ) ; }",
    "resources/libbpf/src/libbpf.c@btf_is_void": "static inline bool btf_is_void ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_UNKN ; }",
    "resources/libbpf/src/libbpf.c@btf_is_int": "static inline bool btf_is_int ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_INT ; }",
    "resources/libbpf/src/libbpf.c@btf_is_ptr": "static inline bool btf_is_ptr ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_PTR ; }",
    "resources/libbpf/src/libbpf.c@btf_is_array": "static inline bool btf_is_array ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_ARRAY ; }",
    "resources/libbpf/src/libbpf.c@btf_is_struct": "static inline bool btf_is_struct ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_STRUCT ; }",
    "resources/libbpf/src/libbpf.c@btf_is_union": "static inline bool btf_is_union ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_UNION ; }",
    "resources/libbpf/src/libbpf.c@btf_is_composite": "static inline bool btf_is_composite ( const struct btf_type * t ) { __u16 kind = btf_kind ( t ) ; return kind == BTF_KIND_STRUCT || kind == BTF_KIND_UNION ; }",
    "resources/libbpf/src/libbpf.c@btf_is_enum": "static inline bool btf_is_enum ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_ENUM ; }",
    "resources/libbpf/src/libbpf.c@btf_is_enum64": "static inline bool btf_is_enum64 ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_ENUM64 ; }",
    "resources/libbpf/src/libbpf.c@btf_is_fwd": "static inline bool btf_is_fwd ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_FWD ; }",
    "resources/libbpf/src/libbpf.c@btf_is_typedef": "static inline bool btf_is_typedef ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_TYPEDEF ; }",
    "resources/libbpf/src/libbpf.c@btf_is_volatile": "static inline bool btf_is_volatile ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_VOLATILE ; }",
    "resources/libbpf/src/libbpf.c@btf_is_const": "static inline bool btf_is_const ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_CONST ; }",
    "resources/libbpf/src/libbpf.c@btf_is_restrict": "static inline bool btf_is_restrict ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_RESTRICT ; }",
    "resources/libbpf/src/libbpf.c@btf_is_mod": "static inline bool btf_is_mod ( const struct btf_type * t ) { __u16 kind = btf_kind ( t ) ; return kind == BTF_KIND_VOLATILE || kind == BTF_KIND_CONST || kind == BTF_KIND_RESTRICT || kind == BTF_KIND_TYPE_TAG ; }",
    "resources/libbpf/src/libbpf.c@btf_is_func": "static inline bool btf_is_func ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_FUNC ; }",
    "resources/libbpf/src/libbpf.c@btf_is_func_proto": "static inline bool btf_is_func_proto ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_FUNC_PROTO ; }",
    "resources/libbpf/src/libbpf.c@btf_is_var": "static inline bool btf_is_var ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_VAR ; }",
    "resources/libbpf/src/libbpf.c@btf_is_datasec": "static inline bool btf_is_datasec ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_DATASEC ; }",
    "resources/libbpf/src/libbpf.c@btf_is_float": "static inline bool btf_is_float ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_FLOAT ; }",
    "resources/libbpf/src/libbpf.c@btf_is_decl_tag": "static inline bool btf_is_decl_tag ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_DECL_TAG ; }",
    "resources/libbpf/src/libbpf.c@btf_is_type_tag": "static inline bool btf_is_type_tag ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_TYPE_TAG ; }",
    "resources/libbpf/src/libbpf.c@btf_is_any_enum": "static inline bool btf_is_any_enum ( const struct btf_type * t ) { return btf_is_enum ( t ) || btf_is_enum64 ( t ) ; }",
    "resources/libbpf/src/libbpf.c@btf_kind_core_compat": "static inline bool btf_kind_core_compat ( const struct btf_type * t1 , const struct btf_type * t2 ) { return btf_kind ( t1 ) == btf_kind ( t2 ) || ( btf_is_any_enum ( t1 ) && btf_is_any_enum ( t2 ) ) ; }",
    "resources/libbpf/src/libbpf.c@btf_int_encoding": "static inline __u8 btf_int_encoding ( const struct btf_type * t ) { return BTF_INT_ENCODING ( * ( __u32 * ) ( t + 1 ) ) ; }",
    "resources/libbpf/src/libbpf.c@btf_int_offset": "static inline __u8 btf_int_offset ( const struct btf_type * t ) { return BTF_INT_OFFSET ( * ( __u32 * ) ( t + 1 ) ) ; }",
    "resources/libbpf/src/libbpf.c@btf_int_bits": "static inline __u8 btf_int_bits ( const struct btf_type * t ) { return BTF_INT_BITS ( * ( __u32 * ) ( t + 1 ) ) ; }",
    "resources/libbpf/src/libbpf.c@btf_array": "static inline struct btf_array * btf_array ( const struct btf_type * t ) { return ( struct btf_array * ) ( t + 1 ) ; }",
    "resources/libbpf/src/libbpf.c@btf_enum": "static inline struct btf_enum * btf_enum ( const struct btf_type * t ) { return ( struct btf_enum * ) ( t + 1 ) ; }",
    "resources/libbpf/src/libbpf.c@btf_enum64": "static inline struct btf_enum64 * btf_enum64 ( const struct btf_type * t ) { return ( struct btf_enum64 * ) ( t + 1 ) ; }",
    "resources/libbpf/src/libbpf.c@btf_enum64_value": "static inline __u64 btf_enum64_value ( const struct btf_enum64 * e ) { /* struct btf_enum64 is introduced in Linux 6.0, which is very\n\t * bleeding-edge. Here we are avoiding relying on struct btf_enum64\n\t * definition coming from kernel UAPI headers to support wider range\n\t * of system-wide kernel headers.\n\t *\n\t * Given this header can be also included from C++ applications, that\n\t * further restricts C tricks we can use (like using compatible\n\t * anonymous struct). So just treat struct btf_enum64 as\n\t * a three-element array of u32 and access second (lo32) and third\n\t * (hi32) elements directly.\n\t *\n\t * For reference, here is a struct btf_enum64 definition:\n\t *\n\t * const struct btf_enum64 {\n\t *\t__u32\tname_off;\n\t *\t__u32\tval_lo32;\n\t *\t__u32\tval_hi32;\n\t * };\n\t */ const __u32 * e64 = ( const __u32 * ) e ; return ( ( __u64 ) e64 [ 2 ] << 32 ) | e64 [ 1 ] ; }",
    "resources/libbpf/src/libbpf.c@btf_members": "static inline struct btf_member * btf_members ( const struct btf_type * t ) { return ( struct btf_member * ) ( t + 1 ) ; }",
    "resources/libbpf/src/libbpf.c@btf_member_bit_offset": "static inline __u32 btf_member_bit_offset ( const struct btf_type * t , __u32 member_idx ) { const struct btf_member * m = btf_members ( t ) + member_idx ; bool kflag = btf_kflag ( t ) ; return kflag ? BTF_MEMBER_BIT_OFFSET ( m -> offset ) : m -> offset ; }",
    "resources/libbpf/src/libbpf.c@btf_member_bitfield_size": "static inline __u32 btf_member_bitfield_size ( const struct btf_type * t , __u32 member_idx ) { const struct btf_member * m = btf_members ( t ) + member_idx ; bool kflag = btf_kflag ( t ) ; return kflag ? BTF_MEMBER_BITFIELD_SIZE ( m -> offset ) : 0 ; }",
    "resources/libbpf/src/libbpf.c@btf_params": "static inline struct btf_param * btf_params ( const struct btf_type * t ) { return ( struct btf_param * ) ( t + 1 ) ; }",
    "resources/libbpf/src/libbpf.c@btf_var": "static inline struct btf_var * btf_var ( const struct btf_type * t ) { return ( struct btf_var * ) ( t + 1 ) ; }",
    "resources/libbpf/src/libbpf.c@btf_var_secinfos": "static inline struct btf_var_secinfo * btf_var_secinfos ( const struct btf_type * t ) { return ( struct btf_var_secinfo * ) ( t + 1 ) ; }",
    "resources/libbpf/src/libbpf.c@btf_decl_tag": "static inline struct btf_decl_tag * btf_decl_tag ( const struct btf_type * t ) { return ( struct btf_decl_tag * ) ( t + 1 ) ; }",
    "resources/libbpf/src/libbpf.c@str_has_sfx": "static inline bool str_has_sfx ( const char * str , const char * sfx ) { size_t str_len = strlen ( str ) ; size_t sfx_len = strlen ( sfx ) ; if ( sfx_len > str_len ) return false ; return strcmp ( str + str_len - sfx_len , sfx ) == 0 ; }",
    "resources/libbpf/src/libbpf.c@libbpf_reallocarray": "static inline void * libbpf_reallocarray ( void * ptr , size_t nmemb , size_t size ) { size_t total ; # if __has_builtin ( __builtin_mul_overflow ) if ( unlikely ( __builtin_mul_overflow ( nmemb , size , & total ) ) ) return NULL ; # else if ( size == 0 || nmemb > ULONG_MAX / size ) return NULL ; total = nmemb * size ; # endif return realloc ( ptr , total ) ; }",
    "resources/libbpf/src/libbpf.c@libbpf_strlcpy": "static inline void libbpf_strlcpy ( char * dst , const char * src , size_t sz ) { size_t i ; if ( sz == 0 ) return ; sz -- ; for ( i = 0 ; i < sz && src [ i ] ; i ++ ) dst [ i ] = src [ i ] ; dst [ i ] = '\\0' ; }",
    "resources/libbpf/src/libbpf.c@btf_func_linkage": "static inline enum btf_func_linkage btf_func_linkage ( const struct btf_type * t ) { return ( enum btf_func_linkage ) ( int ) btf_vlen ( t ) ; }",
    "resources/libbpf/src/libbpf.c@btf_type_info": "static inline __u32 btf_type_info ( int kind , int vlen , int kflag ) { return ( kflag << 31 ) | ( kind << 24 ) | vlen ; }",
    "resources/libbpf/src/libbpf.c@libbpf_is_mem_zeroed": "static inline bool libbpf_is_mem_zeroed ( const char * p , ssize_t len ) { while ( len > 0 ) { if ( * p ) return false ; p ++ ; len -- ; } return true ; }",
    "resources/libbpf/src/libbpf.c@libbpf_validate_opts": "static inline bool libbpf_validate_opts ( const char * opts , size_t opts_sz , size_t user_sz , const char * type_name ) { if ( user_sz < sizeof ( size_t ) ) { pr_warn ( \"%s size (%zu) is too small\\n\" , type_name , user_sz ) ; return false ; } if ( ! libbpf_is_mem_zeroed ( opts + opts_sz , ( ssize_t ) user_sz - opts_sz ) ) { pr_warn ( \"%s has non-zero extra bytes\\n\" , type_name ) ; return false ; } return true ; }",
    "resources/libbpf/src/libbpf.c@libbpf_err": "static inline int libbpf_err ( int ret ) { if ( ret < 0 ) errno = - ret ; return ret ; }",
    "resources/libbpf/src/libbpf.c@libbpf_err_errno": "static inline int libbpf_err_errno ( int ret ) { /* errno is already assumed to be set on error */ return ret < 0 ? - errno : ret ; }",
    "resources/libbpf/src/libbpf.c@libbpf_err_ptr": "static inline void * libbpf_err_ptr ( int err ) { /* set errno on error, this doesn't break anything */ errno = - err ; return NULL ; }",
    "resources/libbpf/src/libbpf.c@libbpf_ptr": "static inline void * libbpf_ptr ( void * ret ) { /* set errno on error, this doesn't break anything */ if ( IS_ERR ( ret ) ) errno = - PTR_ERR ( ret ) ; return IS_ERR ( ret ) ? NULL : ret ; }",
    "resources/libbpf/src/libbpf.c@str_is_empty": "static inline bool str_is_empty ( const char * s ) { return ! s || ! s [ 0 ] ; }",
    "resources/libbpf/src/libbpf.c@is_ldimm64_insn": "static inline bool is_ldimm64_insn ( struct bpf_insn * insn ) { return insn -> code == ( BPF_LD | BPF_IMM | BPF_DW ) ; }",
    "resources/libbpf/src/libbpf.c@dup_good_fd": "static inline int dup_good_fd ( int fd ) { if ( fd < 0 ) return fd ; return fcntl ( fd , F_DUPFD_CLOEXEC , 3 ) ; }",
    "resources/libbpf/src/libbpf.c@ensure_good_fd": "static inline int ensure_good_fd ( int fd ) { int old_fd = fd , saved_errno ; if ( fd < 0 ) return fd ; if ( fd < 3 ) { fd = dup_good_fd ( fd ) ; saved_errno = errno ; close ( old_fd ) ; errno = saved_errno ; if ( fd < 0 ) { pr_warn ( \"failed to dup FD %d to FD > 2: %d\\n\" , old_fd , - saved_errno ) ; errno = saved_errno ; } } return fd ; }",
    "resources/libbpf/src/libbpf.c@sys_dup2": "static inline int sys_dup2 ( int oldfd , int newfd ) { # ifdef __NR_dup2 return syscall ( __NR_dup2 , oldfd , newfd ) ; # else return syscall ( __NR_dup3 , oldfd , newfd , 0 ) ; # endif }",
    "resources/libbpf/src/libbpf.c@reuse_fd": "static inline int reuse_fd ( int fixed_fd , int tmp_fd ) { int err ; err = sys_dup2 ( tmp_fd , fixed_fd ) ; err = err < 0 ? - errno : 0 ; close ( tmp_fd ) ; /* clean up temporary FD */ return err ; }",
    "resources/libbpf/src/libbpf.c@is_pow_of_2": "static inline bool is_pow_of_2 ( size_t x ) { return x && ( x & ( x - 1 ) ) == 0 ; }",
    "resources/libbpf/src/libbpf.c@hash_bits": "static inline size_t hash_bits ( size_t h , int bits ) { /* shuffle bits and return requested number of upper bits */ if ( bits == 0 ) return 0 ; # if ( __SIZEOF_SIZE_T__ == __SIZEOF_LONG_LONG__ ) /* LP64 case */ return ( h * 11400714819323198485llu ) >> ( __SIZEOF_LONG_LONG__ * 8 - bits ) ; # elif ( __SIZEOF_SIZE_T__ <= __SIZEOF_LONG__ ) return ( h * 2654435769lu ) >> ( __SIZEOF_LONG__ * 8 - bits ) ; # else # error \"Unsupported size_t size\" # endif }",
    "resources/libbpf/src/libbpf.c@str_hash": "static inline size_t str_hash ( const char * s ) { size_t h = 0 ; while ( * s ) { h = h * 31 + * s ; s ++ ; } return h ; }",
    "resources/libbpf/src/libbpf.c@__base_pr": "static int __base_pr ( enum libbpf_print_level level , const char * format , va_list args ) { if ( level == LIBBPF_DEBUG ) return 0 ; return vfprintf ( stderr , format , args ) ; }",
    "resources/libbpf/src/libbpf.c@libbpf_set_print": "libbpf_print_fn_t libbpf_set_print ( libbpf_print_fn_t fn ) { libbpf_print_fn_t old_print_fn ; old_print_fn = __atomic_exchange_n ( & __libbpf_pr , fn , __ATOMIC_RELAXED ) ; return old_print_fn ; }",
    "resources/libbpf/src/libbpf.c@libbpf_print": "__attribute__ ( ( format ( printf , a , b ) ) ) static struct bpf_map * bpf_object__add_map ( struct bpf_object * obj ) ; static bool prog_is_subprog ( const struct bpf_object * obj , const struct bpf_program * prog ) ; static int map_set_def_max_entries ( struct bpf_map * map ) ; static const char * const attach_type_name [ ] = { [ BPF_CGROUP_INET_INGRESS ] = \"cgroup_inet_ingress\" , [ BPF_CGROUP_INET_EGRESS ] = \"cgroup_inet_egress\" , [ BPF_CGROUP_INET_SOCK_CREATE ] = \"cgroup_inet_sock_create\" , [ BPF_CGROUP_INET_SOCK_RELEASE ] = \"cgroup_inet_sock_release\" , [ BPF_CGROUP_SOCK_OPS ] = \"cgroup_sock_ops\" , [ BPF_CGROUP_DEVICE ] = \"cgroup_device\" , [ BPF_CGROUP_INET4_BIND ] = \"cgroup_inet4_bind\" , [ BPF_CGROUP_INET6_BIND ] = \"cgroup_inet6_bind\" , [ BPF_CGROUP_INET4_CONNECT ] = \"cgroup_inet4_connect\" , [ BPF_CGROUP_INET6_CONNECT ] = \"cgroup_inet6_connect\" , [ BPF_CGROUP_UNIX_CONNECT ] = \"cgroup_unix_connect\" , [ BPF_CGROUP_INET4_POST_BIND ] = \"cgroup_inet4_post_bind\" , [ BPF_CGROUP_INET6_POST_BIND ] = \"cgroup_inet6_post_bind\" , [ BPF_CGROUP_INET4_GETPEERNAME ] = \"cgroup_inet4_getpeername\" , [ BPF_CGROUP_INET6_GETPEERNAME ] = \"cgroup_inet6_getpeername\" , [ BPF_CGROUP_UNIX_GETPEERNAME ] = \"cgroup_unix_getpeername\" , [ BPF_CGROUP_INET4_GETSOCKNAME ] = \"cgroup_inet4_getsockname\" , [ BPF_CGROUP_INET6_GETSOCKNAME ] = \"cgroup_inet6_getsockname\" , [ BPF_CGROUP_UNIX_GETSOCKNAME ] = \"cgroup_unix_getsockname\" , [ BPF_CGROUP_UDP4_SENDMSG ] = \"cgroup_udp4_sendmsg\" , [ BPF_CGROUP_UDP6_SENDMSG ] = \"cgroup_udp6_sendmsg\" , [ BPF_CGROUP_UNIX_SENDMSG ] = \"cgroup_unix_sendmsg\" , [ BPF_CGROUP_SYSCTL ] = \"cgroup_sysctl\" , [ BPF_CGROUP_UDP4_RECVMSG ] = \"cgroup_udp4_recvmsg\" , [ BPF_CGROUP_UDP6_RECVMSG ] = \"cgroup_udp6_recvmsg\" , [ BPF_CGROUP_UNIX_RECVMSG ] = \"cgroup_unix_recvmsg\" , [ BPF_CGROUP_GETSOCKOPT ] = \"cgroup_getsockopt\" , [ BPF_CGROUP_SETSOCKOPT ] = \"cgroup_setsockopt\" , [ BPF_SK_SKB_STREAM_PARSER ] = \"sk_skb_stream_parser\" , [ BPF_SK_SKB_STREAM_VERDICT ] = \"sk_skb_stream_verdict\" , [ BPF_SK_SKB_VERDICT ] = \"sk_skb_verdict\" , [ BPF_SK_MSG_VERDICT ] = \"sk_msg_verdict\" , [ BPF_LIRC_MODE2 ] = \"lirc_mode2\" , [ BPF_FLOW_DISSECTOR ] = \"flow_dissector\" , [ BPF_TRACE_RAW_TP ] = \"trace_raw_tp\" , [ BPF_TRACE_FENTRY ] = \"trace_fentry\" , [ BPF_TRACE_FEXIT ] = \"trace_fexit\" , [ BPF_MODIFY_RETURN ] = \"modify_return\" , [ BPF_LSM_MAC ] = \"lsm_mac\" , [ BPF_LSM_CGROUP ] = \"lsm_cgroup\" , [ BPF_SK_LOOKUP ] = \"sk_lookup\" , [ BPF_TRACE_ITER ] = \"trace_iter\" , [ BPF_XDP_DEVMAP ] = \"xdp_devmap\" , [ BPF_XDP_CPUMAP ] = \"xdp_cpumap\" , [ BPF_XDP ] = \"xdp\" , [ BPF_SK_REUSEPORT_SELECT ] = \"sk_reuseport_select\" , [ BPF_SK_REUSEPORT_SELECT_OR_MIGRATE ] = \"sk_reuseport_select_or_migrate\" , [ BPF_PERF_EVENT ] = \"perf_event\" , [ BPF_TRACE_KPROBE_MULTI ] = \"trace_kprobe_multi\" , [ BPF_STRUCT_OPS ] = \"struct_ops\" , [ BPF_NETFILTER ] = \"netfilter\" , [ BPF_TCX_INGRESS ] = \"tcx_ingress\" , [ BPF_TCX_EGRESS ] = \"tcx_egress\" , [ BPF_TRACE_UPROBE_MULTI ] = \"trace_uprobe_multi\" , [ BPF_NETKIT_PRIMARY ] = \"netkit_primary\" , [ BPF_NETKIT_PEER ] = \"netkit_peer\" , } ; static const char * const link_type_name [ ] = { [ BPF_LINK_TYPE_UNSPEC ] = \"unspec\" , [ BPF_LINK_TYPE_RAW_TRACEPOINT ] = \"raw_tracepoint\" , [ BPF_LINK_TYPE_TRACING ] = \"tracing\" , [ BPF_LINK_TYPE_CGROUP ] = \"cgroup\" , [ BPF_LINK_TYPE_ITER ] = \"iter\" , [ BPF_LINK_TYPE_NETNS ] = \"netns\" , [ BPF_LINK_TYPE_XDP ] = \"xdp\" , [ BPF_LINK_TYPE_PERF_EVENT ] = \"perf_event\" , [ BPF_LINK_TYPE_KPROBE_MULTI ] = \"kprobe_multi\" , [ BPF_LINK_TYPE_STRUCT_OPS ] = \"struct_ops\" , [ BPF_LINK_TYPE_NETFILTER ] = \"netfilter\" , [ BPF_LINK_TYPE_TCX ] = \"tcx\" , [ BPF_LINK_TYPE_UPROBE_MULTI ] = \"uprobe_multi\" , [ BPF_LINK_TYPE_NETKIT ] = \"netkit\" , } ; static const char * const map_type_name [ ] = { [ BPF_MAP_TYPE_UNSPEC ] = \"unspec\" , [ BPF_MAP_TYPE_HASH ] = \"hash\" , [ BPF_MAP_TYPE_ARRAY ] = \"array\" , [ BPF_MAP_TYPE_PROG_ARRAY ] = \"prog_array\" , [ BPF_MAP_TYPE_PERF_EVENT_ARRAY ] = \"perf_event_array\" , [ BPF_MAP_TYPE_PERCPU_HASH ] = \"percpu_hash\" , [ BPF_MAP_TYPE_PERCPU_ARRAY ] = \"percpu_array\" , [ BPF_MAP_TYPE_STACK_TRACE ] = \"stack_trace\" , [ BPF_MAP_TYPE_CGROUP_ARRAY ] = \"cgroup_array\" , [ BPF_MAP_TYPE_LRU_HASH ] = \"lru_hash\" , [ BPF_MAP_TYPE_LRU_PERCPU_HASH ] = \"lru_percpu_hash\" , [ BPF_MAP_TYPE_LPM_TRIE ] = \"lpm_trie\" , [ BPF_MAP_TYPE_ARRAY_OF_MAPS ] = \"array_of_maps\" , [ BPF_MAP_TYPE_HASH_OF_MAPS ] = \"hash_of_maps\" , [ BPF_MAP_TYPE_DEVMAP ] = \"devmap\" , [ BPF_MAP_TYPE_DEVMAP_HASH ] = \"devmap_hash\" , [ BPF_MAP_TYPE_SOCKMAP ] = \"sockmap\" , [ BPF_MAP_TYPE_CPUMAP ] = \"cpumap\" , [ BPF_MAP_TYPE_XSKMAP ] = \"xskmap\" , [ BPF_MAP_TYPE_SOCKHASH ] = \"sockhash\" , [ BPF_MAP_TYPE_CGROUP_STORAGE ] = \"cgroup_storage\" , [ BPF_MAP_TYPE_REUSEPORT_SOCKARRAY ] = \"reuseport_sockarray\" , [ BPF_MAP_TYPE_PERCPU_CGROUP_STORAGE ] = \"percpu_cgroup_storage\" , [ BPF_MAP_TYPE_QUEUE ] = \"queue\" , [ BPF_MAP_TYPE_STACK ] = \"stack\" , [ BPF_MAP_TYPE_SK_STORAGE ] = \"sk_storage\" , [ BPF_MAP_TYPE_STRUCT_OPS ] = \"struct_ops\" , [ BPF_MAP_TYPE_RINGBUF ] = \"ringbuf\" , [ BPF_MAP_TYPE_INODE_STORAGE ] = \"inode_storage\" , [ BPF_MAP_TYPE_TASK_STORAGE ] = \"task_storage\" , [ BPF_MAP_TYPE_BLOOM_FILTER ] = \"bloom_filter\" , [ BPF_MAP_TYPE_USER_RINGBUF ] = \"user_ringbuf\" , [ BPF_MAP_TYPE_CGRP_STORAGE ] = \"cgrp_storage\" , [ BPF_MAP_TYPE_ARENA ] = \"arena\" , } ; static const char * const prog_type_name [ ] = { [ BPF_PROG_TYPE_UNSPEC ] = \"unspec\" , [ BPF_PROG_TYPE_SOCKET_FILTER ] = \"socket_filter\" , [ BPF_PROG_TYPE_KPROBE ] = \"kprobe\" , [ BPF_PROG_TYPE_SCHED_CLS ] = \"sched_cls\" , [ BPF_PROG_TYPE_SCHED_ACT ] = \"sched_act\" , [ BPF_PROG_TYPE_TRACEPOINT ] = \"tracepoint\" , [ BPF_PROG_TYPE_XDP ] = \"xdp\" , [ BPF_PROG_TYPE_PERF_EVENT ] = \"perf_event\" , [ BPF_PROG_TYPE_CGROUP_SKB ] = \"cgroup_skb\" , [ BPF_PROG_TYPE_CGROUP_SOCK ] = \"cgroup_sock\" , [ BPF_PROG_TYPE_LWT_IN ] = \"lwt_in\" , [ BPF_PROG_TYPE_LWT_OUT ] = \"lwt_out\" , [ BPF_PROG_TYPE_LWT_XMIT ] = \"lwt_xmit\" , [ BPF_PROG_TYPE_SOCK_OPS ] = \"sock_ops\" , [ BPF_PROG_TYPE_SK_SKB ] = \"sk_skb\" , [ BPF_PROG_TYPE_CGROUP_DEVICE ] = \"cgroup_device\" , [ BPF_PROG_TYPE_SK_MSG ] = \"sk_msg\" , [ BPF_PROG_TYPE_RAW_TRACEPOINT ] = \"raw_tracepoint\" , [ BPF_PROG_TYPE_CGROUP_SOCK_ADDR ] = \"cgroup_sock_addr\" , [ BPF_PROG_TYPE_LWT_SEG6LOCAL ] = \"lwt_seg6local\" , [ BPF_PROG_TYPE_LIRC_MODE2 ] = \"lirc_mode2\" , [ BPF_PROG_TYPE_SK_REUSEPORT ] = \"sk_reuseport\" , [ BPF_PROG_TYPE_FLOW_DISSECTOR ] = \"flow_dissector\" , [ BPF_PROG_TYPE_CGROUP_SYSCTL ] = \"cgroup_sysctl\" , [ BPF_PROG_TYPE_RAW_TRACEPOINT_WRITABLE ] = \"raw_tracepoint_writable\" , [ BPF_PROG_TYPE_CGROUP_SOCKOPT ] = \"cgroup_sockopt\" , [ BPF_PROG_TYPE_TRACING ] = \"tracing\" , [ BPF_PROG_TYPE_STRUCT_OPS ] = \"struct_ops\" , [ BPF_PROG_TYPE_EXT ] = \"ext\" , [ BPF_PROG_TYPE_LSM ] = \"lsm\" , [ BPF_PROG_TYPE_SK_LOOKUP ] = \"sk_lookup\" , [ BPF_PROG_TYPE_SYSCALL ] = \"syscall\" , [ BPF_PROG_TYPE_NETFILTER ] = \"netfilter\" , } ; static int __base_pr ( enum libbpf_print_level level , const char * format , va_list args ) { if ( level == LIBBPF_DEBUG ) return 0 ; return vfprintf ( stderr , format , args ) ; } static libbpf_print_fn_t __libbpf_pr = __base_pr ; libbpf_print_fn_t libbpf_set_print ( libbpf_print_fn_t fn ) { libbpf_print_fn_t old_print_fn ; old_print_fn = __atomic_exchange_n ( & __libbpf_pr , fn , __ATOMIC_RELAXED ) ; return old_print_fn ; } __printf ( 2 , 3 ) void libbpf_print ( enum libbpf_print_level level , const char * format , ... ) { va_list args ; int old_errno ; libbpf_print_fn_t print_fn ; print_fn = __atomic_load_n ( & __libbpf_pr , __ATOMIC_RELAXED ) ; if ( ! print_fn ) return ; old_errno = errno ; va_start ( args , format ) ; __libbpf_pr ( level , format , args ) ; va_end ( args ) ; errno = old_errno ; }",
    "resources/libbpf/src/libbpf.c@pr_perm_msg": "static void pr_perm_msg ( int err ) { struct rlimit limit ; char buf [ 100 ] ; if ( err != - EPERM || geteuid ( ) != 0 ) return ; err = getrlimit ( RLIMIT_MEMLOCK , & limit ) ; if ( err ) return ; if ( limit . rlim_cur == RLIM_INFINITY ) return ; if ( limit . rlim_cur < 1024 ) snprintf ( buf , sizeof ( buf ) , \"%zu bytes\" , ( size_t ) limit . rlim_cur ) ; else if ( limit . rlim_cur < 1024 * 1024 ) snprintf ( buf , sizeof ( buf ) , \"%.1f KiB\" , ( double ) limit . rlim_cur / 1024 ) ; else snprintf ( buf , sizeof ( buf ) , \"%.1f MiB\" , ( double ) limit . rlim_cur / ( 1024 * 1024 ) ) ; pr_warn ( \"permission error while running as root; try raising 'ulimit -l'? current value: %s\\n\" , buf ) ; }",
    "resources/libbpf/src/libbpf.c@ptr_to_u64": "static inline __u64 ptr_to_u64 ( const void * ptr ) { return ( __u64 ) ( unsigned long ) ptr ; }",
    "resources/libbpf/src/libbpf.c@libbpf_set_strict_mode": "int libbpf_set_strict_mode ( enum libbpf_strict_mode mode ) { /* as of v1.0 libbpf_set_strict_mode() is a no-op */ return 0 ; }",
    "resources/libbpf/src/libbpf.c@libbpf_major_version": "__u32 libbpf_major_version ( void ) { return LIBBPF_MAJOR_VERSION ; }",
    "resources/libbpf/src/libbpf.c@libbpf_minor_version": "__u32 libbpf_minor_version ( void ) { return LIBBPF_MINOR_VERSION ; }",
    "resources/libbpf/src/libbpf.c@libbpf_version_string": "const char * libbpf_version_string ( void ) { # define __S ( X ) # X # define _S ( X ) __S ( X ) return \"v\" _S ( LIBBPF_MAJOR_VERSION ) \".\" _S ( LIBBPF_MINOR_VERSION ) ; # undef _S # undef __S }",
    "resources/libbpf/src/libbpf.c@bpf_program__unload": "void bpf_program__unload ( struct bpf_program * prog ) { if ( ! prog ) return ; zclose ( prog -> fd ) ; zfree ( & prog -> func_info ) ; zfree ( & prog -> line_info ) ; }",
    "resources/libbpf/src/libbpf.c@bpf_program__exit": "static void bpf_program__exit ( struct bpf_program * prog ) { if ( ! prog ) return ; bpf_program__unload ( prog ) ; zfree ( & prog -> name ) ; zfree ( & prog -> sec_name ) ; zfree ( & prog -> insns ) ; zfree ( & prog -> reloc_desc ) ; prog -> nr_reloc = 0 ; prog -> insns_cnt = 0 ; prog -> sec_idx = - 1 ; }",
    "resources/libbpf/src/libbpf.c@insn_is_subprog_call": "static bool insn_is_subprog_call ( const struct bpf_insn * insn ) { return BPF_CLASS ( insn -> code ) == BPF_JMP && BPF_OP ( insn -> code ) == BPF_CALL && BPF_SRC ( insn -> code ) == BPF_K && insn -> src_reg == BPF_PSEUDO_CALL && insn -> dst_reg == 0 && insn -> off == 0 ; }",
    "resources/libbpf/src/libbpf.c@is_call_insn": "static bool is_call_insn ( const struct bpf_insn * insn ) { return insn -> code == ( BPF_JMP | BPF_CALL ) ; }",
    "resources/libbpf/src/libbpf.c@insn_is_pseudo_func": "static bool insn_is_pseudo_func ( struct bpf_insn * insn ) { return is_ldimm64_insn ( insn ) && insn -> src_reg == BPF_PSEUDO_FUNC ; }",
    "resources/libbpf/src/libbpf.c@bpf_object__init_prog": "static int bpf_object__init_prog ( struct bpf_object * obj , struct bpf_program * prog , const char * name , size_t sec_idx , const char * sec_name , size_t sec_off , void * insn_data , size_t insn_data_sz ) { if ( insn_data_sz == 0 || insn_data_sz % BPF_INSN_SZ || sec_off % BPF_INSN_SZ ) { pr_warn ( \"sec '%s': corrupted program '%s', offset %zu, size %zu\\n\" , sec_name , name , sec_off , insn_data_sz ) ; return - EINVAL ; } memset ( prog , 0 , sizeof ( * prog ) ) ; prog -> obj = obj ; prog -> sec_idx = sec_idx ; prog -> sec_insn_off = sec_off / BPF_INSN_SZ ; prog -> sec_insn_cnt = insn_data_sz / BPF_INSN_SZ ; /* insns_cnt can later be increased by appending used subprograms */ prog -> insns_cnt = prog -> sec_insn_cnt ; prog -> type = BPF_PROG_TYPE_UNSPEC ; prog -> fd = - 1 ; prog -> exception_cb_idx = - 1 ; /* libbpf's convention for SEC(\"?abc...\") is that it's just like\n\t * SEC(\"abc...\") but the corresponding bpf_program starts out with\n\t * autoload set to false.\n\t */ if ( sec_name [ 0 ] == '?' ) { prog -> autoload = false ; /* from now on forget there was ? in section name */ sec_name ++ ; } else { prog -> autoload = true ; } prog -> autoattach = true ; /* inherit object's log_level */ prog -> log_level = obj -> log_level ; prog -> sec_name = strdup ( sec_name ) ; if ( ! prog -> sec_name ) goto errout ; prog -> name = strdup ( name ) ; if ( ! prog -> name ) goto errout ; prog -> insns = malloc ( insn_data_sz ) ; if ( ! prog -> insns ) goto errout ; memcpy ( prog -> insns , insn_data , insn_data_sz ) ; return 0 ; errout : pr_warn ( \"sec '%s': failed to allocate memory for prog '%s'\\n\" , sec_name , name ) ; bpf_program__exit ( prog ) ; return - ENOMEM ; }",
    "resources/libbpf/src/libbpf.c@bpf_object__add_programs": "static int bpf_object__add_programs ( struct bpf_object * obj , Elf_Data * sec_data , const char * sec_name , int sec_idx ) { Elf_Data * symbols = obj -> efile . symbols ; struct bpf_program * prog , * progs ; void * data = sec_data -> d_buf ; size_t sec_sz = sec_data -> d_size , sec_off , prog_sz , nr_syms ; int nr_progs , err , i ; const char * name ; Elf64_Sym * sym ; progs = obj -> programs ; nr_progs = obj -> nr_programs ; nr_syms = symbols -> d_size / sizeof ( Elf64_Sym ) ; for ( i = 0 ; i < nr_syms ; i ++ ) { sym = elf_sym_by_idx ( obj , i ) ; if ( sym -> st_shndx != sec_idx ) continue ; if ( ELF64_ST_TYPE ( sym -> st_info ) != STT_FUNC ) continue ; prog_sz = sym -> st_size ; sec_off = sym -> st_value ; name = elf_sym_str ( obj , sym -> st_name ) ; if ( ! name ) { pr_warn ( \"sec '%s': failed to get symbol name for offset %zu\\n\" , sec_name , sec_off ) ; return - LIBBPF_ERRNO__FORMAT ; } if ( sec_off + prog_sz > sec_sz ) { pr_warn ( \"sec '%s': program at offset %zu crosses section boundary\\n\" , sec_name , sec_off ) ; return - LIBBPF_ERRNO__FORMAT ; } if ( sec_idx != obj -> efile . text_shndx && ELF64_ST_BIND ( sym -> st_info ) == STB_LOCAL ) { pr_warn ( \"sec '%s': program '%s' is static and not supported\\n\" , sec_name , name ) ; return - ENOTSUP ; } pr_debug ( \"sec '%s': found program '%s' at insn offset %zu (%zu bytes), code size %zu insns (%zu bytes)\\n\" , sec_name , name , sec_off / BPF_INSN_SZ , sec_off , prog_sz / BPF_INSN_SZ , prog_sz ) ; progs = libbpf_reallocarray ( progs , nr_progs + 1 , sizeof ( * progs ) ) ; if ( ! progs ) { /*\n\t\t\t * In this case the original obj->programs\n\t\t\t * is still valid, so don't need special treat for\n\t\t\t * bpf_close_object().\n\t\t\t */ pr_warn ( \"sec '%s': failed to alloc memory for new program '%s'\\n\" , sec_name , name ) ; return - ENOMEM ; } obj -> programs = progs ; prog = & progs [ nr_progs ] ; err = bpf_object__init_prog ( obj , prog , name , sec_idx , sec_name , sec_off , data + sec_off , prog_sz ) ; if ( err ) return err ; if ( ELF64_ST_BIND ( sym -> st_info ) != STB_LOCAL ) prog -> sym_global = true ; /* if function is a global/weak symbol, but has restricted\n\t\t * (STV_HIDDEN or STV_INTERNAL) visibility, mark its BTF FUNC\n\t\t * as static to enable more permissive BPF verification mode\n\t\t * with more outside context available to BPF verifier\n\t\t */ if ( prog -> sym_global && ( ELF64_ST_VISIBILITY ( sym -> st_other ) == STV_HIDDEN || ELF64_ST_VISIBILITY ( sym -> st_other ) == STV_INTERNAL ) ) prog -> mark_btf_static = true ; nr_progs ++ ; obj -> nr_programs = nr_progs ; } return 0 ; }",
    "resources/libbpf/src/libbpf.c@find_member_by_offset": "static const struct btf_member * find_member_by_offset ( const struct btf_type * t , __u32 bit_offset ) { struct btf_member * m ; int i ; for ( i = 0 , m = btf_members ( t ) ; i < btf_vlen ( t ) ; i ++ , m ++ ) { if ( btf_member_bit_offset ( t , i ) == bit_offset ) return m ; } return NULL ; }",
    "resources/libbpf/src/libbpf.c@find_member_by_name": "static const struct btf_member * find_member_by_name ( const struct btf * btf , const struct btf_type * t , const char * name ) { struct btf_member * m ; int i ; for ( i = 0 , m = btf_members ( t ) ; i < btf_vlen ( t ) ; i ++ , m ++ ) { if ( ! strcmp ( btf__name_by_offset ( btf , m -> name_off ) , name ) ) return m ; } return NULL ; }",
    "resources/libbpf/src/libbpf.c@find_struct_ops_kern_types": "static int find_struct_ops_kern_types ( struct bpf_object * obj , const char * tname_raw , struct module_btf * * mod_btf , const struct btf_type * * type , __u32 * type_id , const struct btf_type * * vtype , __u32 * vtype_id , const struct btf_member * * data_member ) { const struct btf_type * kern_type , * kern_vtype ; const struct btf_member * kern_data_member ; struct btf * btf ; __s32 kern_vtype_id , kern_type_id ; char tname [ 256 ] ; __u32 i ; snprintf ( tname , sizeof ( tname ) , \"%.*s\" , ( int ) bpf_core_essential_name_len ( tname_raw ) , tname_raw ) ; kern_type_id = find_ksym_btf_id ( obj , tname , BTF_KIND_STRUCT , & btf , mod_btf ) ; if ( kern_type_id < 0 ) { pr_warn ( \"struct_ops init_kern: struct %s is not found in kernel BTF\\n\" , tname ) ; return kern_type_id ; } kern_type = btf__type_by_id ( btf , kern_type_id ) ; /* Find the corresponding \"map_value\" type that will be used\n\t * in map_update(BPF_MAP_TYPE_STRUCT_OPS).  For example,\n\t * find \"struct bpf_struct_ops_tcp_congestion_ops\" from the\n\t * btf_vmlinux.\n\t */ kern_vtype_id = find_btf_by_prefix_kind ( btf , STRUCT_OPS_VALUE_PREFIX , tname , BTF_KIND_STRUCT ) ; if ( kern_vtype_id < 0 ) { pr_warn ( \"struct_ops init_kern: struct %s%s is not found in kernel BTF\\n\" , STRUCT_OPS_VALUE_PREFIX , tname ) ; return kern_vtype_id ; } kern_vtype = btf__type_by_id ( btf , kern_vtype_id ) ; /* Find \"struct tcp_congestion_ops\" from\n\t * struct bpf_struct_ops_tcp_congestion_ops {\n\t *\t[ ... ]\n\t *\tstruct tcp_congestion_ops data;\n\t * }\n\t */ kern_data_member = btf_members ( kern_vtype ) ; for ( i = 0 ; i < btf_vlen ( kern_vtype ) ; i ++ , kern_data_member ++ ) { if ( kern_data_member -> type == kern_type_id ) break ; } if ( i == btf_vlen ( kern_vtype ) ) { pr_warn ( \"struct_ops init_kern: struct %s data is not found in struct %s%s\\n\" , tname , STRUCT_OPS_VALUE_PREFIX , tname ) ; return - EINVAL ; } * type = kern_type ; * type_id = kern_type_id ; * vtype = kern_vtype ; * vtype_id = kern_vtype_id ; * data_member = kern_data_member ; return 0 ; }",
    "resources/libbpf/src/libbpf.c@bpf_map__is_struct_ops": "static bool bpf_map__is_struct_ops ( const struct bpf_map * map ) { return map -> def . type == BPF_MAP_TYPE_STRUCT_OPS ; }",
    "resources/libbpf/src/libbpf.c@is_valid_st_ops_program": "static bool is_valid_st_ops_program ( struct bpf_object * obj , const struct bpf_program * prog ) { int i ; for ( i = 0 ; i < obj -> nr_programs ; i ++ ) { if ( & obj -> programs [ i ] == prog ) return prog -> type == BPF_PROG_TYPE_STRUCT_OPS ; } return false ; }",
    "resources/libbpf/src/libbpf.c@bpf_object_adjust_struct_ops_autoload": "static int bpf_object_adjust_struct_ops_autoload ( struct bpf_object * obj ) { struct bpf_program * prog , * slot_prog ; struct bpf_map * map ; int i , j , k , vlen ; for ( i = 0 ; i < obj -> nr_programs ; ++ i ) { int should_load = false ; int use_cnt = 0 ; prog = & obj -> programs [ i ] ; if ( prog -> type != BPF_PROG_TYPE_STRUCT_OPS ) continue ; for ( j = 0 ; j < obj -> nr_maps ; ++ j ) { map = & obj -> maps [ j ] ; if ( ! bpf_map__is_struct_ops ( map ) ) continue ; vlen = btf_vlen ( map -> st_ops -> type ) ; for ( k = 0 ; k < vlen ; ++ k ) { slot_prog = map -> st_ops -> progs [ k ] ; if ( prog != slot_prog ) continue ; use_cnt ++ ; if ( map -> autocreate ) should_load = true ; } } if ( use_cnt ) prog -> autoload = should_load ; } return 0 ; }",
    "resources/libbpf/src/libbpf.c@bpf_map__init_kern_struct_ops": "static int bpf_map__init_kern_struct_ops ( struct bpf_map * map ) { const struct btf_member * member , * kern_member , * kern_data_member ; const struct btf_type * type , * kern_type , * kern_vtype ; __u32 i , kern_type_id , kern_vtype_id , kern_data_off ; struct bpf_object * obj = map -> obj ; const struct btf * btf = obj -> btf ; struct bpf_struct_ops * st_ops ; const struct btf * kern_btf ; struct module_btf * mod_btf ; void * data , * kern_data ; const char * tname ; int err ; st_ops = map -> st_ops ; type = st_ops -> type ; tname = st_ops -> tname ; err = find_struct_ops_kern_types ( obj , tname , & mod_btf , & kern_type , & kern_type_id , & kern_vtype , & kern_vtype_id , & kern_data_member ) ; if ( err ) return err ; kern_btf = mod_btf ? mod_btf -> btf : obj -> btf_vmlinux ; pr_debug ( \"struct_ops init_kern %s: type_id:%u kern_type_id:%u kern_vtype_id:%u\\n\" , map -> name , st_ops -> type_id , kern_type_id , kern_vtype_id ) ; map -> mod_btf_fd = mod_btf ? mod_btf -> fd : - 1 ; map -> def . value_size = kern_vtype -> size ; map -> btf_vmlinux_value_type_id = kern_vtype_id ; st_ops -> kern_vdata = calloc ( 1 , kern_vtype -> size ) ; if ( ! st_ops -> kern_vdata ) return - ENOMEM ; data = st_ops -> data ; kern_data_off = kern_data_member -> offset / 8 ; kern_data = st_ops -> kern_vdata + kern_data_off ; member = btf_members ( type ) ; for ( i = 0 ; i < btf_vlen ( type ) ; i ++ , member ++ ) { const struct btf_type * mtype , * kern_mtype ; __u32 mtype_id , kern_mtype_id ; void * mdata , * kern_mdata ; __s64 msize , kern_msize ; __u32 moff , kern_moff ; __u32 kern_member_idx ; const char * mname ; mname = btf__name_by_offset ( btf , member -> name_off ) ; moff = member -> offset / 8 ; mdata = data + moff ; msize = btf__resolve_size ( btf , member -> type ) ; if ( msize < 0 ) { pr_warn ( \"struct_ops init_kern %s: failed to resolve the size of member %s\\n\" , map -> name , mname ) ; return msize ; } kern_member = find_member_by_name ( kern_btf , kern_type , mname ) ; if ( ! kern_member ) { /* Skip all zeros or null fields if they are not\n\t\t\t * presented in the kernel BTF.\n\t\t\t */ if ( libbpf_is_mem_zeroed ( mdata , msize ) ) { pr_info ( \"struct_ops %s: member %s not found in kernel, skipping it as it's set to zero\\n\" , map -> name , mname ) ; continue ; } pr_warn ( \"struct_ops init_kern %s: Cannot find member %s in kernel BTF\\n\" , map -> name , mname ) ; return - ENOTSUP ; } kern_member_idx = kern_member - btf_members ( kern_type ) ; if ( btf_member_bitfield_size ( type , i ) || btf_member_bitfield_size ( kern_type , kern_member_idx ) ) { pr_warn ( \"struct_ops init_kern %s: bitfield %s is not supported\\n\" , map -> name , mname ) ; return - ENOTSUP ; } kern_moff = kern_member -> offset / 8 ; kern_mdata = kern_data + kern_moff ; mtype = skip_mods_and_typedefs ( btf , member -> type , & mtype_id ) ; kern_mtype = skip_mods_and_typedefs ( kern_btf , kern_member -> type , & kern_mtype_id ) ; if ( BTF_INFO_KIND ( mtype -> info ) != BTF_INFO_KIND ( kern_mtype -> info ) ) { pr_warn ( \"struct_ops init_kern %s: Unmatched member type %s %u != %u(kernel)\\n\" , map -> name , mname , BTF_INFO_KIND ( mtype -> info ) , BTF_INFO_KIND ( kern_mtype -> info ) ) ; return - ENOTSUP ; } if ( btf_is_ptr ( mtype ) ) { struct bpf_program * prog ; /* Update the value from the shadow type */ prog = * ( void * * ) mdata ; st_ops -> progs [ i ] = prog ; if ( ! prog ) continue ; if ( ! is_valid_st_ops_program ( obj , prog ) ) { pr_warn ( \"struct_ops init_kern %s: member %s is not a struct_ops program\\n\" , map -> name , mname ) ; return - ENOTSUP ; } kern_mtype = skip_mods_and_typedefs ( kern_btf , kern_mtype -> type , & kern_mtype_id ) ; /* mtype->type must be a func_proto which was\n\t\t\t * guaranteed in bpf_object__collect_st_ops_relos(),\n\t\t\t * so only check kern_mtype for func_proto here.\n\t\t\t */ if ( ! btf_is_func_proto ( kern_mtype ) ) { pr_warn ( \"struct_ops init_kern %s: kernel member %s is not a func ptr\\n\" , map -> name , mname ) ; return - ENOTSUP ; } if ( mod_btf ) prog -> attach_btf_obj_fd = mod_btf -> fd ; /* if we haven't yet processed this BPF program, record proper\n\t\t\t * attach_btf_id and member_idx\n\t\t\t */ if ( ! prog -> attach_btf_id ) { prog -> attach_btf_id = kern_type_id ; prog -> expected_attach_type = kern_member_idx ; } /* struct_ops BPF prog can be re-used between multiple\n\t\t\t * .struct_ops & .struct_ops.link as long as it's the\n\t\t\t * same struct_ops struct definition and the same\n\t\t\t * function pointer field\n\t\t\t */ if ( prog -> attach_btf_id != kern_type_id ) { pr_warn ( \"struct_ops init_kern %s func ptr %s: invalid reuse of prog %s in sec %s with type %u: attach_btf_id %u != kern_type_id %u\\n\" , map -> name , mname , prog -> name , prog -> sec_name , prog -> type , prog -> attach_btf_id , kern_type_id ) ; return - EINVAL ; } if ( prog -> expected_attach_type != kern_member_idx ) { pr_warn ( \"struct_ops init_kern %s func ptr %s: invalid reuse of prog %s in sec %s with type %u: expected_attach_type %u != kern_member_idx %u\\n\" , map -> name , mname , prog -> name , prog -> sec_name , prog -> type , prog -> expected_attach_type , kern_member_idx ) ; return - EINVAL ; } st_ops -> kern_func_off [ i ] = kern_data_off + kern_moff ; pr_debug ( \"struct_ops init_kern %s: func ptr %s is set to prog %s from data(+%u) to kern_data(+%u)\\n\" , map -> name , mname , prog -> name , moff , kern_moff ) ; continue ; } kern_msize = btf__resolve_size ( kern_btf , kern_mtype_id ) ; if ( kern_msize < 0 || msize != kern_msize ) { pr_warn ( \"struct_ops init_kern %s: Error in size of member %s: %zd != %zd(kernel)\\n\" , map -> name , mname , ( ssize_t ) msize , ( ssize_t ) kern_msize ) ; return - ENOTSUP ; } pr_debug ( \"struct_ops init_kern %s: copy %s %u bytes from data(+%u) to kern_data(+%u)\\n\" , map -> name , mname , ( unsigned int ) msize , moff , kern_moff ) ; memcpy ( kern_mdata , mdata , msize ) ; } return 0 ; }",
    "resources/libbpf/src/libbpf.c@bpf_object__init_kern_struct_ops_maps": "static int bpf_object__init_kern_struct_ops_maps ( struct bpf_object * obj ) { struct bpf_map * map ; size_t i ; int err ; for ( i = 0 ; i < obj -> nr_maps ; i ++ ) { map = & obj -> maps [ i ] ; if ( ! bpf_map__is_struct_ops ( map ) ) continue ; if ( ! map -> autocreate ) continue ; err = bpf_map__init_kern_struct_ops ( map ) ; if ( err ) return err ; } return 0 ; }",
    "resources/libbpf/src/libbpf.c@init_struct_ops_maps": "static int init_struct_ops_maps ( struct bpf_object * obj , const char * sec_name , int shndx , Elf_Data * data ) { const struct btf_type * type , * datasec ; const struct btf_var_secinfo * vsi ; struct bpf_struct_ops * st_ops ; const char * tname , * var_name ; __s32 type_id , datasec_id ; const struct btf * btf ; struct bpf_map * map ; __u32 i ; if ( shndx == - 1 ) return 0 ; btf = obj -> btf ; datasec_id = btf__find_by_name_kind ( btf , sec_name , BTF_KIND_DATASEC ) ; if ( datasec_id < 0 ) { pr_warn ( \"struct_ops init: DATASEC %s not found\\n\" , sec_name ) ; return - EINVAL ; } datasec = btf__type_by_id ( btf , datasec_id ) ; vsi = btf_var_secinfos ( datasec ) ; for ( i = 0 ; i < btf_vlen ( datasec ) ; i ++ , vsi ++ ) { type = btf__type_by_id ( obj -> btf , vsi -> type ) ; var_name = btf__name_by_offset ( obj -> btf , type -> name_off ) ; type_id = btf__resolve_type ( obj -> btf , vsi -> type ) ; if ( type_id < 0 ) { pr_warn ( \"struct_ops init: Cannot resolve var type_id %u in DATASEC %s\\n\" , vsi -> type , sec_name ) ; return - EINVAL ; } type = btf__type_by_id ( obj -> btf , type_id ) ; tname = btf__name_by_offset ( obj -> btf , type -> name_off ) ; if ( ! tname [ 0 ] ) { pr_warn ( \"struct_ops init: anonymous type is not supported\\n\" ) ; return - ENOTSUP ; } if ( ! btf_is_struct ( type ) ) { pr_warn ( \"struct_ops init: %s is not a struct\\n\" , tname ) ; return - EINVAL ; } map = bpf_object__add_map ( obj ) ; if ( IS_ERR ( map ) ) return PTR_ERR ( map ) ; map -> sec_idx = shndx ; map -> sec_offset = vsi -> offset ; map -> name = strdup ( var_name ) ; if ( ! map -> name ) return - ENOMEM ; map -> btf_value_type_id = type_id ; /* Follow same convention as for programs autoload:\n\t\t * SEC(\"?.struct_ops\") means map is not created by default.\n\t\t */ if ( sec_name [ 0 ] == '?' ) { map -> autocreate = false ; /* from now on forget there was ? in section name */ sec_name ++ ; } map -> def . type = BPF_MAP_TYPE_STRUCT_OPS ; map -> def . key_size = sizeof ( int ) ; map -> def . value_size = type -> size ; map -> def . max_entries = 1 ; map -> def . map_flags = strcmp ( sec_name , STRUCT_OPS_LINK_SEC ) == 0 ? BPF_F_LINK : 0 ; map -> st_ops = calloc ( 1 , sizeof ( * map -> st_ops ) ) ; if ( ! map -> st_ops ) return - ENOMEM ; st_ops = map -> st_ops ; st_ops -> data = malloc ( type -> size ) ; st_ops -> progs = calloc ( btf_vlen ( type ) , sizeof ( * st_ops -> progs ) ) ; st_ops -> kern_func_off = malloc ( btf_vlen ( type ) * sizeof ( * st_ops -> kern_func_off ) ) ; if ( ! st_ops -> data || ! st_ops -> progs || ! st_ops -> kern_func_off ) return - ENOMEM ; if ( vsi -> offset + type -> size > data -> d_size ) { pr_warn ( \"struct_ops init: var %s is beyond the end of DATASEC %s\\n\" , var_name , sec_name ) ; return - EINVAL ; } memcpy ( st_ops -> data , data -> d_buf + vsi -> offset , type -> size ) ; st_ops -> tname = tname ; st_ops -> type = type ; st_ops -> type_id = type_id ; pr_debug ( \"struct_ops init: struct %s(type_id=%u) %s found at offset %u\\n\" , tname , type_id , var_name , vsi -> offset ) ; } return 0 ; }",
    "resources/libbpf/src/libbpf.c@bpf_object_init_struct_ops": "static int bpf_object_init_struct_ops ( struct bpf_object * obj ) { const char * sec_name ; int sec_idx , err ; for ( sec_idx = 0 ; sec_idx < obj -> efile . sec_cnt ; ++ sec_idx ) { struct elf_sec_desc * desc = & obj -> efile . secs [ sec_idx ] ; if ( desc -> sec_type != SEC_ST_OPS ) continue ; sec_name = elf_sec_name ( obj , elf_sec_by_idx ( obj , sec_idx ) ) ; if ( ! sec_name ) return - LIBBPF_ERRNO__FORMAT ; err = init_struct_ops_maps ( obj , sec_name , sec_idx , desc -> data ) ; if ( err ) return err ; } return 0 ; }",
    "resources/libbpf/src/libbpf.c@bpf_object__new": "static struct bpf_object * bpf_object__new ( const char * path , const void * obj_buf , size_t obj_buf_sz , const char * obj_name ) { struct bpf_object * obj ; char * end ; obj = calloc ( 1 , sizeof ( struct bpf_object ) + strlen ( path ) + 1 ) ; if ( ! obj ) { pr_warn ( \"alloc memory failed for %s\\n\" , path ) ; return ERR_PTR ( - ENOMEM ) ; } strcpy ( obj -> path , path ) ; if ( obj_name ) { libbpf_strlcpy ( obj -> name , obj_name , sizeof ( obj -> name ) ) ; } else { /* Using basename() GNU version which doesn't modify arg. */ libbpf_strlcpy ( obj -> name , basename ( ( void * ) path ) , sizeof ( obj -> name ) ) ; end = strchr ( obj -> name , '.' ) ; if ( end ) * end = 0 ; } obj -> efile . fd = - 1 ; /*\n\t * Caller of this function should also call\n\t * bpf_object__elf_finish() after data collection to return\n\t * obj_buf to user. If not, we should duplicate the buffer to\n\t * avoid user freeing them before elf finish.\n\t */ obj -> efile . obj_buf = obj_buf ; obj -> efile . obj_buf_sz = obj_buf_sz ; obj -> efile . btf_maps_shndx = - 1 ; obj -> kconfig_map_idx = - 1 ; obj -> kern_version = get_kernel_version ( ) ; obj -> loaded = false ; return obj ; }",
    "resources/libbpf/src/libbpf.c@bpf_object__elf_finish": "static void bpf_object__elf_finish ( struct bpf_object * obj ) { if ( ! obj -> efile . elf ) return ; elf_end ( obj -> efile . elf ) ; obj -> efile . elf = NULL ; obj -> efile . symbols = NULL ; obj -> efile . arena_data = NULL ; zfree ( & obj -> efile . secs ) ; obj -> efile . sec_cnt = 0 ; zclose ( obj -> efile . fd ) ; obj -> efile . obj_buf = NULL ; obj -> efile . obj_buf_sz = 0 ; }",
    "resources/libbpf/src/libbpf.c@bpf_object__elf_init": "static int bpf_object__elf_init ( struct bpf_object * obj ) { Elf64_Ehdr * ehdr ; int err = 0 ; Elf * elf ; if ( obj -> efile . elf ) { pr_warn ( \"elf: init internal error\\n\" ) ; return - LIBBPF_ERRNO__LIBELF ; } if ( obj -> efile . obj_buf_sz > 0 ) { /* obj_buf should have been validated by bpf_object__open_mem(). */ elf = elf_memory ( ( char * ) obj -> efile . obj_buf , obj -> efile . obj_buf_sz ) ; } else { obj -> efile . fd = open ( obj -> path , O_RDONLY | O_CLOEXEC ) ; if ( obj -> efile . fd < 0 ) { char errmsg [ STRERR_BUFSIZE ] , * cp ; err = - errno ; cp = libbpf_strerror_r ( err , errmsg , sizeof ( errmsg ) ) ; pr_warn ( \"elf: failed to open %s: %s\\n\" , obj -> path , cp ) ; return err ; } elf = elf_begin ( obj -> efile . fd , ELF_C_READ_MMAP , NULL ) ; } if ( ! elf ) { pr_warn ( \"elf: failed to open %s as ELF file: %s\\n\" , obj -> path , elf_errmsg ( - 1 ) ) ; err = - LIBBPF_ERRNO__LIBELF ; goto errout ; } obj -> efile . elf = elf ; if ( elf_kind ( elf ) != ELF_K_ELF ) { err = - LIBBPF_ERRNO__FORMAT ; pr_warn ( \"elf: '%s' is not a proper ELF object\\n\" , obj -> path ) ; goto errout ; } if ( gelf_getclass ( elf ) != ELFCLASS64 ) { err = - LIBBPF_ERRNO__FORMAT ; pr_warn ( \"elf: '%s' is not a 64-bit ELF object\\n\" , obj -> path ) ; goto errout ; } obj -> efile . ehdr = ehdr = elf64_getehdr ( elf ) ; if ( ! obj -> efile . ehdr ) { pr_warn ( \"elf: failed to get ELF header from %s: %s\\n\" , obj -> path , elf_errmsg ( - 1 ) ) ; err = - LIBBPF_ERRNO__FORMAT ; goto errout ; } if ( elf_getshdrstrndx ( elf , & obj -> efile . shstrndx ) ) { pr_warn ( \"elf: failed to get section names section index for %s: %s\\n\" , obj -> path , elf_errmsg ( - 1 ) ) ; err = - LIBBPF_ERRNO__FORMAT ; goto errout ; } /* ELF is corrupted/truncated, avoid calling elf_strptr. */ if ( ! elf_rawdata ( elf_getscn ( elf , obj -> efile . shstrndx ) , NULL ) ) { pr_warn ( \"elf: failed to get section names strings from %s: %s\\n\" , obj -> path , elf_errmsg ( - 1 ) ) ; err = - LIBBPF_ERRNO__FORMAT ; goto errout ; } /* Old LLVM set e_machine to EM_NONE */ if ( ehdr -> e_type != ET_REL || ( ehdr -> e_machine && ehdr -> e_machine != EM_BPF ) ) { pr_warn ( \"elf: %s is not a valid eBPF object file\\n\" , obj -> path ) ; err = - LIBBPF_ERRNO__FORMAT ; goto errout ; } return 0 ; errout : bpf_object__elf_finish ( obj ) ; return err ; }",
    "resources/libbpf/src/libbpf.c@bpf_object__check_endianness": "static int bpf_object__check_endianness ( struct bpf_object * obj ) { # if __BYTE_ORDER__ == __ORDER_LITTLE_ENDIAN__ if ( obj -> efile . ehdr -> e_ident [ EI_DATA ] == ELFDATA2LSB ) return 0 ; # elif __BYTE_ORDER__ == __ORDER_BIG_ENDIAN__ if ( obj -> efile . ehdr -> e_ident [ EI_DATA ] == ELFDATA2MSB ) return 0 ; # else # error \"Unrecognized __BYTE_ORDER__\" # endif pr_warn ( \"elf: endianness mismatch in %s.\\n\" , obj -> path ) ; return - LIBBPF_ERRNO__ENDIAN ; }",
    "resources/libbpf/src/libbpf.c@bpf_object__init_license": "static int bpf_object__init_license ( struct bpf_object * obj , void * data , size_t size ) { if ( ! data ) { pr_warn ( \"invalid license section in %s\\n\" , obj -> path ) ; return - LIBBPF_ERRNO__FORMAT ; } /* libbpf_strlcpy() only copies first N - 1 bytes, so size + 1 won't\n\t * go over allowed ELF data section buffer\n\t */ libbpf_strlcpy ( obj -> license , data , min ( size + 1 , sizeof ( obj -> license ) ) ) ; pr_debug ( \"license of %s is %s\\n\" , obj -> path , obj -> license ) ; return 0 ; }",
    "resources/libbpf/src/libbpf.c@bpf_object__init_kversion": "static int bpf_object__init_kversion ( struct bpf_object * obj , void * data , size_t size ) { __u32 kver ; if ( ! data || size != sizeof ( kver ) ) { pr_warn ( \"invalid kver section in %s\\n\" , obj -> path ) ; return - LIBBPF_ERRNO__FORMAT ; } memcpy ( & kver , data , sizeof ( kver ) ) ; obj -> kern_version = kver ; pr_debug ( \"kernel version of %s is %x\\n\" , obj -> path , obj -> kern_version ) ; return 0 ; }",
    "resources/libbpf/src/libbpf.c@bpf_map_type__is_map_in_map": "static bool bpf_map_type__is_map_in_map ( enum bpf_map_type type ) { if ( type == BPF_MAP_TYPE_ARRAY_OF_MAPS || type == BPF_MAP_TYPE_HASH_OF_MAPS ) return true ; return false ; }",
    "resources/libbpf/src/libbpf.c@find_elf_sec_sz": "static int find_elf_sec_sz ( const struct bpf_object * obj , const char * name , __u32 * size ) { Elf_Data * data ; Elf_Scn * scn ; if ( ! name ) return - EINVAL ; scn = elf_sec_by_name ( obj , name ) ; data = elf_sec_data ( obj , scn ) ; if ( data ) { * size = data -> d_size ; return 0 ; /* found it */ } return - ENOENT ; }",
    "resources/libbpf/src/libbpf.c@find_elf_var_sym": "static Elf64_Sym * find_elf_var_sym ( const struct bpf_object * obj , const char * name ) { Elf_Data * symbols = obj -> efile . symbols ; const char * sname ; size_t si ; for ( si = 0 ; si < symbols -> d_size / sizeof ( Elf64_Sym ) ; si ++ ) { Elf64_Sym * sym = elf_sym_by_idx ( obj , si ) ; if ( ELF64_ST_TYPE ( sym -> st_info ) != STT_OBJECT ) continue ; if ( ELF64_ST_BIND ( sym -> st_info ) != STB_GLOBAL && ELF64_ST_BIND ( sym -> st_info ) != STB_WEAK ) continue ; sname = elf_sym_str ( obj , sym -> st_name ) ; if ( ! sname ) { pr_warn ( \"failed to get sym name string for var %s\\n\" , name ) ; return ERR_PTR ( - EIO ) ; } if ( strcmp ( name , sname ) == 0 ) return sym ; } return ERR_PTR ( - ENOENT ) ; }",
    "resources/libbpf/src/libbpf.c@sys_memfd_create": "static int sys_memfd_create ( const char * name , unsigned flags ) { return syscall ( __NR_memfd_create , name , flags ) ; }",
    "resources/libbpf/src/libbpf.c@create_placeholder_fd": "static int create_placeholder_fd ( void ) { int fd ; fd = ensure_good_fd ( sys_memfd_create ( \"libbpf-placeholder-fd\" , MFD_CLOEXEC ) ) ; if ( fd < 0 ) return - errno ; return fd ; }",
    "resources/libbpf/src/libbpf.c@bpf_object__add_map": "static struct bpf_map * bpf_object__add_map ( struct bpf_object * obj ) { struct bpf_map * map ; int err ; err = libbpf_ensure_mem ( ( void * * ) & obj -> maps , & obj -> maps_cap , sizeof ( * obj -> maps ) , obj -> nr_maps + 1 ) ; if ( err ) return ERR_PTR ( err ) ; map = & obj -> maps [ obj -> nr_maps ++ ] ; map -> obj = obj ; /* Preallocate map FD without actually creating BPF map just yet.\n\t * These map FD \"placeholders\" will be reused later without changing\n\t * FD value when map is actually created in the kernel.\n\t *\n\t * This is useful to be able to perform BPF program relocations\n\t * without having to create BPF maps before that step. This allows us\n\t * to finalize and load BTF very late in BPF object's loading phase,\n\t * right before BPF maps have to be created and BPF programs have to\n\t * be loaded. By having these map FD placeholders we can perform all\n\t * the sanitizations, relocations, and any other adjustments before we\n\t * start creating actual BPF kernel objects (BTF, maps, progs).\n\t */ map -> fd = create_placeholder_fd ( ) ; if ( map -> fd < 0 ) return ERR_PTR ( map -> fd ) ; map -> inner_map_fd = - 1 ; map -> autocreate = true ; return map ; }",
    "resources/libbpf/src/libbpf.c@array_map_mmap_sz": "static size_t array_map_mmap_sz ( unsigned int value_sz , unsigned int max_entries ) { const long page_sz = sysconf ( _SC_PAGE_SIZE ) ; size_t map_sz ; map_sz = ( size_t ) roundup ( value_sz , 8 ) * max_entries ; map_sz = roundup ( map_sz , page_sz ) ; return map_sz ; }",
    "resources/libbpf/src/libbpf.c@bpf_map_mmap_sz": "static size_t bpf_map_mmap_sz ( const struct bpf_map * map ) { const long page_sz = sysconf ( _SC_PAGE_SIZE ) ; switch ( map -> def . type ) { case BPF_MAP_TYPE_ARRAY : return array_map_mmap_sz ( map -> def . value_size , map -> def . max_entries ) ; case BPF_MAP_TYPE_ARENA : return page_sz * map -> def . max_entries ; default : return 0 ; /* not supported */ } }",
    "resources/libbpf/src/libbpf.c@bpf_map_mmap_resize": "static int bpf_map_mmap_resize ( struct bpf_map * map , size_t old_sz , size_t new_sz ) { void * mmaped ; if ( ! map -> mmaped ) return - EINVAL ; if ( old_sz == new_sz ) return 0 ; mmaped = mmap ( NULL , new_sz , PROT_READ | PROT_WRITE , MAP_SHARED | MAP_ANONYMOUS , - 1 , 0 ) ; if ( mmaped == MAP_FAILED ) return - errno ; memcpy ( mmaped , map -> mmaped , min ( old_sz , new_sz ) ) ; munmap ( map -> mmaped , old_sz ) ; map -> mmaped = mmaped ; return 0 ; }",
    "resources/libbpf/src/libbpf.c@internal_map_name": "static char * internal_map_name ( struct bpf_object * obj , const char * real_name ) { char map_name [ BPF_OBJ_NAME_LEN ] , * p ; int pfx_len , sfx_len = max ( ( size_t ) 7 , strlen ( real_name ) ) ; /* This is one of the more confusing parts of libbpf for various\n\t * reasons, some of which are historical. The original idea for naming\n\t * internal names was to include as much of BPF object name prefix as\n\t * possible, so that it can be distinguished from similar internal\n\t * maps of a different BPF object.\n\t * As an example, let's say we have bpf_object named 'my_object_name'\n\t * and internal map corresponding to '.rodata' ELF section. The final\n\t * map name advertised to user and to the kernel will be\n\t * 'my_objec.rodata', taking first 8 characters of object name and\n\t * entire 7 characters of '.rodata'.\n\t * Somewhat confusingly, if internal map ELF section name is shorter\n\t * than 7 characters, e.g., '.bss', we still reserve 7 characters\n\t * for the suffix, even though we only have 4 actual characters, and\n\t * resulting map will be called 'my_objec.bss', not even using all 15\n\t * characters allowed by the kernel. Oh well, at least the truncated\n\t * object name is somewhat consistent in this case. But if the map\n\t * name is '.kconfig', we'll still have entirety of '.kconfig' added\n\t * (8 chars) and thus will be left with only first 7 characters of the\n\t * object name ('my_obje'). Happy guessing, user, that the final map\n\t * name will be \"my_obje.kconfig\".\n\t * Now, with libbpf starting to support arbitrarily named .rodata.*\n\t * and .data.* data sections, it's possible that ELF section name is\n\t * longer than allowed 15 chars, so we now need to be careful to take\n\t * only up to 15 first characters of ELF name, taking no BPF object\n\t * name characters at all. So '.rodata.abracadabra' will result in\n\t * '.rodata.abracad' kernel and user-visible name.\n\t * We need to keep this convoluted logic intact for .data, .bss and\n\t * .rodata maps, but for new custom .data.custom and .rodata.custom\n\t * maps we use their ELF names as is, not prepending bpf_object name\n\t * in front. We still need to truncate them to 15 characters for the\n\t * kernel. Full name can be recovered for such maps by using DATASEC\n\t * BTF type associated with such map's value type, though.\n\t */ if ( sfx_len >= BPF_OBJ_NAME_LEN ) sfx_len = BPF_OBJ_NAME_LEN - 1 ; /* if there are two or more dots in map name, it's a custom dot map */ if ( strchr ( real_name + 1 , '.' ) != NULL ) pfx_len = 0 ; else pfx_len = min ( ( size_t ) BPF_OBJ_NAME_LEN - sfx_len - 1 , strlen ( obj -> name ) ) ; snprintf ( map_name , sizeof ( map_name ) , \"%.*s%.*s\" , pfx_len , obj -> name , sfx_len , real_name ) ; /* sanitise map name to characters allowed by kernel */ for ( p = map_name ; * p && p < map_name + sizeof ( map_name ) ; p ++ ) if ( ! isalnum ( * p ) && * p != '_' && * p != '.' ) * p = '_' ; return strdup ( map_name ) ; }",
    "resources/libbpf/src/libbpf.c@map_is_mmapable": "static bool map_is_mmapable ( struct bpf_object * obj , struct bpf_map * map ) { const struct btf_type * t , * vt ; struct btf_var_secinfo * vsi ; int i , n ; if ( ! map -> btf_value_type_id ) return false ; t = btf__type_by_id ( obj -> btf , map -> btf_value_type_id ) ; if ( ! btf_is_datasec ( t ) ) return false ; vsi = btf_var_secinfos ( t ) ; for ( i = 0 , n = btf_vlen ( t ) ; i < n ; i ++ , vsi ++ ) { vt = btf__type_by_id ( obj -> btf , vsi -> type ) ; if ( ! btf_is_var ( vt ) ) continue ; if ( btf_var ( vt ) -> linkage != BTF_VAR_STATIC ) return true ; } return false ; }",
    "resources/libbpf/src/libbpf.c@bpf_object__init_internal_map": "static int bpf_object__init_internal_map ( struct bpf_object * obj , enum libbpf_map_type type , const char * real_name , int sec_idx , void * data , size_t data_sz ) { struct bpf_map_def * def ; struct bpf_map * map ; size_t mmap_sz ; int err ; map = bpf_object__add_map ( obj ) ; if ( IS_ERR ( map ) ) return PTR_ERR ( map ) ; map -> libbpf_type = type ; map -> sec_idx = sec_idx ; map -> sec_offset = 0 ; map -> real_name = strdup ( real_name ) ; map -> name = internal_map_name ( obj , real_name ) ; if ( ! map -> real_name || ! map -> name ) { zfree ( & map -> real_name ) ; zfree ( & map -> name ) ; return - ENOMEM ; } def = & map -> def ; def -> type = BPF_MAP_TYPE_ARRAY ; def -> key_size = sizeof ( int ) ; def -> value_size = data_sz ; def -> max_entries = 1 ; def -> map_flags = type == LIBBPF_MAP_RODATA || type == LIBBPF_MAP_KCONFIG ? BPF_F_RDONLY_PROG : 0 ; /* failures are fine because of maps like .rodata.str1.1 */ ( void ) map_fill_btf_type_info ( obj , map ) ; if ( map_is_mmapable ( obj , map ) ) def -> map_flags |= BPF_F_MMAPABLE ; pr_debug ( \"map '%s' (global data): at sec_idx %d, offset %zu, flags %x.\\n\" , map -> name , map -> sec_idx , map -> sec_offset , def -> map_flags ) ; mmap_sz = bpf_map_mmap_sz ( map ) ; map -> mmaped = mmap ( NULL , mmap_sz , PROT_READ | PROT_WRITE , MAP_SHARED | MAP_ANONYMOUS , - 1 , 0 ) ; if ( map -> mmaped == MAP_FAILED ) { err = - errno ; map -> mmaped = NULL ; pr_warn ( \"failed to alloc map '%s' content buffer: %d\\n\" , map -> name , err ) ; zfree ( & map -> real_name ) ; zfree ( & map -> name ) ; return err ; } if ( data ) memcpy ( map -> mmaped , data , data_sz ) ; pr_debug ( \"map %td is \\\"%s\\\"\\n\" , map - obj -> maps , map -> name ) ; return 0 ; }",
    "resources/libbpf/src/libbpf.c@bpf_object__init_global_data_maps": "static int bpf_object__init_global_data_maps ( struct bpf_object * obj ) { struct elf_sec_desc * sec_desc ; const char * sec_name ; int err = 0 , sec_idx ; /*\n\t * Populate obj->maps with libbpf internal maps.\n\t */ for ( sec_idx = 1 ; sec_idx < obj -> efile . sec_cnt ; sec_idx ++ ) { sec_desc = & obj -> efile . secs [ sec_idx ] ; /* Skip recognized sections with size 0. */ if ( ! sec_desc -> data || sec_desc -> data -> d_size == 0 ) continue ; switch ( sec_desc -> sec_type ) { case SEC_DATA : sec_name = elf_sec_name ( obj , elf_sec_by_idx ( obj , sec_idx ) ) ; err = bpf_object__init_internal_map ( obj , LIBBPF_MAP_DATA , sec_name , sec_idx , sec_desc -> data -> d_buf , sec_desc -> data -> d_size ) ; break ; case SEC_RODATA : obj -> has_rodata = true ; sec_name = elf_sec_name ( obj , elf_sec_by_idx ( obj , sec_idx ) ) ; err = bpf_object__init_internal_map ( obj , LIBBPF_MAP_RODATA , sec_name , sec_idx , sec_desc -> data -> d_buf , sec_desc -> data -> d_size ) ; break ; case SEC_BSS : sec_name = elf_sec_name ( obj , elf_sec_by_idx ( obj , sec_idx ) ) ; err = bpf_object__init_internal_map ( obj , LIBBPF_MAP_BSS , sec_name , sec_idx , NULL , sec_desc -> data -> d_size ) ; break ; default : /* skip */ break ; } if ( err ) return err ; } return 0 ; }",
    "resources/libbpf/src/libbpf.c@find_extern_by_name": "static struct extern_desc * find_extern_by_name ( const struct bpf_object * obj , const void * name ) { int i ; for ( i = 0 ; i < obj -> nr_extern ; i ++ ) { if ( strcmp ( obj -> externs [ i ] . name , name ) == 0 ) return & obj -> externs [ i ] ; } return NULL ; }",
    "resources/libbpf/src/libbpf.c@set_kcfg_value_tri": "static int set_kcfg_value_tri ( struct extern_desc * ext , void * ext_val , char value ) { switch ( ext -> kcfg . type ) { case KCFG_BOOL : if ( value == 'm' ) { pr_warn ( \"extern (kcfg) '%s': value '%c' implies tristate or char type\\n\" , ext -> name , value ) ; return - EINVAL ; } * ( bool * ) ext_val = value == 'y' ? true : false ; break ; case KCFG_TRISTATE : if ( value == 'y' ) * ( enum libbpf_tristate * ) ext_val = TRI_YES ; else if ( value == 'm' ) * ( enum libbpf_tristate * ) ext_val = TRI_MODULE ; else /* value == 'n' */ * ( enum libbpf_tristate * ) ext_val = TRI_NO ; break ; case KCFG_CHAR : * ( char * ) ext_val = value ; break ; case KCFG_UNKNOWN : case KCFG_INT : case KCFG_CHAR_ARR : default : pr_warn ( \"extern (kcfg) '%s': value '%c' implies bool, tristate, or char type\\n\" , ext -> name , value ) ; return - EINVAL ; } ext -> is_set = true ; return 0 ; }",
    "resources/libbpf/src/libbpf.c@set_kcfg_value_str": "static int set_kcfg_value_str ( struct extern_desc * ext , char * ext_val , const char * value ) { size_t len ; if ( ext -> kcfg . type != KCFG_CHAR_ARR ) { pr_warn ( \"extern (kcfg) '%s': value '%s' implies char array type\\n\" , ext -> name , value ) ; return - EINVAL ; } len = strlen ( value ) ; if ( value [ len - 1 ] != '\"' ) { pr_warn ( \"extern (kcfg) '%s': invalid string config '%s'\\n\" , ext -> name , value ) ; return - EINVAL ; } /* strip quotes */ len -= 2 ; if ( len >= ext -> kcfg . sz ) { pr_warn ( \"extern (kcfg) '%s': long string '%s' of (%zu bytes) truncated to %d bytes\\n\" , ext -> name , value , len , ext -> kcfg . sz - 1 ) ; len = ext -> kcfg . sz - 1 ; } memcpy ( ext_val , value + 1 , len ) ; ext_val [ len ] = '\\0' ; ext -> is_set = true ; return 0 ; }",
    "resources/libbpf/src/libbpf.c@parse_u64": "static int parse_u64 ( const char * value , __u64 * res ) { char * value_end ; int err ; errno = 0 ; * res = strtoull ( value , & value_end , 0 ) ; if ( errno ) { err = - errno ; pr_warn ( \"failed to parse '%s' as integer: %d\\n\" , value , err ) ; return err ; } if ( * value_end ) { pr_warn ( \"failed to parse '%s' as integer completely\\n\" , value ) ; return - EINVAL ; } return 0 ; }",
    "resources/libbpf/src/libbpf.c@is_kcfg_value_in_range": "static bool is_kcfg_value_in_range ( const struct extern_desc * ext , __u64 v ) { int bit_sz = ext -> kcfg . sz * 8 ; if ( ext -> kcfg . sz == 8 ) return true ; /* Validate that value stored in u64 fits in integer of `ext->sz`\n\t * bytes size without any loss of information. If the target integer\n\t * is signed, we rely on the following limits of integer type of\n\t * Y bits and subsequent transformation:\n\t *\n\t *     -2^(Y-1) <= X           <= 2^(Y-1) - 1\n\t *            0 <= X + 2^(Y-1) <= 2^Y - 1\n\t *            0 <= X + 2^(Y-1) <  2^Y\n\t *\n\t *  For unsigned target integer, check that all the (64 - Y) bits are\n\t *  zero.\n\t */ if ( ext -> kcfg . is_signed ) return v + ( 1ULL << ( bit_sz - 1 ) ) < ( 1ULL << bit_sz ) ; else return ( v >> bit_sz ) == 0 ; }",
    "resources/libbpf/src/libbpf.c@set_kcfg_value_num": "static int set_kcfg_value_num ( struct extern_desc * ext , void * ext_val , __u64 value ) { if ( ext -> kcfg . type != KCFG_INT && ext -> kcfg . type != KCFG_CHAR && ext -> kcfg . type != KCFG_BOOL ) { pr_warn ( \"extern (kcfg) '%s': value '%llu' implies integer, char, or boolean type\\n\" , ext -> name , ( unsigned long long ) value ) ; return - EINVAL ; } if ( ext -> kcfg . type == KCFG_BOOL && value > 1 ) { pr_warn ( \"extern (kcfg) '%s': value '%llu' isn't boolean compatible\\n\" , ext -> name , ( unsigned long long ) value ) ; return - EINVAL ; } if ( ! is_kcfg_value_in_range ( ext , value ) ) { pr_warn ( \"extern (kcfg) '%s': value '%llu' doesn't fit in %d bytes\\n\" , ext -> name , ( unsigned long long ) value , ext -> kcfg . sz ) ; return - ERANGE ; } switch ( ext -> kcfg . sz ) { case 1 : * ( __u8 * ) ext_val = value ; break ; case 2 : * ( __u16 * ) ext_val = value ; break ; case 4 : * ( __u32 * ) ext_val = value ; break ; case 8 : * ( __u64 * ) ext_val = value ; break ; default : return - EINVAL ; } ext -> is_set = true ; return 0 ; }",
    "resources/libbpf/src/libbpf.c@bpf_object__process_kconfig_line": "static int bpf_object__process_kconfig_line ( struct bpf_object * obj , char * buf , void * data ) { struct extern_desc * ext ; char * sep , * value ; int len , err = 0 ; void * ext_val ; __u64 num ; if ( ! str_has_pfx ( buf , \"CONFIG_\" ) ) return 0 ; sep = strchr ( buf , '=' ) ; if ( ! sep ) { pr_warn ( \"failed to parse '%s': no separator\\n\" , buf ) ; return - EINVAL ; } /* Trim ending '\\n' */ len = strlen ( buf ) ; if ( buf [ len - 1 ] == '\\n' ) buf [ len - 1 ] = '\\0' ; /* Split on '=' and ensure that a value is present. */ * sep = '\\0' ; if ( ! sep [ 1 ] ) { * sep = '=' ; pr_warn ( \"failed to parse '%s': no value\\n\" , buf ) ; return - EINVAL ; } ext = find_extern_by_name ( obj , buf ) ; if ( ! ext || ext -> is_set ) return 0 ; ext_val = data + ext -> kcfg . data_off ; value = sep + 1 ; switch ( * value ) { case 'y' : case 'n' : case 'm' : err = set_kcfg_value_tri ( ext , ext_val , * value ) ; break ; case '\"' : err = set_kcfg_value_str ( ext , ext_val , value ) ; break ; default : /* assume integer */ err = parse_u64 ( value , & num ) ; if ( err ) { pr_warn ( \"extern (kcfg) '%s': value '%s' isn't a valid integer\\n\" , ext -> name , value ) ; return err ; } if ( ext -> kcfg . type != KCFG_INT && ext -> kcfg . type != KCFG_CHAR ) { pr_warn ( \"extern (kcfg) '%s': value '%s' implies integer type\\n\" , ext -> name , value ) ; return - EINVAL ; } err = set_kcfg_value_num ( ext , ext_val , num ) ; break ; } if ( err ) return err ; pr_debug ( \"extern (kcfg) '%s': set to %s\\n\" , ext -> name , value ) ; return 0 ; }",
    "resources/libbpf/src/libbpf.c@bpf_object__read_kconfig_file": "static int bpf_object__read_kconfig_file ( struct bpf_object * obj , void * data ) { char buf [ PATH_MAX ] ; struct utsname uts ; int len , err = 0 ; gzFile file ; uname ( & uts ) ; len = snprintf ( buf , PATH_MAX , \"/boot/config-%s\" , uts . release ) ; if ( len < 0 ) return - EINVAL ; else if ( len >= PATH_MAX ) return - ENAMETOOLONG ; /* gzopen also accepts uncompressed files. */ file = gzopen ( buf , \"re\" ) ; if ( ! file ) file = gzopen ( \"/proc/config.gz\" , \"re\" ) ; if ( ! file ) { pr_warn ( \"failed to open system Kconfig\\n\" ) ; return - ENOENT ; } while ( gzgets ( file , buf , sizeof ( buf ) ) ) { err = bpf_object__process_kconfig_line ( obj , buf , data ) ; if ( err ) { pr_warn ( \"error parsing system Kconfig line '%s': %d\\n\" , buf , err ) ; goto out ; } } out : gzclose ( file ) ; return err ; }",
    "resources/libbpf/src/libbpf.c@bpf_object__read_kconfig_mem": "static int bpf_object__read_kconfig_mem ( struct bpf_object * obj , const char * config , void * data ) { char buf [ PATH_MAX ] ; int err = 0 ; FILE * file ; file = fmemopen ( ( void * ) config , strlen ( config ) , \"r\" ) ; if ( ! file ) { err = - errno ; pr_warn ( \"failed to open in-memory Kconfig: %d\\n\" , err ) ; return err ; } while ( fgets ( buf , sizeof ( buf ) , file ) ) { err = bpf_object__process_kconfig_line ( obj , buf , data ) ; if ( err ) { pr_warn ( \"error parsing in-memory Kconfig line '%s': %d\\n\" , buf , err ) ; break ; } } fclose ( file ) ; return err ; }",
    "resources/libbpf/src/libbpf.c@bpf_object__init_kconfig_map": "static int bpf_object__init_kconfig_map ( struct bpf_object * obj ) { struct extern_desc * last_ext = NULL , * ext ; size_t map_sz ; int i , err ; for ( i = 0 ; i < obj -> nr_extern ; i ++ ) { ext = & obj -> externs [ i ] ; if ( ext -> type == EXT_KCFG ) last_ext = ext ; } if ( ! last_ext ) return 0 ; map_sz = last_ext -> kcfg . data_off + last_ext -> kcfg . sz ; err = bpf_object__init_internal_map ( obj , LIBBPF_MAP_KCONFIG , \".kconfig\" , obj -> efile . symbols_shndx , NULL , map_sz ) ; if ( err ) return err ; obj -> kconfig_map_idx = obj -> nr_maps - 1 ; return 0 ; }",
    "resources/libbpf/src/libbpf.c@skip_mods_and_typedefs": "const struct btf_type * skip_mods_and_typedefs ( const struct btf * btf , __u32 id , __u32 * res_id ) { const struct btf_type * t = btf__type_by_id ( btf , id ) ; if ( res_id ) * res_id = id ; while ( btf_is_mod ( t ) || btf_is_typedef ( t ) ) { if ( res_id ) * res_id = t -> type ; t = btf__type_by_id ( btf , t -> type ) ; } return t ; }",
    "resources/libbpf/src/libbpf.c@resolve_func_ptr": "static const struct btf_type * resolve_func_ptr ( const struct btf * btf , __u32 id , __u32 * res_id ) { const struct btf_type * t ; t = skip_mods_and_typedefs ( btf , id , NULL ) ; if ( ! btf_is_ptr ( t ) ) return NULL ; t = skip_mods_and_typedefs ( btf , t -> type , res_id ) ; return btf_is_func_proto ( t ) ? t : NULL ; }",
    "resources/libbpf/src/libbpf.c@__btf_kind_str": "static const char * __btf_kind_str ( __u16 kind ) { switch ( kind ) { case BTF_KIND_UNKN : return \"void\" ; case BTF_KIND_INT : return \"int\" ; case BTF_KIND_PTR : return \"ptr\" ; case BTF_KIND_ARRAY : return \"array\" ; case BTF_KIND_STRUCT : return \"struct\" ; case BTF_KIND_UNION : return \"union\" ; case BTF_KIND_ENUM : return \"enum\" ; case BTF_KIND_FWD : return \"fwd\" ; case BTF_KIND_TYPEDEF : return \"typedef\" ; case BTF_KIND_VOLATILE : return \"volatile\" ; case BTF_KIND_CONST : return \"const\" ; case BTF_KIND_RESTRICT : return \"restrict\" ; case BTF_KIND_FUNC : return \"func\" ; case BTF_KIND_FUNC_PROTO : return \"func_proto\" ; case BTF_KIND_VAR : return \"var\" ; case BTF_KIND_DATASEC : return \"datasec\" ; case BTF_KIND_FLOAT : return \"float\" ; case BTF_KIND_DECL_TAG : return \"decl_tag\" ; case BTF_KIND_TYPE_TAG : return \"type_tag\" ; case BTF_KIND_ENUM64 : return \"enum64\" ; default : return \"unknown\" ; } }",
    "resources/libbpf/src/libbpf.c@btf_kind_str": "const char * btf_kind_str ( const struct btf_type * t ) { return __btf_kind_str ( btf_kind ( t ) ) ; }",
    "resources/libbpf/src/libbpf.c@get_map_field_int": "static bool get_map_field_int ( const char * map_name , const struct btf * btf , const struct btf_member * m , __u32 * res ) { const struct btf_type * t = skip_mods_and_typedefs ( btf , m -> type , NULL ) ; const char * name = btf__name_by_offset ( btf , m -> name_off ) ; const struct btf_array * arr_info ; const struct btf_type * arr_t ; if ( ! btf_is_ptr ( t ) ) { pr_warn ( \"map '%s': attr '%s': expected PTR, got %s.\\n\" , map_name , name , btf_kind_str ( t ) ) ; return false ; } arr_t = btf__type_by_id ( btf , t -> type ) ; if ( ! arr_t ) { pr_warn ( \"map '%s': attr '%s': type [%u] not found.\\n\" , map_name , name , t -> type ) ; return false ; } if ( ! btf_is_array ( arr_t ) ) { pr_warn ( \"map '%s': attr '%s': expected ARRAY, got %s.\\n\" , map_name , name , btf_kind_str ( arr_t ) ) ; return false ; } arr_info = btf_array ( arr_t ) ; * res = arr_info -> nelems ; return true ; }",
    "resources/libbpf/src/libbpf.c@get_map_field_long": "static bool get_map_field_long ( const char * map_name , const struct btf * btf , const struct btf_member * m , __u64 * res ) { const struct btf_type * t = skip_mods_and_typedefs ( btf , m -> type , NULL ) ; const char * name = btf__name_by_offset ( btf , m -> name_off ) ; if ( btf_is_ptr ( t ) ) { __u32 res32 ; bool ret ; ret = get_map_field_int ( map_name , btf , m , & res32 ) ; if ( ret ) * res = ( __u64 ) res32 ; return ret ; } if ( ! btf_is_enum ( t ) && ! btf_is_enum64 ( t ) ) { pr_warn ( \"map '%s': attr '%s': expected ENUM or ENUM64, got %s.\\n\" , map_name , name , btf_kind_str ( t ) ) ; return false ; } if ( btf_vlen ( t ) != 1 ) { pr_warn ( \"map '%s': attr '%s': invalid __ulong\\n\" , map_name , name ) ; return false ; } if ( btf_is_enum ( t ) ) { const struct btf_enum * e = btf_enum ( t ) ; * res = e -> val ; } else { const struct btf_enum64 * e = btf_enum64 ( t ) ; * res = btf_enum64_value ( e ) ; } return true ; }",
    "resources/libbpf/src/libbpf.c@pathname_concat": "static int pathname_concat ( char * buf , size_t buf_sz , const char * path , const char * name ) { int len ; len = snprintf ( buf , buf_sz , \"%s/%s\" , path , name ) ; if ( len < 0 ) return - EINVAL ; if ( len >= buf_sz ) return - ENAMETOOLONG ; return 0 ; }",
    "resources/libbpf/src/libbpf.c@build_map_pin_path": "static int build_map_pin_path ( struct bpf_map * map , const char * path ) { char buf [ PATH_MAX ] ; int err ; if ( ! path ) path = BPF_FS_DEFAULT_PATH ; err = pathname_concat ( buf , sizeof ( buf ) , path , bpf_map__name ( map ) ) ; if ( err ) return err ; return bpf_map__set_pin_path ( map , buf ) ; }",
    "resources/libbpf/src/libbpf.c@parse_btf_map_def": "int parse_btf_map_def ( const char * map_name , struct btf * btf , const struct btf_type * def_t , bool strict , struct btf_map_def * map_def , struct btf_map_def * inner_def ) { const struct btf_type * t ; const struct btf_member * m ; bool is_inner = inner_def == NULL ; int vlen , i ; vlen = btf_vlen ( def_t ) ; m = btf_members ( def_t ) ; for ( i = 0 ; i < vlen ; i ++ , m ++ ) { const char * name = btf__name_by_offset ( btf , m -> name_off ) ; if ( ! name ) { pr_warn ( \"map '%s': invalid field #%d.\\n\" , map_name , i ) ; return - EINVAL ; } if ( strcmp ( name , \"type\" ) == 0 ) { if ( ! get_map_field_int ( map_name , btf , m , & map_def -> map_type ) ) return - EINVAL ; map_def -> parts |= MAP_DEF_MAP_TYPE ; } else if ( strcmp ( name , \"max_entries\" ) == 0 ) { if ( ! get_map_field_int ( map_name , btf , m , & map_def -> max_entries ) ) return - EINVAL ; map_def -> parts |= MAP_DEF_MAX_ENTRIES ; } else if ( strcmp ( name , \"map_flags\" ) == 0 ) { if ( ! get_map_field_int ( map_name , btf , m , & map_def -> map_flags ) ) return - EINVAL ; map_def -> parts |= MAP_DEF_MAP_FLAGS ; } else if ( strcmp ( name , \"numa_node\" ) == 0 ) { if ( ! get_map_field_int ( map_name , btf , m , & map_def -> numa_node ) ) return - EINVAL ; map_def -> parts |= MAP_DEF_NUMA_NODE ; } else if ( strcmp ( name , \"key_size\" ) == 0 ) { __u32 sz ; if ( ! get_map_field_int ( map_name , btf , m , & sz ) ) return - EINVAL ; if ( map_def -> key_size && map_def -> key_size != sz ) { pr_warn ( \"map '%s': conflicting key size %u != %u.\\n\" , map_name , map_def -> key_size , sz ) ; return - EINVAL ; } map_def -> key_size = sz ; map_def -> parts |= MAP_DEF_KEY_SIZE ; } else if ( strcmp ( name , \"key\" ) == 0 ) { __s64 sz ; t = btf__type_by_id ( btf , m -> type ) ; if ( ! t ) { pr_warn ( \"map '%s': key type [%d] not found.\\n\" , map_name , m -> type ) ; return - EINVAL ; } if ( ! btf_is_ptr ( t ) ) { pr_warn ( \"map '%s': key spec is not PTR: %s.\\n\" , map_name , btf_kind_str ( t ) ) ; return - EINVAL ; } sz = btf__resolve_size ( btf , t -> type ) ; if ( sz < 0 ) { pr_warn ( \"map '%s': can't determine key size for type [%u]: %zd.\\n\" , map_name , t -> type , ( ssize_t ) sz ) ; return sz ; } if ( map_def -> key_size && map_def -> key_size != sz ) { pr_warn ( \"map '%s': conflicting key size %u != %zd.\\n\" , map_name , map_def -> key_size , ( ssize_t ) sz ) ; return - EINVAL ; } map_def -> key_size = sz ; map_def -> key_type_id = t -> type ; map_def -> parts |= MAP_DEF_KEY_SIZE | MAP_DEF_KEY_TYPE ; } else if ( strcmp ( name , \"value_size\" ) == 0 ) { __u32 sz ; if ( ! get_map_field_int ( map_name , btf , m , & sz ) ) return - EINVAL ; if ( map_def -> value_size && map_def -> value_size != sz ) { pr_warn ( \"map '%s': conflicting value size %u != %u.\\n\" , map_name , map_def -> value_size , sz ) ; return - EINVAL ; } map_def -> value_size = sz ; map_def -> parts |= MAP_DEF_VALUE_SIZE ; } else if ( strcmp ( name , \"value\" ) == 0 ) { __s64 sz ; t = btf__type_by_id ( btf , m -> type ) ; if ( ! t ) { pr_warn ( \"map '%s': value type [%d] not found.\\n\" , map_name , m -> type ) ; return - EINVAL ; } if ( ! btf_is_ptr ( t ) ) { pr_warn ( \"map '%s': value spec is not PTR: %s.\\n\" , map_name , btf_kind_str ( t ) ) ; return - EINVAL ; } sz = btf__resolve_size ( btf , t -> type ) ; if ( sz < 0 ) { pr_warn ( \"map '%s': can't determine value size for type [%u]: %zd.\\n\" , map_name , t -> type , ( ssize_t ) sz ) ; return sz ; } if ( map_def -> value_size && map_def -> value_size != sz ) { pr_warn ( \"map '%s': conflicting value size %u != %zd.\\n\" , map_name , map_def -> value_size , ( ssize_t ) sz ) ; return - EINVAL ; } map_def -> value_size = sz ; map_def -> value_type_id = t -> type ; map_def -> parts |= MAP_DEF_VALUE_SIZE | MAP_DEF_VALUE_TYPE ; } else if ( strcmp ( name , \"values\" ) == 0 ) { bool is_map_in_map = bpf_map_type__is_map_in_map ( map_def -> map_type ) ; bool is_prog_array = map_def -> map_type == BPF_MAP_TYPE_PROG_ARRAY ; const char * desc = is_map_in_map ? \"map-in-map inner\" : \"prog-array value\" ; char inner_map_name [ 128 ] ; int err ; if ( is_inner ) { pr_warn ( \"map '%s': multi-level inner maps not supported.\\n\" , map_name ) ; return - ENOTSUP ; } if ( i != vlen - 1 ) { pr_warn ( \"map '%s': '%s' member should be last.\\n\" , map_name , name ) ; return - EINVAL ; } if ( ! is_map_in_map && ! is_prog_array ) { pr_warn ( \"map '%s': should be map-in-map or prog-array.\\n\" , map_name ) ; return - ENOTSUP ; } if ( map_def -> value_size && map_def -> value_size != 4 ) { pr_warn ( \"map '%s': conflicting value size %u != 4.\\n\" , map_name , map_def -> value_size ) ; return - EINVAL ; } map_def -> value_size = 4 ; t = btf__type_by_id ( btf , m -> type ) ; if ( ! t ) { pr_warn ( \"map '%s': %s type [%d] not found.\\n\" , map_name , desc , m -> type ) ; return - EINVAL ; } if ( ! btf_is_array ( t ) || btf_array ( t ) -> nelems ) { pr_warn ( \"map '%s': %s spec is not a zero-sized array.\\n\" , map_name , desc ) ; return - EINVAL ; } t = skip_mods_and_typedefs ( btf , btf_array ( t ) -> type , NULL ) ; if ( ! btf_is_ptr ( t ) ) { pr_warn ( \"map '%s': %s def is of unexpected kind %s.\\n\" , map_name , desc , btf_kind_str ( t ) ) ; return - EINVAL ; } t = skip_mods_and_typedefs ( btf , t -> type , NULL ) ; if ( is_prog_array ) { if ( ! btf_is_func_proto ( t ) ) { pr_warn ( \"map '%s': prog-array value def is of unexpected kind %s.\\n\" , map_name , btf_kind_str ( t ) ) ; return - EINVAL ; } continue ; } if ( ! btf_is_struct ( t ) ) { pr_warn ( \"map '%s': map-in-map inner def is of unexpected kind %s.\\n\" , map_name , btf_kind_str ( t ) ) ; return - EINVAL ; } snprintf ( inner_map_name , sizeof ( inner_map_name ) , \"%s.inner\" , map_name ) ; err = parse_btf_map_def ( inner_map_name , btf , t , strict , inner_def , NULL ) ; if ( err ) return err ; map_def -> parts |= MAP_DEF_INNER_MAP ; } else if ( strcmp ( name , \"pinning\" ) == 0 ) { __u32 val ; if ( is_inner ) { pr_warn ( \"map '%s': inner def can't be pinned.\\n\" , map_name ) ; return - EINVAL ; } if ( ! get_map_field_int ( map_name , btf , m , & val ) ) return - EINVAL ; if ( val != LIBBPF_PIN_NONE && val != LIBBPF_PIN_BY_NAME ) { pr_warn ( \"map '%s': invalid pinning value %u.\\n\" , map_name , val ) ; return - EINVAL ; } map_def -> pinning = val ; map_def -> parts |= MAP_DEF_PINNING ; } else if ( strcmp ( name , \"map_extra\" ) == 0 ) { __u64 map_extra ; if ( ! get_map_field_long ( map_name , btf , m , & map_extra ) ) return - EINVAL ; map_def -> map_extra = map_extra ; map_def -> parts |= MAP_DEF_MAP_EXTRA ; } else { if ( strict ) { pr_warn ( \"map '%s': unknown field '%s'.\\n\" , map_name , name ) ; return - ENOTSUP ; } pr_debug ( \"map '%s': ignoring unknown field '%s'.\\n\" , map_name , name ) ; } } if ( map_def -> map_type == BPF_MAP_TYPE_UNSPEC ) { pr_warn ( \"map '%s': map type isn't specified.\\n\" , map_name ) ; return - EINVAL ; } return 0 ; }",
    "resources/libbpf/src/libbpf.c@adjust_ringbuf_sz": "static size_t adjust_ringbuf_sz ( size_t sz ) { __u32 page_sz = sysconf ( _SC_PAGE_SIZE ) ; __u32 mul ; /* if user forgot to set any size, make sure they see error */ if ( sz == 0 ) return 0 ; /* Kernel expects BPF_MAP_TYPE_RINGBUF's max_entries to be\n\t * a power-of-2 multiple of kernel's page size. If user diligently\n\t * satisified these conditions, pass the size through.\n\t */ if ( ( sz % page_sz ) == 0 && is_pow_of_2 ( sz / page_sz ) ) return sz ; /* Otherwise find closest (page_sz * power_of_2) product bigger than\n\t * user-set size to satisfy both user size request and kernel\n\t * requirements and substitute correct max_entries for map creation.\n\t */ for ( mul = 1 ; mul <= UINT_MAX / page_sz ; mul <<= 1 ) { if ( mul * page_sz > sz ) return mul * page_sz ; } /* if it's impossible to satisfy the conditions (i.e., user size is\n\t * very close to UINT_MAX but is not a power-of-2 multiple of\n\t * page_size) then just return original size and let kernel reject it\n\t */ return sz ; }",
    "resources/libbpf/src/libbpf.c@map_is_ringbuf": "static bool map_is_ringbuf ( const struct bpf_map * map ) { return map -> def . type == BPF_MAP_TYPE_RINGBUF || map -> def . type == BPF_MAP_TYPE_USER_RINGBUF ; }",
    "resources/libbpf/src/libbpf.c@fill_map_from_def": "static void fill_map_from_def ( struct bpf_map * map , const struct btf_map_def * def ) { map -> def . type = def -> map_type ; map -> def . key_size = def -> key_size ; map -> def . value_size = def -> value_size ; map -> def . max_entries = def -> max_entries ; map -> def . map_flags = def -> map_flags ; map -> map_extra = def -> map_extra ; map -> numa_node = def -> numa_node ; map -> btf_key_type_id = def -> key_type_id ; map -> btf_value_type_id = def -> value_type_id ; /* auto-adjust BPF ringbuf map max_entries to be a multiple of page size */ if ( map_is_ringbuf ( map ) ) map -> def . max_entries = adjust_ringbuf_sz ( map -> def . max_entries ) ; if ( def -> parts & MAP_DEF_MAP_TYPE ) pr_debug ( \"map '%s': found type = %u.\\n\" , map -> name , def -> map_type ) ; if ( def -> parts & MAP_DEF_KEY_TYPE ) pr_debug ( \"map '%s': found key [%u], sz = %u.\\n\" , map -> name , def -> key_type_id , def -> key_size ) ; else if ( def -> parts & MAP_DEF_KEY_SIZE ) pr_debug ( \"map '%s': found key_size = %u.\\n\" , map -> name , def -> key_size ) ; if ( def -> parts & MAP_DEF_VALUE_TYPE ) pr_debug ( \"map '%s': found value [%u], sz = %u.\\n\" , map -> name , def -> value_type_id , def -> value_size ) ; else if ( def -> parts & MAP_DEF_VALUE_SIZE ) pr_debug ( \"map '%s': found value_size = %u.\\n\" , map -> name , def -> value_size ) ; if ( def -> parts & MAP_DEF_MAX_ENTRIES ) pr_debug ( \"map '%s': found max_entries = %u.\\n\" , map -> name , def -> max_entries ) ; if ( def -> parts & MAP_DEF_MAP_FLAGS ) pr_debug ( \"map '%s': found map_flags = 0x%x.\\n\" , map -> name , def -> map_flags ) ; if ( def -> parts & MAP_DEF_MAP_EXTRA ) pr_debug ( \"map '%s': found map_extra = 0x%llx.\\n\" , map -> name , ( unsigned long long ) def -> map_extra ) ; if ( def -> parts & MAP_DEF_PINNING ) pr_debug ( \"map '%s': found pinning = %u.\\n\" , map -> name , def -> pinning ) ; if ( def -> parts & MAP_DEF_NUMA_NODE ) pr_debug ( \"map '%s': found numa_node = %u.\\n\" , map -> name , def -> numa_node ) ; if ( def -> parts & MAP_DEF_INNER_MAP ) pr_debug ( \"map '%s': found inner map definition.\\n\" , map -> name ) ; }",
    "resources/libbpf/src/libbpf.c@btf_var_linkage_str": "static const char * btf_var_linkage_str ( __u32 linkage ) { switch ( linkage ) { case BTF_VAR_STATIC : return \"static\" ; case BTF_VAR_GLOBAL_ALLOCATED : return \"global\" ; case BTF_VAR_GLOBAL_EXTERN : return \"extern\" ; default : return \"unknown\" ; } }",
    "resources/libbpf/src/libbpf.c@bpf_object__init_user_btf_map": "static int bpf_object__init_user_btf_map ( struct bpf_object * obj , const struct btf_type * sec , int var_idx , int sec_idx , const Elf_Data * data , bool strict , const char * pin_root_path ) { struct btf_map_def map_def = { } , inner_def = { } ; const struct btf_type * var , * def ; const struct btf_var_secinfo * vi ; const struct btf_var * var_extra ; const char * map_name ; struct bpf_map * map ; int err ; vi = btf_var_secinfos ( sec ) + var_idx ; var = btf__type_by_id ( obj -> btf , vi -> type ) ; var_extra = btf_var ( var ) ; map_name = btf__name_by_offset ( obj -> btf , var -> name_off ) ; if ( map_name == NULL || map_name [ 0 ] == '\\0' ) { pr_warn ( \"map #%d: empty name.\\n\" , var_idx ) ; return - EINVAL ; } if ( ( __u64 ) vi -> offset + vi -> size > data -> d_size ) { pr_warn ( \"map '%s' BTF data is corrupted.\\n\" , map_name ) ; return - EINVAL ; } if ( ! btf_is_var ( var ) ) { pr_warn ( \"map '%s': unexpected var kind %s.\\n\" , map_name , btf_kind_str ( var ) ) ; return - EINVAL ; } if ( var_extra -> linkage != BTF_VAR_GLOBAL_ALLOCATED ) { pr_warn ( \"map '%s': unsupported map linkage %s.\\n\" , map_name , btf_var_linkage_str ( var_extra -> linkage ) ) ; return - EOPNOTSUPP ; } def = skip_mods_and_typedefs ( obj -> btf , var -> type , NULL ) ; if ( ! btf_is_struct ( def ) ) { pr_warn ( \"map '%s': unexpected def kind %s.\\n\" , map_name , btf_kind_str ( var ) ) ; return - EINVAL ; } if ( def -> size > vi -> size ) { pr_warn ( \"map '%s': invalid def size.\\n\" , map_name ) ; return - EINVAL ; } map = bpf_object__add_map ( obj ) ; if ( IS_ERR ( map ) ) return PTR_ERR ( map ) ; map -> name = strdup ( map_name ) ; if ( ! map -> name ) { pr_warn ( \"map '%s': failed to alloc map name.\\n\" , map_name ) ; return - ENOMEM ; } map -> libbpf_type = LIBBPF_MAP_UNSPEC ; map -> def . type = BPF_MAP_TYPE_UNSPEC ; map -> sec_idx = sec_idx ; map -> sec_offset = vi -> offset ; map -> btf_var_idx = var_idx ; pr_debug ( \"map '%s': at sec_idx %d, offset %zu.\\n\" , map_name , map -> sec_idx , map -> sec_offset ) ; err = parse_btf_map_def ( map -> name , obj -> btf , def , strict , & map_def , & inner_def ) ; if ( err ) return err ; fill_map_from_def ( map , & map_def ) ; if ( map_def . pinning == LIBBPF_PIN_BY_NAME ) { err = build_map_pin_path ( map , pin_root_path ) ; if ( err ) { pr_warn ( \"map '%s': couldn't build pin path.\\n\" , map -> name ) ; return err ; } } if ( map_def . parts & MAP_DEF_INNER_MAP ) { map -> inner_map = calloc ( 1 , sizeof ( * map -> inner_map ) ) ; if ( ! map -> inner_map ) return - ENOMEM ; map -> inner_map -> fd = create_placeholder_fd ( ) ; if ( map -> inner_map -> fd < 0 ) return map -> inner_map -> fd ; map -> inner_map -> sec_idx = sec_idx ; map -> inner_map -> name = malloc ( strlen ( map_name ) + sizeof ( \".inner\" ) + 1 ) ; if ( ! map -> inner_map -> name ) return - ENOMEM ; sprintf ( map -> inner_map -> name , \"%s.inner\" , map_name ) ; fill_map_from_def ( map -> inner_map , & inner_def ) ; } err = map_fill_btf_type_info ( obj , map ) ; if ( err ) return err ; return 0 ; }",
    "resources/libbpf/src/libbpf.c@init_arena_map_data": "static int init_arena_map_data ( struct bpf_object * obj , struct bpf_map * map , const char * sec_name , int sec_idx , void * data , size_t data_sz ) { const long page_sz = sysconf ( _SC_PAGE_SIZE ) ; size_t mmap_sz ; mmap_sz = bpf_map_mmap_sz ( obj -> arena_map ) ; if ( roundup ( data_sz , page_sz ) > mmap_sz ) { pr_warn ( \"elf: sec '%s': declared ARENA map size (%zu) is too small to hold global __arena variables of size %zu\\n\" , sec_name , mmap_sz , data_sz ) ; return - E2BIG ; } obj -> arena_data = malloc ( data_sz ) ; if ( ! obj -> arena_data ) return - ENOMEM ; memcpy ( obj -> arena_data , data , data_sz ) ; obj -> arena_data_sz = data_sz ; /* make bpf_map__init_value() work for ARENA maps */ map -> mmaped = obj -> arena_data ; return 0 ; }",
    "resources/libbpf/src/libbpf.c@bpf_object__init_user_btf_maps": "static int bpf_object__init_user_btf_maps ( struct bpf_object * obj , bool strict , const char * pin_root_path ) { const struct btf_type * sec = NULL ; int nr_types , i , vlen , err ; const struct btf_type * t ; const char * name ; Elf_Data * data ; Elf_Scn * scn ; if ( obj -> efile . btf_maps_shndx < 0 ) return 0 ; scn = elf_sec_by_idx ( obj , obj -> efile . btf_maps_shndx ) ; data = elf_sec_data ( obj , scn ) ; if ( ! scn || ! data ) { pr_warn ( \"elf: failed to get %s map definitions for %s\\n\" , MAPS_ELF_SEC , obj -> path ) ; return - EINVAL ; } nr_types = btf__type_cnt ( obj -> btf ) ; for ( i = 1 ; i < nr_types ; i ++ ) { t = btf__type_by_id ( obj -> btf , i ) ; if ( ! btf_is_datasec ( t ) ) continue ; name = btf__name_by_offset ( obj -> btf , t -> name_off ) ; if ( strcmp ( name , MAPS_ELF_SEC ) == 0 ) { sec = t ; obj -> efile . btf_maps_sec_btf_id = i ; break ; } } if ( ! sec ) { pr_warn ( \"DATASEC '%s' not found.\\n\" , MAPS_ELF_SEC ) ; return - ENOENT ; } vlen = btf_vlen ( sec ) ; for ( i = 0 ; i < vlen ; i ++ ) { err = bpf_object__init_user_btf_map ( obj , sec , i , obj -> efile . btf_maps_shndx , data , strict , pin_root_path ) ; if ( err ) return err ; } for ( i = 0 ; i < obj -> nr_maps ; i ++ ) { struct bpf_map * map = & obj -> maps [ i ] ; if ( map -> def . type != BPF_MAP_TYPE_ARENA ) continue ; if ( obj -> arena_map ) { pr_warn ( \"map '%s': only single ARENA map is supported (map '%s' is also ARENA)\\n\" , map -> name , obj -> arena_map -> name ) ; return - EINVAL ; } obj -> arena_map = map ; if ( obj -> efile . arena_data ) { err = init_arena_map_data ( obj , map , ARENA_SEC , obj -> efile . arena_data_shndx , obj -> efile . arena_data -> d_buf , obj -> efile . arena_data -> d_size ) ; if ( err ) return err ; } } if ( obj -> efile . arena_data && ! obj -> arena_map ) { pr_warn ( \"elf: sec '%s': to use global __arena variables the ARENA map should be explicitly declared in SEC(\\\".maps\\\")\\n\" , ARENA_SEC ) ; return - ENOENT ; } return 0 ; }",
    "resources/libbpf/src/libbpf.c@bpf_object__init_maps": "static int bpf_object__init_maps ( struct bpf_object * obj , const struct bpf_object_open_opts * opts ) { const char * pin_root_path ; bool strict ; int err = 0 ; strict = ! OPTS_GET ( opts , relaxed_maps , false ) ; pin_root_path = OPTS_GET ( opts , pin_root_path , NULL ) ; err = bpf_object__init_user_btf_maps ( obj , strict , pin_root_path ) ; err = err ? : bpf_object__init_global_data_maps ( obj ) ; err = err ? : bpf_object__init_kconfig_map ( obj ) ; err = err ? : bpf_object_init_struct_ops ( obj ) ; return err ; }",
    "resources/libbpf/src/libbpf.c@section_have_execinstr": "static bool section_have_execinstr ( struct bpf_object * obj , int idx ) { Elf64_Shdr * sh ; sh = elf_sec_hdr ( obj , elf_sec_by_idx ( obj , idx ) ) ; if ( ! sh ) return false ; return sh -> sh_flags & SHF_EXECINSTR ; }",
    "resources/libbpf/src/libbpf.c@starts_with_qmark": "static bool starts_with_qmark ( const char * s ) { return s && s [ 0 ] == '?' ; }",
    "resources/libbpf/src/libbpf.c@btf_needs_sanitization": "static bool btf_needs_sanitization ( struct bpf_object * obj ) { bool has_func_global = kernel_supports ( obj , FEAT_BTF_GLOBAL_FUNC ) ; bool has_datasec = kernel_supports ( obj , FEAT_BTF_DATASEC ) ; bool has_float = kernel_supports ( obj , FEAT_BTF_FLOAT ) ; bool has_func = kernel_supports ( obj , FEAT_BTF_FUNC ) ; bool has_decl_tag = kernel_supports ( obj , FEAT_BTF_DECL_TAG ) ; bool has_type_tag = kernel_supports ( obj , FEAT_BTF_TYPE_TAG ) ; bool has_enum64 = kernel_supports ( obj , FEAT_BTF_ENUM64 ) ; bool has_qmark_datasec = kernel_supports ( obj , FEAT_BTF_QMARK_DATASEC ) ; return ! has_func || ! has_datasec || ! has_func_global || ! has_float || ! has_decl_tag || ! has_type_tag || ! has_enum64 || ! has_qmark_datasec ; }",
    "resources/libbpf/src/libbpf.c@bpf_object__sanitize_btf": "static int bpf_object__sanitize_btf ( struct bpf_object * obj , struct btf * btf ) { bool has_func_global = kernel_supports ( obj , FEAT_BTF_GLOBAL_FUNC ) ; bool has_datasec = kernel_supports ( obj , FEAT_BTF_DATASEC ) ; bool has_float = kernel_supports ( obj , FEAT_BTF_FLOAT ) ; bool has_func = kernel_supports ( obj , FEAT_BTF_FUNC ) ; bool has_decl_tag = kernel_supports ( obj , FEAT_BTF_DECL_TAG ) ; bool has_type_tag = kernel_supports ( obj , FEAT_BTF_TYPE_TAG ) ; bool has_enum64 = kernel_supports ( obj , FEAT_BTF_ENUM64 ) ; bool has_qmark_datasec = kernel_supports ( obj , FEAT_BTF_QMARK_DATASEC ) ; int enum64_placeholder_id = 0 ; struct btf_type * t ; int i , j , vlen ; for ( i = 1 ; i < btf__type_cnt ( btf ) ; i ++ ) { t = ( struct btf_type * ) btf__type_by_id ( btf , i ) ; if ( ( ! has_datasec && btf_is_var ( t ) ) || ( ! has_decl_tag && btf_is_decl_tag ( t ) ) ) { /* replace VAR/DECL_TAG with INT */ t -> info = BTF_INFO_ENC ( BTF_KIND_INT , 0 , 0 ) ; /*\n\t\t\t * using size = 1 is the safest choice, 4 will be too\n\t\t\t * big and cause kernel BTF validation failure if\n\t\t\t * original variable took less than 4 bytes\n\t\t\t */ t -> size = 1 ; * ( int * ) ( t + 1 ) = BTF_INT_ENC ( 0 , 0 , 8 ) ; } else if ( ! has_datasec && btf_is_datasec ( t ) ) { /* replace DATASEC with STRUCT */ const struct btf_var_secinfo * v = btf_var_secinfos ( t ) ; struct btf_member * m = btf_members ( t ) ; struct btf_type * vt ; char * name ; name = ( char * ) btf__name_by_offset ( btf , t -> name_off ) ; while ( * name ) { if ( * name == '.' || * name == '?' ) * name = '_' ; name ++ ; } vlen = btf_vlen ( t ) ; t -> info = BTF_INFO_ENC ( BTF_KIND_STRUCT , 0 , vlen ) ; for ( j = 0 ; j < vlen ; j ++ , v ++ , m ++ ) { /* order of field assignments is important */ m -> offset = v -> offset * 8 ; m -> type = v -> type ; /* preserve variable name as member name */ vt = ( void * ) btf__type_by_id ( btf , v -> type ) ; m -> name_off = vt -> name_off ; } } else if ( ! has_qmark_datasec && btf_is_datasec ( t ) && starts_with_qmark ( btf__name_by_offset ( btf , t -> name_off ) ) ) { /* replace '?' prefix with '_' for DATASEC names */ char * name ; name = ( char * ) btf__name_by_offset ( btf , t -> name_off ) ; if ( name [ 0 ] == '?' ) name [ 0 ] = '_' ; } else if ( ! has_func && btf_is_func_proto ( t ) ) { /* replace FUNC_PROTO with ENUM */ vlen = btf_vlen ( t ) ; t -> info = BTF_INFO_ENC ( BTF_KIND_ENUM , 0 , vlen ) ; t -> size = sizeof ( __u32 ) ; /* kernel enforced */ } else if ( ! has_func && btf_is_func ( t ) ) { /* replace FUNC with TYPEDEF */ t -> info = BTF_INFO_ENC ( BTF_KIND_TYPEDEF , 0 , 0 ) ; } else if ( ! has_func_global && btf_is_func ( t ) ) { /* replace BTF_FUNC_GLOBAL with BTF_FUNC_STATIC */ t -> info = BTF_INFO_ENC ( BTF_KIND_FUNC , 0 , 0 ) ; } else if ( ! has_float && btf_is_float ( t ) ) { /* replace FLOAT with an equally-sized empty STRUCT;\n\t\t\t * since C compilers do not accept e.g. \"float\" as a\n\t\t\t * valid struct name, make it anonymous\n\t\t\t */ t -> name_off = 0 ; t -> info = BTF_INFO_ENC ( BTF_KIND_STRUCT , 0 , 0 ) ; } else if ( ! has_type_tag && btf_is_type_tag ( t ) ) { /* replace TYPE_TAG with a CONST */ t -> name_off = 0 ; t -> info = BTF_INFO_ENC ( BTF_KIND_CONST , 0 , 0 ) ; } else if ( ! has_enum64 && btf_is_enum ( t ) ) { /* clear the kflag */ t -> info = btf_type_info ( btf_kind ( t ) , btf_vlen ( t ) , false ) ; } else if ( ! has_enum64 && btf_is_enum64 ( t ) ) { /* replace ENUM64 with a union */ struct btf_member * m ; if ( enum64_placeholder_id == 0 ) { enum64_placeholder_id = btf__add_int ( btf , \"enum64_placeholder\" , 1 , 0 ) ; if ( enum64_placeholder_id < 0 ) return enum64_placeholder_id ; t = ( struct btf_type * ) btf__type_by_id ( btf , i ) ; } m = btf_members ( t ) ; vlen = btf_vlen ( t ) ; t -> info = BTF_INFO_ENC ( BTF_KIND_UNION , 0 , vlen ) ; for ( j = 0 ; j < vlen ; j ++ , m ++ ) { m -> type = enum64_placeholder_id ; m -> offset = 0 ; } } } return 0 ; }",
    "resources/libbpf/src/libbpf.c@libbpf_needs_btf": "static bool libbpf_needs_btf ( const struct bpf_object * obj ) { return obj -> efile . btf_maps_shndx >= 0 || obj -> efile . has_st_ops || obj -> nr_extern > 0 ; }",
    "resources/libbpf/src/libbpf.c@kernel_needs_btf": "static bool kernel_needs_btf ( const struct bpf_object * obj ) { return obj -> efile . has_st_ops ; }",
    "resources/libbpf/src/libbpf.c@bpf_object__init_btf": "static int bpf_object__init_btf ( struct bpf_object * obj , Elf_Data * btf_data , Elf_Data * btf_ext_data ) { int err = - ENOENT ; if ( btf_data ) { obj -> btf = btf__new ( btf_data -> d_buf , btf_data -> d_size ) ; err = libbpf_get_error ( obj -> btf ) ; if ( err ) { obj -> btf = NULL ; pr_warn ( \"Error loading ELF section %s: %d.\\n\" , BTF_ELF_SEC , err ) ; goto out ; } /* enforce 8-byte pointers for BPF-targeted BTFs */ btf__set_pointer_size ( obj -> btf , 8 ) ; } if ( btf_ext_data ) { struct btf_ext_info * ext_segs [ 3 ] ; int seg_num , sec_num ; if ( ! obj -> btf ) { pr_debug ( \"Ignore ELF section %s because its depending ELF section %s is not found.\\n\" , BTF_EXT_ELF_SEC , BTF_ELF_SEC ) ; goto out ; } obj -> btf_ext = btf_ext__new ( btf_ext_data -> d_buf , btf_ext_data -> d_size ) ; err = libbpf_get_error ( obj -> btf_ext ) ; if ( err ) { pr_warn ( \"Error loading ELF section %s: %d. Ignored and continue.\\n\" , BTF_EXT_ELF_SEC , err ) ; obj -> btf_ext = NULL ; goto out ; } /* setup .BTF.ext to ELF section mapping */ ext_segs [ 0 ] = & obj -> btf_ext -> func_info ; ext_segs [ 1 ] = & obj -> btf_ext -> line_info ; ext_segs [ 2 ] = & obj -> btf_ext -> core_relo_info ; for ( seg_num = 0 ; seg_num < ARRAY_SIZE ( ext_segs ) ; seg_num ++ ) { struct btf_ext_info * seg = ext_segs [ seg_num ] ; const struct btf_ext_info_sec * sec ; const char * sec_name ; Elf_Scn * scn ; if ( seg -> sec_cnt == 0 ) continue ; seg -> sec_idxs = calloc ( seg -> sec_cnt , sizeof ( * seg -> sec_idxs ) ) ; if ( ! seg -> sec_idxs ) { err = - ENOMEM ; goto out ; } sec_num = 0 ; for_each_btf_ext_sec ( seg , sec ) { /* preventively increment index to avoid doing\n\t\t\t\t * this before every continue below\n\t\t\t\t */ sec_num ++ ; sec_name = btf__name_by_offset ( obj -> btf , sec -> sec_name_off ) ; if ( str_is_empty ( sec_name ) ) continue ; scn = elf_sec_by_name ( obj , sec_name ) ; if ( ! scn ) continue ; seg -> sec_idxs [ sec_num - 1 ] = elf_ndxscn ( scn ) ; } } } out : if ( err && libbpf_needs_btf ( obj ) ) { pr_warn ( \"BTF is required, but is missing or corrupted.\\n\" ) ; return err ; } return 0 ; }",
    "resources/libbpf/src/libbpf.c@compare_vsi_off": "static int compare_vsi_off ( const void * _a , const void * _b ) { const struct btf_var_secinfo * a = _a ; const struct btf_var_secinfo * b = _b ; return a -> offset - b -> offset ; }",
    "resources/libbpf/src/libbpf.c@btf_fixup_datasec": "static int btf_fixup_datasec ( struct bpf_object * obj , struct btf * btf , struct btf_type * t ) { __u32 size = 0 , i , vars = btf_vlen ( t ) ; const char * sec_name = btf__name_by_offset ( btf , t -> name_off ) ; struct btf_var_secinfo * vsi ; bool fixup_offsets = false ; int err ; if ( ! sec_name ) { pr_debug ( \"No name found in string section for DATASEC kind.\\n\" ) ; return - ENOENT ; } /* Extern-backing datasecs (.ksyms, .kconfig) have their size and\n\t * variable offsets set at the previous step. Further, not every\n\t * extern BTF VAR has corresponding ELF symbol preserved, so we skip\n\t * all fixups altogether for such sections and go straight to sorting\n\t * VARs within their DATASEC.\n\t */ if ( strcmp ( sec_name , KCONFIG_SEC ) == 0 || strcmp ( sec_name , KSYMS_SEC ) == 0 ) goto sort_vars ; /* Clang leaves DATASEC size and VAR offsets as zeroes, so we need to\n\t * fix this up. But BPF static linker already fixes this up and fills\n\t * all the sizes and offsets during static linking. So this step has\n\t * to be optional. But the STV_HIDDEN handling is non-optional for any\n\t * non-extern DATASEC, so the variable fixup loop below handles both\n\t * functions at the same time, paying the cost of BTF VAR <-> ELF\n\t * symbol matching just once.\n\t */ if ( t -> size == 0 ) { err = find_elf_sec_sz ( obj , sec_name , & size ) ; if ( err || ! size ) { pr_debug ( \"sec '%s': failed to determine size from ELF: size %u, err %d\\n\" , sec_name , size , err ) ; return - ENOENT ; } t -> size = size ; fixup_offsets = true ; } for ( i = 0 , vsi = btf_var_secinfos ( t ) ; i < vars ; i ++ , vsi ++ ) { const struct btf_type * t_var ; struct btf_var * var ; const char * var_name ; Elf64_Sym * sym ; t_var = btf__type_by_id ( btf , vsi -> type ) ; if ( ! t_var || ! btf_is_var ( t_var ) ) { pr_debug ( \"sec '%s': unexpected non-VAR type found\\n\" , sec_name ) ; return - EINVAL ; } var = btf_var ( t_var ) ; if ( var -> linkage == BTF_VAR_STATIC || var -> linkage == BTF_VAR_GLOBAL_EXTERN ) continue ; var_name = btf__name_by_offset ( btf , t_var -> name_off ) ; if ( ! var_name ) { pr_debug ( \"sec '%s': failed to find name of DATASEC's member #%d\\n\" , sec_name , i ) ; return - ENOENT ; } sym = find_elf_var_sym ( obj , var_name ) ; if ( IS_ERR ( sym ) ) { pr_debug ( \"sec '%s': failed to find ELF symbol for VAR '%s'\\n\" , sec_name , var_name ) ; return - ENOENT ; } if ( fixup_offsets ) vsi -> offset = sym -> st_value ; /* if variable is a global/weak symbol, but has restricted\n\t\t * (STV_HIDDEN or STV_INTERNAL) visibility, mark its BTF VAR\n\t\t * as static. This follows similar logic for functions (BPF\n\t\t * subprogs) and influences libbpf's further decisions about\n\t\t * whether to make global data BPF array maps as\n\t\t * BPF_F_MMAPABLE.\n\t\t */ if ( ELF64_ST_VISIBILITY ( sym -> st_other ) == STV_HIDDEN || ELF64_ST_VISIBILITY ( sym -> st_other ) == STV_INTERNAL ) var -> linkage = BTF_VAR_STATIC ; } sort_vars : qsort ( btf_var_secinfos ( t ) , vars , sizeof ( * vsi ) , compare_vsi_off ) ; return 0 ; }",
    "resources/libbpf/src/libbpf.c@bpf_object_fixup_btf": "static int bpf_object_fixup_btf ( struct bpf_object * obj ) { int i , n , err = 0 ; if ( ! obj -> btf ) return 0 ; n = btf__type_cnt ( obj -> btf ) ; for ( i = 1 ; i < n ; i ++ ) { struct btf_type * t = btf_type_by_id ( obj -> btf , i ) ; /* Loader needs to fix up some of the things compiler\n\t\t * couldn't get its hands on while emitting BTF. This\n\t\t * is section size and global variable offset. We use\n\t\t * the info from the ELF itself for this purpose.\n\t\t */ if ( btf_is_datasec ( t ) ) { err = btf_fixup_datasec ( obj , obj -> btf , t ) ; if ( err ) return err ; } } return 0 ; }",
    "resources/libbpf/src/libbpf.c@prog_needs_vmlinux_btf": "static bool prog_needs_vmlinux_btf ( struct bpf_program * prog ) { if ( prog -> type == BPF_PROG_TYPE_STRUCT_OPS || prog -> type == BPF_PROG_TYPE_LSM ) return true ; /* BPF_PROG_TYPE_TRACING programs which do not attach to other programs\n\t * also need vmlinux BTF\n\t */ if ( prog -> type == BPF_PROG_TYPE_TRACING && ! prog -> attach_prog_fd ) return true ; return false ; }",
    "resources/libbpf/src/libbpf.c@map_needs_vmlinux_btf": "static bool map_needs_vmlinux_btf ( struct bpf_map * map ) { return bpf_map__is_struct_ops ( map ) ; }",
    "resources/libbpf/src/libbpf.c@obj_needs_vmlinux_btf": "static bool obj_needs_vmlinux_btf ( const struct bpf_object * obj ) { struct bpf_program * prog ; struct bpf_map * map ; int i ; /* CO-RE relocations need kernel BTF, only when btf_custom_path\n\t * is not specified\n\t */ if ( obj -> btf_ext && obj -> btf_ext -> core_relo_info . len && ! obj -> btf_custom_path ) return true ; /* Support for typed ksyms needs kernel BTF */ for ( i = 0 ; i < obj -> nr_extern ; i ++ ) { const struct extern_desc * ext ; ext = & obj -> externs [ i ] ; if ( ext -> type == EXT_KSYM && ext -> ksym . type_id ) return true ; } bpf_object__for_each_program ( prog , obj ) { if ( ! prog -> autoload ) continue ; if ( prog_needs_vmlinux_btf ( prog ) ) return true ; } bpf_object__for_each_map ( map , obj ) { if ( map_needs_vmlinux_btf ( map ) ) return true ; } return false ; }",
    "resources/libbpf/src/libbpf.c@bpf_object__load_vmlinux_btf": "static int bpf_object__load_vmlinux_btf ( struct bpf_object * obj , bool force ) { int err ; /* btf_vmlinux could be loaded earlier */ if ( obj -> btf_vmlinux || obj -> gen_loader ) return 0 ; if ( ! force && ! obj_needs_vmlinux_btf ( obj ) ) return 0 ; obj -> btf_vmlinux = btf__load_vmlinux_btf ( ) ; err = libbpf_get_error ( obj -> btf_vmlinux ) ; if ( err ) { pr_warn ( \"Error loading vmlinux BTF: %d\\n\" , err ) ; obj -> btf_vmlinux = NULL ; return err ; } return 0 ; }",
    "resources/libbpf/src/libbpf.c@bpf_object__sanitize_and_load_btf": "static int bpf_object__sanitize_and_load_btf ( struct bpf_object * obj ) { struct btf * kern_btf = obj -> btf ; bool btf_mandatory , sanitize ; int i , err = 0 ; if ( ! obj -> btf ) return 0 ; if ( ! kernel_supports ( obj , FEAT_BTF ) ) { if ( kernel_needs_btf ( obj ) ) { err = - EOPNOTSUPP ; goto report ; } pr_debug ( \"Kernel doesn't support BTF, skipping uploading it.\\n\" ) ; return 0 ; } /* Even though some subprogs are global/weak, user might prefer more\n\t * permissive BPF verification process that BPF verifier performs for\n\t * static functions, taking into account more context from the caller\n\t * functions. In such case, they need to mark such subprogs with\n\t * __attribute__((visibility(\"hidden\"))) and libbpf will adjust\n\t * corresponding FUNC BTF type to be marked as static and trigger more\n\t * involved BPF verification process.\n\t */ for ( i = 0 ; i < obj -> nr_programs ; i ++ ) { struct bpf_program * prog = & obj -> programs [ i ] ; struct btf_type * t ; const char * name ; int j , n ; if ( ! prog -> mark_btf_static || ! prog_is_subprog ( obj , prog ) ) continue ; n = btf__type_cnt ( obj -> btf ) ; for ( j = 1 ; j < n ; j ++ ) { t = btf_type_by_id ( obj -> btf , j ) ; if ( ! btf_is_func ( t ) || btf_func_linkage ( t ) != BTF_FUNC_GLOBAL ) continue ; name = btf__str_by_offset ( obj -> btf , t -> name_off ) ; if ( strcmp ( name , prog -> name ) != 0 ) continue ; t -> info = btf_type_info ( BTF_KIND_FUNC , BTF_FUNC_STATIC , 0 ) ; break ; } } sanitize = btf_needs_sanitization ( obj ) ; if ( sanitize ) { const void * raw_data ; __u32 sz ; /* clone BTF to sanitize a copy and leave the original intact */ raw_data = btf__raw_data ( obj -> btf , & sz ) ; kern_btf = btf__new ( raw_data , sz ) ; err = libbpf_get_error ( kern_btf ) ; if ( err ) return err ; /* enforce 8-byte pointers for BPF-targeted BTFs */ btf__set_pointer_size ( obj -> btf , 8 ) ; err = bpf_object__sanitize_btf ( obj , kern_btf ) ; if ( err ) return err ; } if ( obj -> gen_loader ) { __u32 raw_size = 0 ; const void * raw_data = btf__raw_data ( kern_btf , & raw_size ) ; if ( ! raw_data ) return - ENOMEM ; bpf_gen__load_btf ( obj -> gen_loader , raw_data , raw_size ) ; /* Pretend to have valid FD to pass various fd >= 0 checks.\n\t\t * This fd == 0 will not be used with any syscall and will be reset to -1 eventually.\n\t\t */ btf__set_fd ( kern_btf , 0 ) ; } else { /* currently BPF_BTF_LOAD only supports log_level 1 */ err = btf_load_into_kernel ( kern_btf , obj -> log_buf , obj -> log_size , obj -> log_level ? 1 : 0 , obj -> token_fd ) ; } if ( sanitize ) { if ( ! err ) { /* move fd to libbpf's BTF */ btf__set_fd ( obj -> btf , btf__fd ( kern_btf ) ) ; btf__set_fd ( kern_btf , - 1 ) ; } btf__free ( kern_btf ) ; } report : if ( err ) { btf_mandatory = kernel_needs_btf ( obj ) ; pr_warn ( \"Error loading .BTF into kernel: %d. %s\\n\" , err , btf_mandatory ? \"BTF is mandatory, can't proceed.\" : \"BTF is optional, ignoring.\" ) ; if ( ! btf_mandatory ) err = 0 ; } return err ; }",
    "resources/libbpf/src/libbpf.c@elf_sym_str": "static const char * elf_sym_str ( const struct bpf_object * obj , size_t off ) { const char * name ; name = elf_strptr ( obj -> efile . elf , obj -> efile . strtabidx , off ) ; if ( ! name ) { pr_warn ( \"elf: failed to get section name string at offset %zu from %s: %s\\n\" , off , obj -> path , elf_errmsg ( - 1 ) ) ; return NULL ; } return name ; }",
    "resources/libbpf/src/libbpf.c@elf_sec_str": "static const char * elf_sec_str ( const struct bpf_object * obj , size_t off ) { const char * name ; name = elf_strptr ( obj -> efile . elf , obj -> efile . shstrndx , off ) ; if ( ! name ) { pr_warn ( \"elf: failed to get section name string at offset %zu from %s: %s\\n\" , off , obj -> path , elf_errmsg ( - 1 ) ) ; return NULL ; } return name ; }",
    "resources/libbpf/src/libbpf.c@elf_sec_by_idx": "static Elf_Scn * elf_sec_by_idx ( const struct bpf_object * obj , size_t idx ) { Elf_Scn * scn ; scn = elf_getscn ( obj -> efile . elf , idx ) ; if ( ! scn ) { pr_warn ( \"elf: failed to get section(%zu) from %s: %s\\n\" , idx , obj -> path , elf_errmsg ( - 1 ) ) ; return NULL ; } return scn ; }",
    "resources/libbpf/src/libbpf.c@elf_sec_by_name": "static Elf_Scn * elf_sec_by_name ( const struct bpf_object * obj , const char * name ) { Elf_Scn * scn = NULL ; Elf * elf = obj -> efile . elf ; const char * sec_name ; while ( ( scn = elf_nextscn ( elf , scn ) ) != NULL ) { sec_name = elf_sec_name ( obj , scn ) ; if ( ! sec_name ) return NULL ; if ( strcmp ( sec_name , name ) != 0 ) continue ; return scn ; } return NULL ; }",
    "resources/libbpf/src/libbpf.c@elf_sec_hdr": "static Elf64_Shdr * elf_sec_hdr ( const struct bpf_object * obj , Elf_Scn * scn ) { Elf64_Shdr * shdr ; if ( ! scn ) return NULL ; shdr = elf64_getshdr ( scn ) ; if ( ! shdr ) { pr_warn ( \"elf: failed to get section(%zu) header from %s: %s\\n\" , elf_ndxscn ( scn ) , obj -> path , elf_errmsg ( - 1 ) ) ; return NULL ; } return shdr ; }",
    "resources/libbpf/src/libbpf.c@elf_sec_name": "static const char * elf_sec_name ( const struct bpf_object * obj , Elf_Scn * scn ) { const char * name ; Elf64_Shdr * sh ; if ( ! scn ) return NULL ; sh = elf_sec_hdr ( obj , scn ) ; if ( ! sh ) return NULL ; name = elf_sec_str ( obj , sh -> sh_name ) ; if ( ! name ) { pr_warn ( \"elf: failed to get section(%zu) name from %s: %s\\n\" , elf_ndxscn ( scn ) , obj -> path , elf_errmsg ( - 1 ) ) ; return NULL ; } return name ; }",
    "resources/libbpf/src/libbpf.c@elf_sec_data": "static Elf_Data * elf_sec_data ( const struct bpf_object * obj , Elf_Scn * scn ) { Elf_Data * data ; if ( ! scn ) return NULL ; data = elf_getdata ( scn , 0 ) ; if ( ! data ) { pr_warn ( \"elf: failed to get section(%zu) %s data from %s: %s\\n\" , elf_ndxscn ( scn ) , elf_sec_name ( obj , scn ) ? : \"<?>\" , obj -> path , elf_errmsg ( - 1 ) ) ; return NULL ; } return data ; }",
    "resources/libbpf/src/libbpf.c@elf_sym_by_idx": "static Elf64_Sym * elf_sym_by_idx ( const struct bpf_object * obj , size_t idx ) { if ( idx >= obj -> efile . symbols -> d_size / sizeof ( Elf64_Sym ) ) return NULL ; return ( Elf64_Sym * ) obj -> efile . symbols -> d_buf + idx ; }",
    "resources/libbpf/src/libbpf.c@elf_rel_by_idx": "static Elf64_Rel * elf_rel_by_idx ( Elf_Data * data , size_t idx ) { if ( idx >= data -> d_size / sizeof ( Elf64_Rel ) ) return NULL ; return ( Elf64_Rel * ) data -> d_buf + idx ; }",
    "resources/libbpf/src/libbpf.c@is_sec_name_dwarf": "static bool is_sec_name_dwarf ( const char * name ) { /* approximation, but the actual list is too long */ return str_has_pfx ( name , \".debug_\" ) ; }",
    "resources/libbpf/src/libbpf.c@ignore_elf_section": "static bool ignore_elf_section ( Elf64_Shdr * hdr , const char * name ) { /* no special handling of .strtab */ if ( hdr -> sh_type == SHT_STRTAB ) return true ; /* ignore .llvm_addrsig section as well */ if ( hdr -> sh_type == SHT_LLVM_ADDRSIG ) return true ; /* no subprograms will lead to an empty .text section, ignore it */ if ( hdr -> sh_type == SHT_PROGBITS && hdr -> sh_size == 0 && strcmp ( name , \".text\" ) == 0 ) return true ; /* DWARF sections */ if ( is_sec_name_dwarf ( name ) ) return true ; if ( str_has_pfx ( name , \".rel\" ) ) { name += sizeof ( \".rel\" ) - 1 ; /* DWARF section relocations */ if ( is_sec_name_dwarf ( name ) ) return true ; /* .BTF and .BTF.ext don't need relocations */ if ( strcmp ( name , BTF_ELF_SEC ) == 0 || strcmp ( name , BTF_EXT_ELF_SEC ) == 0 ) return true ; } return false ; }",
    "resources/libbpf/src/libbpf.c@cmp_progs": "static int cmp_progs ( const void * _a , const void * _b ) { const struct bpf_program * a = _a ; const struct bpf_program * b = _b ; if ( a -> sec_idx != b -> sec_idx ) return a -> sec_idx < b -> sec_idx ? - 1 : 1 ; /* sec_insn_off can't be the same within the section */ return a -> sec_insn_off < b -> sec_insn_off ? - 1 : 1 ; }",
    "resources/libbpf/src/libbpf.c@bpf_object__elf_collect": "static int bpf_object__elf_collect ( struct bpf_object * obj ) { struct elf_sec_desc * sec_desc ; Elf * elf = obj -> efile . elf ; Elf_Data * btf_ext_data = NULL ; Elf_Data * btf_data = NULL ; int idx = 0 , err = 0 ; const char * name ; Elf_Data * data ; Elf_Scn * scn ; Elf64_Shdr * sh ; /* ELF section indices are 0-based, but sec #0 is special \"invalid\"\n\t * section. Since section count retrieved by elf_getshdrnum() does\n\t * include sec #0, it is already the necessary size of an array to keep\n\t * all the sections.\n\t */ if ( elf_getshdrnum ( obj -> efile . elf , & obj -> efile . sec_cnt ) ) { pr_warn ( \"elf: failed to get the number of sections for %s: %s\\n\" , obj -> path , elf_errmsg ( - 1 ) ) ; return - LIBBPF_ERRNO__FORMAT ; } obj -> efile . secs = calloc ( obj -> efile . sec_cnt , sizeof ( * obj -> efile . secs ) ) ; if ( ! obj -> efile . secs ) return - ENOMEM ; /* a bunch of ELF parsing functionality depends on processing symbols,\n\t * so do the first pass and find the symbol table\n\t */ scn = NULL ; while ( ( scn = elf_nextscn ( elf , scn ) ) != NULL ) { sh = elf_sec_hdr ( obj , scn ) ; if ( ! sh ) return - LIBBPF_ERRNO__FORMAT ; if ( sh -> sh_type == SHT_SYMTAB ) { if ( obj -> efile . symbols ) { pr_warn ( \"elf: multiple symbol tables in %s\\n\" , obj -> path ) ; return - LIBBPF_ERRNO__FORMAT ; } data = elf_sec_data ( obj , scn ) ; if ( ! data ) return - LIBBPF_ERRNO__FORMAT ; idx = elf_ndxscn ( scn ) ; obj -> efile . symbols = data ; obj -> efile . symbols_shndx = idx ; obj -> efile . strtabidx = sh -> sh_link ; } } if ( ! obj -> efile . symbols ) { pr_warn ( \"elf: couldn't find symbol table in %s, stripped object file?\\n\" , obj -> path ) ; return - ENOENT ; } scn = NULL ; while ( ( scn = elf_nextscn ( elf , scn ) ) != NULL ) { idx = elf_ndxscn ( scn ) ; sec_desc = & obj -> efile . secs [ idx ] ; sh = elf_sec_hdr ( obj , scn ) ; if ( ! sh ) return - LIBBPF_ERRNO__FORMAT ; name = elf_sec_str ( obj , sh -> sh_name ) ; if ( ! name ) return - LIBBPF_ERRNO__FORMAT ; if ( ignore_elf_section ( sh , name ) ) continue ; data = elf_sec_data ( obj , scn ) ; if ( ! data ) return - LIBBPF_ERRNO__FORMAT ; pr_debug ( \"elf: section(%d) %s, size %ld, link %d, flags %lx, type=%d\\n\" , idx , name , ( unsigned long ) data -> d_size , ( int ) sh -> sh_link , ( unsigned long ) sh -> sh_flags , ( int ) sh -> sh_type ) ; if ( strcmp ( name , \"license\" ) == 0 ) { err = bpf_object__init_license ( obj , data -> d_buf , data -> d_size ) ; if ( err ) return err ; } else if ( strcmp ( name , \"version\" ) == 0 ) { err = bpf_object__init_kversion ( obj , data -> d_buf , data -> d_size ) ; if ( err ) return err ; } else if ( strcmp ( name , \"maps\" ) == 0 ) { pr_warn ( \"elf: legacy map definitions in 'maps' section are not supported by libbpf v1.0+\\n\" ) ; return - ENOTSUP ; } else if ( strcmp ( name , MAPS_ELF_SEC ) == 0 ) { obj -> efile . btf_maps_shndx = idx ; } else if ( strcmp ( name , BTF_ELF_SEC ) == 0 ) { if ( sh -> sh_type != SHT_PROGBITS ) return - LIBBPF_ERRNO__FORMAT ; btf_data = data ; } else if ( strcmp ( name , BTF_EXT_ELF_SEC ) == 0 ) { if ( sh -> sh_type != SHT_PROGBITS ) return - LIBBPF_ERRNO__FORMAT ; btf_ext_data = data ; } else if ( sh -> sh_type == SHT_SYMTAB ) { /* already processed during the first pass above */ } else if ( sh -> sh_type == SHT_PROGBITS && data -> d_size > 0 ) { if ( sh -> sh_flags & SHF_EXECINSTR ) { if ( strcmp ( name , \".text\" ) == 0 ) obj -> efile . text_shndx = idx ; err = bpf_object__add_programs ( obj , data , name , idx ) ; if ( err ) return err ; } else if ( strcmp ( name , DATA_SEC ) == 0 || str_has_pfx ( name , DATA_SEC \".\" ) ) { sec_desc -> sec_type = SEC_DATA ; sec_desc -> shdr = sh ; sec_desc -> data = data ; } else if ( strcmp ( name , RODATA_SEC ) == 0 || str_has_pfx ( name , RODATA_SEC \".\" ) ) { sec_desc -> sec_type = SEC_RODATA ; sec_desc -> shdr = sh ; sec_desc -> data = data ; } else if ( strcmp ( name , STRUCT_OPS_SEC ) == 0 || strcmp ( name , STRUCT_OPS_LINK_SEC ) == 0 || strcmp ( name , \"?\" STRUCT_OPS_SEC ) == 0 || strcmp ( name , \"?\" STRUCT_OPS_LINK_SEC ) == 0 ) { sec_desc -> sec_type = SEC_ST_OPS ; sec_desc -> shdr = sh ; sec_desc -> data = data ; obj -> efile . has_st_ops = true ; } else if ( strcmp ( name , ARENA_SEC ) == 0 ) { obj -> efile . arena_data = data ; obj -> efile . arena_data_shndx = idx ; } else { pr_info ( \"elf: skipping unrecognized data section(%d) %s\\n\" , idx , name ) ; } } else if ( sh -> sh_type == SHT_REL ) { int targ_sec_idx = sh -> sh_info ; /* points to other section */ if ( sh -> sh_entsize != sizeof ( Elf64_Rel ) || targ_sec_idx >= obj -> efile . sec_cnt ) return - LIBBPF_ERRNO__FORMAT ; /* Only do relo for section with exec instructions */ if ( ! section_have_execinstr ( obj , targ_sec_idx ) && strcmp ( name , \".rel\" STRUCT_OPS_SEC ) && strcmp ( name , \".rel\" STRUCT_OPS_LINK_SEC ) && strcmp ( name , \".rel?\" STRUCT_OPS_SEC ) && strcmp ( name , \".rel?\" STRUCT_OPS_LINK_SEC ) && strcmp ( name , \".rel\" MAPS_ELF_SEC ) ) { pr_info ( \"elf: skipping relo section(%d) %s for section(%d) %s\\n\" , idx , name , targ_sec_idx , elf_sec_name ( obj , elf_sec_by_idx ( obj , targ_sec_idx ) ) ? : \"<?>\" ) ; continue ; } sec_desc -> sec_type = SEC_RELO ; sec_desc -> shdr = sh ; sec_desc -> data = data ; } else if ( sh -> sh_type == SHT_NOBITS && ( strcmp ( name , BSS_SEC ) == 0 || str_has_pfx ( name , BSS_SEC \".\" ) ) ) { sec_desc -> sec_type = SEC_BSS ; sec_desc -> shdr = sh ; sec_desc -> data = data ; } else { pr_info ( \"elf: skipping section(%d) %s (size %zu)\\n\" , idx , name , ( size_t ) sh -> sh_size ) ; } } if ( ! obj -> efile . strtabidx || obj -> efile . strtabidx > idx ) { pr_warn ( \"elf: symbol strings section missing or invalid in %s\\n\" , obj -> path ) ; return - LIBBPF_ERRNO__FORMAT ; } /* sort BPF programs by section name and in-section instruction offset\n\t * for faster search\n\t */ if ( obj -> nr_programs ) qsort ( obj -> programs , obj -> nr_programs , sizeof ( * obj -> programs ) , cmp_progs ) ; return bpf_object__init_btf ( obj , btf_data , btf_ext_data ) ; }",
    "resources/libbpf/src/libbpf.c@sym_is_extern": "static bool sym_is_extern ( const Elf64_Sym * sym ) { int bind = ELF64_ST_BIND ( sym -> st_info ) ; /* externs are symbols w/ type=NOTYPE, bind=GLOBAL|WEAK, section=UND */ return sym -> st_shndx == SHN_UNDEF && ( bind == STB_GLOBAL || bind == STB_WEAK ) && ELF64_ST_TYPE ( sym -> st_info ) == STT_NOTYPE ; }",
    "resources/libbpf/src/libbpf.c@sym_is_subprog": "static bool sym_is_subprog ( const Elf64_Sym * sym , int text_shndx ) { int bind = ELF64_ST_BIND ( sym -> st_info ) ; int type = ELF64_ST_TYPE ( sym -> st_info ) ; /* in .text section */ if ( sym -> st_shndx != text_shndx ) return false ; /* local function */ if ( bind == STB_LOCAL && type == STT_SECTION ) return true ; /* global function */ return bind == STB_GLOBAL && type == STT_FUNC ; }",
    "resources/libbpf/src/libbpf.c@find_extern_btf_id": "static int find_extern_btf_id ( const struct btf * btf , const char * ext_name ) { const struct btf_type * t ; const char * tname ; int i , n ; if ( ! btf ) return - ESRCH ; n = btf__type_cnt ( btf ) ; for ( i = 1 ; i < n ; i ++ ) { t = btf__type_by_id ( btf , i ) ; if ( ! btf_is_var ( t ) && ! btf_is_func ( t ) ) continue ; tname = btf__name_by_offset ( btf , t -> name_off ) ; if ( strcmp ( tname , ext_name ) ) continue ; if ( btf_is_var ( t ) && btf_var ( t ) -> linkage != BTF_VAR_GLOBAL_EXTERN ) return - EINVAL ; if ( btf_is_func ( t ) && btf_func_linkage ( t ) != BTF_FUNC_EXTERN ) return - EINVAL ; return i ; } return - ENOENT ; }",
    "resources/libbpf/src/libbpf.c@find_extern_sec_btf_id": "static int find_extern_sec_btf_id ( struct btf * btf , int ext_btf_id ) { const struct btf_var_secinfo * vs ; const struct btf_type * t ; int i , j , n ; if ( ! btf ) return - ESRCH ; n = btf__type_cnt ( btf ) ; for ( i = 1 ; i < n ; i ++ ) { t = btf__type_by_id ( btf , i ) ; if ( ! btf_is_datasec ( t ) ) continue ; vs = btf_var_secinfos ( t ) ; for ( j = 0 ; j < btf_vlen ( t ) ; j ++ , vs ++ ) { if ( vs -> type == ext_btf_id ) return i ; } } return - ENOENT ; }",
    "resources/libbpf/src/libbpf.c@find_kcfg_type": "static enum kcfg_type find_kcfg_type ( const struct btf * btf , int id , bool * is_signed ) { const struct btf_type * t ; const char * name ; t = skip_mods_and_typedefs ( btf , id , NULL ) ; name = btf__name_by_offset ( btf , t -> name_off ) ; if ( is_signed ) * is_signed = false ; switch ( btf_kind ( t ) ) { case BTF_KIND_INT : { int enc = btf_int_encoding ( t ) ; if ( enc & BTF_INT_BOOL ) return t -> size == 1 ? KCFG_BOOL : KCFG_UNKNOWN ; if ( is_signed ) * is_signed = enc & BTF_INT_SIGNED ; if ( t -> size == 1 ) return KCFG_CHAR ; if ( t -> size < 1 || t -> size > 8 || ( t -> size & ( t -> size - 1 ) ) ) return KCFG_UNKNOWN ; return KCFG_INT ; } case BTF_KIND_ENUM : if ( t -> size != 4 ) return KCFG_UNKNOWN ; if ( strcmp ( name , \"libbpf_tristate\" ) ) return KCFG_UNKNOWN ; return KCFG_TRISTATE ; case BTF_KIND_ENUM64 : if ( strcmp ( name , \"libbpf_tristate\" ) ) return KCFG_UNKNOWN ; return KCFG_TRISTATE ; case BTF_KIND_ARRAY : if ( btf_array ( t ) -> nelems == 0 ) return KCFG_UNKNOWN ; if ( find_kcfg_type ( btf , btf_array ( t ) -> type , NULL ) != KCFG_CHAR ) return KCFG_UNKNOWN ; return KCFG_CHAR_ARR ; default : return KCFG_UNKNOWN ; } }",
    "resources/libbpf/src/libbpf.c@cmp_externs": "static int cmp_externs ( const void * _a , const void * _b ) { const struct extern_desc * a = _a ; const struct extern_desc * b = _b ; if ( a -> type != b -> type ) return a -> type < b -> type ? - 1 : 1 ; if ( a -> type == EXT_KCFG ) { /* descending order by alignment requirements */ if ( a -> kcfg . align != b -> kcfg . align ) return a -> kcfg . align > b -> kcfg . align ? - 1 : 1 ; /* ascending order by size, within same alignment class */ if ( a -> kcfg . sz != b -> kcfg . sz ) return a -> kcfg . sz < b -> kcfg . sz ? - 1 : 1 ; } /* resolve ties by name */ return strcmp ( a -> name , b -> name ) ; }",
    "resources/libbpf/src/libbpf.c@find_int_btf_id": "static int find_int_btf_id ( const struct btf * btf ) { const struct btf_type * t ; int i , n ; n = btf__type_cnt ( btf ) ; for ( i = 1 ; i < n ; i ++ ) { t = btf__type_by_id ( btf , i ) ; if ( btf_is_int ( t ) && btf_int_bits ( t ) == 32 ) return i ; } return 0 ; }",
    "resources/libbpf/src/libbpf.c@add_dummy_ksym_var": "static int add_dummy_ksym_var ( struct btf * btf ) { int i , int_btf_id , sec_btf_id , dummy_var_btf_id ; const struct btf_var_secinfo * vs ; const struct btf_type * sec ; if ( ! btf ) return 0 ; sec_btf_id = btf__find_by_name_kind ( btf , KSYMS_SEC , BTF_KIND_DATASEC ) ; if ( sec_btf_id < 0 ) return 0 ; sec = btf__type_by_id ( btf , sec_btf_id ) ; vs = btf_var_secinfos ( sec ) ; for ( i = 0 ; i < btf_vlen ( sec ) ; i ++ , vs ++ ) { const struct btf_type * vt ; vt = btf__type_by_id ( btf , vs -> type ) ; if ( btf_is_func ( vt ) ) break ; } /* No func in ksyms sec.  No need to add dummy var. */ if ( i == btf_vlen ( sec ) ) return 0 ; int_btf_id = find_int_btf_id ( btf ) ; dummy_var_btf_id = btf__add_var ( btf , \"dummy_ksym\" , BTF_VAR_GLOBAL_ALLOCATED , int_btf_id ) ; if ( dummy_var_btf_id < 0 ) pr_warn ( \"cannot create a dummy_ksym var\\n\" ) ; return dummy_var_btf_id ; }",
    "resources/libbpf/src/libbpf.c@bpf_object__collect_externs": "static int bpf_object__collect_externs ( struct bpf_object * obj ) { struct btf_type * sec , * kcfg_sec = NULL , * ksym_sec = NULL ; const struct btf_type * t ; struct extern_desc * ext ; int i , n , off , dummy_var_btf_id ; const char * ext_name , * sec_name ; size_t ext_essent_len ; Elf_Scn * scn ; Elf64_Shdr * sh ; if ( ! obj -> efile . symbols ) return 0 ; scn = elf_sec_by_idx ( obj , obj -> efile . symbols_shndx ) ; sh = elf_sec_hdr ( obj , scn ) ; if ( ! sh || sh -> sh_entsize != sizeof ( Elf64_Sym ) ) return - LIBBPF_ERRNO__FORMAT ; dummy_var_btf_id = add_dummy_ksym_var ( obj -> btf ) ; if ( dummy_var_btf_id < 0 ) return dummy_var_btf_id ; n = sh -> sh_size / sh -> sh_entsize ; pr_debug ( \"looking for externs among %d symbols...\\n\" , n ) ; for ( i = 0 ; i < n ; i ++ ) { Elf64_Sym * sym = elf_sym_by_idx ( obj , i ) ; if ( ! sym ) return - LIBBPF_ERRNO__FORMAT ; if ( ! sym_is_extern ( sym ) ) continue ; ext_name = elf_sym_str ( obj , sym -> st_name ) ; if ( ! ext_name || ! ext_name [ 0 ] ) continue ; ext = obj -> externs ; ext = libbpf_reallocarray ( ext , obj -> nr_extern + 1 , sizeof ( * ext ) ) ; if ( ! ext ) return - ENOMEM ; obj -> externs = ext ; ext = & ext [ obj -> nr_extern ] ; memset ( ext , 0 , sizeof ( * ext ) ) ; obj -> nr_extern ++ ; ext -> btf_id = find_extern_btf_id ( obj -> btf , ext_name ) ; if ( ext -> btf_id <= 0 ) { pr_warn ( \"failed to find BTF for extern '%s': %d\\n\" , ext_name , ext -> btf_id ) ; return ext -> btf_id ; } t = btf__type_by_id ( obj -> btf , ext -> btf_id ) ; ext -> name = btf__name_by_offset ( obj -> btf , t -> name_off ) ; ext -> sym_idx = i ; ext -> is_weak = ELF64_ST_BIND ( sym -> st_info ) == STB_WEAK ; ext_essent_len = bpf_core_essential_name_len ( ext -> name ) ; ext -> essent_name = NULL ; if ( ext_essent_len != strlen ( ext -> name ) ) { ext -> essent_name = strndup ( ext -> name , ext_essent_len ) ; if ( ! ext -> essent_name ) return - ENOMEM ; } ext -> sec_btf_id = find_extern_sec_btf_id ( obj -> btf , ext -> btf_id ) ; if ( ext -> sec_btf_id <= 0 ) { pr_warn ( \"failed to find BTF for extern '%s' [%d] section: %d\\n\" , ext_name , ext -> btf_id , ext -> sec_btf_id ) ; return ext -> sec_btf_id ; } sec = ( void * ) btf__type_by_id ( obj -> btf , ext -> sec_btf_id ) ; sec_name = btf__name_by_offset ( obj -> btf , sec -> name_off ) ; if ( strcmp ( sec_name , KCONFIG_SEC ) == 0 ) { if ( btf_is_func ( t ) ) { pr_warn ( \"extern function %s is unsupported under %s section\\n\" , ext -> name , KCONFIG_SEC ) ; return - ENOTSUP ; } kcfg_sec = sec ; ext -> type = EXT_KCFG ; ext -> kcfg . sz = btf__resolve_size ( obj -> btf , t -> type ) ; if ( ext -> kcfg . sz <= 0 ) { pr_warn ( \"failed to resolve size of extern (kcfg) '%s': %d\\n\" , ext_name , ext -> kcfg . sz ) ; return ext -> kcfg . sz ; } ext -> kcfg . align = btf__align_of ( obj -> btf , t -> type ) ; if ( ext -> kcfg . align <= 0 ) { pr_warn ( \"failed to determine alignment of extern (kcfg) '%s': %d\\n\" , ext_name , ext -> kcfg . align ) ; return - EINVAL ; } ext -> kcfg . type = find_kcfg_type ( obj -> btf , t -> type , & ext -> kcfg . is_signed ) ; if ( ext -> kcfg . type == KCFG_UNKNOWN ) { pr_warn ( \"extern (kcfg) '%s': type is unsupported\\n\" , ext_name ) ; return - ENOTSUP ; } } else if ( strcmp ( sec_name , KSYMS_SEC ) == 0 ) { ksym_sec = sec ; ext -> type = EXT_KSYM ; skip_mods_and_typedefs ( obj -> btf , t -> type , & ext -> ksym . type_id ) ; } else { pr_warn ( \"unrecognized extern section '%s'\\n\" , sec_name ) ; return - ENOTSUP ; } } pr_debug ( \"collected %d externs total\\n\" , obj -> nr_extern ) ; if ( ! obj -> nr_extern ) return 0 ; /* sort externs by type, for kcfg ones also by (align, size, name) */ qsort ( obj -> externs , obj -> nr_extern , sizeof ( * ext ) , cmp_externs ) ; /* for .ksyms section, we need to turn all externs into allocated\n\t * variables in BTF to pass kernel verification; we do this by\n\t * pretending that each extern is a 8-byte variable\n\t */ if ( ksym_sec ) { /* find existing 4-byte integer type in BTF to use for fake\n\t\t * extern variables in DATASEC\n\t\t */ int int_btf_id = find_int_btf_id ( obj -> btf ) ; /* For extern function, a dummy_var added earlier\n\t\t * will be used to replace the vs->type and\n\t\t * its name string will be used to refill\n\t\t * the missing param's name.\n\t\t */ const struct btf_type * dummy_var ; dummy_var = btf__type_by_id ( obj -> btf , dummy_var_btf_id ) ; for ( i = 0 ; i < obj -> nr_extern ; i ++ ) { ext = & obj -> externs [ i ] ; if ( ext -> type != EXT_KSYM ) continue ; pr_debug ( \"extern (ksym) #%d: symbol %d, name %s\\n\" , i , ext -> sym_idx , ext -> name ) ; } sec = ksym_sec ; n = btf_vlen ( sec ) ; for ( i = 0 , off = 0 ; i < n ; i ++ , off += sizeof ( int ) ) { struct btf_var_secinfo * vs = btf_var_secinfos ( sec ) + i ; struct btf_type * vt ; vt = ( void * ) btf__type_by_id ( obj -> btf , vs -> type ) ; ext_name = btf__name_by_offset ( obj -> btf , vt -> name_off ) ; ext = find_extern_by_name ( obj , ext_name ) ; if ( ! ext ) { pr_warn ( \"failed to find extern definition for BTF %s '%s'\\n\" , btf_kind_str ( vt ) , ext_name ) ; return - ESRCH ; } if ( btf_is_func ( vt ) ) { const struct btf_type * func_proto ; struct btf_param * param ; int j ; func_proto = btf__type_by_id ( obj -> btf , vt -> type ) ; param = btf_params ( func_proto ) ; /* Reuse the dummy_var string if the\n\t\t\t\t * func proto does not have param name.\n\t\t\t\t */ for ( j = 0 ; j < btf_vlen ( func_proto ) ; j ++ ) if ( param [ j ] . type && ! param [ j ] . name_off ) param [ j ] . name_off = dummy_var -> name_off ; vs -> type = dummy_var_btf_id ; vt -> info &= ~ 0xffff ; vt -> info |= BTF_FUNC_GLOBAL ; } else { btf_var ( vt ) -> linkage = BTF_VAR_GLOBAL_ALLOCATED ; vt -> type = int_btf_id ; } vs -> offset = off ; vs -> size = sizeof ( int ) ; } sec -> size = off ; } if ( kcfg_sec ) { sec = kcfg_sec ; /* for kcfg externs calculate their offsets within a .kconfig map */ off = 0 ; for ( i = 0 ; i < obj -> nr_extern ; i ++ ) { ext = & obj -> externs [ i ] ; if ( ext -> type != EXT_KCFG ) continue ; ext -> kcfg . data_off = roundup ( off , ext -> kcfg . align ) ; off = ext -> kcfg . data_off + ext -> kcfg . sz ; pr_debug ( \"extern (kcfg) #%d: symbol %d, off %u, name %s\\n\" , i , ext -> sym_idx , ext -> kcfg . data_off , ext -> name ) ; } sec -> size = off ; n = btf_vlen ( sec ) ; for ( i = 0 ; i < n ; i ++ ) { struct btf_var_secinfo * vs = btf_var_secinfos ( sec ) + i ; t = btf__type_by_id ( obj -> btf , vs -> type ) ; ext_name = btf__name_by_offset ( obj -> btf , t -> name_off ) ; ext = find_extern_by_name ( obj , ext_name ) ; if ( ! ext ) { pr_warn ( \"failed to find extern definition for BTF var '%s'\\n\" , ext_name ) ; return - ESRCH ; } btf_var ( t ) -> linkage = BTF_VAR_GLOBAL_ALLOCATED ; vs -> offset = ext -> kcfg . data_off ; } } return 0 ; }",
    "resources/libbpf/src/libbpf.c@prog_is_subprog": "static bool prog_is_subprog ( const struct bpf_object * obj , const struct bpf_program * prog ) { return prog -> sec_idx == obj -> efile . text_shndx && obj -> nr_programs > 1 ; }",
    "resources/libbpf/src/libbpf.c@bpf_object__find_program_by_name": "struct bpf_program * bpf_object__find_program_by_name ( const struct bpf_object * obj , const char * name ) { struct bpf_program * prog ; bpf_object__for_each_program ( prog , obj ) { if ( prog_is_subprog ( obj , prog ) ) continue ; if ( ! strcmp ( prog -> name , name ) ) return prog ; } return errno = ENOENT , NULL ; }",
    "resources/libbpf/src/libbpf.c@bpf_object__shndx_is_data": "static bool bpf_object__shndx_is_data ( const struct bpf_object * obj , int shndx ) { switch ( obj -> efile . secs [ shndx ] . sec_type ) { case SEC_BSS : case SEC_DATA : case SEC_RODATA : return true ; default : return false ; } }",
    "resources/libbpf/src/libbpf.c@bpf_object__shndx_is_maps": "static bool bpf_object__shndx_is_maps ( const struct bpf_object * obj , int shndx ) { return shndx == obj -> efile . btf_maps_shndx ; }",
    "resources/libbpf/src/libbpf.c@bpf_object__section_to_libbpf_map_type": "static enum libbpf_map_type bpf_object__section_to_libbpf_map_type ( const struct bpf_object * obj , int shndx ) { if ( shndx == obj -> efile . symbols_shndx ) return LIBBPF_MAP_KCONFIG ; switch ( obj -> efile . secs [ shndx ] . sec_type ) { case SEC_BSS : return LIBBPF_MAP_BSS ; case SEC_DATA : return LIBBPF_MAP_DATA ; case SEC_RODATA : return LIBBPF_MAP_RODATA ; default : return LIBBPF_MAP_UNSPEC ; } }",
    "resources/libbpf/src/libbpf.c@bpf_program__record_reloc": "static int bpf_program__record_reloc ( struct bpf_program * prog , struct reloc_desc * reloc_desc , __u32 insn_idx , const char * sym_name , const Elf64_Sym * sym , const Elf64_Rel * rel ) { struct bpf_insn * insn = & prog -> insns [ insn_idx ] ; size_t map_idx , nr_maps = prog -> obj -> nr_maps ; struct bpf_object * obj = prog -> obj ; __u32 shdr_idx = sym -> st_shndx ; enum libbpf_map_type type ; const char * sym_sec_name ; struct bpf_map * map ; if ( ! is_call_insn ( insn ) && ! is_ldimm64_insn ( insn ) ) { pr_warn ( \"prog '%s': invalid relo against '%s' for insns[%d].code 0x%x\\n\" , prog -> name , sym_name , insn_idx , insn -> code ) ; return - LIBBPF_ERRNO__RELOC ; } if ( sym_is_extern ( sym ) ) { int sym_idx = ELF64_R_SYM ( rel -> r_info ) ; int i , n = obj -> nr_extern ; struct extern_desc * ext ; for ( i = 0 ; i < n ; i ++ ) { ext = & obj -> externs [ i ] ; if ( ext -> sym_idx == sym_idx ) break ; } if ( i >= n ) { pr_warn ( \"prog '%s': extern relo failed to find extern for '%s' (%d)\\n\" , prog -> name , sym_name , sym_idx ) ; return - LIBBPF_ERRNO__RELOC ; } pr_debug ( \"prog '%s': found extern #%d '%s' (sym %d) for insn #%u\\n\" , prog -> name , i , ext -> name , ext -> sym_idx , insn_idx ) ; if ( insn -> code == ( BPF_JMP | BPF_CALL ) ) reloc_desc -> type = RELO_EXTERN_CALL ; else reloc_desc -> type = RELO_EXTERN_LD64 ; reloc_desc -> insn_idx = insn_idx ; reloc_desc -> ext_idx = i ; return 0 ; } /* sub-program call relocation */ if ( is_call_insn ( insn ) ) { if ( insn -> src_reg != BPF_PSEUDO_CALL ) { pr_warn ( \"prog '%s': incorrect bpf_call opcode\\n\" , prog -> name ) ; return - LIBBPF_ERRNO__RELOC ; } /* text_shndx can be 0, if no default \"main\" program exists */ if ( ! shdr_idx || shdr_idx != obj -> efile . text_shndx ) { sym_sec_name = elf_sec_name ( obj , elf_sec_by_idx ( obj , shdr_idx ) ) ; pr_warn ( \"prog '%s': bad call relo against '%s' in section '%s'\\n\" , prog -> name , sym_name , sym_sec_name ) ; return - LIBBPF_ERRNO__RELOC ; } if ( sym -> st_value % BPF_INSN_SZ ) { pr_warn ( \"prog '%s': bad call relo against '%s' at offset %zu\\n\" , prog -> name , sym_name , ( size_t ) sym -> st_value ) ; return - LIBBPF_ERRNO__RELOC ; } reloc_desc -> type = RELO_CALL ; reloc_desc -> insn_idx = insn_idx ; reloc_desc -> sym_off = sym -> st_value ; return 0 ; } if ( ! shdr_idx || shdr_idx >= SHN_LORESERVE ) { pr_warn ( \"prog '%s': invalid relo against '%s' in special section 0x%x; forgot to initialize global var?..\\n\" , prog -> name , sym_name , shdr_idx ) ; return - LIBBPF_ERRNO__RELOC ; } /* loading subprog addresses */ if ( sym_is_subprog ( sym , obj -> efile . text_shndx ) ) { /* global_func: sym->st_value = offset in the section, insn->imm = 0.\n\t\t * local_func: sym->st_value = 0, insn->imm = offset in the section.\n\t\t */ if ( ( sym -> st_value % BPF_INSN_SZ ) || ( insn -> imm % BPF_INSN_SZ ) ) { pr_warn ( \"prog '%s': bad subprog addr relo against '%s' at offset %zu+%d\\n\" , prog -> name , sym_name , ( size_t ) sym -> st_value , insn -> imm ) ; return - LIBBPF_ERRNO__RELOC ; } reloc_desc -> type = RELO_SUBPROG_ADDR ; reloc_desc -> insn_idx = insn_idx ; reloc_desc -> sym_off = sym -> st_value ; return 0 ; } type = bpf_object__section_to_libbpf_map_type ( obj , shdr_idx ) ; sym_sec_name = elf_sec_name ( obj , elf_sec_by_idx ( obj , shdr_idx ) ) ; /* arena data relocation */ if ( shdr_idx == obj -> efile . arena_data_shndx ) { reloc_desc -> type = RELO_DATA ; reloc_desc -> insn_idx = insn_idx ; reloc_desc -> map_idx = obj -> arena_map - obj -> maps ; reloc_desc -> sym_off = sym -> st_value ; return 0 ; } /* generic map reference relocation */ if ( type == LIBBPF_MAP_UNSPEC ) { if ( ! bpf_object__shndx_is_maps ( obj , shdr_idx ) ) { pr_warn ( \"prog '%s': bad map relo against '%s' in section '%s'\\n\" , prog -> name , sym_name , sym_sec_name ) ; return - LIBBPF_ERRNO__RELOC ; } for ( map_idx = 0 ; map_idx < nr_maps ; map_idx ++ ) { map = & obj -> maps [ map_idx ] ; if ( map -> libbpf_type != type || map -> sec_idx != sym -> st_shndx || map -> sec_offset != sym -> st_value ) continue ; pr_debug ( \"prog '%s': found map %zd (%s, sec %d, off %zu) for insn #%u\\n\" , prog -> name , map_idx , map -> name , map -> sec_idx , map -> sec_offset , insn_idx ) ; break ; } if ( map_idx >= nr_maps ) { pr_warn ( \"prog '%s': map relo failed to find map for section '%s', off %zu\\n\" , prog -> name , sym_sec_name , ( size_t ) sym -> st_value ) ; return - LIBBPF_ERRNO__RELOC ; } reloc_desc -> type = RELO_LD64 ; reloc_desc -> insn_idx = insn_idx ; reloc_desc -> map_idx = map_idx ; reloc_desc -> sym_off = 0 ; /* sym->st_value determines map_idx */ return 0 ; } /* global data map relocation */ if ( ! bpf_object__shndx_is_data ( obj , shdr_idx ) ) { pr_warn ( \"prog '%s': bad data relo against section '%s'\\n\" , prog -> name , sym_sec_name ) ; return - LIBBPF_ERRNO__RELOC ; } for ( map_idx = 0 ; map_idx < nr_maps ; map_idx ++ ) { map = & obj -> maps [ map_idx ] ; if ( map -> libbpf_type != type || map -> sec_idx != sym -> st_shndx ) continue ; pr_debug ( \"prog '%s': found data map %zd (%s, sec %d, off %zu) for insn %u\\n\" , prog -> name , map_idx , map -> name , map -> sec_idx , map -> sec_offset , insn_idx ) ; break ; } if ( map_idx >= nr_maps ) { pr_warn ( \"prog '%s': data relo failed to find map for section '%s'\\n\" , prog -> name , sym_sec_name ) ; return - LIBBPF_ERRNO__RELOC ; } reloc_desc -> type = RELO_DATA ; reloc_desc -> insn_idx = insn_idx ; reloc_desc -> map_idx = map_idx ; reloc_desc -> sym_off = sym -> st_value ; return 0 ; }",
    "resources/libbpf/src/libbpf.c@prog_contains_insn": "static bool prog_contains_insn ( const struct bpf_program * prog , size_t insn_idx ) { return insn_idx >= prog -> sec_insn_off && insn_idx < prog -> sec_insn_off + prog -> sec_insn_cnt ; }",
    "resources/libbpf/src/libbpf.c@find_prog_by_sec_insn": "static struct bpf_program * find_prog_by_sec_insn ( const struct bpf_object * obj , size_t sec_idx , size_t insn_idx ) { int l = 0 , r = obj -> nr_programs - 1 , m ; struct bpf_program * prog ; if ( ! obj -> nr_programs ) return NULL ; while ( l < r ) { m = l + ( r - l + 1 ) / 2 ; prog = & obj -> programs [ m ] ; if ( prog -> sec_idx < sec_idx || ( prog -> sec_idx == sec_idx && prog -> sec_insn_off <= insn_idx ) ) l = m ; else r = m - 1 ; } /* matching program could be at index l, but it still might be the\n\t * wrong one, so we need to double check conditions for the last time\n\t */ prog = & obj -> programs [ l ] ; if ( prog -> sec_idx == sec_idx && prog_contains_insn ( prog , insn_idx ) ) return prog ; return NULL ; }",
    "resources/libbpf/src/libbpf.c@bpf_object__collect_prog_relos": "static int bpf_object__collect_prog_relos ( struct bpf_object * obj , Elf64_Shdr * shdr , Elf_Data * data ) { const char * relo_sec_name , * sec_name ; size_t sec_idx = shdr -> sh_info , sym_idx ; struct bpf_program * prog ; struct reloc_desc * relos ; int err , i , nrels ; const char * sym_name ; __u32 insn_idx ; Elf_Scn * scn ; Elf_Data * scn_data ; Elf64_Sym * sym ; Elf64_Rel * rel ; if ( sec_idx >= obj -> efile . sec_cnt ) return - EINVAL ; scn = elf_sec_by_idx ( obj , sec_idx ) ; scn_data = elf_sec_data ( obj , scn ) ; if ( ! scn_data ) return - LIBBPF_ERRNO__FORMAT ; relo_sec_name = elf_sec_str ( obj , shdr -> sh_name ) ; sec_name = elf_sec_name ( obj , scn ) ; if ( ! relo_sec_name || ! sec_name ) return - EINVAL ; pr_debug ( \"sec '%s': collecting relocation for section(%zu) '%s'\\n\" , relo_sec_name , sec_idx , sec_name ) ; nrels = shdr -> sh_size / shdr -> sh_entsize ; for ( i = 0 ; i < nrels ; i ++ ) { rel = elf_rel_by_idx ( data , i ) ; if ( ! rel ) { pr_warn ( \"sec '%s': failed to get relo #%d\\n\" , relo_sec_name , i ) ; return - LIBBPF_ERRNO__FORMAT ; } sym_idx = ELF64_R_SYM ( rel -> r_info ) ; sym = elf_sym_by_idx ( obj , sym_idx ) ; if ( ! sym ) { pr_warn ( \"sec '%s': symbol #%zu not found for relo #%d\\n\" , relo_sec_name , sym_idx , i ) ; return - LIBBPF_ERRNO__FORMAT ; } if ( sym -> st_shndx >= obj -> efile . sec_cnt ) { pr_warn ( \"sec '%s': corrupted symbol #%zu pointing to invalid section #%zu for relo #%d\\n\" , relo_sec_name , sym_idx , ( size_t ) sym -> st_shndx , i ) ; return - LIBBPF_ERRNO__FORMAT ; } if ( rel -> r_offset % BPF_INSN_SZ || rel -> r_offset >= scn_data -> d_size ) { pr_warn ( \"sec '%s': invalid offset 0x%zx for relo #%d\\n\" , relo_sec_name , ( size_t ) rel -> r_offset , i ) ; return - LIBBPF_ERRNO__FORMAT ; } insn_idx = rel -> r_offset / BPF_INSN_SZ ; /* relocations against static functions are recorded as\n\t\t * relocations against the section that contains a function;\n\t\t * in such case, symbol will be STT_SECTION and sym.st_name\n\t\t * will point to empty string (0), so fetch section name\n\t\t * instead\n\t\t */ if ( ELF64_ST_TYPE ( sym -> st_info ) == STT_SECTION && sym -> st_name == 0 ) sym_name = elf_sec_name ( obj , elf_sec_by_idx ( obj , sym -> st_shndx ) ) ; else sym_name = elf_sym_str ( obj , sym -> st_name ) ; sym_name = sym_name ? : \"<?\" ; pr_debug ( \"sec '%s': relo #%d: insn #%u against '%s'\\n\" , relo_sec_name , i , insn_idx , sym_name ) ; prog = find_prog_by_sec_insn ( obj , sec_idx , insn_idx ) ; if ( ! prog ) { pr_debug ( \"sec '%s': relo #%d: couldn't find program in section '%s' for insn #%u, probably overridden weak function, skipping...\\n\" , relo_sec_name , i , sec_name , insn_idx ) ; continue ; } relos = libbpf_reallocarray ( prog -> reloc_desc , prog -> nr_reloc + 1 , sizeof ( * relos ) ) ; if ( ! relos ) return - ENOMEM ; prog -> reloc_desc = relos ; /* adjust insn_idx to local BPF program frame of reference */ insn_idx -= prog -> sec_insn_off ; err = bpf_program__record_reloc ( prog , & relos [ prog -> nr_reloc ] , insn_idx , sym_name , sym , rel ) ; if ( err ) return err ; prog -> nr_reloc ++ ; } return 0 ; }",
    "resources/libbpf/src/libbpf.c@map_fill_btf_type_info": "static int map_fill_btf_type_info ( struct bpf_object * obj , struct bpf_map * map ) { int id ; if ( ! obj -> btf ) return - ENOENT ; /* if it's BTF-defined map, we don't need to search for type IDs.\n\t * For struct_ops map, it does not need btf_key_type_id and\n\t * btf_value_type_id.\n\t */ if ( map -> sec_idx == obj -> efile . btf_maps_shndx || bpf_map__is_struct_ops ( map ) ) return 0 ; /*\n\t * LLVM annotates global data differently in BTF, that is,\n\t * only as '.data', '.bss' or '.rodata'.\n\t */ if ( ! bpf_map__is_internal ( map ) ) return - ENOENT ; id = btf__find_by_name ( obj -> btf , map -> real_name ) ; if ( id < 0 ) return id ; map -> btf_key_type_id = 0 ; map -> btf_value_type_id = id ; return 0 ; }",
    "resources/libbpf/src/libbpf.c@bpf_get_map_info_from_fdinfo": "static int bpf_get_map_info_from_fdinfo ( int fd , struct bpf_map_info * info ) { char file [ PATH_MAX ] , buff [ 4096 ] ; FILE * fp ; __u32 val ; int err ; snprintf ( file , sizeof ( file ) , \"/proc/%d/fdinfo/%d\" , getpid ( ) , fd ) ; memset ( info , 0 , sizeof ( * info ) ) ; fp = fopen ( file , \"re\" ) ; if ( ! fp ) { err = - errno ; pr_warn ( \"failed to open %s: %d. No procfs support?\\n\" , file , err ) ; return err ; } while ( fgets ( buff , sizeof ( buff ) , fp ) ) { if ( sscanf ( buff , \"map_type:\\t%u\" , & val ) == 1 ) info -> type = val ; else if ( sscanf ( buff , \"key_size:\\t%u\" , & val ) == 1 ) info -> key_size = val ; else if ( sscanf ( buff , \"value_size:\\t%u\" , & val ) == 1 ) info -> value_size = val ; else if ( sscanf ( buff , \"max_entries:\\t%u\" , & val ) == 1 ) info -> max_entries = val ; else if ( sscanf ( buff , \"map_flags:\\t%i\" , & val ) == 1 ) info -> map_flags = val ; } fclose ( fp ) ; return 0 ; }",
    "resources/libbpf/src/libbpf.c@bpf_map__autocreate": "",
    "resources/libbpf/src/libbpf.c@bpf_map__set_autocreate": "int bpf_map__set_autocreate ( struct bpf_map * map , bool autocreate ) { if ( map -> obj -> loaded ) return libbpf_err ( - EBUSY ) ; map -> autocreate = autocreate ; return 0 ; }",
    "resources/libbpf/src/libbpf.c@bpf_map__reuse_fd": "int bpf_map__reuse_fd ( struct bpf_map * map , int fd ) { struct bpf_map_info info ; __u32 len = sizeof ( info ) , name_len ; int new_fd , err ; char * new_name ; memset ( & info , 0 , len ) ; err = bpf_map_get_info_by_fd ( fd , & info , & len ) ; if ( err && errno == EINVAL ) err = bpf_get_map_info_from_fdinfo ( fd , & info ) ; if ( err ) return libbpf_err ( err ) ; name_len = strlen ( info . name ) ; if ( name_len == BPF_OBJ_NAME_LEN - 1 && strncmp ( map -> name , info . name , name_len ) == 0 ) new_name = strdup ( map -> name ) ; else new_name = strdup ( info . name ) ; if ( ! new_name ) return libbpf_err ( - errno ) ; /*\n\t * Like dup(), but make sure new FD is >= 3 and has O_CLOEXEC set.\n\t * This is similar to what we do in ensure_good_fd(), but without\n\t * closing original FD.\n\t */ new_fd = fcntl ( fd , F_DUPFD_CLOEXEC , 3 ) ; if ( new_fd < 0 ) { err = - errno ; goto err_free_new_name ; } err = reuse_fd ( map -> fd , new_fd ) ; if ( err ) goto err_free_new_name ; free ( map -> name ) ; map -> name = new_name ; map -> def . type = info . type ; map -> def . key_size = info . key_size ; map -> def . value_size = info . value_size ; map -> def . max_entries = info . max_entries ; map -> def . map_flags = info . map_flags ; map -> btf_key_type_id = info . btf_key_type_id ; map -> btf_value_type_id = info . btf_value_type_id ; map -> reused = true ; map -> map_extra = info . map_extra ; return 0 ; err_free_new_name : free ( new_name ) ; return libbpf_err ( err ) ; }",
    "resources/libbpf/src/libbpf.c@bpf_map__max_entries": "__u32 bpf_map__max_entries ( const struct bpf_map * map ) { return map -> def . max_entries ; }",
    "resources/libbpf/src/libbpf.c@bpf_map__inner_map": "struct bpf_map * bpf_map__inner_map ( struct bpf_map * map ) { if ( ! bpf_map_type__is_map_in_map ( map -> def . type ) ) return errno = EINVAL , NULL ; return map -> inner_map ; }",
    "resources/libbpf/src/libbpf.c@bpf_map__set_max_entries": "int bpf_map__set_max_entries ( struct bpf_map * map , __u32 max_entries ) { if ( map -> obj -> loaded ) return libbpf_err ( - EBUSY ) ; map -> def . max_entries = max_entries ; /* auto-adjust BPF ringbuf map max_entries to be a multiple of page size */ if ( map_is_ringbuf ( map ) ) map -> def . max_entries = adjust_ringbuf_sz ( map -> def . max_entries ) ; return 0 ; }",
    "resources/libbpf/src/libbpf.c@bpf_object_prepare_token": "static int bpf_object_prepare_token ( struct bpf_object * obj ) { const char * bpffs_path ; int bpffs_fd = - 1 , token_fd , err ; bool mandatory ; enum libbpf_print_level level ; /* token is explicitly prevented */ if ( obj -> token_path && obj -> token_path [ 0 ] == '\\0' ) { pr_debug ( \"object '%s': token is prevented, skipping...\\n\" , obj -> name ) ; return 0 ; } mandatory = obj -> token_path != NULL ; level = mandatory ? LIBBPF_WARN : LIBBPF_DEBUG ; bpffs_path = obj -> token_path ? : BPF_FS_DEFAULT_PATH ; bpffs_fd = open ( bpffs_path , O_DIRECTORY , O_RDWR ) ; if ( bpffs_fd < 0 ) { err = - errno ; __pr ( level , \"object '%s': failed (%d) to open BPF FS mount at '%s'%s\\n\" , obj -> name , err , bpffs_path , mandatory ? \"\" : \", skipping optional step...\" ) ; return mandatory ? err : 0 ; } token_fd = bpf_token_create ( bpffs_fd , 0 ) ; close ( bpffs_fd ) ; if ( token_fd < 0 ) { if ( ! mandatory && token_fd == - ENOENT ) { pr_debug ( \"object '%s': BPF FS at '%s' doesn't have BPF token delegation set up, skipping...\\n\" , obj -> name , bpffs_path ) ; return 0 ; } __pr ( level , \"object '%s': failed (%d) to create BPF token from '%s'%s\\n\" , obj -> name , token_fd , bpffs_path , mandatory ? \"\" : \", skipping optional step...\" ) ; return mandatory ? token_fd : 0 ; } obj -> feat_cache = calloc ( 1 , sizeof ( * obj -> feat_cache ) ) ; if ( ! obj -> feat_cache ) { close ( token_fd ) ; return - ENOMEM ; } obj -> token_fd = token_fd ; obj -> feat_cache -> token_fd = token_fd ; return 0 ; }",
    "resources/libbpf/src/libbpf.c@bpf_object__probe_loading": "static int bpf_object__probe_loading ( struct bpf_object * obj ) { char * cp , errmsg [ STRERR_BUFSIZE ] ; struct bpf_insn insns [ ] = { BPF_MOV64_IMM ( BPF_REG_0 , 0 ) , BPF_EXIT_INSN ( ) , } ; int ret , insn_cnt = ARRAY_SIZE ( insns ) ; LIBBPF_OPTS ( bpf_prog_load_opts , opts , . token_fd = obj -> token_fd , . prog_flags = obj -> token_fd ? BPF_F_TOKEN_FD : 0 , ) ; if ( obj -> gen_loader ) return 0 ; ret = bump_rlimit_memlock ( ) ; if ( ret ) pr_warn ( \"Failed to bump RLIMIT_MEMLOCK (err = %d), you might need to do it explicitly!\\n\" , ret ) ; /* make sure basic loading works */ ret = bpf_prog_load ( BPF_PROG_TYPE_SOCKET_FILTER , NULL , \"GPL\" , insns , insn_cnt , & opts ) ; if ( ret < 0 ) ret = bpf_prog_load ( BPF_PROG_TYPE_TRACEPOINT , NULL , \"GPL\" , insns , insn_cnt , & opts ) ; if ( ret < 0 ) { ret = errno ; cp = libbpf_strerror_r ( ret , errmsg , sizeof ( errmsg ) ) ; pr_warn ( \"Error in %s():%s(%d). Couldn't load trivial BPF \" \"program. Make sure your kernel supports BPF \" \"(CONFIG_BPF_SYSCALL=y) and/or that RLIMIT_MEMLOCK is \" \"set to big enough value.\\n\" , __func__ , cp , ret ) ; return - ret ; } close ( ret ) ; return 0 ; }",
    "resources/libbpf/src/libbpf.c@kernel_supports": "",
    "resources/libbpf/src/libbpf.c@map_is_reuse_compat": "static bool map_is_reuse_compat ( const struct bpf_map * map , int map_fd ) { struct bpf_map_info map_info ; char msg [ STRERR_BUFSIZE ] ; __u32 map_info_len = sizeof ( map_info ) ; int err ; memset ( & map_info , 0 , map_info_len ) ; err = bpf_map_get_info_by_fd ( map_fd , & map_info , & map_info_len ) ; if ( err && errno == EINVAL ) err = bpf_get_map_info_from_fdinfo ( map_fd , & map_info ) ; if ( err ) { pr_warn ( \"failed to get map info for map FD %d: %s\\n\" , map_fd , libbpf_strerror_r ( errno , msg , sizeof ( msg ) ) ) ; return false ; } return ( map_info . type == map -> def . type && map_info . key_size == map -> def . key_size && map_info . value_size == map -> def . value_size && map_info . max_entries == map -> def . max_entries && map_info . map_flags == map -> def . map_flags && map_info . map_extra == map -> map_extra ) ; }",
    "resources/libbpf/src/libbpf.c@bpf_object__reuse_map": "static int bpf_object__reuse_map ( struct bpf_map * map ) { char * cp , errmsg [ STRERR_BUFSIZE ] ; int err , pin_fd ; pin_fd = bpf_obj_get ( map -> pin_path ) ; if ( pin_fd < 0 ) { err = - errno ; if ( err == - ENOENT ) { pr_debug ( \"found no pinned map to reuse at '%s'\\n\" , map -> pin_path ) ; return 0 ; } cp = libbpf_strerror_r ( - err , errmsg , sizeof ( errmsg ) ) ; pr_warn ( \"couldn't retrieve pinned map '%s': %s\\n\" , map -> pin_path , cp ) ; return err ; } if ( ! map_is_reuse_compat ( map , pin_fd ) ) { pr_warn ( \"couldn't reuse pinned map at '%s': parameter mismatch\\n\" , map -> pin_path ) ; close ( pin_fd ) ; return - EINVAL ; } err = bpf_map__reuse_fd ( map , pin_fd ) ; close ( pin_fd ) ; if ( err ) return err ; map -> pinned = true ; pr_debug ( \"reused pinned map at '%s'\\n\" , map -> pin_path ) ; return 0 ; }",
    "resources/libbpf/src/libbpf.c@bpf_object__populate_internal_map": "static int bpf_object__populate_internal_map ( struct bpf_object * obj , struct bpf_map * map ) { enum libbpf_map_type map_type = map -> libbpf_type ; char * cp , errmsg [ STRERR_BUFSIZE ] ; int err , zero = 0 ; if ( obj -> gen_loader ) { bpf_gen__map_update_elem ( obj -> gen_loader , map - obj -> maps , map -> mmaped , map -> def . value_size ) ; if ( map_type == LIBBPF_MAP_RODATA || map_type == LIBBPF_MAP_KCONFIG ) bpf_gen__map_freeze ( obj -> gen_loader , map - obj -> maps ) ; return 0 ; } err = bpf_map_update_elem ( map -> fd , & zero , map -> mmaped , 0 ) ; if ( err ) { err = - errno ; cp = libbpf_strerror_r ( err , errmsg , sizeof ( errmsg ) ) ; pr_warn ( \"Error setting initial map(%s) contents: %s\\n\" , map -> name , cp ) ; return err ; } /* Freeze .rodata and .kconfig map as read-only from syscall side. */ if ( map_type == LIBBPF_MAP_RODATA || map_type == LIBBPF_MAP_KCONFIG ) { err = bpf_map_freeze ( map -> fd ) ; if ( err ) { err = - errno ; cp = libbpf_strerror_r ( err , errmsg , sizeof ( errmsg ) ) ; pr_warn ( \"Error freezing map(%s) as read-only: %s\\n\" , map -> name , cp ) ; return err ; } } return 0 ; }",
    "resources/libbpf/src/libbpf.c@map_is_created": "static bool map_is_created ( const struct bpf_map * map ) { return map -> obj -> loaded || map -> reused ; }",
    "resources/libbpf/src/libbpf.c@bpf_object__create_map": "static int bpf_object__create_map ( struct bpf_object * obj , struct bpf_map * map , bool is_inner ) { LIBBPF_OPTS ( bpf_map_create_opts , create_attr ) ; struct bpf_map_def * def = & map -> def ; const char * map_name = NULL ; int err = 0 , map_fd ; if ( kernel_supports ( obj , FEAT_PROG_NAME ) ) map_name = map -> name ; create_attr . map_ifindex = map -> map_ifindex ; create_attr . map_flags = def -> map_flags ; create_attr . numa_node = map -> numa_node ; create_attr . map_extra = map -> map_extra ; create_attr . token_fd = obj -> token_fd ; if ( obj -> token_fd ) create_attr . map_flags |= BPF_F_TOKEN_FD ; if ( bpf_map__is_struct_ops ( map ) ) { create_attr . btf_vmlinux_value_type_id = map -> btf_vmlinux_value_type_id ; if ( map -> mod_btf_fd >= 0 ) { create_attr . value_type_btf_obj_fd = map -> mod_btf_fd ; create_attr . map_flags |= BPF_F_VTYPE_BTF_OBJ_FD ; } } if ( obj -> btf && btf__fd ( obj -> btf ) >= 0 ) { create_attr . btf_fd = btf__fd ( obj -> btf ) ; create_attr . btf_key_type_id = map -> btf_key_type_id ; create_attr . btf_value_type_id = map -> btf_value_type_id ; } if ( bpf_map_type__is_map_in_map ( def -> type ) ) { if ( map -> inner_map ) { err = map_set_def_max_entries ( map -> inner_map ) ; if ( err ) return err ; err = bpf_object__create_map ( obj , map -> inner_map , true ) ; if ( err ) { pr_warn ( \"map '%s': failed to create inner map: %d\\n\" , map -> name , err ) ; return err ; } map -> inner_map_fd = map -> inner_map -> fd ; } if ( map -> inner_map_fd >= 0 ) create_attr . inner_map_fd = map -> inner_map_fd ; } switch ( def -> type ) { case BPF_MAP_TYPE_PERF_EVENT_ARRAY : case BPF_MAP_TYPE_CGROUP_ARRAY : case BPF_MAP_TYPE_STACK_TRACE : case BPF_MAP_TYPE_ARRAY_OF_MAPS : case BPF_MAP_TYPE_HASH_OF_MAPS : case BPF_MAP_TYPE_DEVMAP : case BPF_MAP_TYPE_DEVMAP_HASH : case BPF_MAP_TYPE_CPUMAP : case BPF_MAP_TYPE_XSKMAP : case BPF_MAP_TYPE_SOCKMAP : case BPF_MAP_TYPE_SOCKHASH : case BPF_MAP_TYPE_QUEUE : case BPF_MAP_TYPE_STACK : case BPF_MAP_TYPE_ARENA : create_attr . btf_fd = 0 ; create_attr . btf_key_type_id = 0 ; create_attr . btf_value_type_id = 0 ; map -> btf_key_type_id = 0 ; map -> btf_value_type_id = 0 ; break ; case BPF_MAP_TYPE_STRUCT_OPS : create_attr . btf_value_type_id = 0 ; break ; default : break ; } if ( obj -> gen_loader ) { bpf_gen__map_create ( obj -> gen_loader , def -> type , map_name , def -> key_size , def -> value_size , def -> max_entries , & create_attr , is_inner ? - 1 : map - obj -> maps ) ; /* We keep pretenting we have valid FD to pass various fd >= 0\n\t\t * checks by just keeping original placeholder FDs in place.\n\t\t * See bpf_object__add_map() comment.\n\t\t * This placeholder fd will not be used with any syscall and\n\t\t * will be reset to -1 eventually.\n\t\t */ map_fd = map -> fd ; } else { map_fd = bpf_map_create ( def -> type , map_name , def -> key_size , def -> value_size , def -> max_entries , & create_attr ) ; } if ( map_fd < 0 && ( create_attr . btf_key_type_id || create_attr . btf_value_type_id ) ) { char * cp , errmsg [ STRERR_BUFSIZE ] ; err = - errno ; cp = libbpf_strerror_r ( err , errmsg , sizeof ( errmsg ) ) ; pr_warn ( \"Error in bpf_create_map_xattr(%s):%s(%d). Retrying without BTF.\\n\" , map -> name , cp , err ) ; create_attr . btf_fd = 0 ; create_attr . btf_key_type_id = 0 ; create_attr . btf_value_type_id = 0 ; map -> btf_key_type_id = 0 ; map -> btf_value_type_id = 0 ; map_fd = bpf_map_create ( def -> type , map_name , def -> key_size , def -> value_size , def -> max_entries , & create_attr ) ; } if ( bpf_map_type__is_map_in_map ( def -> type ) && map -> inner_map ) { if ( obj -> gen_loader ) map -> inner_map -> fd = - 1 ; bpf_map__destroy ( map -> inner_map ) ; zfree ( & map -> inner_map ) ; } if ( map_fd < 0 ) return map_fd ; /* obj->gen_loader case, prevent reuse_fd() from closing map_fd */ if ( map -> fd == map_fd ) return 0 ; /* Keep placeholder FD value but now point it to the BPF map object.\n\t * This way everything that relied on this map's FD (e.g., relocated\n\t * ldimm64 instructions) will stay valid and won't need adjustments.\n\t * map->fd stays valid but now point to what map_fd points to.\n\t */ return reuse_fd ( map -> fd , map_fd ) ; }",
    "resources/libbpf/src/libbpf.c@init_map_in_map_slots": "static int init_map_in_map_slots ( struct bpf_object * obj , struct bpf_map * map ) { const struct bpf_map * targ_map ; unsigned int i ; int fd , err = 0 ; for ( i = 0 ; i < map -> init_slots_sz ; i ++ ) { if ( ! map -> init_slots [ i ] ) continue ; targ_map = map -> init_slots [ i ] ; fd = targ_map -> fd ; if ( obj -> gen_loader ) { bpf_gen__populate_outer_map ( obj -> gen_loader , map - obj -> maps , i , targ_map - obj -> maps ) ; } else { err = bpf_map_update_elem ( map -> fd , & i , & fd , 0 ) ; } if ( err ) { err = - errno ; pr_warn ( \"map '%s': failed to initialize slot [%d] to map '%s' fd=%d: %d\\n\" , map -> name , i , targ_map -> name , fd , err ) ; return err ; } pr_debug ( \"map '%s': slot [%d] set to map '%s' fd=%d\\n\" , map -> name , i , targ_map -> name , fd ) ; } zfree ( & map -> init_slots ) ; map -> init_slots_sz = 0 ; return 0 ; }",
    "resources/libbpf/src/libbpf.c@init_prog_array_slots": "static int init_prog_array_slots ( struct bpf_object * obj , struct bpf_map * map ) { const struct bpf_program * targ_prog ; unsigned int i ; int fd , err ; if ( obj -> gen_loader ) return - ENOTSUP ; for ( i = 0 ; i < map -> init_slots_sz ; i ++ ) { if ( ! map -> init_slots [ i ] ) continue ; targ_prog = map -> init_slots [ i ] ; fd = bpf_program__fd ( targ_prog ) ; err = bpf_map_update_elem ( map -> fd , & i , & fd , 0 ) ; if ( err ) { err = - errno ; pr_warn ( \"map '%s': failed to initialize slot [%d] to prog '%s' fd=%d: %d\\n\" , map -> name , i , targ_prog -> name , fd , err ) ; return err ; } pr_debug ( \"map '%s': slot [%d] set to prog '%s' fd=%d\\n\" , map -> name , i , targ_prog -> name , fd ) ; } zfree ( & map -> init_slots ) ; map -> init_slots_sz = 0 ; return 0 ; }",
    "resources/libbpf/src/libbpf.c@bpf_object_init_prog_arrays": "static int bpf_object_init_prog_arrays ( struct bpf_object * obj ) { struct bpf_map * map ; int i , err ; for ( i = 0 ; i < obj -> nr_maps ; i ++ ) { map = & obj -> maps [ i ] ; if ( ! map -> init_slots_sz || map -> def . type != BPF_MAP_TYPE_PROG_ARRAY ) continue ; err = init_prog_array_slots ( obj , map ) ; if ( err < 0 ) return err ; } return 0 ; }",
    "resources/libbpf/src/libbpf.c@map_set_def_max_entries": "static int map_set_def_max_entries ( struct bpf_map * map ) { if ( map -> def . type == BPF_MAP_TYPE_PERF_EVENT_ARRAY && ! map -> def . max_entries ) { int nr_cpus ; nr_cpus = libbpf_num_possible_cpus ( ) ; if ( nr_cpus < 0 ) { pr_warn ( \"map '%s': failed to determine number of system CPUs: %d\\n\" , map -> name , nr_cpus ) ; return nr_cpus ; } pr_debug ( \"map '%s': setting size to %d\\n\" , map -> name , nr_cpus ) ; map -> def . max_entries = nr_cpus ; } return 0 ; }",
    "resources/libbpf/src/libbpf.c@bpf_object__create_maps": "static int bpf_object__create_maps ( struct bpf_object * obj ) { struct bpf_map * map ; char * cp , errmsg [ STRERR_BUFSIZE ] ; unsigned int i , j ; int err ; bool retried ; for ( i = 0 ; i < obj -> nr_maps ; i ++ ) { map = & obj -> maps [ i ] ; /* To support old kernels, we skip creating global data maps\n\t\t * (.rodata, .data, .kconfig, etc); later on, during program\n\t\t * loading, if we detect that at least one of the to-be-loaded\n\t\t * programs is referencing any global data map, we'll error\n\t\t * out with program name and relocation index logged.\n\t\t * This approach allows to accommodate Clang emitting\n\t\t * unnecessary .rodata.str1.1 sections for string literals,\n\t\t * but also it allows to have CO-RE applications that use\n\t\t * global variables in some of BPF programs, but not others.\n\t\t * If those global variable-using programs are not loaded at\n\t\t * runtime due to bpf_program__set_autoload(prog, false),\n\t\t * bpf_object loading will succeed just fine even on old\n\t\t * kernels.\n\t\t */ if ( bpf_map__is_internal ( map ) && ! kernel_supports ( obj , FEAT_GLOBAL_DATA ) ) map -> autocreate = false ; if ( ! map -> autocreate ) { pr_debug ( \"map '%s': skipped auto-creating...\\n\" , map -> name ) ; continue ; } err = map_set_def_max_entries ( map ) ; if ( err ) goto err_out ; retried = false ; retry : if ( map -> pin_path ) { err = bpf_object__reuse_map ( map ) ; if ( err ) { pr_warn ( \"map '%s': error reusing pinned map\\n\" , map -> name ) ; goto err_out ; } if ( retried && map -> fd < 0 ) { pr_warn ( \"map '%s': cannot find pinned map\\n\" , map -> name ) ; err = - ENOENT ; goto err_out ; } } if ( map -> reused ) { pr_debug ( \"map '%s': skipping creation (preset fd=%d)\\n\" , map -> name , map -> fd ) ; } else { err = bpf_object__create_map ( obj , map , false ) ; if ( err ) goto err_out ; pr_debug ( \"map '%s': created successfully, fd=%d\\n\" , map -> name , map -> fd ) ; if ( bpf_map__is_internal ( map ) ) { err = bpf_object__populate_internal_map ( obj , map ) ; if ( err < 0 ) goto err_out ; } if ( map -> def . type == BPF_MAP_TYPE_ARENA ) { map -> mmaped = mmap ( ( void * ) ( long ) map -> map_extra , bpf_map_mmap_sz ( map ) , PROT_READ | PROT_WRITE , map -> map_extra ? MAP_SHARED | MAP_FIXED : MAP_SHARED , map -> fd , 0 ) ; if ( map -> mmaped == MAP_FAILED ) { err = - errno ; map -> mmaped = NULL ; pr_warn ( \"map '%s': failed to mmap arena: %d\\n\" , map -> name , err ) ; return err ; } if ( obj -> arena_data ) { memcpy ( map -> mmaped , obj -> arena_data , obj -> arena_data_sz ) ; zfree ( & obj -> arena_data ) ; } } if ( map -> init_slots_sz && map -> def . type != BPF_MAP_TYPE_PROG_ARRAY ) { err = init_map_in_map_slots ( obj , map ) ; if ( err < 0 ) goto err_out ; } } if ( map -> pin_path && ! map -> pinned ) { err = bpf_map__pin ( map , NULL ) ; if ( err ) { if ( ! retried && err == - EEXIST ) { retried = true ; goto retry ; } pr_warn ( \"map '%s': failed to auto-pin at '%s': %d\\n\" , map -> name , map -> pin_path , err ) ; goto err_out ; } } } return 0 ; err_out : cp = libbpf_strerror_r ( err , errmsg , sizeof ( errmsg ) ) ; pr_warn ( \"map '%s': failed to create: %s(%d)\\n\" , map -> name , cp , err ) ; pr_perm_msg ( err ) ; for ( j = 0 ; j < i ; j ++ ) zclose ( obj -> maps [ j ] . fd ) ; return err ; }",
    "resources/libbpf/src/libbpf.c@bpf_core_is_flavor_sep": "static bool bpf_core_is_flavor_sep ( const char * s ) { /* check X___Y name pattern, where X and Y are not underscores */ return s [ 0 ] != '_' && /* X */ s [ 1 ] == '_' && s [ 2 ] == '_' && s [ 3 ] == '_' && /* ___ */ s [ 4 ] != '_' ; /* Y */ }",
    "resources/libbpf/src/libbpf.c@bpf_core_essential_name_len": "size_t bpf_core_essential_name_len ( const char * name ) { size_t n = strlen ( name ) ; int i ; for ( i = n - 5 ; i >= 0 ; i -- ) { if ( bpf_core_is_flavor_sep ( name + i ) ) return i + 1 ; } return n ; }",
    "resources/libbpf/src/libbpf.c@bpf_core_free_cands": "void bpf_core_free_cands ( struct bpf_core_cand_list * cands ) { if ( ! cands ) return ; free ( cands -> cands ) ; free ( cands ) ; }",
    "resources/libbpf/src/libbpf.c@bpf_core_add_cands": "int bpf_core_add_cands ( struct bpf_core_cand * local_cand , size_t local_essent_len , const struct btf * targ_btf , const char * targ_btf_name , int targ_start_id , struct bpf_core_cand_list * cands ) { struct bpf_core_cand * new_cands , * cand ; const struct btf_type * t , * local_t ; const char * targ_name , * local_name ; size_t targ_essent_len ; int n , i ; local_t = btf__type_by_id ( local_cand -> btf , local_cand -> id ) ; local_name = btf__str_by_offset ( local_cand -> btf , local_t -> name_off ) ; n = btf__type_cnt ( targ_btf ) ; for ( i = targ_start_id ; i < n ; i ++ ) { t = btf__type_by_id ( targ_btf , i ) ; if ( ! btf_kind_core_compat ( t , local_t ) ) continue ; targ_name = btf__name_by_offset ( targ_btf , t -> name_off ) ; if ( str_is_empty ( targ_name ) ) continue ; targ_essent_len = bpf_core_essential_name_len ( targ_name ) ; if ( targ_essent_len != local_essent_len ) continue ; if ( strncmp ( local_name , targ_name , local_essent_len ) != 0 ) continue ; pr_debug ( \"CO-RE relocating [%d] %s %s: found target candidate [%d] %s %s in [%s]\\n\" , local_cand -> id , btf_kind_str ( local_t ) , local_name , i , btf_kind_str ( t ) , targ_name , targ_btf_name ) ; new_cands = libbpf_reallocarray ( cands -> cands , cands -> len + 1 , sizeof ( * cands -> cands ) ) ; if ( ! new_cands ) return - ENOMEM ; cand = & new_cands [ cands -> len ] ; cand -> btf = targ_btf ; cand -> id = i ; cands -> cands = new_cands ; cands -> len ++ ; } return 0 ; }",
    "resources/libbpf/src/libbpf.c@load_module_btfs": "static int load_module_btfs ( struct bpf_object * obj ) { struct bpf_btf_info info ; struct module_btf * mod_btf ; struct btf * btf ; char name [ 64 ] ; __u32 id = 0 , len ; int err , fd ; if ( obj -> btf_modules_loaded ) return 0 ; if ( obj -> gen_loader ) return 0 ; /* don't do this again, even if we find no module BTFs */ obj -> btf_modules_loaded = true ; /* kernel too old to support module BTFs */ if ( ! kernel_supports ( obj , FEAT_MODULE_BTF ) ) return 0 ; while ( true ) { err = bpf_btf_get_next_id ( id , & id ) ; if ( err && errno == ENOENT ) return 0 ; if ( err && errno == EPERM ) { pr_debug ( \"skipping module BTFs loading, missing privileges\\n\" ) ; return 0 ; } if ( err ) { err = - errno ; pr_warn ( \"failed to iterate BTF objects: %d\\n\" , err ) ; return err ; } fd = bpf_btf_get_fd_by_id ( id ) ; if ( fd < 0 ) { if ( errno == ENOENT ) continue ; /* expected race: BTF was unloaded */ err = - errno ; pr_warn ( \"failed to get BTF object #%d FD: %d\\n\" , id , err ) ; return err ; } len = sizeof ( info ) ; memset ( & info , 0 , sizeof ( info ) ) ; info . name = ptr_to_u64 ( name ) ; info . name_len = sizeof ( name ) ; err = bpf_btf_get_info_by_fd ( fd , & info , & len ) ; if ( err ) { err = - errno ; pr_warn ( \"failed to get BTF object #%d info: %d\\n\" , id , err ) ; goto err_out ; } /* ignore non-module BTFs */ if ( ! info . kernel_btf || strcmp ( name , \"vmlinux\" ) == 0 ) { close ( fd ) ; continue ; } btf = btf_get_from_fd ( fd , obj -> btf_vmlinux ) ; err = libbpf_get_error ( btf ) ; if ( err ) { pr_warn ( \"failed to load module [%s]'s BTF object #%d: %d\\n\" , name , id , err ) ; goto err_out ; } err = libbpf_ensure_mem ( ( void * * ) & obj -> btf_modules , & obj -> btf_module_cap , sizeof ( * obj -> btf_modules ) , obj -> btf_module_cnt + 1 ) ; if ( err ) goto err_out ; mod_btf = & obj -> btf_modules [ obj -> btf_module_cnt ++ ] ; mod_btf -> btf = btf ; mod_btf -> id = id ; mod_btf -> fd = fd ; mod_btf -> name = strdup ( name ) ; if ( ! mod_btf -> name ) { err = - ENOMEM ; goto err_out ; } continue ; err_out : close ( fd ) ; return err ; } return 0 ; }",
    "resources/libbpf/src/libbpf.c@bpf_core_find_cands": "static struct bpf_core_cand_list * bpf_core_find_cands ( struct bpf_object * obj , const struct btf * local_btf , __u32 local_type_id ) { struct bpf_core_cand local_cand = { } ; struct bpf_core_cand_list * cands ; const struct btf * main_btf ; const struct btf_type * local_t ; const char * local_name ; size_t local_essent_len ; int err , i ; local_cand . btf = local_btf ; local_cand . id = local_type_id ; local_t = btf__type_by_id ( local_btf , local_type_id ) ; if ( ! local_t ) return ERR_PTR ( - EINVAL ) ; local_name = btf__name_by_offset ( local_btf , local_t -> name_off ) ; if ( str_is_empty ( local_name ) ) return ERR_PTR ( - EINVAL ) ; local_essent_len = bpf_core_essential_name_len ( local_name ) ; cands = calloc ( 1 , sizeof ( * cands ) ) ; if ( ! cands ) return ERR_PTR ( - ENOMEM ) ; /* Attempt to find target candidates in vmlinux BTF first */ main_btf = obj -> btf_vmlinux_override ? : obj -> btf_vmlinux ; err = bpf_core_add_cands ( & local_cand , local_essent_len , main_btf , \"vmlinux\" , 1 , cands ) ; if ( err ) goto err_out ; /* if vmlinux BTF has any candidate, don't got for module BTFs */ if ( cands -> len ) return cands ; /* if vmlinux BTF was overridden, don't attempt to load module BTFs */ if ( obj -> btf_vmlinux_override ) return cands ; /* now look through module BTFs, trying to still find candidates */ err = load_module_btfs ( obj ) ; if ( err ) goto err_out ; for ( i = 0 ; i < obj -> btf_module_cnt ; i ++ ) { err = bpf_core_add_cands ( & local_cand , local_essent_len , obj -> btf_modules [ i ] . btf , obj -> btf_modules [ i ] . name , btf__type_cnt ( obj -> btf_vmlinux ) , cands ) ; if ( err ) goto err_out ; } return cands ; err_out : bpf_core_free_cands ( cands ) ; return ERR_PTR ( err ) ; }",
    "resources/libbpf/src/libbpf.c@bpf_core_types_are_compat": "int bpf_core_types_are_compat ( const struct btf * local_btf , __u32 local_id , const struct btf * targ_btf , __u32 targ_id ) { return __bpf_core_types_are_compat ( local_btf , local_id , targ_btf , targ_id , 32 ) ; }",
    "resources/libbpf/src/libbpf.c@bpf_core_types_match": "int bpf_core_types_match ( const struct btf * local_btf , __u32 local_id , const struct btf * targ_btf , __u32 targ_id ) { return __bpf_core_types_match ( local_btf , local_id , targ_btf , targ_id , false , 32 ) ; }",
    "resources/libbpf/src/libbpf.c@bpf_core_hash_fn": "static size_t bpf_core_hash_fn ( const long key , void * ctx ) { return key ; }",
    "resources/libbpf/src/libbpf.c@bpf_core_equal_fn": "static bool bpf_core_equal_fn ( const long k1 , const long k2 , void * ctx ) { return k1 == k2 ; }",
    "resources/libbpf/src/libbpf.c@record_relo_core": "static int record_relo_core ( struct bpf_program * prog , const struct bpf_core_relo * core_relo , int insn_idx ) { struct reloc_desc * relos , * relo ; relos = libbpf_reallocarray ( prog -> reloc_desc , prog -> nr_reloc + 1 , sizeof ( * relos ) ) ; if ( ! relos ) return - ENOMEM ; relo = & relos [ prog -> nr_reloc ] ; relo -> type = RELO_CORE ; relo -> insn_idx = insn_idx ; relo -> core_relo = core_relo ; prog -> reloc_desc = relos ; prog -> nr_reloc ++ ; return 0 ; }",
    "resources/libbpf/src/libbpf.c@find_relo_core": "static const struct bpf_core_relo * find_relo_core ( struct bpf_program * prog , int insn_idx ) { struct reloc_desc * relo ; int i ; for ( i = 0 ; i < prog -> nr_reloc ; i ++ ) { relo = & prog -> reloc_desc [ i ] ; if ( relo -> type != RELO_CORE || relo -> insn_idx != insn_idx ) continue ; return relo -> core_relo ; } return NULL ; }",
    "resources/libbpf/src/libbpf.c@bpf_core_resolve_relo": "static int bpf_core_resolve_relo ( struct bpf_program * prog , const struct bpf_core_relo * relo , int relo_idx , const struct btf * local_btf , struct hashmap * cand_cache , struct bpf_core_relo_res * targ_res ) { struct bpf_core_spec specs_scratch [ 3 ] = { } ; struct bpf_core_cand_list * cands = NULL ; const char * prog_name = prog -> name ; const struct btf_type * local_type ; const char * local_name ; __u32 local_id = relo -> type_id ; int err ; local_type = btf__type_by_id ( local_btf , local_id ) ; if ( ! local_type ) return - EINVAL ; local_name = btf__name_by_offset ( local_btf , local_type -> name_off ) ; if ( ! local_name ) return - EINVAL ; if ( relo -> kind != BPF_CORE_TYPE_ID_LOCAL && ! hashmap__find ( cand_cache , local_id , & cands ) ) { cands = bpf_core_find_cands ( prog -> obj , local_btf , local_id ) ; if ( IS_ERR ( cands ) ) { pr_warn ( \"prog '%s': relo #%d: target candidate search failed for [%d] %s %s: %ld\\n\" , prog_name , relo_idx , local_id , btf_kind_str ( local_type ) , local_name , PTR_ERR ( cands ) ) ; return PTR_ERR ( cands ) ; } err = hashmap__set ( cand_cache , local_id , cands , NULL , NULL ) ; if ( err ) { bpf_core_free_cands ( cands ) ; return err ; } } return bpf_core_calc_relo_insn ( prog_name , relo , relo_idx , local_btf , cands , specs_scratch , targ_res ) ; }",
    "resources/libbpf/src/libbpf.c@bpf_object__relocate_core": "static int bpf_object__relocate_core ( struct bpf_object * obj , const char * targ_btf_path ) { const struct btf_ext_info_sec * sec ; struct bpf_core_relo_res targ_res ; const struct bpf_core_relo * rec ; const struct btf_ext_info * seg ; struct hashmap_entry * entry ; struct hashmap * cand_cache = NULL ; struct bpf_program * prog ; struct bpf_insn * insn ; const char * sec_name ; int i , err = 0 , insn_idx , sec_idx , sec_num ; if ( obj -> btf_ext -> core_relo_info . len == 0 ) return 0 ; if ( targ_btf_path ) { obj -> btf_vmlinux_override = btf__parse ( targ_btf_path , NULL ) ; err = libbpf_get_error ( obj -> btf_vmlinux_override ) ; if ( err ) { pr_warn ( \"failed to parse target BTF: %d\\n\" , err ) ; return err ; } } cand_cache = hashmap__new ( bpf_core_hash_fn , bpf_core_equal_fn , NULL ) ; if ( IS_ERR ( cand_cache ) ) { err = PTR_ERR ( cand_cache ) ; goto out ; } seg = & obj -> btf_ext -> core_relo_info ; sec_num = 0 ; for_each_btf_ext_sec ( seg , sec ) { sec_idx = seg -> sec_idxs [ sec_num ] ; sec_num ++ ; sec_name = btf__name_by_offset ( obj -> btf , sec -> sec_name_off ) ; if ( str_is_empty ( sec_name ) ) { err = - EINVAL ; goto out ; } pr_debug ( \"sec '%s': found %d CO-RE relocations\\n\" , sec_name , sec -> num_info ) ; for_each_btf_ext_rec ( seg , sec , i , rec ) { if ( rec -> insn_off % BPF_INSN_SZ ) return - EINVAL ; insn_idx = rec -> insn_off / BPF_INSN_SZ ; prog = find_prog_by_sec_insn ( obj , sec_idx , insn_idx ) ; if ( ! prog ) { /* When __weak subprog is \"overridden\" by another instance\n\t\t\t\t * of the subprog from a different object file, linker still\n\t\t\t\t * appends all the .BTF.ext info that used to belong to that\n\t\t\t\t * eliminated subprogram.\n\t\t\t\t * This is similar to what x86-64 linker does for relocations.\n\t\t\t\t * So just ignore such relocations just like we ignore\n\t\t\t\t * subprog instructions when discovering subprograms.\n\t\t\t\t */ pr_debug ( \"sec '%s': skipping CO-RE relocation #%d for insn #%d belonging to eliminated weak subprogram\\n\" , sec_name , i , insn_idx ) ; continue ; } /* no need to apply CO-RE relocation if the program is\n\t\t\t * not going to be loaded\n\t\t\t */ if ( ! prog -> autoload ) continue ; /* adjust insn_idx from section frame of reference to the local\n\t\t\t * program's frame of reference; (sub-)program code is not yet\n\t\t\t * relocated, so it's enough to just subtract in-section offset\n\t\t\t */ insn_idx = insn_idx - prog -> sec_insn_off ; if ( insn_idx >= prog -> insns_cnt ) return - EINVAL ; insn = & prog -> insns [ insn_idx ] ; err = record_relo_core ( prog , rec , insn_idx ) ; if ( err ) { pr_warn ( \"prog '%s': relo #%d: failed to record relocation: %d\\n\" , prog -> name , i , err ) ; goto out ; } if ( prog -> obj -> gen_loader ) continue ; err = bpf_core_resolve_relo ( prog , rec , i , obj -> btf , cand_cache , & targ_res ) ; if ( err ) { pr_warn ( \"prog '%s': relo #%d: failed to relocate: %d\\n\" , prog -> name , i , err ) ; goto out ; } err = bpf_core_patch_insn ( prog -> name , insn , insn_idx , rec , i , & targ_res ) ; if ( err ) { pr_warn ( \"prog '%s': relo #%d: failed to patch insn #%u: %d\\n\" , prog -> name , i , insn_idx , err ) ; goto out ; } } } out : /* obj->btf_vmlinux and module BTFs are freed after object load */ btf__free ( obj -> btf_vmlinux_override ) ; obj -> btf_vmlinux_override = NULL ; if ( ! IS_ERR_OR_NULL ( cand_cache ) ) { hashmap__for_each_entry ( cand_cache , entry , i ) { bpf_core_free_cands ( entry -> pvalue ) ; } hashmap__free ( cand_cache ) ; } return err ; }",
    "resources/libbpf/src/libbpf.c@poison_map_ldimm64": "static void poison_map_ldimm64 ( struct bpf_program * prog , int relo_idx , int insn_idx , struct bpf_insn * insn , int map_idx , const struct bpf_map * map ) { int i ; pr_debug ( \"prog '%s': relo #%d: poisoning insn #%d that loads map #%d '%s'\\n\" , prog -> name , relo_idx , insn_idx , map_idx , map -> name ) ; /* we turn single ldimm64 into two identical invalid calls */ for ( i = 0 ; i < 2 ; i ++ ) { insn -> code = BPF_JMP | BPF_CALL ; insn -> dst_reg = 0 ; insn -> src_reg = 0 ; insn -> off = 0 ; /* if this instruction is reachable (not a dead code),\n\t\t * verifier will complain with something like:\n\t\t * invalid func unknown#2001000123\n\t\t * where lower 123 is map index into obj->maps[] array\n\t\t */ insn -> imm = POISON_LDIMM64_MAP_BASE + map_idx ; insn ++ ; } }",
    "resources/libbpf/src/libbpf.c@poison_kfunc_call": "static void poison_kfunc_call ( struct bpf_program * prog , int relo_idx , int insn_idx , struct bpf_insn * insn , int ext_idx , const struct extern_desc * ext ) { pr_debug ( \"prog '%s': relo #%d: poisoning insn #%d that calls kfunc '%s'\\n\" , prog -> name , relo_idx , insn_idx , ext -> name ) ; /* we turn kfunc call into invalid helper call with identifiable constant */ insn -> code = BPF_JMP | BPF_CALL ; insn -> dst_reg = 0 ; insn -> src_reg = 0 ; insn -> off = 0 ; /* if this instruction is reachable (not a dead code),\n\t * verifier will complain with something like:\n\t * invalid func unknown#2001000123\n\t * where lower 123 is extern index into obj->externs[] array\n\t */ insn -> imm = POISON_CALL_KFUNC_BASE + ext_idx ; }",
    "resources/libbpf/src/libbpf.c@bpf_object__relocate_data": "static int bpf_object__relocate_data ( struct bpf_object * obj , struct bpf_program * prog ) { int i ; for ( i = 0 ; i < prog -> nr_reloc ; i ++ ) { struct reloc_desc * relo = & prog -> reloc_desc [ i ] ; struct bpf_insn * insn = & prog -> insns [ relo -> insn_idx ] ; const struct bpf_map * map ; struct extern_desc * ext ; switch ( relo -> type ) { case RELO_LD64 : map = & obj -> maps [ relo -> map_idx ] ; if ( obj -> gen_loader ) { insn [ 0 ] . src_reg = BPF_PSEUDO_MAP_IDX ; insn [ 0 ] . imm = relo -> map_idx ; } else if ( map -> autocreate ) { insn [ 0 ] . src_reg = BPF_PSEUDO_MAP_FD ; insn [ 0 ] . imm = map -> fd ; } else { poison_map_ldimm64 ( prog , i , relo -> insn_idx , insn , relo -> map_idx , map ) ; } break ; case RELO_DATA : map = & obj -> maps [ relo -> map_idx ] ; insn [ 1 ] . imm = insn [ 0 ] . imm + relo -> sym_off ; if ( obj -> gen_loader ) { insn [ 0 ] . src_reg = BPF_PSEUDO_MAP_IDX_VALUE ; insn [ 0 ] . imm = relo -> map_idx ; } else if ( map -> autocreate ) { insn [ 0 ] . src_reg = BPF_PSEUDO_MAP_VALUE ; insn [ 0 ] . imm = map -> fd ; } else { poison_map_ldimm64 ( prog , i , relo -> insn_idx , insn , relo -> map_idx , map ) ; } break ; case RELO_EXTERN_LD64 : ext = & obj -> externs [ relo -> ext_idx ] ; if ( ext -> type == EXT_KCFG ) { if ( obj -> gen_loader ) { insn [ 0 ] . src_reg = BPF_PSEUDO_MAP_IDX_VALUE ; insn [ 0 ] . imm = obj -> kconfig_map_idx ; } else { insn [ 0 ] . src_reg = BPF_PSEUDO_MAP_VALUE ; insn [ 0 ] . imm = obj -> maps [ obj -> kconfig_map_idx ] . fd ; } insn [ 1 ] . imm = ext -> kcfg . data_off ; } else /* EXT_KSYM */ { if ( ext -> ksym . type_id && ext -> is_set ) { /* typed ksyms */ insn [ 0 ] . src_reg = BPF_PSEUDO_BTF_ID ; insn [ 0 ] . imm = ext -> ksym . kernel_btf_id ; insn [ 1 ] . imm = ext -> ksym . kernel_btf_obj_fd ; } else { /* typeless ksyms or unresolved typed ksyms */ insn [ 0 ] . imm = ( __u32 ) ext -> ksym . addr ; insn [ 1 ] . imm = ext -> ksym . addr >> 32 ; } } break ; case RELO_EXTERN_CALL : ext = & obj -> externs [ relo -> ext_idx ] ; insn [ 0 ] . src_reg = BPF_PSEUDO_KFUNC_CALL ; if ( ext -> is_set ) { insn [ 0 ] . imm = ext -> ksym . kernel_btf_id ; insn [ 0 ] . off = ext -> ksym . btf_fd_idx ; } else { /* unresolved weak kfunc call */ poison_kfunc_call ( prog , i , relo -> insn_idx , insn , relo -> ext_idx , ext ) ; } break ; case RELO_SUBPROG_ADDR : if ( insn [ 0 ] . src_reg != BPF_PSEUDO_FUNC ) { pr_warn ( \"prog '%s': relo #%d: bad insn\\n\" , prog -> name , i ) ; return - EINVAL ; } /* handled already */ break ; case RELO_CALL : /* handled already */ break ; case RELO_CORE : /* will be handled by bpf_program_record_relos() */ break ; default : pr_warn ( \"prog '%s': relo #%d: bad relo type %d\\n\" , prog -> name , i , relo -> type ) ; return - EINVAL ; } } return 0 ; }",
    "resources/libbpf/src/libbpf.c@adjust_prog_btf_ext_info": "static int adjust_prog_btf_ext_info ( const struct bpf_object * obj , const struct bpf_program * prog , const struct btf_ext_info * ext_info , void * * prog_info , __u32 * prog_rec_cnt , __u32 * prog_rec_sz ) { void * copy_start = NULL , * copy_end = NULL ; void * rec , * rec_end , * new_prog_info ; const struct btf_ext_info_sec * sec ; size_t old_sz , new_sz ; int i , sec_num , sec_idx , off_adj ; sec_num = 0 ; for_each_btf_ext_sec ( ext_info , sec ) { sec_idx = ext_info -> sec_idxs [ sec_num ] ; sec_num ++ ; if ( prog -> sec_idx != sec_idx ) continue ; for_each_btf_ext_rec ( ext_info , sec , i , rec ) { __u32 insn_off = * ( __u32 * ) rec / BPF_INSN_SZ ; if ( insn_off < prog -> sec_insn_off ) continue ; if ( insn_off >= prog -> sec_insn_off + prog -> sec_insn_cnt ) break ; if ( ! copy_start ) copy_start = rec ; copy_end = rec + ext_info -> rec_size ; } if ( ! copy_start ) return - ENOENT ; /* append func/line info of a given (sub-)program to the main\n\t\t * program func/line info\n\t\t */ old_sz = ( size_t ) ( * prog_rec_cnt ) * ext_info -> rec_size ; new_sz = old_sz + ( copy_end - copy_start ) ; new_prog_info = realloc ( * prog_info , new_sz ) ; if ( ! new_prog_info ) return - ENOMEM ; * prog_info = new_prog_info ; * prog_rec_cnt = new_sz / ext_info -> rec_size ; memcpy ( new_prog_info + old_sz , copy_start , copy_end - copy_start ) ; /* Kernel instruction offsets are in units of 8-byte\n\t\t * instructions, while .BTF.ext instruction offsets generated\n\t\t * by Clang are in units of bytes. So convert Clang offsets\n\t\t * into kernel offsets and adjust offset according to program\n\t\t * relocated position.\n\t\t */ off_adj = prog -> sub_insn_off - prog -> sec_insn_off ; rec = new_prog_info + old_sz ; rec_end = new_prog_info + new_sz ; for ( ; rec < rec_end ; rec += ext_info -> rec_size ) { __u32 * insn_off = rec ; * insn_off = * insn_off / BPF_INSN_SZ + off_adj ; } * prog_rec_sz = ext_info -> rec_size ; return 0 ; } return - ENOENT ; }",
    "resources/libbpf/src/libbpf.c@reloc_prog_func_and_line_info": "static int reloc_prog_func_and_line_info ( const struct bpf_object * obj , struct bpf_program * main_prog , const struct bpf_program * prog ) { int err ; /* no .BTF.ext relocation if .BTF.ext is missing or kernel doesn't\n\t * support func/line info\n\t */ if ( ! obj -> btf_ext || ! kernel_supports ( obj , FEAT_BTF_FUNC ) ) return 0 ; /* only attempt func info relocation if main program's func_info\n\t * relocation was successful\n\t */ if ( main_prog != prog && ! main_prog -> func_info ) goto line_info ; err = adjust_prog_btf_ext_info ( obj , prog , & obj -> btf_ext -> func_info , & main_prog -> func_info , & main_prog -> func_info_cnt , & main_prog -> func_info_rec_size ) ; if ( err ) { if ( err != - ENOENT ) { pr_warn ( \"prog '%s': error relocating .BTF.ext function info: %d\\n\" , prog -> name , err ) ; return err ; } if ( main_prog -> func_info ) { /*\n\t\t\t * Some info has already been found but has problem\n\t\t\t * in the last btf_ext reloc. Must have to error out.\n\t\t\t */ pr_warn ( \"prog '%s': missing .BTF.ext function info.\\n\" , prog -> name ) ; return err ; } /* Have problem loading the very first info. Ignore the rest. */ pr_warn ( \"prog '%s': missing .BTF.ext function info for the main program, skipping all of .BTF.ext func info.\\n\" , prog -> name ) ; } line_info : /* don't relocate line info if main program's relocation failed */ if ( main_prog != prog && ! main_prog -> line_info ) return 0 ; err = adjust_prog_btf_ext_info ( obj , prog , & obj -> btf_ext -> line_info , & main_prog -> line_info , & main_prog -> line_info_cnt , & main_prog -> line_info_rec_size ) ; if ( err ) { if ( err != - ENOENT ) { pr_warn ( \"prog '%s': error relocating .BTF.ext line info: %d\\n\" , prog -> name , err ) ; return err ; } if ( main_prog -> line_info ) { /*\n\t\t\t * Some info has already been found but has problem\n\t\t\t * in the last btf_ext reloc. Must have to error out.\n\t\t\t */ pr_warn ( \"prog '%s': missing .BTF.ext line info.\\n\" , prog -> name ) ; return err ; } /* Have problem loading the very first info. Ignore the rest. */ pr_warn ( \"prog '%s': missing .BTF.ext line info for the main program, skipping all of .BTF.ext line info.\\n\" , prog -> name ) ; } return 0 ; }",
    "resources/libbpf/src/libbpf.c@cmp_relo_by_insn_idx": "static int cmp_relo_by_insn_idx ( const void * key , const void * elem ) { size_t insn_idx = * ( const size_t * ) key ; const struct reloc_desc * relo = elem ; if ( insn_idx == relo -> insn_idx ) return 0 ; return insn_idx < relo -> insn_idx ? - 1 : 1 ; }",
    "resources/libbpf/src/libbpf.c@find_prog_insn_relo": "static struct reloc_desc * find_prog_insn_relo ( const struct bpf_program * prog , size_t insn_idx ) { if ( ! prog -> nr_reloc ) return NULL ; return bsearch ( & insn_idx , prog -> reloc_desc , prog -> nr_reloc , sizeof ( * prog -> reloc_desc ) , cmp_relo_by_insn_idx ) ; }",
    "resources/libbpf/src/libbpf.c@append_subprog_relos": "static int append_subprog_relos ( struct bpf_program * main_prog , struct bpf_program * subprog ) { int new_cnt = main_prog -> nr_reloc + subprog -> nr_reloc ; struct reloc_desc * relos ; int i ; if ( main_prog == subprog ) return 0 ; relos = libbpf_reallocarray ( main_prog -> reloc_desc , new_cnt , sizeof ( * relos ) ) ; /* if new count is zero, reallocarray can return a valid NULL result;\n\t * in this case the previous pointer will be freed, so we *have to*\n\t * reassign old pointer to the new value (even if it's NULL)\n\t */ if ( ! relos && new_cnt ) return - ENOMEM ; if ( subprog -> nr_reloc ) memcpy ( relos + main_prog -> nr_reloc , subprog -> reloc_desc , sizeof ( * relos ) * subprog -> nr_reloc ) ; for ( i = main_prog -> nr_reloc ; i < new_cnt ; i ++ ) relos [ i ] . insn_idx += subprog -> sub_insn_off ; /* After insn_idx adjustment the 'relos' array is still sorted\n\t * by insn_idx and doesn't break bsearch.\n\t */ main_prog -> reloc_desc = relos ; main_prog -> nr_reloc = new_cnt ; return 0 ; }",
    "resources/libbpf/src/libbpf.c@bpf_object__append_subprog_code": "static int bpf_object__append_subprog_code ( struct bpf_object * obj , struct bpf_program * main_prog , struct bpf_program * subprog ) { struct bpf_insn * insns ; size_t new_cnt ; int err ; subprog -> sub_insn_off = main_prog -> insns_cnt ; new_cnt = main_prog -> insns_cnt + subprog -> insns_cnt ; insns = libbpf_reallocarray ( main_prog -> insns , new_cnt , sizeof ( * insns ) ) ; if ( ! insns ) { pr_warn ( \"prog '%s': failed to realloc prog code\\n\" , main_prog -> name ) ; return - ENOMEM ; } main_prog -> insns = insns ; main_prog -> insns_cnt = new_cnt ; memcpy ( main_prog -> insns + subprog -> sub_insn_off , subprog -> insns , subprog -> insns_cnt * sizeof ( * insns ) ) ; pr_debug ( \"prog '%s': added %zu insns from sub-prog '%s'\\n\" , main_prog -> name , subprog -> insns_cnt , subprog -> name ) ; /* The subprog insns are now appended. Append its relos too. */ err = append_subprog_relos ( main_prog , subprog ) ; if ( err ) return err ; return 0 ; }",
    "resources/libbpf/src/libbpf.c@bpf_object__reloc_code": "static int bpf_object__reloc_code ( struct bpf_object * obj , struct bpf_program * main_prog , struct bpf_program * prog ) { size_t sub_insn_idx , insn_idx ; struct bpf_program * subprog ; struct reloc_desc * relo ; struct bpf_insn * insn ; int err ; err = reloc_prog_func_and_line_info ( obj , main_prog , prog ) ; if ( err ) return err ; for ( insn_idx = 0 ; insn_idx < prog -> sec_insn_cnt ; insn_idx ++ ) { insn = & main_prog -> insns [ prog -> sub_insn_off + insn_idx ] ; if ( ! insn_is_subprog_call ( insn ) && ! insn_is_pseudo_func ( insn ) ) continue ; relo = find_prog_insn_relo ( prog , insn_idx ) ; if ( relo && relo -> type == RELO_EXTERN_CALL ) /* kfunc relocations will be handled later\n\t\t\t * in bpf_object__relocate_data()\n\t\t\t */ continue ; if ( relo && relo -> type != RELO_CALL && relo -> type != RELO_SUBPROG_ADDR ) { pr_warn ( \"prog '%s': unexpected relo for insn #%zu, type %d\\n\" , prog -> name , insn_idx , relo -> type ) ; return - LIBBPF_ERRNO__RELOC ; } if ( relo ) { /* sub-program instruction index is a combination of\n\t\t\t * an offset of a symbol pointed to by relocation and\n\t\t\t * call instruction's imm field; for global functions,\n\t\t\t * call always has imm = -1, but for static functions\n\t\t\t * relocation is against STT_SECTION and insn->imm\n\t\t\t * points to a start of a static function\n\t\t\t *\n\t\t\t * for subprog addr relocation, the relo->sym_off + insn->imm is\n\t\t\t * the byte offset in the corresponding section.\n\t\t\t */ if ( relo -> type == RELO_CALL ) sub_insn_idx = relo -> sym_off / BPF_INSN_SZ + insn -> imm + 1 ; else sub_insn_idx = ( relo -> sym_off + insn -> imm ) / BPF_INSN_SZ ; } else if ( insn_is_pseudo_func ( insn ) ) { /*\n\t\t\t * RELO_SUBPROG_ADDR relo is always emitted even if both\n\t\t\t * functions are in the same section, so it shouldn't reach here.\n\t\t\t */ pr_warn ( \"prog '%s': missing subprog addr relo for insn #%zu\\n\" , prog -> name , insn_idx ) ; return - LIBBPF_ERRNO__RELOC ; } else { /* if subprogram call is to a static function within\n\t\t\t * the same ELF section, there won't be any relocation\n\t\t\t * emitted, but it also means there is no additional\n\t\t\t * offset necessary, insns->imm is relative to\n\t\t\t * instruction's original position within the section\n\t\t\t */ sub_insn_idx = prog -> sec_insn_off + insn_idx + insn -> imm + 1 ; } /* we enforce that sub-programs should be in .text section */ subprog = find_prog_by_sec_insn ( obj , obj -> efile . text_shndx , sub_insn_idx ) ; if ( ! subprog ) { pr_warn ( \"prog '%s': no .text section found yet sub-program call exists\\n\" , prog -> name ) ; return - LIBBPF_ERRNO__RELOC ; } /* if it's the first call instruction calling into this\n\t\t * subprogram (meaning this subprog hasn't been processed\n\t\t * yet) within the context of current main program:\n\t\t *   - append it at the end of main program's instructions blog;\n\t\t *   - process is recursively, while current program is put on hold;\n\t\t *   - if that subprogram calls some other not yet processes\n\t\t *   subprogram, same thing will happen recursively until\n\t\t *   there are no more unprocesses subprograms left to append\n\t\t *   and relocate.\n\t\t */ if ( subprog -> sub_insn_off == 0 ) { err = bpf_object__append_subprog_code ( obj , main_prog , subprog ) ; if ( err ) return err ; err = bpf_object__reloc_code ( obj , main_prog , subprog ) ; if ( err ) return err ; } /* main_prog->insns memory could have been re-allocated, so\n\t\t * calculate pointer again\n\t\t */ insn = & main_prog -> insns [ prog -> sub_insn_off + insn_idx ] ; /* calculate correct instruction position within current main\n\t\t * prog; each main prog can have a different set of\n\t\t * subprograms appended (potentially in different order as\n\t\t * well), so position of any subprog can be different for\n\t\t * different main programs\n\t\t */ insn -> imm = subprog -> sub_insn_off - ( prog -> sub_insn_off + insn_idx ) - 1 ; pr_debug ( \"prog '%s': insn #%zu relocated, imm %d points to subprog '%s' (now at %zu offset)\\n\" , prog -> name , insn_idx , insn -> imm , subprog -> name , subprog -> sub_insn_off ) ; } return 0 ; }",
    "resources/libbpf/src/libbpf.c@bpf_object__relocate_calls": "static int bpf_object__relocate_calls ( struct bpf_object * obj , struct bpf_program * prog ) { struct bpf_program * subprog ; int i , err ; /* mark all subprogs as not relocated (yet) within the context of\n\t * current main program\n\t */ for ( i = 0 ; i < obj -> nr_programs ; i ++ ) { subprog = & obj -> programs [ i ] ; if ( ! prog_is_subprog ( obj , subprog ) ) continue ; subprog -> sub_insn_off = 0 ; } err = bpf_object__reloc_code ( obj , prog , prog ) ; if ( err ) return err ; return 0 ; }",
    "resources/libbpf/src/libbpf.c@bpf_object__free_relocs": "static void bpf_object__free_relocs ( struct bpf_object * obj ) { struct bpf_program * prog ; int i ; /* free up relocation descriptors */ for ( i = 0 ; i < obj -> nr_programs ; i ++ ) { prog = & obj -> programs [ i ] ; zfree ( & prog -> reloc_desc ) ; prog -> nr_reloc = 0 ; } }",
    "resources/libbpf/src/libbpf.c@cmp_relocs": "static int cmp_relocs ( const void * _a , const void * _b ) { const struct reloc_desc * a = _a ; const struct reloc_desc * b = _b ; if ( a -> insn_idx != b -> insn_idx ) return a -> insn_idx < b -> insn_idx ? - 1 : 1 ; /* no two relocations should have the same insn_idx, but ... */ if ( a -> type != b -> type ) return a -> type < b -> type ? - 1 : 1 ; return 0 ; }",
    "resources/libbpf/src/libbpf.c@bpf_object__sort_relos": "static void bpf_object__sort_relos ( struct bpf_object * obj ) { int i ; for ( i = 0 ; i < obj -> nr_programs ; i ++ ) { struct bpf_program * p = & obj -> programs [ i ] ; if ( ! p -> nr_reloc ) continue ; qsort ( p -> reloc_desc , p -> nr_reloc , sizeof ( * p -> reloc_desc ) , cmp_relocs ) ; } }",
    "resources/libbpf/src/libbpf.c@bpf_prog_assign_exc_cb": "static int bpf_prog_assign_exc_cb ( struct bpf_object * obj , struct bpf_program * prog ) { const char * str = \"exception_callback:\" ; size_t pfx_len = strlen ( str ) ; int i , j , n ; if ( ! obj -> btf || ! kernel_supports ( obj , FEAT_BTF_DECL_TAG ) ) return 0 ; n = btf__type_cnt ( obj -> btf ) ; for ( i = 1 ; i < n ; i ++ ) { const char * name ; struct btf_type * t ; t = btf_type_by_id ( obj -> btf , i ) ; if ( ! btf_is_decl_tag ( t ) || btf_decl_tag ( t ) -> component_idx != - 1 ) continue ; name = btf__str_by_offset ( obj -> btf , t -> name_off ) ; if ( strncmp ( name , str , pfx_len ) != 0 ) continue ; t = btf_type_by_id ( obj -> btf , t -> type ) ; if ( ! btf_is_func ( t ) || btf_func_linkage ( t ) != BTF_FUNC_GLOBAL ) { pr_warn ( \"prog '%s': exception_callback:<value> decl tag not applied to the main program\\n\" , prog -> name ) ; return - EINVAL ; } if ( strcmp ( prog -> name , btf__str_by_offset ( obj -> btf , t -> name_off ) ) != 0 ) continue ; /* Multiple callbacks are specified for the same prog,\n\t\t * the verifier will eventually return an error for this\n\t\t * case, hence simply skip appending a subprog.\n\t\t */ if ( prog -> exception_cb_idx >= 0 ) { prog -> exception_cb_idx = - 1 ; break ; } name += pfx_len ; if ( str_is_empty ( name ) ) { pr_warn ( \"prog '%s': exception_callback:<value> decl tag contains empty value\\n\" , prog -> name ) ; return - EINVAL ; } for ( j = 0 ; j < obj -> nr_programs ; j ++ ) { struct bpf_program * subprog = & obj -> programs [ j ] ; if ( ! prog_is_subprog ( obj , subprog ) ) continue ; if ( strcmp ( name , subprog -> name ) != 0 ) continue ; /* Enforce non-hidden, as from verifier point of\n\t\t\t * view it expects global functions, whereas the\n\t\t\t * mark_btf_static fixes up linkage as static.\n\t\t\t */ if ( ! subprog -> sym_global || subprog -> mark_btf_static ) { pr_warn ( \"prog '%s': exception callback %s must be a global non-hidden function\\n\" , prog -> name , subprog -> name ) ; return - EINVAL ; } /* Let's see if we already saw a static exception callback with the same name */ if ( prog -> exception_cb_idx >= 0 ) { pr_warn ( \"prog '%s': multiple subprogs with same name as exception callback '%s'\\n\" , prog -> name , subprog -> name ) ; return - EINVAL ; } prog -> exception_cb_idx = j ; break ; } if ( prog -> exception_cb_idx >= 0 ) continue ; pr_warn ( \"prog '%s': cannot find exception callback '%s'\\n\" , prog -> name , name ) ; return - ENOENT ; } return 0 ; }",
    "resources/libbpf/src/libbpf.c@need_func_arg_type_fixup": "static bool need_func_arg_type_fixup ( const struct btf * btf , const struct bpf_program * prog , const char * subprog_name , int arg_idx , int arg_type_id , const char * ctx_name ) { const struct btf_type * t ; const char * tname ; /* check if existing parameter already matches verifier expectations */ t = skip_mods_and_typedefs ( btf , arg_type_id , NULL ) ; if ( ! btf_is_ptr ( t ) ) goto out_warn ; /* typedef bpf_user_pt_regs_t is a special PITA case, valid for kprobe\n\t * and perf_event programs, so check this case early on and forget\n\t * about it for subsequent checks\n\t */ while ( btf_is_mod ( t ) ) t = btf__type_by_id ( btf , t -> type ) ; if ( btf_is_typedef ( t ) && ( prog -> type == BPF_PROG_TYPE_KPROBE || prog -> type == BPF_PROG_TYPE_PERF_EVENT ) ) { tname = btf__str_by_offset ( btf , t -> name_off ) ? : \"<anon>\" ; if ( strcmp ( tname , \"bpf_user_pt_regs_t\" ) == 0 ) return false ; /* canonical type for kprobe/perf_event */ } /* now we can ignore typedefs moving forward */ t = skip_mods_and_typedefs ( btf , t -> type , NULL ) ; /* if it's `void *`, definitely fix up BTF info */ if ( btf_is_void ( t ) ) return true ; /* if it's already proper canonical type, no need to fix up */ tname = btf__str_by_offset ( btf , t -> name_off ) ? : \"<anon>\" ; if ( btf_is_struct ( t ) && strcmp ( tname , ctx_name ) == 0 ) return false ; /* special cases */ switch ( prog -> type ) { case BPF_PROG_TYPE_KPROBE : /* `struct pt_regs *` is expected, but we need to fix up */ if ( btf_is_struct ( t ) && strcmp ( tname , \"pt_regs\" ) == 0 ) return true ; break ; case BPF_PROG_TYPE_PERF_EVENT : if ( __builtin_types_compatible_p ( bpf_user_pt_regs_t , struct pt_regs ) && btf_is_struct ( t ) && strcmp ( tname , \"pt_regs\" ) == 0 ) return true ; if ( __builtin_types_compatible_p ( bpf_user_pt_regs_t , struct user_pt_regs ) && btf_is_struct ( t ) && strcmp ( tname , \"user_pt_regs\" ) == 0 ) return true ; if ( __builtin_types_compatible_p ( bpf_user_pt_regs_t , struct user_regs_struct ) && btf_is_struct ( t ) && strcmp ( tname , \"user_regs_struct\" ) == 0 ) return true ; break ; case BPF_PROG_TYPE_RAW_TRACEPOINT : case BPF_PROG_TYPE_RAW_TRACEPOINT_WRITABLE : /* allow u64* as ctx */ if ( btf_is_int ( t ) && t -> size == 8 ) return true ; break ; default : break ; } out_warn : pr_warn ( \"prog '%s': subprog '%s' arg#%d is expected to be of `struct %s *` type\\n\" , prog -> name , subprog_name , arg_idx , ctx_name ) ; return false ; }",
    "resources/libbpf/src/libbpf.c@clone_func_btf_info": "static int clone_func_btf_info ( struct btf * btf , int orig_fn_id , struct bpf_program * prog ) { int fn_id , fn_proto_id , ret_type_id , orig_proto_id ; int i , err , arg_cnt , fn_name_off , linkage ; struct btf_type * fn_t , * fn_proto_t , * t ; struct btf_param * p ; /* caller already validated FUNC -> FUNC_PROTO validity */ fn_t = btf_type_by_id ( btf , orig_fn_id ) ; fn_proto_t = btf_type_by_id ( btf , fn_t -> type ) ; /* Note that each btf__add_xxx() operation invalidates\n\t * all btf_type and string pointers, so we need to be\n\t * very careful when cloning BTF types. BTF type\n\t * pointers have to be always refetched. And to avoid\n\t * problems with invalidated string pointers, we\n\t * add empty strings initially, then just fix up\n\t * name_off offsets in place. Offsets are stable for\n\t * existing strings, so that works out.\n\t */ fn_name_off = fn_t -> name_off ; /* we are about to invalidate fn_t */ linkage = btf_func_linkage ( fn_t ) ; orig_proto_id = fn_t -> type ; /* original FUNC_PROTO ID */ ret_type_id = fn_proto_t -> type ; /* fn_proto_t will be invalidated */ arg_cnt = btf_vlen ( fn_proto_t ) ; /* clone FUNC_PROTO and its params */ fn_proto_id = btf__add_func_proto ( btf , ret_type_id ) ; if ( fn_proto_id < 0 ) return - EINVAL ; for ( i = 0 ; i < arg_cnt ; i ++ ) { int name_off ; /* copy original parameter data */ t = btf_type_by_id ( btf , orig_proto_id ) ; p = & btf_params ( t ) [ i ] ; name_off = p -> name_off ; err = btf__add_func_param ( btf , \"\" , p -> type ) ; if ( err ) return err ; fn_proto_t = btf_type_by_id ( btf , fn_proto_id ) ; p = & btf_params ( fn_proto_t ) [ i ] ; p -> name_off = name_off ; /* use remembered str offset */ } /* clone FUNC now, btf__add_func() enforces non-empty name, so use\n\t * entry program's name as a placeholder, which we replace immediately\n\t * with original name_off\n\t */ fn_id = btf__add_func ( btf , prog -> name , linkage , fn_proto_id ) ; if ( fn_id < 0 ) return - EINVAL ; fn_t = btf_type_by_id ( btf , fn_id ) ; fn_t -> name_off = fn_name_off ; /* reuse original string */ return fn_id ; }",
    "resources/libbpf/src/libbpf.c@bpf_program_fixup_func_info": "static int bpf_program_fixup_func_info ( struct bpf_object * obj , struct bpf_program * prog ) { const char * ctx_name = NULL , * ctx_tag = \"arg:ctx\" , * fn_name ; struct bpf_func_info_min * func_rec ; struct btf_type * fn_t , * fn_proto_t ; struct btf * btf = obj -> btf ; const struct btf_type * t ; struct btf_param * p ; int ptr_id = 0 , struct_id , tag_id , orig_fn_id ; int i , n , arg_idx , arg_cnt , err , rec_idx ; int * orig_ids ; /* no .BTF.ext, no problem */ if ( ! obj -> btf_ext || ! prog -> func_info ) return 0 ; /* don't do any fix ups if kernel natively supports __arg_ctx */ if ( kernel_supports ( obj , FEAT_ARG_CTX_TAG ) ) return 0 ; /* some BPF program types just don't have named context structs, so\n\t * this fallback mechanism doesn't work for them\n\t */ for ( i = 0 ; i < ARRAY_SIZE ( global_ctx_map ) ; i ++ ) { if ( global_ctx_map [ i ] . prog_type != prog -> type ) continue ; ctx_name = global_ctx_map [ i ] . ctx_name ; break ; } if ( ! ctx_name ) return 0 ; /* remember original func BTF IDs to detect if we already cloned them */ orig_ids = calloc ( prog -> func_info_cnt , sizeof ( * orig_ids ) ) ; if ( ! orig_ids ) return - ENOMEM ; for ( i = 0 ; i < prog -> func_info_cnt ; i ++ ) { func_rec = prog -> func_info + prog -> func_info_rec_size * i ; orig_ids [ i ] = func_rec -> type_id ; } /* go through each DECL_TAG with \"arg:ctx\" and see if it points to one\n\t * of our subprogs; if yes and subprog is global and needs adjustment,\n\t * clone and adjust FUNC -> FUNC_PROTO combo\n\t */ for ( i = 1 , n = btf__type_cnt ( btf ) ; i < n ; i ++ ) { /* only DECL_TAG with \"arg:ctx\" value are interesting */ t = btf__type_by_id ( btf , i ) ; if ( ! btf_is_decl_tag ( t ) ) continue ; if ( strcmp ( btf__str_by_offset ( btf , t -> name_off ) , ctx_tag ) != 0 ) continue ; /* only global funcs need adjustment, if at all */ orig_fn_id = t -> type ; fn_t = btf_type_by_id ( btf , orig_fn_id ) ; if ( ! btf_is_func ( fn_t ) || btf_func_linkage ( fn_t ) != BTF_FUNC_GLOBAL ) continue ; /* sanity check FUNC -> FUNC_PROTO chain, just in case */ fn_proto_t = btf_type_by_id ( btf , fn_t -> type ) ; if ( ! fn_proto_t || ! btf_is_func_proto ( fn_proto_t ) ) continue ; /* find corresponding func_info record */ func_rec = NULL ; for ( rec_idx = 0 ; rec_idx < prog -> func_info_cnt ; rec_idx ++ ) { if ( orig_ids [ rec_idx ] == t -> type ) { func_rec = prog -> func_info + prog -> func_info_rec_size * rec_idx ; break ; } } /* current main program doesn't call into this subprog */ if ( ! func_rec ) continue ; /* some more sanity checking of DECL_TAG */ arg_cnt = btf_vlen ( fn_proto_t ) ; arg_idx = btf_decl_tag ( t ) -> component_idx ; if ( arg_idx < 0 || arg_idx >= arg_cnt ) continue ; /* check if we should fix up argument type */ p = & btf_params ( fn_proto_t ) [ arg_idx ] ; fn_name = btf__str_by_offset ( btf , fn_t -> name_off ) ? : \"<anon>\" ; if ( ! need_func_arg_type_fixup ( btf , prog , fn_name , arg_idx , p -> type , ctx_name ) ) continue ; /* clone fn/fn_proto, unless we already did it for another arg */ if ( func_rec -> type_id == orig_fn_id ) { int fn_id ; fn_id = clone_func_btf_info ( btf , orig_fn_id , prog ) ; if ( fn_id < 0 ) { err = fn_id ; goto err_out ; } /* point func_info record to a cloned FUNC type */ func_rec -> type_id = fn_id ; } /* create PTR -> STRUCT type chain to mark PTR_TO_CTX argument;\n\t\t * we do it just once per main BPF program, as all global\n\t\t * funcs share the same program type, so need only PTR ->\n\t\t * STRUCT type chain\n\t\t */ if ( ptr_id == 0 ) { struct_id = btf__add_struct ( btf , ctx_name , 0 ) ; ptr_id = btf__add_ptr ( btf , struct_id ) ; if ( ptr_id < 0 || struct_id < 0 ) { err = - EINVAL ; goto err_out ; } } /* for completeness, clone DECL_TAG and point it to cloned param */ tag_id = btf__add_decl_tag ( btf , ctx_tag , func_rec -> type_id , arg_idx ) ; if ( tag_id < 0 ) { err = - EINVAL ; goto err_out ; } /* all the BTF manipulations invalidated pointers, refetch them */ fn_t = btf_type_by_id ( btf , func_rec -> type_id ) ; fn_proto_t = btf_type_by_id ( btf , fn_t -> type ) ; /* fix up type ID pointed to by param */ p = & btf_params ( fn_proto_t ) [ arg_idx ] ; p -> type = ptr_id ; } free ( orig_ids ) ; return 0 ; err_out : free ( orig_ids ) ; return err ; }",
    "resources/libbpf/src/libbpf.c@bpf_object__relocate": "static int bpf_object__relocate ( struct bpf_object * obj , const char * targ_btf_path ) { struct bpf_program * prog ; size_t i , j ; int err ; if ( obj -> btf_ext ) { err = bpf_object__relocate_core ( obj , targ_btf_path ) ; if ( err ) { pr_warn ( \"failed to perform CO-RE relocations: %d\\n\" , err ) ; return err ; } bpf_object__sort_relos ( obj ) ; } /* Before relocating calls pre-process relocations and mark\n\t * few ld_imm64 instructions that points to subprogs.\n\t * Otherwise bpf_object__reloc_code() later would have to consider\n\t * all ld_imm64 insns as relocation candidates. That would\n\t * reduce relocation speed, since amount of find_prog_insn_relo()\n\t * would increase and most of them will fail to find a relo.\n\t */ for ( i = 0 ; i < obj -> nr_programs ; i ++ ) { prog = & obj -> programs [ i ] ; for ( j = 0 ; j < prog -> nr_reloc ; j ++ ) { struct reloc_desc * relo = & prog -> reloc_desc [ j ] ; struct bpf_insn * insn = & prog -> insns [ relo -> insn_idx ] ; /* mark the insn, so it's recognized by insn_is_pseudo_func() */ if ( relo -> type == RELO_SUBPROG_ADDR ) insn [ 0 ] . src_reg = BPF_PSEUDO_FUNC ; } } /* relocate subprogram calls and append used subprograms to main\n\t * programs; each copy of subprogram code needs to be relocated\n\t * differently for each main program, because its code location might\n\t * have changed.\n\t * Append subprog relos to main programs to allow data relos to be\n\t * processed after text is completely relocated.\n\t */ for ( i = 0 ; i < obj -> nr_programs ; i ++ ) { prog = & obj -> programs [ i ] ; /* sub-program's sub-calls are relocated within the context of\n\t\t * its main program only\n\t\t */ if ( prog_is_subprog ( obj , prog ) ) continue ; if ( ! prog -> autoload ) continue ; err = bpf_object__relocate_calls ( obj , prog ) ; if ( err ) { pr_warn ( \"prog '%s': failed to relocate calls: %d\\n\" , prog -> name , err ) ; return err ; } err = bpf_prog_assign_exc_cb ( obj , prog ) ; if ( err ) return err ; /* Now, also append exception callback if it has not been done already. */ if ( prog -> exception_cb_idx >= 0 ) { struct bpf_program * subprog = & obj -> programs [ prog -> exception_cb_idx ] ; /* Calling exception callback directly is disallowed, which the\n\t\t\t * verifier will reject later. In case it was processed already,\n\t\t\t * we can skip this step, otherwise for all other valid cases we\n\t\t\t * have to append exception callback now.\n\t\t\t */ if ( subprog -> sub_insn_off == 0 ) { err = bpf_object__append_subprog_code ( obj , prog , subprog ) ; if ( err ) return err ; err = bpf_object__reloc_code ( obj , prog , subprog ) ; if ( err ) return err ; } } } for ( i = 0 ; i < obj -> nr_programs ; i ++ ) { prog = & obj -> programs [ i ] ; if ( prog_is_subprog ( obj , prog ) ) continue ; if ( ! prog -> autoload ) continue ; /* Process data relos for main programs */ err = bpf_object__relocate_data ( obj , prog ) ; if ( err ) { pr_warn ( \"prog '%s': failed to relocate data references: %d\\n\" , prog -> name , err ) ; return err ; } /* Fix up .BTF.ext information, if necessary */ err = bpf_program_fixup_func_info ( obj , prog ) ; if ( err ) { pr_warn ( \"prog '%s': failed to perform .BTF.ext fix ups: %d\\n\" , prog -> name , err ) ; return err ; } } return 0 ; }",
    "resources/libbpf/src/libbpf.c@bpf_object__collect_map_relos": "static int bpf_object__collect_map_relos ( struct bpf_object * obj , Elf64_Shdr * shdr , Elf_Data * data ) { const int bpf_ptr_sz = 8 , host_ptr_sz = sizeof ( void * ) ; int i , j , nrels , new_sz ; const struct btf_var_secinfo * vi = NULL ; const struct btf_type * sec , * var , * def ; struct bpf_map * map = NULL , * targ_map = NULL ; struct bpf_program * targ_prog = NULL ; bool is_prog_array , is_map_in_map ; const struct btf_member * member ; const char * name , * mname , * type ; unsigned int moff ; Elf64_Sym * sym ; Elf64_Rel * rel ; void * tmp ; if ( ! obj -> efile . btf_maps_sec_btf_id || ! obj -> btf ) return - EINVAL ; sec = btf__type_by_id ( obj -> btf , obj -> efile . btf_maps_sec_btf_id ) ; if ( ! sec ) return - EINVAL ; nrels = shdr -> sh_size / shdr -> sh_entsize ; for ( i = 0 ; i < nrels ; i ++ ) { rel = elf_rel_by_idx ( data , i ) ; if ( ! rel ) { pr_warn ( \".maps relo #%d: failed to get ELF relo\\n\" , i ) ; return - LIBBPF_ERRNO__FORMAT ; } sym = elf_sym_by_idx ( obj , ELF64_R_SYM ( rel -> r_info ) ) ; if ( ! sym ) { pr_warn ( \".maps relo #%d: symbol %zx not found\\n\" , i , ( size_t ) ELF64_R_SYM ( rel -> r_info ) ) ; return - LIBBPF_ERRNO__FORMAT ; } name = elf_sym_str ( obj , sym -> st_name ) ? : \"<?>\" ; pr_debug ( \".maps relo #%d: for %zd value %zd rel->r_offset %zu name %d ('%s')\\n\" , i , ( ssize_t ) ( rel -> r_info >> 32 ) , ( size_t ) sym -> st_value , ( size_t ) rel -> r_offset , sym -> st_name , name ) ; for ( j = 0 ; j < obj -> nr_maps ; j ++ ) { map = & obj -> maps [ j ] ; if ( map -> sec_idx != obj -> efile . btf_maps_shndx ) continue ; vi = btf_var_secinfos ( sec ) + map -> btf_var_idx ; if ( vi -> offset <= rel -> r_offset && rel -> r_offset + bpf_ptr_sz <= vi -> offset + vi -> size ) break ; } if ( j == obj -> nr_maps ) { pr_warn ( \".maps relo #%d: cannot find map '%s' at rel->r_offset %zu\\n\" , i , name , ( size_t ) rel -> r_offset ) ; return - EINVAL ; } is_map_in_map = bpf_map_type__is_map_in_map ( map -> def . type ) ; is_prog_array = map -> def . type == BPF_MAP_TYPE_PROG_ARRAY ; type = is_map_in_map ? \"map\" : \"prog\" ; if ( is_map_in_map ) { if ( sym -> st_shndx != obj -> efile . btf_maps_shndx ) { pr_warn ( \".maps relo #%d: '%s' isn't a BTF-defined map\\n\" , i , name ) ; return - LIBBPF_ERRNO__RELOC ; } if ( map -> def . type == BPF_MAP_TYPE_HASH_OF_MAPS && map -> def . key_size != sizeof ( int ) ) { pr_warn ( \".maps relo #%d: hash-of-maps '%s' should have key size %zu.\\n\" , i , map -> name , sizeof ( int ) ) ; return - EINVAL ; } targ_map = bpf_object__find_map_by_name ( obj , name ) ; if ( ! targ_map ) { pr_warn ( \".maps relo #%d: '%s' isn't a valid map reference\\n\" , i , name ) ; return - ESRCH ; } } else if ( is_prog_array ) { targ_prog = bpf_object__find_program_by_name ( obj , name ) ; if ( ! targ_prog ) { pr_warn ( \".maps relo #%d: '%s' isn't a valid program reference\\n\" , i , name ) ; return - ESRCH ; } if ( targ_prog -> sec_idx != sym -> st_shndx || targ_prog -> sec_insn_off * 8 != sym -> st_value || prog_is_subprog ( obj , targ_prog ) ) { pr_warn ( \".maps relo #%d: '%s' isn't an entry-point program\\n\" , i , name ) ; return - LIBBPF_ERRNO__RELOC ; } } else { return - EINVAL ; } var = btf__type_by_id ( obj -> btf , vi -> type ) ; def = skip_mods_and_typedefs ( obj -> btf , var -> type , NULL ) ; if ( btf_vlen ( def ) == 0 ) return - EINVAL ; member = btf_members ( def ) + btf_vlen ( def ) - 1 ; mname = btf__name_by_offset ( obj -> btf , member -> name_off ) ; if ( strcmp ( mname , \"values\" ) ) return - EINVAL ; moff = btf_member_bit_offset ( def , btf_vlen ( def ) - 1 ) / 8 ; if ( rel -> r_offset - vi -> offset < moff ) return - EINVAL ; moff = rel -> r_offset - vi -> offset - moff ; /* here we use BPF pointer size, which is always 64 bit, as we\n\t\t * are parsing ELF that was built for BPF target\n\t\t */ if ( moff % bpf_ptr_sz ) return - EINVAL ; moff /= bpf_ptr_sz ; if ( moff >= map -> init_slots_sz ) { new_sz = moff + 1 ; tmp = libbpf_reallocarray ( map -> init_slots , new_sz , host_ptr_sz ) ; if ( ! tmp ) return - ENOMEM ; map -> init_slots = tmp ; memset ( map -> init_slots + map -> init_slots_sz , 0 , ( new_sz - map -> init_slots_sz ) * host_ptr_sz ) ; map -> init_slots_sz = new_sz ; } map -> init_slots [ moff ] = is_map_in_map ? ( void * ) targ_map : ( void * ) targ_prog ; pr_debug ( \".maps relo #%d: map '%s' slot [%d] points to %s '%s'\\n\" , i , map -> name , moff , type , name ) ; } return 0 ; }",
    "resources/libbpf/src/libbpf.c@bpf_object__collect_relos": "static int bpf_object__collect_relos ( struct bpf_object * obj ) { int i , err ; for ( i = 0 ; i < obj -> efile . sec_cnt ; i ++ ) { struct elf_sec_desc * sec_desc = & obj -> efile . secs [ i ] ; Elf64_Shdr * shdr ; Elf_Data * data ; int idx ; if ( sec_desc -> sec_type != SEC_RELO ) continue ; shdr = sec_desc -> shdr ; data = sec_desc -> data ; idx = shdr -> sh_info ; if ( shdr -> sh_type != SHT_REL || idx < 0 || idx >= obj -> efile . sec_cnt ) { pr_warn ( \"internal error at %d\\n\" , __LINE__ ) ; return - LIBBPF_ERRNO__INTERNAL ; } if ( obj -> efile . secs [ idx ] . sec_type == SEC_ST_OPS ) err = bpf_object__collect_st_ops_relos ( obj , shdr , data ) ; else if ( idx == obj -> efile . btf_maps_shndx ) err = bpf_object__collect_map_relos ( obj , shdr , data ) ; else err = bpf_object__collect_prog_relos ( obj , shdr , data ) ; if ( err ) return err ; } bpf_object__sort_relos ( obj ) ; return 0 ; }",
    "resources/libbpf/src/libbpf.c@insn_is_helper_call": "static bool insn_is_helper_call ( struct bpf_insn * insn , enum bpf_func_id * func_id ) { if ( BPF_CLASS ( insn -> code ) == BPF_JMP && BPF_OP ( insn -> code ) == BPF_CALL && BPF_SRC ( insn -> code ) == BPF_K && insn -> src_reg == 0 && insn -> dst_reg == 0 ) { * func_id = insn -> imm ; return true ; } return false ; }",
    "resources/libbpf/src/libbpf.c@bpf_object__sanitize_prog": "static int bpf_object__sanitize_prog ( struct bpf_object * obj , struct bpf_program * prog ) { struct bpf_insn * insn = prog -> insns ; enum bpf_func_id func_id ; int i ; if ( obj -> gen_loader ) return 0 ; for ( i = 0 ; i < prog -> insns_cnt ; i ++ , insn ++ ) { if ( ! insn_is_helper_call ( insn , & func_id ) ) continue ; /* on kernels that don't yet support\n\t\t * bpf_probe_read_{kernel,user}[_str] helpers, fall back\n\t\t * to bpf_probe_read() which works well for old kernels\n\t\t */ switch ( func_id ) { case BPF_FUNC_probe_read_kernel : case BPF_FUNC_probe_read_user : if ( ! kernel_supports ( obj , FEAT_PROBE_READ_KERN ) ) insn -> imm = BPF_FUNC_probe_read ; break ; case BPF_FUNC_probe_read_kernel_str : case BPF_FUNC_probe_read_user_str : if ( ! kernel_supports ( obj , FEAT_PROBE_READ_KERN ) ) insn -> imm = BPF_FUNC_probe_read_str ; break ; default : break ; } } return 0 ; }",
    "resources/libbpf/src/libbpf.c@libbpf_prepare_prog_load": "static int libbpf_prepare_prog_load ( struct bpf_program * prog , struct bpf_prog_load_opts * opts , long cookie ) { enum sec_def_flags def = cookie ; /* old kernels might not support specifying expected_attach_type */ if ( ( def & SEC_EXP_ATTACH_OPT ) && ! kernel_supports ( prog -> obj , FEAT_EXP_ATTACH_TYPE ) ) opts -> expected_attach_type = 0 ; if ( def & SEC_SLEEPABLE ) opts -> prog_flags |= BPF_F_SLEEPABLE ; if ( prog -> type == BPF_PROG_TYPE_XDP && ( def & SEC_XDP_FRAGS ) ) opts -> prog_flags |= BPF_F_XDP_HAS_FRAGS ; /* special check for usdt to use uprobe_multi link */ if ( ( def & SEC_USDT ) && kernel_supports ( prog -> obj , FEAT_UPROBE_MULTI_LINK ) ) prog -> expected_attach_type = BPF_TRACE_UPROBE_MULTI ; if ( ( def & SEC_ATTACH_BTF ) && ! prog -> attach_btf_id ) { int btf_obj_fd = 0 , btf_type_id = 0 , err ; const char * attach_name ; attach_name = strchr ( prog -> sec_name , '/' ) ; if ( ! attach_name ) { /* if BPF program is annotated with just SEC(\"fentry\")\n\t\t\t * (or similar) without declaratively specifying\n\t\t\t * target, then it is expected that target will be\n\t\t\t * specified with bpf_program__set_attach_target() at\n\t\t\t * runtime before BPF object load step. If not, then\n\t\t\t * there is nothing to load into the kernel as BPF\n\t\t\t * verifier won't be able to validate BPF program\n\t\t\t * correctness anyways.\n\t\t\t */ pr_warn ( \"prog '%s': no BTF-based attach target is specified, use bpf_program__set_attach_target()\\n\" , prog -> name ) ; return - EINVAL ; } attach_name ++ ; /* skip over / */ err = libbpf_find_attach_btf_id ( prog , attach_name , & btf_obj_fd , & btf_type_id ) ; if ( err ) return err ; /* cache resolved BTF FD and BTF type ID in the prog */ prog -> attach_btf_obj_fd = btf_obj_fd ; prog -> attach_btf_id = btf_type_id ; /* but by now libbpf common logic is not utilizing\n\t\t * prog->atach_btf_obj_fd/prog->attach_btf_id anymore because\n\t\t * this callback is called after opts were populated by\n\t\t * libbpf, so this callback has to update opts explicitly here\n\t\t */ opts -> attach_btf_obj_fd = btf_obj_fd ; opts -> attach_btf_id = btf_type_id ; } return 0 ; }",
    "resources/libbpf/src/libbpf.c@bpf_object_load_prog": "static int bpf_object_load_prog ( struct bpf_object * obj , struct bpf_program * prog , struct bpf_insn * insns , int insns_cnt , const char * license , __u32 kern_version , int * prog_fd ) { LIBBPF_OPTS ( bpf_prog_load_opts , load_attr ) ; const char * prog_name = NULL ; char * cp , errmsg [ STRERR_BUFSIZE ] ; size_t log_buf_size = 0 ; char * log_buf = NULL , * tmp ; bool own_log_buf = true ; __u32 log_level = prog -> log_level ; int ret , err ; if ( prog -> type == BPF_PROG_TYPE_UNSPEC ) { /*\n\t\t * The program type must be set.  Most likely we couldn't find a proper\n\t\t * section definition at load time, and thus we didn't infer the type.\n\t\t */ pr_warn ( \"prog '%s': missing BPF prog type, check ELF section name '%s'\\n\" , prog -> name , prog -> sec_name ) ; return - EINVAL ; } if ( ! insns || ! insns_cnt ) return - EINVAL ; if ( kernel_supports ( obj , FEAT_PROG_NAME ) ) prog_name = prog -> name ; load_attr . attach_prog_fd = prog -> attach_prog_fd ; load_attr . attach_btf_obj_fd = prog -> attach_btf_obj_fd ; load_attr . attach_btf_id = prog -> attach_btf_id ; load_attr . kern_version = kern_version ; load_attr . prog_ifindex = prog -> prog_ifindex ; /* specify func_info/line_info only if kernel supports them */ if ( obj -> btf && btf__fd ( obj -> btf ) >= 0 && kernel_supports ( obj , FEAT_BTF_FUNC ) ) { load_attr . prog_btf_fd = btf__fd ( obj -> btf ) ; load_attr . func_info = prog -> func_info ; load_attr . func_info_rec_size = prog -> func_info_rec_size ; load_attr . func_info_cnt = prog -> func_info_cnt ; load_attr . line_info = prog -> line_info ; load_attr . line_info_rec_size = prog -> line_info_rec_size ; load_attr . line_info_cnt = prog -> line_info_cnt ; } load_attr . log_level = log_level ; load_attr . prog_flags = prog -> prog_flags ; load_attr . fd_array = obj -> fd_array ; load_attr . token_fd = obj -> token_fd ; if ( obj -> token_fd ) load_attr . prog_flags |= BPF_F_TOKEN_FD ; /* adjust load_attr if sec_def provides custom preload callback */ if ( prog -> sec_def && prog -> sec_def -> prog_prepare_load_fn ) { err = prog -> sec_def -> prog_prepare_load_fn ( prog , & load_attr , prog -> sec_def -> cookie ) ; if ( err < 0 ) { pr_warn ( \"prog '%s': failed to prepare load attributes: %d\\n\" , prog -> name , err ) ; return err ; } insns = prog -> insns ; insns_cnt = prog -> insns_cnt ; } /* allow prog_prepare_load_fn to change expected_attach_type */ load_attr . expected_attach_type = prog -> expected_attach_type ; if ( obj -> gen_loader ) { bpf_gen__prog_load ( obj -> gen_loader , prog -> type , prog -> name , license , insns , insns_cnt , & load_attr , prog - obj -> programs ) ; * prog_fd = - 1 ; return 0 ; } retry_load : /* if log_level is zero, we don't request logs initially even if\n\t * custom log_buf is specified; if the program load fails, then we'll\n\t * bump log_level to 1 and use either custom log_buf or we'll allocate\n\t * our own and retry the load to get details on what failed\n\t */ if ( log_level ) { if ( prog -> log_buf ) { log_buf = prog -> log_buf ; log_buf_size = prog -> log_size ; own_log_buf = false ; } else if ( obj -> log_buf ) { log_buf = obj -> log_buf ; log_buf_size = obj -> log_size ; own_log_buf = false ; } else { log_buf_size = max ( ( size_t ) BPF_LOG_BUF_SIZE , log_buf_size * 2 ) ; tmp = realloc ( log_buf , log_buf_size ) ; if ( ! tmp ) { ret = - ENOMEM ; goto out ; } log_buf = tmp ; log_buf [ 0 ] = '\\0' ; own_log_buf = true ; } } load_attr . log_buf = log_buf ; load_attr . log_size = log_buf_size ; load_attr . log_level = log_level ; ret = bpf_prog_load ( prog -> type , prog_name , license , insns , insns_cnt , & load_attr ) ; if ( ret >= 0 ) { if ( log_level && own_log_buf ) { pr_debug ( \"prog '%s': -- BEGIN PROG LOAD LOG --\\n%s-- END PROG LOAD LOG --\\n\" , prog -> name , log_buf ) ; } if ( obj -> has_rodata && kernel_supports ( obj , FEAT_PROG_BIND_MAP ) ) { struct bpf_map * map ; int i ; for ( i = 0 ; i < obj -> nr_maps ; i ++ ) { map = & prog -> obj -> maps [ i ] ; if ( map -> libbpf_type != LIBBPF_MAP_RODATA ) continue ; if ( bpf_prog_bind_map ( ret , map -> fd , NULL ) ) { cp = libbpf_strerror_r ( errno , errmsg , sizeof ( errmsg ) ) ; pr_warn ( \"prog '%s': failed to bind map '%s': %s\\n\" , prog -> name , map -> real_name , cp ) ; /* Don't fail hard if can't bind rodata. */ } } } * prog_fd = ret ; ret = 0 ; goto out ; } if ( log_level == 0 ) { log_level = 1 ; goto retry_load ; } /* On ENOSPC, increase log buffer size and retry, unless custom\n\t * log_buf is specified.\n\t * Be careful to not overflow u32, though. Kernel's log buf size limit\n\t * isn't part of UAPI so it can always be bumped to full 4GB. So don't\n\t * multiply by 2 unless we are sure we'll fit within 32 bits.\n\t * Currently, we'll get -EINVAL when we reach (UINT_MAX >> 2).\n\t */ if ( own_log_buf && errno == ENOSPC && log_buf_size <= UINT_MAX / 2 ) goto retry_load ; ret = - errno ; /* post-process verifier log to improve error descriptions */ fixup_verifier_log ( prog , log_buf , log_buf_size ) ; cp = libbpf_strerror_r ( errno , errmsg , sizeof ( errmsg ) ) ; pr_warn ( \"prog '%s': BPF program load failed: %s\\n\" , prog -> name , cp ) ; pr_perm_msg ( ret ) ; if ( own_log_buf && log_buf && log_buf [ 0 ] != '\\0' ) { pr_warn ( \"prog '%s': -- BEGIN PROG LOAD LOG --\\n%s-- END PROG LOAD LOG --\\n\" , prog -> name , log_buf ) ; } out : if ( own_log_buf ) free ( log_buf ) ; return ret ; }",
    "resources/libbpf/src/libbpf.c@find_prev_line": "static char * find_prev_line ( char * buf , char * cur ) { char * p ; if ( cur == buf ) /* end of a log buf */ return NULL ; p = cur - 1 ; while ( p - 1 >= buf && * ( p - 1 ) != '\\n' ) p -- ; return p ; }",
    "resources/libbpf/src/libbpf.c@patch_log": "static void patch_log ( char * buf , size_t buf_sz , size_t log_sz , char * orig , size_t orig_sz , const char * patch ) { /* size of the remaining log content to the right from the to-be-replaced part */ size_t rem_sz = ( buf + log_sz ) - ( orig + orig_sz ) ; size_t patch_sz = strlen ( patch ) ; if ( patch_sz != orig_sz ) { /* If patch line(s) are longer than original piece of verifier log,\n\t\t * shift log contents by (patch_sz - orig_sz) bytes to the right\n\t\t * starting from after to-be-replaced part of the log.\n\t\t *\n\t\t * If patch line(s) are shorter than original piece of verifier log,\n\t\t * shift log contents by (orig_sz - patch_sz) bytes to the left\n\t\t * starting from after to-be-replaced part of the log\n\t\t *\n\t\t * We need to be careful about not overflowing available\n\t\t * buf_sz capacity. If that's the case, we'll truncate the end\n\t\t * of the original log, as necessary.\n\t\t */ if ( patch_sz > orig_sz ) { if ( orig + patch_sz >= buf + buf_sz ) { /* patch is big enough to cover remaining space completely */ patch_sz -= ( orig + patch_sz ) - ( buf + buf_sz ) + 1 ; rem_sz = 0 ; } else if ( patch_sz - orig_sz > buf_sz - log_sz ) { /* patch causes part of remaining log to be truncated */ rem_sz -= ( patch_sz - orig_sz ) - ( buf_sz - log_sz ) ; } } /* shift remaining log to the right by calculated amount */ memmove ( orig + patch_sz , orig + orig_sz , rem_sz ) ; } memcpy ( orig , patch , patch_sz ) ; }",
    "resources/libbpf/src/libbpf.c@fixup_log_failed_core_relo": "static void fixup_log_failed_core_relo ( struct bpf_program * prog , char * buf , size_t buf_sz , size_t log_sz , char * line1 , char * line2 , char * line3 ) { /* Expected log for failed and not properly guarded CO-RE relocation:\n\t * line1 -> 123: (85) call unknown#195896080\n\t * line2 -> invalid func unknown#195896080\n\t * line3 -> <anything else or end of buffer>\n\t *\n\t * \"123\" is the index of the instruction that was poisoned. We extract\n\t * instruction index to find corresponding CO-RE relocation and\n\t * replace this part of the log with more relevant information about\n\t * failed CO-RE relocation.\n\t */ const struct bpf_core_relo * relo ; struct bpf_core_spec spec ; char patch [ 512 ] , spec_buf [ 256 ] ; int insn_idx , err , spec_len ; if ( sscanf ( line1 , \"%d: (%*d) call unknown#195896080\\n\" , & insn_idx ) != 1 ) return ; relo = find_relo_core ( prog , insn_idx ) ; if ( ! relo ) return ; err = bpf_core_parse_spec ( prog -> name , prog -> obj -> btf , relo , & spec ) ; if ( err ) return ; spec_len = bpf_core_format_spec ( spec_buf , sizeof ( spec_buf ) , & spec ) ; snprintf ( patch , sizeof ( patch ) , \"%d: <invalid CO-RE relocation>\\n\" \"failed to resolve CO-RE relocation %s%s\\n\" , insn_idx , spec_buf , spec_len >= sizeof ( spec_buf ) ? \"...\" : \"\" ) ; patch_log ( buf , buf_sz , log_sz , line1 , line3 - line1 , patch ) ; }",
    "resources/libbpf/src/libbpf.c@fixup_log_missing_map_load": "static void fixup_log_missing_map_load ( struct bpf_program * prog , char * buf , size_t buf_sz , size_t log_sz , char * line1 , char * line2 , char * line3 ) { /* Expected log for failed and not properly guarded map reference:\n\t * line1 -> 123: (85) call unknown#2001000345\n\t * line2 -> invalid func unknown#2001000345\n\t * line3 -> <anything else or end of buffer>\n\t *\n\t * \"123\" is the index of the instruction that was poisoned.\n\t * \"345\" in \"2001000345\" is a map index in obj->maps to fetch map name.\n\t */ struct bpf_object * obj = prog -> obj ; const struct bpf_map * map ; int insn_idx , map_idx ; char patch [ 128 ] ; if ( sscanf ( line1 , \"%d: (%*d) call unknown#%d\\n\" , & insn_idx , & map_idx ) != 2 ) return ; map_idx -= POISON_LDIMM64_MAP_BASE ; if ( map_idx < 0 || map_idx >= obj -> nr_maps ) return ; map = & obj -> maps [ map_idx ] ; snprintf ( patch , sizeof ( patch ) , \"%d: <invalid BPF map reference>\\n\" \"BPF map '%s' is referenced but wasn't created\\n\" , insn_idx , map -> name ) ; patch_log ( buf , buf_sz , log_sz , line1 , line3 - line1 , patch ) ; }",
    "resources/libbpf/src/libbpf.c@fixup_log_missing_kfunc_call": "static void fixup_log_missing_kfunc_call ( struct bpf_program * prog , char * buf , size_t buf_sz , size_t log_sz , char * line1 , char * line2 , char * line3 ) { /* Expected log for failed and not properly guarded kfunc call:\n\t * line1 -> 123: (85) call unknown#2002000345\n\t * line2 -> invalid func unknown#2002000345\n\t * line3 -> <anything else or end of buffer>\n\t *\n\t * \"123\" is the index of the instruction that was poisoned.\n\t * \"345\" in \"2002000345\" is an extern index in obj->externs to fetch kfunc name.\n\t */ struct bpf_object * obj = prog -> obj ; const struct extern_desc * ext ; int insn_idx , ext_idx ; char patch [ 128 ] ; if ( sscanf ( line1 , \"%d: (%*d) call unknown#%d\\n\" , & insn_idx , & ext_idx ) != 2 ) return ; ext_idx -= POISON_CALL_KFUNC_BASE ; if ( ext_idx < 0 || ext_idx >= obj -> nr_extern ) return ; ext = & obj -> externs [ ext_idx ] ; snprintf ( patch , sizeof ( patch ) , \"%d: <invalid kfunc call>\\n\" \"kfunc '%s' is referenced but wasn't resolved\\n\" , insn_idx , ext -> name ) ; patch_log ( buf , buf_sz , log_sz , line1 , line3 - line1 , patch ) ; }",
    "resources/libbpf/src/libbpf.c@fixup_verifier_log": "static void fixup_verifier_log ( struct bpf_program * prog , char * buf , size_t buf_sz ) { /* look for familiar error patterns in last N lines of the log */ const size_t max_last_line_cnt = 10 ; char * prev_line , * cur_line , * next_line ; size_t log_sz ; int i ; if ( ! buf ) return ; log_sz = strlen ( buf ) + 1 ; next_line = buf + log_sz - 1 ; for ( i = 0 ; i < max_last_line_cnt ; i ++ , next_line = cur_line ) { cur_line = find_prev_line ( buf , next_line ) ; if ( ! cur_line ) return ; if ( str_has_pfx ( cur_line , \"invalid func unknown#195896080\\n\" ) ) { prev_line = find_prev_line ( buf , cur_line ) ; if ( ! prev_line ) continue ; /* failed CO-RE relocation case */ fixup_log_failed_core_relo ( prog , buf , buf_sz , log_sz , prev_line , cur_line , next_line ) ; return ; } else if ( str_has_pfx ( cur_line , \"invalid func unknown#\" POISON_LDIMM64_MAP_PFX ) ) { prev_line = find_prev_line ( buf , cur_line ) ; if ( ! prev_line ) continue ; /* reference to uncreated BPF map */ fixup_log_missing_map_load ( prog , buf , buf_sz , log_sz , prev_line , cur_line , next_line ) ; return ; } else if ( str_has_pfx ( cur_line , \"invalid func unknown#\" POISON_CALL_KFUNC_PFX ) ) { prev_line = find_prev_line ( buf , cur_line ) ; if ( ! prev_line ) continue ; /* reference to unresolved kfunc */ fixup_log_missing_kfunc_call ( prog , buf , buf_sz , log_sz , prev_line , cur_line , next_line ) ; return ; } } }",
    "resources/libbpf/src/libbpf.c@bpf_program_record_relos": "static int bpf_program_record_relos ( struct bpf_program * prog ) { struct bpf_object * obj = prog -> obj ; int i ; for ( i = 0 ; i < prog -> nr_reloc ; i ++ ) { struct reloc_desc * relo = & prog -> reloc_desc [ i ] ; struct extern_desc * ext = & obj -> externs [ relo -> ext_idx ] ; int kind ; switch ( relo -> type ) { case RELO_EXTERN_LD64 : if ( ext -> type != EXT_KSYM ) continue ; kind = btf_is_var ( btf__type_by_id ( obj -> btf , ext -> btf_id ) ) ? BTF_KIND_VAR : BTF_KIND_FUNC ; bpf_gen__record_extern ( obj -> gen_loader , ext -> name , ext -> is_weak , ! ext -> ksym . type_id , true , kind , relo -> insn_idx ) ; break ; case RELO_EXTERN_CALL : bpf_gen__record_extern ( obj -> gen_loader , ext -> name , ext -> is_weak , false , false , BTF_KIND_FUNC , relo -> insn_idx ) ; break ; case RELO_CORE : { struct bpf_core_relo cr = { . insn_off = relo -> insn_idx * 8 , . type_id = relo -> core_relo -> type_id , . access_str_off = relo -> core_relo -> access_str_off , . kind = relo -> core_relo -> kind , } ; bpf_gen__record_relo_core ( obj -> gen_loader , & cr ) ; break ; } default : continue ; } } return 0 ; }",
    "resources/libbpf/src/libbpf.c@bpf_object__load_progs": "static int bpf_object__load_progs ( struct bpf_object * obj , int log_level ) { struct bpf_program * prog ; size_t i ; int err ; for ( i = 0 ; i < obj -> nr_programs ; i ++ ) { prog = & obj -> programs [ i ] ; err = bpf_object__sanitize_prog ( obj , prog ) ; if ( err ) return err ; } for ( i = 0 ; i < obj -> nr_programs ; i ++ ) { prog = & obj -> programs [ i ] ; if ( prog_is_subprog ( obj , prog ) ) continue ; if ( ! prog -> autoload ) { pr_debug ( \"prog '%s': skipped loading\\n\" , prog -> name ) ; continue ; } prog -> log_level |= log_level ; if ( obj -> gen_loader ) bpf_program_record_relos ( prog ) ; err = bpf_object_load_prog ( obj , prog , prog -> insns , prog -> insns_cnt , obj -> license , obj -> kern_version , & prog -> fd ) ; if ( err ) { pr_warn ( \"prog '%s': failed to load: %d\\n\" , prog -> name , err ) ; return err ; } } bpf_object__free_relocs ( obj ) ; return 0 ; }",
    "resources/libbpf/src/libbpf.c@bpf_object_init_progs": "static int bpf_object_init_progs ( struct bpf_object * obj , const struct bpf_object_open_opts * opts ) { struct bpf_program * prog ; int err ; bpf_object__for_each_program ( prog , obj ) { prog -> sec_def = find_sec_def ( prog -> sec_name ) ; if ( ! prog -> sec_def ) { /* couldn't guess, but user might manually specify */ pr_debug ( \"prog '%s': unrecognized ELF section name '%s'\\n\" , prog -> name , prog -> sec_name ) ; continue ; } prog -> type = prog -> sec_def -> prog_type ; prog -> expected_attach_type = prog -> sec_def -> expected_attach_type ; /* sec_def can have custom callback which should be called\n\t\t * after bpf_program is initialized to adjust its properties\n\t\t */ if ( prog -> sec_def -> prog_setup_fn ) { err = prog -> sec_def -> prog_setup_fn ( prog , prog -> sec_def -> cookie ) ; if ( err < 0 ) { pr_warn ( \"prog '%s': failed to initialize: %d\\n\" , prog -> name , err ) ; return err ; } } } return 0 ; }",
    "resources/libbpf/src/libbpf.c@bpf_object_open": "static struct bpf_object * bpf_object_open ( const char * path , const void * obj_buf , size_t obj_buf_sz , const struct bpf_object_open_opts * opts ) { const char * obj_name , * kconfig , * btf_tmp_path , * token_path ; struct bpf_object * obj ; char tmp_name [ 64 ] ; int err ; char * log_buf ; size_t log_size ; __u32 log_level ; if ( elf_version ( EV_CURRENT ) == EV_NONE ) { pr_warn ( \"failed to init libelf for %s\\n\" , path ? : \"(mem buf)\" ) ; return ERR_PTR ( - LIBBPF_ERRNO__LIBELF ) ; } if ( ! OPTS_VALID ( opts , bpf_object_open_opts ) ) return ERR_PTR ( - EINVAL ) ; obj_name = OPTS_GET ( opts , object_name , NULL ) ; if ( obj_buf ) { if ( ! obj_name ) { snprintf ( tmp_name , sizeof ( tmp_name ) , \"%lx-%lx\" , ( unsigned long ) obj_buf , ( unsigned long ) obj_buf_sz ) ; obj_name = tmp_name ; } path = obj_name ; pr_debug ( \"loading object '%s' from buffer\\n\" , obj_name ) ; } log_buf = OPTS_GET ( opts , kernel_log_buf , NULL ) ; log_size = OPTS_GET ( opts , kernel_log_size , 0 ) ; log_level = OPTS_GET ( opts , kernel_log_level , 0 ) ; if ( log_size > UINT_MAX ) return ERR_PTR ( - EINVAL ) ; if ( log_size && ! log_buf ) return ERR_PTR ( - EINVAL ) ; token_path = OPTS_GET ( opts , bpf_token_path , NULL ) ; /* if user didn't specify bpf_token_path explicitly, check if\n\t * LIBBPF_BPF_TOKEN_PATH envvar was set and treat it as bpf_token_path\n\t * option\n\t */ if ( ! token_path ) token_path = getenv ( \"LIBBPF_BPF_TOKEN_PATH\" ) ; if ( token_path && strlen ( token_path ) >= PATH_MAX ) return ERR_PTR ( - ENAMETOOLONG ) ; obj = bpf_object__new ( path , obj_buf , obj_buf_sz , obj_name ) ; if ( IS_ERR ( obj ) ) return obj ; obj -> log_buf = log_buf ; obj -> log_size = log_size ; obj -> log_level = log_level ; if ( token_path ) { obj -> token_path = strdup ( token_path ) ; if ( ! obj -> token_path ) { err = - ENOMEM ; goto out ; } } btf_tmp_path = OPTS_GET ( opts , btf_custom_path , NULL ) ; if ( btf_tmp_path ) { if ( strlen ( btf_tmp_path ) >= PATH_MAX ) { err = - ENAMETOOLONG ; goto out ; } obj -> btf_custom_path = strdup ( btf_tmp_path ) ; if ( ! obj -> btf_custom_path ) { err = - ENOMEM ; goto out ; } } kconfig = OPTS_GET ( opts , kconfig , NULL ) ; if ( kconfig ) { obj -> kconfig = strdup ( kconfig ) ; if ( ! obj -> kconfig ) { err = - ENOMEM ; goto out ; } } err = bpf_object__elf_init ( obj ) ; err = err ? : bpf_object__check_endianness ( obj ) ; err = err ? : bpf_object__elf_collect ( obj ) ; err = err ? : bpf_object__collect_externs ( obj ) ; err = err ? : bpf_object_fixup_btf ( obj ) ; err = err ? : bpf_object__init_maps ( obj , opts ) ; err = err ? : bpf_object_init_progs ( obj , opts ) ; err = err ? : bpf_object__collect_relos ( obj ) ; if ( err ) goto out ; bpf_object__elf_finish ( obj ) ; return obj ; out : bpf_object__close ( obj ) ; return ERR_PTR ( err ) ; }",
    "resources/libbpf/src/libbpf.c@bpf_object__open_file": "struct bpf_object * bpf_object__open_file ( const char * path , const struct bpf_object_open_opts * opts ) { if ( ! path ) return libbpf_err_ptr ( - EINVAL ) ; pr_debug ( \"loading %s\\n\" , path ) ; return libbpf_ptr ( bpf_object_open ( path , NULL , 0 , opts ) ) ; }",
    "resources/libbpf/src/libbpf.c@bpf_object__open": "struct bpf_object * bpf_object__open ( const char * path ) { return bpf_object__open_file ( path , NULL ) ; }",
    "resources/libbpf/src/libbpf.c@bpf_object__open_mem": "struct bpf_object * bpf_object__open_mem ( const void * obj_buf , size_t obj_buf_sz , const struct bpf_object_open_opts * opts ) { if ( ! obj_buf || obj_buf_sz == 0 ) return libbpf_err_ptr ( - EINVAL ) ; return libbpf_ptr ( bpf_object_open ( NULL , obj_buf , obj_buf_sz , opts ) ) ; }",
    "resources/libbpf/src/libbpf.c@bpf_object_unload": "static int bpf_object_unload ( struct bpf_object * obj ) { size_t i ; if ( ! obj ) return libbpf_err ( - EINVAL ) ; for ( i = 0 ; i < obj -> nr_maps ; i ++ ) { zclose ( obj -> maps [ i ] . fd ) ; if ( obj -> maps [ i ] . st_ops ) zfree ( & obj -> maps [ i ] . st_ops -> kern_vdata ) ; } for ( i = 0 ; i < obj -> nr_programs ; i ++ ) bpf_program__unload ( & obj -> programs [ i ] ) ; return 0 ; }",
    "resources/libbpf/src/libbpf.c@bpf_object__sanitize_maps": "static int bpf_object__sanitize_maps ( struct bpf_object * obj ) { struct bpf_map * m ; bpf_object__for_each_map ( m , obj ) { if ( ! bpf_map__is_internal ( m ) ) continue ; if ( ! kernel_supports ( obj , FEAT_ARRAY_MMAP ) ) m -> def . map_flags &= ~ BPF_F_MMAPABLE ; } return 0 ; }",
    "resources/libbpf/src/libbpf.c@libbpf_kallsyms_parse": "int libbpf_kallsyms_parse ( kallsyms_cb_t cb , void * ctx ) { char sym_type , sym_name [ 500 ] ; unsigned long long sym_addr ; int ret , err = 0 ; FILE * f ; f = fopen ( \"/proc/kallsyms\" , \"re\" ) ; if ( ! f ) { err = - errno ; pr_warn ( \"failed to open /proc/kallsyms: %d\\n\" , err ) ; return err ; } while ( true ) { ret = fscanf ( f , \"%llx %c %499s%*[^\\n]\\n\" , & sym_addr , & sym_type , sym_name ) ; if ( ret == EOF && feof ( f ) ) break ; if ( ret != 3 ) { pr_warn ( \"failed to read kallsyms entry: %d\\n\" , ret ) ; err = - EINVAL ; break ; } err = cb ( sym_addr , sym_type , sym_name , ctx ) ; if ( err ) break ; } fclose ( f ) ; return err ; }",
    "resources/libbpf/src/libbpf.c@kallsyms_cb": "static int kallsyms_cb ( unsigned long long sym_addr , char sym_type , const char * sym_name , void * ctx ) { struct bpf_object * obj = ctx ; const struct btf_type * t ; struct extern_desc * ext ; ext = find_extern_by_name ( obj , sym_name ) ; if ( ! ext || ext -> type != EXT_KSYM ) return 0 ; t = btf__type_by_id ( obj -> btf , ext -> btf_id ) ; if ( ! btf_is_var ( t ) ) return 0 ; if ( ext -> is_set && ext -> ksym . addr != sym_addr ) { pr_warn ( \"extern (ksym) '%s': resolution is ambiguous: 0x%llx or 0x%llx\\n\" , sym_name , ext -> ksym . addr , sym_addr ) ; return - EINVAL ; } if ( ! ext -> is_set ) { ext -> is_set = true ; ext -> ksym . addr = sym_addr ; pr_debug ( \"extern (ksym) '%s': set to 0x%llx\\n\" , sym_name , sym_addr ) ; } return 0 ; }",
    "resources/libbpf/src/libbpf.c@bpf_object__read_kallsyms_file": "static int bpf_object__read_kallsyms_file ( struct bpf_object * obj ) { return libbpf_kallsyms_parse ( kallsyms_cb , obj ) ; }",
    "resources/libbpf/src/libbpf.c@find_ksym_btf_id": "static int find_ksym_btf_id ( struct bpf_object * obj , const char * ksym_name , __u16 kind , struct btf * * res_btf , struct module_btf * * res_mod_btf ) { struct module_btf * mod_btf ; struct btf * btf ; int i , id , err ; btf = obj -> btf_vmlinux ; mod_btf = NULL ; id = btf__find_by_name_kind ( btf , ksym_name , kind ) ; if ( id == - ENOENT ) { err = load_module_btfs ( obj ) ; if ( err ) return err ; for ( i = 0 ; i < obj -> btf_module_cnt ; i ++ ) { /* we assume module_btf's BTF FD is always >0 */ mod_btf = & obj -> btf_modules [ i ] ; btf = mod_btf -> btf ; id = btf__find_by_name_kind_own ( btf , ksym_name , kind ) ; if ( id != - ENOENT ) break ; } } if ( id <= 0 ) return - ESRCH ; * res_btf = btf ; * res_mod_btf = mod_btf ; return id ; }",
    "resources/libbpf/src/libbpf.c@bpf_object__resolve_ksym_var_btf_id": "static int bpf_object__resolve_ksym_var_btf_id ( struct bpf_object * obj , struct extern_desc * ext ) { const struct btf_type * targ_var , * targ_type ; __u32 targ_type_id , local_type_id ; struct module_btf * mod_btf = NULL ; const char * targ_var_name ; struct btf * btf = NULL ; int id , err ; id = find_ksym_btf_id ( obj , ext -> name , BTF_KIND_VAR , & btf , & mod_btf ) ; if ( id < 0 ) { if ( id == - ESRCH && ext -> is_weak ) return 0 ; pr_warn ( \"extern (var ksym) '%s': not found in kernel BTF\\n\" , ext -> name ) ; return id ; } /* find local type_id */ local_type_id = ext -> ksym . type_id ; /* find target type_id */ targ_var = btf__type_by_id ( btf , id ) ; targ_var_name = btf__name_by_offset ( btf , targ_var -> name_off ) ; targ_type = skip_mods_and_typedefs ( btf , targ_var -> type , & targ_type_id ) ; err = bpf_core_types_are_compat ( obj -> btf , local_type_id , btf , targ_type_id ) ; if ( err <= 0 ) { const struct btf_type * local_type ; const char * targ_name , * local_name ; local_type = btf__type_by_id ( obj -> btf , local_type_id ) ; local_name = btf__name_by_offset ( obj -> btf , local_type -> name_off ) ; targ_name = btf__name_by_offset ( btf , targ_type -> name_off ) ; pr_warn ( \"extern (var ksym) '%s': incompatible types, expected [%d] %s %s, but kernel has [%d] %s %s\\n\" , ext -> name , local_type_id , btf_kind_str ( local_type ) , local_name , targ_type_id , btf_kind_str ( targ_type ) , targ_name ) ; return - EINVAL ; } ext -> is_set = true ; ext -> ksym . kernel_btf_obj_fd = mod_btf ? mod_btf -> fd : 0 ; ext -> ksym . kernel_btf_id = id ; pr_debug ( \"extern (var ksym) '%s': resolved to [%d] %s %s\\n\" , ext -> name , id , btf_kind_str ( targ_var ) , targ_var_name ) ; return 0 ; }",
    "resources/libbpf/src/libbpf.c@bpf_object__resolve_ksym_func_btf_id": "static int bpf_object__resolve_ksym_func_btf_id ( struct bpf_object * obj , struct extern_desc * ext ) { int local_func_proto_id , kfunc_proto_id , kfunc_id ; struct module_btf * mod_btf = NULL ; const struct btf_type * kern_func ; struct btf * kern_btf = NULL ; int ret ; local_func_proto_id = ext -> ksym . type_id ; kfunc_id = find_ksym_btf_id ( obj , ext -> essent_name ? : ext -> name , BTF_KIND_FUNC , & kern_btf , & mod_btf ) ; if ( kfunc_id < 0 ) { if ( kfunc_id == - ESRCH && ext -> is_weak ) return 0 ; pr_warn ( \"extern (func ksym) '%s': not found in kernel or module BTFs\\n\" , ext -> name ) ; return kfunc_id ; } kern_func = btf__type_by_id ( kern_btf , kfunc_id ) ; kfunc_proto_id = kern_func -> type ; ret = bpf_core_types_are_compat ( obj -> btf , local_func_proto_id , kern_btf , kfunc_proto_id ) ; if ( ret <= 0 ) { if ( ext -> is_weak ) return 0 ; pr_warn ( \"extern (func ksym) '%s': func_proto [%d] incompatible with %s [%d]\\n\" , ext -> name , local_func_proto_id , mod_btf ? mod_btf -> name : \"vmlinux\" , kfunc_proto_id ) ; return - EINVAL ; } /* set index for module BTF fd in fd_array, if unset */ if ( mod_btf && ! mod_btf -> fd_array_idx ) { /* insn->off is s16 */ if ( obj -> fd_array_cnt == INT16_MAX ) { pr_warn ( \"extern (func ksym) '%s': module BTF fd index %d too big to fit in bpf_insn offset\\n\" , ext -> name , mod_btf -> fd_array_idx ) ; return - E2BIG ; } /* Cannot use index 0 for module BTF fd */ if ( ! obj -> fd_array_cnt ) obj -> fd_array_cnt = 1 ; ret = libbpf_ensure_mem ( ( void * * ) & obj -> fd_array , & obj -> fd_array_cap , sizeof ( int ) , obj -> fd_array_cnt + 1 ) ; if ( ret ) return ret ; mod_btf -> fd_array_idx = obj -> fd_array_cnt ; /* we assume module BTF FD is always >0 */ obj -> fd_array [ obj -> fd_array_cnt ++ ] = mod_btf -> fd ; } ext -> is_set = true ; ext -> ksym . kernel_btf_id = kfunc_id ; ext -> ksym . btf_fd_idx = mod_btf ? mod_btf -> fd_array_idx : 0 ; /* Also set kernel_btf_obj_fd to make sure that bpf_object__relocate_data()\n\t * populates FD into ld_imm64 insn when it's used to point to kfunc.\n\t * {kernel_btf_id, btf_fd_idx} -> fixup bpf_call.\n\t * {kernel_btf_id, kernel_btf_obj_fd} -> fixup ld_imm64.\n\t */ ext -> ksym . kernel_btf_obj_fd = mod_btf ? mod_btf -> fd : 0 ; pr_debug ( \"extern (func ksym) '%s': resolved to %s [%d]\\n\" , ext -> name , mod_btf ? mod_btf -> name : \"vmlinux\" , kfunc_id ) ; return 0 ; }",
    "resources/libbpf/src/libbpf.c@bpf_object__resolve_ksyms_btf_id": "static int bpf_object__resolve_ksyms_btf_id ( struct bpf_object * obj ) { const struct btf_type * t ; struct extern_desc * ext ; int i , err ; for ( i = 0 ; i < obj -> nr_extern ; i ++ ) { ext = & obj -> externs [ i ] ; if ( ext -> type != EXT_KSYM || ! ext -> ksym . type_id ) continue ; if ( obj -> gen_loader ) { ext -> is_set = true ; ext -> ksym . kernel_btf_obj_fd = 0 ; ext -> ksym . kernel_btf_id = 0 ; continue ; } t = btf__type_by_id ( obj -> btf , ext -> btf_id ) ; if ( btf_is_var ( t ) ) err = bpf_object__resolve_ksym_var_btf_id ( obj , ext ) ; else err = bpf_object__resolve_ksym_func_btf_id ( obj , ext ) ; if ( err ) return err ; } return 0 ; }",
    "resources/libbpf/src/libbpf.c@bpf_object__resolve_externs": "static int bpf_object__resolve_externs ( struct bpf_object * obj , const char * extra_kconfig ) { bool need_config = false , need_kallsyms = false ; bool need_vmlinux_btf = false ; struct extern_desc * ext ; void * kcfg_data = NULL ; int err , i ; if ( obj -> nr_extern == 0 ) return 0 ; if ( obj -> kconfig_map_idx >= 0 ) kcfg_data = obj -> maps [ obj -> kconfig_map_idx ] . mmaped ; for ( i = 0 ; i < obj -> nr_extern ; i ++ ) { ext = & obj -> externs [ i ] ; if ( ext -> type == EXT_KSYM ) { if ( ext -> ksym . type_id ) need_vmlinux_btf = true ; else need_kallsyms = true ; continue ; } else if ( ext -> type == EXT_KCFG ) { void * ext_ptr = kcfg_data + ext -> kcfg . data_off ; __u64 value = 0 ; /* Kconfig externs need actual /proc/config.gz */ if ( str_has_pfx ( ext -> name , \"CONFIG_\" ) ) { need_config = true ; continue ; } /* Virtual kcfg externs are customly handled by libbpf */ if ( strcmp ( ext -> name , \"LINUX_KERNEL_VERSION\" ) == 0 ) { value = get_kernel_version ( ) ; if ( ! value ) { pr_warn ( \"extern (kcfg) '%s': failed to get kernel version\\n\" , ext -> name ) ; return - EINVAL ; } } else if ( strcmp ( ext -> name , \"LINUX_HAS_BPF_COOKIE\" ) == 0 ) { value = kernel_supports ( obj , FEAT_BPF_COOKIE ) ; } else if ( strcmp ( ext -> name , \"LINUX_HAS_SYSCALL_WRAPPER\" ) == 0 ) { value = kernel_supports ( obj , FEAT_SYSCALL_WRAPPER ) ; } else if ( ! str_has_pfx ( ext -> name , \"LINUX_\" ) || ! ext -> is_weak ) { /* Currently libbpf supports only CONFIG_ and LINUX_ prefixed\n\t\t\t\t * __kconfig externs, where LINUX_ ones are virtual and filled out\n\t\t\t\t * customly by libbpf (their values don't come from Kconfig).\n\t\t\t\t * If LINUX_xxx variable is not recognized by libbpf, but is marked\n\t\t\t\t * __weak, it defaults to zero value, just like for CONFIG_xxx\n\t\t\t\t * externs.\n\t\t\t\t */ pr_warn ( \"extern (kcfg) '%s': unrecognized virtual extern\\n\" , ext -> name ) ; return - EINVAL ; } err = set_kcfg_value_num ( ext , ext_ptr , value ) ; if ( err ) return err ; pr_debug ( \"extern (kcfg) '%s': set to 0x%llx\\n\" , ext -> name , ( long long ) value ) ; } else { pr_warn ( \"extern '%s': unrecognized extern kind\\n\" , ext -> name ) ; return - EINVAL ; } } if ( need_config && extra_kconfig ) { err = bpf_object__read_kconfig_mem ( obj , extra_kconfig , kcfg_data ) ; if ( err ) return - EINVAL ; need_config = false ; for ( i = 0 ; i < obj -> nr_extern ; i ++ ) { ext = & obj -> externs [ i ] ; if ( ext -> type == EXT_KCFG && ! ext -> is_set ) { need_config = true ; break ; } } } if ( need_config ) { err = bpf_object__read_kconfig_file ( obj , kcfg_data ) ; if ( err ) return - EINVAL ; } if ( need_kallsyms ) { err = bpf_object__read_kallsyms_file ( obj ) ; if ( err ) return - EINVAL ; } if ( need_vmlinux_btf ) { err = bpf_object__resolve_ksyms_btf_id ( obj ) ; if ( err ) return - EINVAL ; } for ( i = 0 ; i < obj -> nr_extern ; i ++ ) { ext = & obj -> externs [ i ] ; if ( ! ext -> is_set && ! ext -> is_weak ) { pr_warn ( \"extern '%s' (strong): not resolved\\n\" , ext -> name ) ; return - ESRCH ; } else if ( ! ext -> is_set ) { pr_debug ( \"extern '%s' (weak): not resolved, defaulting to zero\\n\" , ext -> name ) ; } } return 0 ; }",
    "resources/libbpf/src/libbpf.c@bpf_map_prepare_vdata": "static void bpf_map_prepare_vdata ( const struct bpf_map * map ) { struct bpf_struct_ops * st_ops ; __u32 i ; st_ops = map -> st_ops ; for ( i = 0 ; i < btf_vlen ( st_ops -> type ) ; i ++ ) { struct bpf_program * prog = st_ops -> progs [ i ] ; void * kern_data ; int prog_fd ; if ( ! prog ) continue ; prog_fd = bpf_program__fd ( prog ) ; kern_data = st_ops -> kern_vdata + st_ops -> kern_func_off [ i ] ; * ( unsigned long * ) kern_data = prog_fd ; } }",
    "resources/libbpf/src/libbpf.c@bpf_object_prepare_struct_ops": "static int bpf_object_prepare_struct_ops ( struct bpf_object * obj ) { struct bpf_map * map ; int i ; for ( i = 0 ; i < obj -> nr_maps ; i ++ ) { map = & obj -> maps [ i ] ; if ( ! bpf_map__is_struct_ops ( map ) ) continue ; if ( ! map -> autocreate ) continue ; bpf_map_prepare_vdata ( map ) ; } return 0 ; }",
    "resources/libbpf/src/libbpf.c@bpf_object_load": "static int bpf_object_load ( struct bpf_object * obj , int extra_log_level , const char * target_btf_path ) { int err , i ; if ( ! obj ) return libbpf_err ( - EINVAL ) ; if ( obj -> loaded ) { pr_warn ( \"object '%s': load can't be attempted twice\\n\" , obj -> name ) ; return libbpf_err ( - EINVAL ) ; } if ( obj -> gen_loader ) bpf_gen__init ( obj -> gen_loader , extra_log_level , obj -> nr_programs , obj -> nr_maps ) ; err = bpf_object_prepare_token ( obj ) ; err = err ? : bpf_object__probe_loading ( obj ) ; err = err ? : bpf_object__load_vmlinux_btf ( obj , false ) ; err = err ? : bpf_object__resolve_externs ( obj , obj -> kconfig ) ; err = err ? : bpf_object__sanitize_maps ( obj ) ; err = err ? : bpf_object__init_kern_struct_ops_maps ( obj ) ; err = err ? : bpf_object_adjust_struct_ops_autoload ( obj ) ; err = err ? : bpf_object__relocate ( obj , obj -> btf_custom_path ? : target_btf_path ) ; err = err ? : bpf_object__sanitize_and_load_btf ( obj ) ; err = err ? : bpf_object__create_maps ( obj ) ; err = err ? : bpf_object__load_progs ( obj , extra_log_level ) ; err = err ? : bpf_object_init_prog_arrays ( obj ) ; err = err ? : bpf_object_prepare_struct_ops ( obj ) ; if ( obj -> gen_loader ) { /* reset FDs */ if ( obj -> btf ) btf__set_fd ( obj -> btf , - 1 ) ; if ( ! err ) err = bpf_gen__finish ( obj -> gen_loader , obj -> nr_programs , obj -> nr_maps ) ; } /* clean up fd_array */ zfree ( & obj -> fd_array ) ; /* clean up module BTFs */ for ( i = 0 ; i < obj -> btf_module_cnt ; i ++ ) { close ( obj -> btf_modules [ i ] . fd ) ; btf__free ( obj -> btf_modules [ i ] . btf ) ; free ( obj -> btf_modules [ i ] . name ) ; } free ( obj -> btf_modules ) ; /* clean up vmlinux BTF */ btf__free ( obj -> btf_vmlinux ) ; obj -> btf_vmlinux = NULL ; obj -> loaded = true ; /* doesn't matter if successfully or not */ if ( err ) goto out ; return 0 ; out : /* unpin any maps that were auto-pinned during load */ for ( i = 0 ; i < obj -> nr_maps ; i ++ ) if ( obj -> maps [ i ] . pinned && ! obj -> maps [ i ] . reused ) bpf_map__unpin ( & obj -> maps [ i ] , NULL ) ; bpf_object_unload ( obj ) ; pr_warn ( \"failed to load object '%s'\\n\" , obj -> path ) ; return libbpf_err ( err ) ; }",
    "resources/libbpf/src/libbpf.c@bpf_object__load": "int bpf_object__load ( struct bpf_object * obj ) { return bpf_object_load ( obj , 0 , NULL ) ; }",
    "resources/libbpf/src/libbpf.c@make_parent_dir": "static int make_parent_dir ( const char * path ) { char * cp , errmsg [ STRERR_BUFSIZE ] ; char * dname , * dir ; int err = 0 ; dname = strdup ( path ) ; if ( dname == NULL ) return - ENOMEM ; dir = dirname ( dname ) ; if ( mkdir ( dir , 0700 ) && errno != EEXIST ) err = - errno ; free ( dname ) ; if ( err ) { cp = libbpf_strerror_r ( - err , errmsg , sizeof ( errmsg ) ) ; pr_warn ( \"failed to mkdir %s: %s\\n\" , path , cp ) ; } return err ; }",
    "resources/libbpf/src/libbpf.c@check_path": "static int check_path ( const char * path ) { char * cp , errmsg [ STRERR_BUFSIZE ] ; struct statfs st_fs ; char * dname , * dir ; int err = 0 ; if ( path == NULL ) return - EINVAL ; dname = strdup ( path ) ; if ( dname == NULL ) return - ENOMEM ; dir = dirname ( dname ) ; if ( statfs ( dir , & st_fs ) ) { cp = libbpf_strerror_r ( errno , errmsg , sizeof ( errmsg ) ) ; pr_warn ( \"failed to statfs %s: %s\\n\" , dir , cp ) ; err = - errno ; } free ( dname ) ; if ( ! err && st_fs . f_type != BPF_FS_MAGIC ) { pr_warn ( \"specified path %s is not on BPF FS\\n\" , path ) ; err = - EINVAL ; } return err ; }",
    "resources/libbpf/src/libbpf.c@bpf_program__pin": "int bpf_program__pin ( struct bpf_program * prog , const char * path ) { char * cp , errmsg [ STRERR_BUFSIZE ] ; int err ; if ( prog -> fd < 0 ) { pr_warn ( \"prog '%s': can't pin program that wasn't loaded\\n\" , prog -> name ) ; return libbpf_err ( - EINVAL ) ; } err = make_parent_dir ( path ) ; if ( err ) return libbpf_err ( err ) ; err = check_path ( path ) ; if ( err ) return libbpf_err ( err ) ; if ( bpf_obj_pin ( prog -> fd , path ) ) { err = - errno ; cp = libbpf_strerror_r ( err , errmsg , sizeof ( errmsg ) ) ; pr_warn ( \"prog '%s': failed to pin at '%s': %s\\n\" , prog -> name , path , cp ) ; return libbpf_err ( err ) ; } pr_debug ( \"prog '%s': pinned at '%s'\\n\" , prog -> name , path ) ; return 0 ; }",
    "resources/libbpf/src/libbpf.c@bpf_program__unpin": "int bpf_program__unpin ( struct bpf_program * prog , const char * path ) { int err ; if ( prog -> fd < 0 ) { pr_warn ( \"prog '%s': can't unpin program that wasn't loaded\\n\" , prog -> name ) ; return libbpf_err ( - EINVAL ) ; } err = check_path ( path ) ; if ( err ) return libbpf_err ( err ) ; err = unlink ( path ) ; if ( err ) return libbpf_err ( - errno ) ; pr_debug ( \"prog '%s': unpinned from '%s'\\n\" , prog -> name , path ) ; return 0 ; }",
    "resources/libbpf/src/libbpf.c@bpf_map__pin": "int bpf_map__pin ( struct bpf_map * map , const char * path ) { char * cp , errmsg [ STRERR_BUFSIZE ] ; int err ; if ( map == NULL ) { pr_warn ( \"invalid map pointer\\n\" ) ; return libbpf_err ( - EINVAL ) ; } if ( map -> fd < 0 ) { pr_warn ( \"map '%s': can't pin BPF map without FD (was it created?)\\n\" , map -> name ) ; return libbpf_err ( - EINVAL ) ; } if ( map -> pin_path ) { if ( path && strcmp ( path , map -> pin_path ) ) { pr_warn ( \"map '%s' already has pin path '%s' different from '%s'\\n\" , bpf_map__name ( map ) , map -> pin_path , path ) ; return libbpf_err ( - EINVAL ) ; } else if ( map -> pinned ) { pr_debug ( \"map '%s' already pinned at '%s'; not re-pinning\\n\" , bpf_map__name ( map ) , map -> pin_path ) ; return 0 ; } } else { if ( ! path ) { pr_warn ( \"missing a path to pin map '%s' at\\n\" , bpf_map__name ( map ) ) ; return libbpf_err ( - EINVAL ) ; } else if ( map -> pinned ) { pr_warn ( \"map '%s' already pinned\\n\" , bpf_map__name ( map ) ) ; return libbpf_err ( - EEXIST ) ; } map -> pin_path = strdup ( path ) ; if ( ! map -> pin_path ) { err = - errno ; goto out_err ; } } err = make_parent_dir ( map -> pin_path ) ; if ( err ) return libbpf_err ( err ) ; err = check_path ( map -> pin_path ) ; if ( err ) return libbpf_err ( err ) ; if ( bpf_obj_pin ( map -> fd , map -> pin_path ) ) { err = - errno ; goto out_err ; } map -> pinned = true ; pr_debug ( \"pinned map '%s'\\n\" , map -> pin_path ) ; return 0 ; out_err : cp = libbpf_strerror_r ( - err , errmsg , sizeof ( errmsg ) ) ; pr_warn ( \"failed to pin map: %s\\n\" , cp ) ; return libbpf_err ( err ) ; }",
    "resources/libbpf/src/libbpf.c@bpf_map__unpin": "int bpf_map__unpin ( struct bpf_map * map , const char * path ) { int err ; if ( map == NULL ) { pr_warn ( \"invalid map pointer\\n\" ) ; return libbpf_err ( - EINVAL ) ; } if ( map -> pin_path ) { if ( path && strcmp ( path , map -> pin_path ) ) { pr_warn ( \"map '%s' already has pin path '%s' different from '%s'\\n\" , bpf_map__name ( map ) , map -> pin_path , path ) ; return libbpf_err ( - EINVAL ) ; } path = map -> pin_path ; } else if ( ! path ) { pr_warn ( \"no path to unpin map '%s' from\\n\" , bpf_map__name ( map ) ) ; return libbpf_err ( - EINVAL ) ; } err = check_path ( path ) ; if ( err ) return libbpf_err ( err ) ; err = unlink ( path ) ; if ( err != 0 ) return libbpf_err ( - errno ) ; map -> pinned = false ; pr_debug ( \"unpinned map '%s' from '%s'\\n\" , bpf_map__name ( map ) , path ) ; return 0 ; }",
    "resources/libbpf/src/libbpf.c@bpf_map__set_pin_path": "int bpf_map__set_pin_path ( struct bpf_map * map , const char * path ) { char * new = NULL ; if ( path ) { new = strdup ( path ) ; if ( ! new ) return libbpf_err ( - errno ) ; } free ( map -> pin_path ) ; map -> pin_path = new ; return 0 ; }",
    "resources/libbpf/src/libbpf.c@bpf_map__pin_path": "const char * bpf_map__pin_path ( const struct bpf_map * map ) { return map -> pin_path ; }",
    "resources/libbpf/src/libbpf.c@bpf_map__is_pinned": "",
    "resources/libbpf/src/libbpf.c@sanitize_pin_path": "static void sanitize_pin_path ( char * s ) { /* bpffs disallows periods in path names */ while ( * s ) { if ( * s == '.' ) * s = '_' ; s ++ ; } }",
    "resources/libbpf/src/libbpf.c@bpf_object__pin_maps": "int bpf_object__pin_maps ( struct bpf_object * obj , const char * path ) { struct bpf_map * map ; int err ; if ( ! obj ) return libbpf_err ( - ENOENT ) ; if ( ! obj -> loaded ) { pr_warn ( \"object not yet loaded; load it first\\n\" ) ; return libbpf_err ( - ENOENT ) ; } bpf_object__for_each_map ( map , obj ) { char * pin_path = NULL ; char buf [ PATH_MAX ] ; if ( ! map -> autocreate ) continue ; if ( path ) { err = pathname_concat ( buf , sizeof ( buf ) , path , bpf_map__name ( map ) ) ; if ( err ) goto err_unpin_maps ; sanitize_pin_path ( buf ) ; pin_path = buf ; } else if ( ! map -> pin_path ) { continue ; } err = bpf_map__pin ( map , pin_path ) ; if ( err ) goto err_unpin_maps ; } return 0 ; err_unpin_maps : while ( ( map = bpf_object__prev_map ( obj , map ) ) ) { if ( ! map -> pin_path ) continue ; bpf_map__unpin ( map , NULL ) ; } return libbpf_err ( err ) ; }",
    "resources/libbpf/src/libbpf.c@bpf_object__unpin_maps": "int bpf_object__unpin_maps ( struct bpf_object * obj , const char * path ) { struct bpf_map * map ; int err ; if ( ! obj ) return libbpf_err ( - ENOENT ) ; bpf_object__for_each_map ( map , obj ) { char * pin_path = NULL ; char buf [ PATH_MAX ] ; if ( path ) { err = pathname_concat ( buf , sizeof ( buf ) , path , bpf_map__name ( map ) ) ; if ( err ) return libbpf_err ( err ) ; sanitize_pin_path ( buf ) ; pin_path = buf ; } else if ( ! map -> pin_path ) { continue ; } err = bpf_map__unpin ( map , pin_path ) ; if ( err ) return libbpf_err ( err ) ; } return 0 ; }",
    "resources/libbpf/src/libbpf.c@bpf_object__pin_programs": "int bpf_object__pin_programs ( struct bpf_object * obj , const char * path ) { struct bpf_program * prog ; char buf [ PATH_MAX ] ; int err ; if ( ! obj ) return libbpf_err ( - ENOENT ) ; if ( ! obj -> loaded ) { pr_warn ( \"object not yet loaded; load it first\\n\" ) ; return libbpf_err ( - ENOENT ) ; } bpf_object__for_each_program ( prog , obj ) { err = pathname_concat ( buf , sizeof ( buf ) , path , prog -> name ) ; if ( err ) goto err_unpin_programs ; err = bpf_program__pin ( prog , buf ) ; if ( err ) goto err_unpin_programs ; } return 0 ; err_unpin_programs : while ( ( prog = bpf_object__prev_program ( obj , prog ) ) ) { if ( pathname_concat ( buf , sizeof ( buf ) , path , prog -> name ) ) continue ; bpf_program__unpin ( prog , buf ) ; } return libbpf_err ( err ) ; }",
    "resources/libbpf/src/libbpf.c@bpf_object__unpin_programs": "int bpf_object__unpin_programs ( struct bpf_object * obj , const char * path ) { struct bpf_program * prog ; int err ; if ( ! obj ) return libbpf_err ( - ENOENT ) ; bpf_object__for_each_program ( prog , obj ) { char buf [ PATH_MAX ] ; err = pathname_concat ( buf , sizeof ( buf ) , path , prog -> name ) ; if ( err ) return libbpf_err ( err ) ; err = bpf_program__unpin ( prog , buf ) ; if ( err ) return libbpf_err ( err ) ; } return 0 ; }",
    "resources/libbpf/src/libbpf.c@bpf_object__pin": "int bpf_object__pin ( struct bpf_object * obj , const char * path ) { int err ; err = bpf_object__pin_maps ( obj , path ) ; if ( err ) return libbpf_err ( err ) ; err = bpf_object__pin_programs ( obj , path ) ; if ( err ) { bpf_object__unpin_maps ( obj , path ) ; return libbpf_err ( err ) ; } return 0 ; }",
    "resources/libbpf/src/libbpf.c@bpf_object__unpin": "int bpf_object__unpin ( struct bpf_object * obj , const char * path ) { int err ; err = bpf_object__unpin_programs ( obj , path ) ; if ( err ) return libbpf_err ( err ) ; err = bpf_object__unpin_maps ( obj , path ) ; if ( err ) return libbpf_err ( err ) ; return 0 ; }",
    "resources/libbpf/src/libbpf.c@bpf_map__destroy": "static void bpf_map__destroy ( struct bpf_map * map ) { if ( map -> inner_map ) { bpf_map__destroy ( map -> inner_map ) ; zfree ( & map -> inner_map ) ; } zfree ( & map -> init_slots ) ; map -> init_slots_sz = 0 ; if ( map -> mmaped && map -> mmaped != map -> obj -> arena_data ) munmap ( map -> mmaped , bpf_map_mmap_sz ( map ) ) ; map -> mmaped = NULL ; if ( map -> st_ops ) { zfree ( & map -> st_ops -> data ) ; zfree ( & map -> st_ops -> progs ) ; zfree ( & map -> st_ops -> kern_func_off ) ; zfree ( & map -> st_ops ) ; } zfree ( & map -> name ) ; zfree ( & map -> real_name ) ; zfree ( & map -> pin_path ) ; if ( map -> fd >= 0 ) zclose ( map -> fd ) ; }",
    "resources/libbpf/src/libbpf.c@bpf_object__close": "void bpf_object__close ( struct bpf_object * obj ) { size_t i ; if ( IS_ERR_OR_NULL ( obj ) ) return ; usdt_manager_free ( obj -> usdt_man ) ; obj -> usdt_man = NULL ; bpf_gen__free ( obj -> gen_loader ) ; bpf_object__elf_finish ( obj ) ; bpf_object_unload ( obj ) ; btf__free ( obj -> btf ) ; btf__free ( obj -> btf_vmlinux ) ; btf_ext__free ( obj -> btf_ext ) ; for ( i = 0 ; i < obj -> nr_maps ; i ++ ) bpf_map__destroy ( & obj -> maps [ i ] ) ; zfree ( & obj -> btf_custom_path ) ; zfree ( & obj -> kconfig ) ; for ( i = 0 ; i < obj -> nr_extern ; i ++ ) zfree ( & obj -> externs [ i ] . essent_name ) ; zfree ( & obj -> externs ) ; obj -> nr_extern = 0 ; zfree ( & obj -> maps ) ; obj -> nr_maps = 0 ; if ( obj -> programs && obj -> nr_programs ) { for ( i = 0 ; i < obj -> nr_programs ; i ++ ) bpf_program__exit ( & obj -> programs [ i ] ) ; } zfree ( & obj -> programs ) ; zfree ( & obj -> feat_cache ) ; zfree ( & obj -> token_path ) ; if ( obj -> token_fd > 0 ) close ( obj -> token_fd ) ; zfree ( & obj -> arena_data ) ; free ( obj ) ; }",
    "resources/libbpf/src/libbpf.c@bpf_object__name": "const char * bpf_object__name ( const struct bpf_object * obj ) { return obj ? obj -> name : libbpf_err_ptr ( - EINVAL ) ; }",
    "resources/libbpf/src/libbpf.c@bpf_object__kversion": "unsigned int bpf_object__kversion ( const struct bpf_object * obj ) { return obj ? obj -> kern_version : 0 ; }",
    "resources/libbpf/src/libbpf.c@bpf_object__btf": "struct btf * bpf_object__btf ( const struct bpf_object * obj ) { return obj ? obj -> btf : NULL ; }",
    "resources/libbpf/src/libbpf.c@bpf_object__btf_fd": "int bpf_object__btf_fd ( const struct bpf_object * obj ) { return obj -> btf ? btf__fd ( obj -> btf ) : - 1 ; }",
    "resources/libbpf/src/libbpf.c@bpf_object__set_kversion": "int bpf_object__set_kversion ( struct bpf_object * obj , __u32 kern_version ) { if ( obj -> loaded ) return libbpf_err ( - EINVAL ) ; obj -> kern_version = kern_version ; return 0 ; }",
    "resources/libbpf/src/libbpf.c@bpf_object__gen_loader": "int bpf_object__gen_loader ( struct bpf_object * obj , struct gen_loader_opts * opts ) { struct bpf_gen * gen ; if ( ! opts ) return - EFAULT ; if ( ! OPTS_VALID ( opts , gen_loader_opts ) ) return - EINVAL ; gen = calloc ( sizeof ( * gen ) , 1 ) ; if ( ! gen ) return - ENOMEM ; gen -> opts = opts ; obj -> gen_loader = gen ; return 0 ; }",
    "resources/libbpf/src/libbpf.c@__bpf_program__iter": "static struct bpf_program * __bpf_program__iter ( const struct bpf_program * p , const struct bpf_object * obj , bool forward ) { size_t nr_programs = obj -> nr_programs ; ssize_t idx ; if ( ! nr_programs ) return NULL ; if ( ! p ) /* Iter from the beginning */ return forward ? & obj -> programs [ 0 ] : & obj -> programs [ nr_programs - 1 ] ; if ( p -> obj != obj ) { pr_warn ( \"error: program handler doesn't match object\\n\" ) ; return errno = EINVAL , NULL ; } idx = ( p - obj -> programs ) + ( forward ? 1 : - 1 ) ; if ( idx >= obj -> nr_programs || idx < 0 ) return NULL ; return & obj -> programs [ idx ] ; }",
    "resources/libbpf/src/libbpf.c@bpf_object__next_program": "struct bpf_program * bpf_object__next_program ( const struct bpf_object * obj , struct bpf_program * prev ) { struct bpf_program * prog = prev ; do { prog = __bpf_program__iter ( prog , obj , true ) ; } while ( prog && prog_is_subprog ( obj , prog ) ) ; return prog ; }",
    "resources/libbpf/src/libbpf.c@bpf_object__prev_program": "struct bpf_program * bpf_object__prev_program ( const struct bpf_object * obj , struct bpf_program * next ) { struct bpf_program * prog = next ; do { prog = __bpf_program__iter ( prog , obj , false ) ; } while ( prog && prog_is_subprog ( obj , prog ) ) ; return prog ; }",
    "resources/libbpf/src/libbpf.c@bpf_program__set_ifindex": "void bpf_program__set_ifindex ( struct bpf_program * prog , __u32 ifindex ) { prog -> prog_ifindex = ifindex ; }",
    "resources/libbpf/src/libbpf.c@bpf_program__name": "const char * bpf_program__name ( const struct bpf_program * prog ) { return prog -> name ; }",
    "resources/libbpf/src/libbpf.c@bpf_program__section_name": "const char * bpf_program__section_name ( const struct bpf_program * prog ) { return prog -> sec_name ; }",
    "resources/libbpf/src/libbpf.c@bpf_program__autoload": "",
    "resources/libbpf/src/libbpf.c@bpf_program__set_autoload": "int bpf_program__set_autoload ( struct bpf_program * prog , bool autoload ) { if ( prog -> obj -> loaded ) return libbpf_err ( - EINVAL ) ; prog -> autoload = autoload ; return 0 ; }",
    "resources/libbpf/src/libbpf.c@bpf_program__autoattach": "",
    "resources/libbpf/src/libbpf.c@bpf_program__set_autoattach": "void bpf_program__set_autoattach ( struct bpf_program * prog , bool autoattach ) { prog -> autoattach = autoattach ; }",
    "resources/libbpf/src/libbpf.c@bpf_program__insns": "const struct bpf_insn * bpf_program__insns ( const struct bpf_program * prog ) { return prog -> insns ; }",
    "resources/libbpf/src/libbpf.c@bpf_program__insn_cnt": "size_t bpf_program__insn_cnt ( const struct bpf_program * prog ) { return prog -> insns_cnt ; }",
    "resources/libbpf/src/libbpf.c@bpf_program__set_insns": "int bpf_program__set_insns ( struct bpf_program * prog , struct bpf_insn * new_insns , size_t new_insn_cnt ) { struct bpf_insn * insns ; if ( prog -> obj -> loaded ) return - EBUSY ; insns = libbpf_reallocarray ( prog -> insns , new_insn_cnt , sizeof ( * insns ) ) ; /* NULL is a valid return from reallocarray if the new count is zero */ if ( ! insns && new_insn_cnt ) { pr_warn ( \"prog '%s': failed to realloc prog code\\n\" , prog -> name ) ; return - ENOMEM ; } memcpy ( insns , new_insns , new_insn_cnt * sizeof ( * insns ) ) ; prog -> insns = insns ; prog -> insns_cnt = new_insn_cnt ; return 0 ; }",
    "resources/libbpf/src/libbpf.c@bpf_program__fd": "int bpf_program__fd ( const struct bpf_program * prog ) { if ( ! prog ) return libbpf_err ( - EINVAL ) ; if ( prog -> fd < 0 ) return libbpf_err ( - ENOENT ) ; return prog -> fd ; }",
    "resources/libbpf/src/libbpf.c@bpf_program__type": "enum bpf_prog_type bpf_program__type ( const struct bpf_program * prog ) { return prog -> type ; }",
    "resources/libbpf/src/libbpf.c@bpf_program__set_type": "int bpf_program__set_type ( struct bpf_program * prog , enum bpf_prog_type type ) { if ( prog -> obj -> loaded ) return libbpf_err ( - EBUSY ) ; /* if type is not changed, do nothing */ if ( prog -> type == type ) return 0 ; prog -> type = type ; /* If a program type was changed, we need to reset associated SEC()\n\t * handler, as it will be invalid now. The only exception is a generic\n\t * fallback handler, which by definition is program type-agnostic and\n\t * is a catch-all custom handler, optionally set by the application,\n\t * so should be able to handle any type of BPF program.\n\t */ if ( prog -> sec_def != & custom_fallback_def ) prog -> sec_def = NULL ; return 0 ; }",
    "resources/libbpf/src/libbpf.c@bpf_program__expected_attach_type": "enum bpf_attach_type bpf_program__expected_attach_type ( const struct bpf_program * prog ) { return prog -> expected_attach_type ; }",
    "resources/libbpf/src/libbpf.c@bpf_program__set_expected_attach_type": "int bpf_program__set_expected_attach_type ( struct bpf_program * prog , enum bpf_attach_type type ) { if ( prog -> obj -> loaded ) return libbpf_err ( - EBUSY ) ; prog -> expected_attach_type = type ; return 0 ; }",
    "resources/libbpf/src/libbpf.c@bpf_program__flags": "__u32 bpf_program__flags ( const struct bpf_program * prog ) { return prog -> prog_flags ; }",
    "resources/libbpf/src/libbpf.c@bpf_program__set_flags": "int bpf_program__set_flags ( struct bpf_program * prog , __u32 flags ) { if ( prog -> obj -> loaded ) return libbpf_err ( - EBUSY ) ; prog -> prog_flags = flags ; return 0 ; }",
    "resources/libbpf/src/libbpf.c@bpf_program__log_level": "__u32 bpf_program__log_level ( const struct bpf_program * prog ) { return prog -> log_level ; }",
    "resources/libbpf/src/libbpf.c@bpf_program__set_log_level": "int bpf_program__set_log_level ( struct bpf_program * prog , __u32 log_level ) { if ( prog -> obj -> loaded ) return libbpf_err ( - EBUSY ) ; prog -> log_level = log_level ; return 0 ; }",
    "resources/libbpf/src/libbpf.c@bpf_program__log_buf": "const char * bpf_program__log_buf ( const struct bpf_program * prog , size_t * log_size ) { * log_size = prog -> log_size ; return prog -> log_buf ; }",
    "resources/libbpf/src/libbpf.c@bpf_program__set_log_buf": "int bpf_program__set_log_buf ( struct bpf_program * prog , char * log_buf , size_t log_size ) { if ( log_size && ! log_buf ) return - EINVAL ; if ( prog -> log_size > UINT_MAX ) return - EINVAL ; if ( prog -> obj -> loaded ) return - EBUSY ; prog -> log_buf = log_buf ; prog -> log_size = log_size ; return 0 ; }",
    "resources/libbpf/src/libbpf.c@libbpf_register_prog_handler": "int libbpf_register_prog_handler ( const char * sec , enum bpf_prog_type prog_type , enum bpf_attach_type exp_attach_type , const struct libbpf_prog_handler_opts * opts ) { struct bpf_sec_def * sec_def ; if ( ! OPTS_VALID ( opts , libbpf_prog_handler_opts ) ) return libbpf_err ( - EINVAL ) ; if ( last_custom_sec_def_handler_id == INT_MAX ) /* prevent overflow */ return libbpf_err ( - E2BIG ) ; if ( sec ) { sec_def = libbpf_reallocarray ( custom_sec_defs , custom_sec_def_cnt + 1 , sizeof ( * sec_def ) ) ; if ( ! sec_def ) return libbpf_err ( - ENOMEM ) ; custom_sec_defs = sec_def ; sec_def = & custom_sec_defs [ custom_sec_def_cnt ] ; } else { if ( has_custom_fallback_def ) return libbpf_err ( - EBUSY ) ; sec_def = & custom_fallback_def ; } sec_def -> sec = sec ? strdup ( sec ) : NULL ; if ( sec && ! sec_def -> sec ) return libbpf_err ( - ENOMEM ) ; sec_def -> prog_type = prog_type ; sec_def -> expected_attach_type = exp_attach_type ; sec_def -> cookie = OPTS_GET ( opts , cookie , 0 ) ; sec_def -> prog_setup_fn = OPTS_GET ( opts , prog_setup_fn , NULL ) ; sec_def -> prog_prepare_load_fn = OPTS_GET ( opts , prog_prepare_load_fn , NULL ) ; sec_def -> prog_attach_fn = OPTS_GET ( opts , prog_attach_fn , NULL ) ; sec_def -> handler_id = ++ last_custom_sec_def_handler_id ; if ( sec ) custom_sec_def_cnt ++ ; else has_custom_fallback_def = true ; return sec_def -> handler_id ; }",
    "resources/libbpf/src/libbpf.c@libbpf_unregister_prog_handler": "int libbpf_unregister_prog_handler ( int handler_id ) { struct bpf_sec_def * sec_defs ; int i ; if ( handler_id <= 0 ) return libbpf_err ( - EINVAL ) ; if ( has_custom_fallback_def && custom_fallback_def . handler_id == handler_id ) { memset ( & custom_fallback_def , 0 , sizeof ( custom_fallback_def ) ) ; has_custom_fallback_def = false ; return 0 ; } for ( i = 0 ; i < custom_sec_def_cnt ; i ++ ) { if ( custom_sec_defs [ i ] . handler_id == handler_id ) break ; } if ( i == custom_sec_def_cnt ) return libbpf_err ( - ENOENT ) ; free ( custom_sec_defs [ i ] . sec ) ; for ( i = i + 1 ; i < custom_sec_def_cnt ; i ++ ) custom_sec_defs [ i - 1 ] = custom_sec_defs [ i ] ; custom_sec_def_cnt -- ; /* try to shrink the array, but it's ok if we couldn't */ sec_defs = libbpf_reallocarray ( custom_sec_defs , custom_sec_def_cnt , sizeof ( * sec_defs ) ) ; /* if new count is zero, reallocarray can return a valid NULL result;\n\t * in this case the previous pointer will be freed, so we *have to*\n\t * reassign old pointer to the new value (even if it's NULL)\n\t */ if ( sec_defs || custom_sec_def_cnt == 0 ) custom_sec_defs = sec_defs ; return 0 ; }",
    "resources/libbpf/src/libbpf.c@sec_def_matches": "static bool sec_def_matches ( const struct bpf_sec_def * sec_def , const char * sec_name ) { size_t len = strlen ( sec_def -> sec ) ; /* \"type/\" always has to have proper SEC(\"type/extras\") form */ if ( sec_def -> sec [ len - 1 ] == '/' ) { if ( str_has_pfx ( sec_name , sec_def -> sec ) ) return true ; return false ; } /* \"type+\" means it can be either exact SEC(\"type\") or\n\t * well-formed SEC(\"type/extras\") with proper '/' separator\n\t */ if ( sec_def -> sec [ len - 1 ] == '+' ) { len -- ; /* not even a prefix */ if ( strncmp ( sec_name , sec_def -> sec , len ) != 0 ) return false ; /* exact match or has '/' separator */ if ( sec_name [ len ] == '\\0' || sec_name [ len ] == '/' ) return true ; return false ; } return strcmp ( sec_name , sec_def -> sec ) == 0 ; }",
    "resources/libbpf/src/libbpf.c@find_sec_def": "static const struct bpf_sec_def * find_sec_def ( const char * sec_name ) { const struct bpf_sec_def * sec_def ; int i , n ; n = custom_sec_def_cnt ; for ( i = 0 ; i < n ; i ++ ) { sec_def = & custom_sec_defs [ i ] ; if ( sec_def_matches ( sec_def , sec_name ) ) return sec_def ; } n = ARRAY_SIZE ( section_defs ) ; for ( i = 0 ; i < n ; i ++ ) { sec_def = & section_defs [ i ] ; if ( sec_def_matches ( sec_def , sec_name ) ) return sec_def ; } if ( has_custom_fallback_def ) return & custom_fallback_def ; return NULL ; }",
    "resources/libbpf/src/libbpf.c@libbpf_get_type_names": "static char * libbpf_get_type_names ( bool attach_type ) { int i , len = ARRAY_SIZE ( section_defs ) * MAX_TYPE_NAME_SIZE ; char * buf ; buf = malloc ( len ) ; if ( ! buf ) return NULL ; buf [ 0 ] = '\\0' ; /* Forge string buf with all available names */ for ( i = 0 ; i < ARRAY_SIZE ( section_defs ) ; i ++ ) { const struct bpf_sec_def * sec_def = & section_defs [ i ] ; if ( attach_type ) { if ( sec_def -> prog_prepare_load_fn != libbpf_prepare_prog_load ) continue ; if ( ! ( sec_def -> cookie & SEC_ATTACHABLE ) ) continue ; } if ( strlen ( buf ) + strlen ( section_defs [ i ] . sec ) + 2 > len ) { free ( buf ) ; return NULL ; } strcat ( buf , \" \" ) ; strcat ( buf , section_defs [ i ] . sec ) ; } return buf ; }",
    "resources/libbpf/src/libbpf.c@libbpf_prog_type_by_name": "int libbpf_prog_type_by_name ( const char * name , enum bpf_prog_type * prog_type , enum bpf_attach_type * expected_attach_type ) { const struct bpf_sec_def * sec_def ; char * type_names ; if ( ! name ) return libbpf_err ( - EINVAL ) ; sec_def = find_sec_def ( name ) ; if ( sec_def ) { * prog_type = sec_def -> prog_type ; * expected_attach_type = sec_def -> expected_attach_type ; return 0 ; } pr_debug ( \"failed to guess program type from ELF section '%s'\\n\" , name ) ; type_names = libbpf_get_type_names ( false ) ; if ( type_names != NULL ) { pr_debug ( \"supported section(type) names are:%s\\n\" , type_names ) ; free ( type_names ) ; } return libbpf_err ( - ESRCH ) ; }",
    "resources/libbpf/src/libbpf.c@libbpf_bpf_attach_type_str": "const char * libbpf_bpf_attach_type_str ( enum bpf_attach_type t ) { if ( t < 0 || t >= ARRAY_SIZE ( attach_type_name ) ) return NULL ; return attach_type_name [ t ] ; }",
    "resources/libbpf/src/libbpf.c@libbpf_bpf_link_type_str": "const char * libbpf_bpf_link_type_str ( enum bpf_link_type t ) { if ( t < 0 || t >= ARRAY_SIZE ( link_type_name ) ) return NULL ; return link_type_name [ t ] ; }",
    "resources/libbpf/src/libbpf.c@libbpf_bpf_map_type_str": "const char * libbpf_bpf_map_type_str ( enum bpf_map_type t ) { if ( t < 0 || t >= ARRAY_SIZE ( map_type_name ) ) return NULL ; return map_type_name [ t ] ; }",
    "resources/libbpf/src/libbpf.c@libbpf_bpf_prog_type_str": "const char * libbpf_bpf_prog_type_str ( enum bpf_prog_type t ) { if ( t < 0 || t >= ARRAY_SIZE ( prog_type_name ) ) return NULL ; return prog_type_name [ t ] ; }",
    "resources/libbpf/src/libbpf.c@find_struct_ops_map_by_offset": "static struct bpf_map * find_struct_ops_map_by_offset ( struct bpf_object * obj , int sec_idx , size_t offset ) { struct bpf_map * map ; size_t i ; for ( i = 0 ; i < obj -> nr_maps ; i ++ ) { map = & obj -> maps [ i ] ; if ( ! bpf_map__is_struct_ops ( map ) ) continue ; if ( map -> sec_idx == sec_idx && map -> sec_offset <= offset && offset - map -> sec_offset < map -> def . value_size ) return map ; } return NULL ; }",
    "resources/libbpf/src/libbpf.c@bpf_object__collect_st_ops_relos": "static int bpf_object__collect_st_ops_relos ( struct bpf_object * obj , Elf64_Shdr * shdr , Elf_Data * data ) { const struct btf_member * member ; struct bpf_struct_ops * st_ops ; struct bpf_program * prog ; unsigned int shdr_idx ; const struct btf * btf ; struct bpf_map * map ; unsigned int moff , insn_idx ; const char * name ; __u32 member_idx ; Elf64_Sym * sym ; Elf64_Rel * rel ; int i , nrels ; btf = obj -> btf ; nrels = shdr -> sh_size / shdr -> sh_entsize ; for ( i = 0 ; i < nrels ; i ++ ) { rel = elf_rel_by_idx ( data , i ) ; if ( ! rel ) { pr_warn ( \"struct_ops reloc: failed to get %d reloc\\n\" , i ) ; return - LIBBPF_ERRNO__FORMAT ; } sym = elf_sym_by_idx ( obj , ELF64_R_SYM ( rel -> r_info ) ) ; if ( ! sym ) { pr_warn ( \"struct_ops reloc: symbol %zx not found\\n\" , ( size_t ) ELF64_R_SYM ( rel -> r_info ) ) ; return - LIBBPF_ERRNO__FORMAT ; } name = elf_sym_str ( obj , sym -> st_name ) ? : \"<?>\" ; map = find_struct_ops_map_by_offset ( obj , shdr -> sh_info , rel -> r_offset ) ; if ( ! map ) { pr_warn ( \"struct_ops reloc: cannot find map at rel->r_offset %zu\\n\" , ( size_t ) rel -> r_offset ) ; return - EINVAL ; } moff = rel -> r_offset - map -> sec_offset ; shdr_idx = sym -> st_shndx ; st_ops = map -> st_ops ; pr_debug ( \"struct_ops reloc %s: for %lld value %lld shdr_idx %u rel->r_offset %zu map->sec_offset %zu name %d (\\'%s\\')\\n\" , map -> name , ( long long ) ( rel -> r_info >> 32 ) , ( long long ) sym -> st_value , shdr_idx , ( size_t ) rel -> r_offset , map -> sec_offset , sym -> st_name , name ) ; if ( shdr_idx >= SHN_LORESERVE ) { pr_warn ( \"struct_ops reloc %s: rel->r_offset %zu shdr_idx %u unsupported non-static function\\n\" , map -> name , ( size_t ) rel -> r_offset , shdr_idx ) ; return - LIBBPF_ERRNO__RELOC ; } if ( sym -> st_value % BPF_INSN_SZ ) { pr_warn ( \"struct_ops reloc %s: invalid target program offset %llu\\n\" , map -> name , ( unsigned long long ) sym -> st_value ) ; return - LIBBPF_ERRNO__FORMAT ; } insn_idx = sym -> st_value / BPF_INSN_SZ ; member = find_member_by_offset ( st_ops -> type , moff * 8 ) ; if ( ! member ) { pr_warn ( \"struct_ops reloc %s: cannot find member at moff %u\\n\" , map -> name , moff ) ; return - EINVAL ; } member_idx = member - btf_members ( st_ops -> type ) ; name = btf__name_by_offset ( btf , member -> name_off ) ; if ( ! resolve_func_ptr ( btf , member -> type , NULL ) ) { pr_warn ( \"struct_ops reloc %s: cannot relocate non func ptr %s\\n\" , map -> name , name ) ; return - EINVAL ; } prog = find_prog_by_sec_insn ( obj , shdr_idx , insn_idx ) ; if ( ! prog ) { pr_warn ( \"struct_ops reloc %s: cannot find prog at shdr_idx %u to relocate func ptr %s\\n\" , map -> name , shdr_idx , name ) ; return - EINVAL ; } /* prevent the use of BPF prog with invalid type */ if ( prog -> type != BPF_PROG_TYPE_STRUCT_OPS ) { pr_warn ( \"struct_ops reloc %s: prog %s is not struct_ops BPF program\\n\" , map -> name , prog -> name ) ; return - EINVAL ; } st_ops -> progs [ member_idx ] = prog ; /* st_ops->data will be exposed to users, being returned by\n\t\t * bpf_map__initial_value() as a pointer to the shadow\n\t\t * type. All function pointers in the original struct type\n\t\t * should be converted to a pointer to struct bpf_program\n\t\t * in the shadow type.\n\t\t */ * ( ( struct bpf_program * * ) ( st_ops -> data + moff ) ) = prog ; } return 0 ; }",
    "resources/libbpf/src/libbpf.c@btf_get_kernel_prefix_kind": "void btf_get_kernel_prefix_kind ( enum bpf_attach_type attach_type , const char * * prefix , int * kind ) { switch ( attach_type ) { case BPF_TRACE_RAW_TP : * prefix = BTF_TRACE_PREFIX ; * kind = BTF_KIND_TYPEDEF ; break ; case BPF_LSM_MAC : case BPF_LSM_CGROUP : * prefix = BTF_LSM_PREFIX ; * kind = BTF_KIND_FUNC ; break ; case BPF_TRACE_ITER : * prefix = BTF_ITER_PREFIX ; * kind = BTF_KIND_FUNC ; break ; default : * prefix = \"\" ; * kind = BTF_KIND_FUNC ; } }",
    "resources/libbpf/src/libbpf.c@find_btf_by_prefix_kind": "static int find_btf_by_prefix_kind ( const struct btf * btf , const char * prefix , const char * name , __u32 kind ) { char btf_type_name [ BTF_MAX_NAME_SIZE ] ; int ret ; ret = snprintf ( btf_type_name , sizeof ( btf_type_name ) , \"%s%s\" , prefix , name ) ; /* snprintf returns the number of characters written excluding the\n\t * terminating null. So, if >= BTF_MAX_NAME_SIZE are written, it\n\t * indicates truncation.\n\t */ if ( ret < 0 || ret >= sizeof ( btf_type_name ) ) return - ENAMETOOLONG ; return btf__find_by_name_kind ( btf , btf_type_name , kind ) ; }",
    "resources/libbpf/src/libbpf.c@find_attach_btf_id": "static inline int find_attach_btf_id ( struct btf * btf , const char * name , enum bpf_attach_type attach_type ) { const char * prefix ; int kind ; btf_get_kernel_prefix_kind ( attach_type , & prefix , & kind ) ; return find_btf_by_prefix_kind ( btf , prefix , name , kind ) ; }",
    "resources/libbpf/src/libbpf.c@libbpf_find_vmlinux_btf_id": "int libbpf_find_vmlinux_btf_id ( const char * name , enum bpf_attach_type attach_type ) { struct btf * btf ; int err ; btf = btf__load_vmlinux_btf ( ) ; err = libbpf_get_error ( btf ) ; if ( err ) { pr_warn ( \"vmlinux BTF is not found\\n\" ) ; return libbpf_err ( err ) ; } err = find_attach_btf_id ( btf , name , attach_type ) ; if ( err <= 0 ) pr_warn ( \"%s is not found in vmlinux BTF\\n\" , name ) ; btf__free ( btf ) ; return libbpf_err ( err ) ; }",
    "resources/libbpf/src/libbpf.c@libbpf_find_prog_btf_id": "static int libbpf_find_prog_btf_id ( const char * name , __u32 attach_prog_fd ) { struct bpf_prog_info info ; __u32 info_len = sizeof ( info ) ; struct btf * btf ; int err ; memset ( & info , 0 , info_len ) ; err = bpf_prog_get_info_by_fd ( attach_prog_fd , & info , & info_len ) ; if ( err ) { pr_warn ( \"failed bpf_prog_get_info_by_fd for FD %d: %d\\n\" , attach_prog_fd , err ) ; return err ; } err = - EINVAL ; if ( ! info . btf_id ) { pr_warn ( \"The target program doesn't have BTF\\n\" ) ; goto out ; } btf = btf__load_from_kernel_by_id ( info . btf_id ) ; err = libbpf_get_error ( btf ) ; if ( err ) { pr_warn ( \"Failed to get BTF %d of the program: %d\\n\" , info . btf_id , err ) ; goto out ; } err = btf__find_by_name_kind ( btf , name , BTF_KIND_FUNC ) ; btf__free ( btf ) ; if ( err <= 0 ) { pr_warn ( \"%s is not found in prog's BTF\\n\" , name ) ; goto out ; } out : return err ; }",
    "resources/libbpf/src/libbpf.c@find_kernel_btf_id": "static int find_kernel_btf_id ( struct bpf_object * obj , const char * attach_name , enum bpf_attach_type attach_type , int * btf_obj_fd , int * btf_type_id ) { int ret , i ; ret = find_attach_btf_id ( obj -> btf_vmlinux , attach_name , attach_type ) ; if ( ret > 0 ) { * btf_obj_fd = 0 ; /* vmlinux BTF */ * btf_type_id = ret ; return 0 ; } if ( ret != - ENOENT ) return ret ; ret = load_module_btfs ( obj ) ; if ( ret ) return ret ; for ( i = 0 ; i < obj -> btf_module_cnt ; i ++ ) { const struct module_btf * mod = & obj -> btf_modules [ i ] ; ret = find_attach_btf_id ( mod -> btf , attach_name , attach_type ) ; if ( ret > 0 ) { * btf_obj_fd = mod -> fd ; * btf_type_id = ret ; return 0 ; } if ( ret == - ENOENT ) continue ; return ret ; } return - ESRCH ; }",
    "resources/libbpf/src/libbpf.c@libbpf_find_attach_btf_id": "static int libbpf_find_attach_btf_id ( struct bpf_program * prog , const char * attach_name , int * btf_obj_fd , int * btf_type_id ) { enum bpf_attach_type attach_type = prog -> expected_attach_type ; __u32 attach_prog_fd = prog -> attach_prog_fd ; int err = 0 ; /* BPF program's BTF ID */ if ( prog -> type == BPF_PROG_TYPE_EXT || attach_prog_fd ) { if ( ! attach_prog_fd ) { pr_warn ( \"prog '%s': attach program FD is not set\\n\" , prog -> name ) ; return - EINVAL ; } err = libbpf_find_prog_btf_id ( attach_name , attach_prog_fd ) ; if ( err < 0 ) { pr_warn ( \"prog '%s': failed to find BPF program (FD %d) BTF ID for '%s': %d\\n\" , prog -> name , attach_prog_fd , attach_name , err ) ; return err ; } * btf_obj_fd = 0 ; * btf_type_id = err ; return 0 ; } /* kernel/module BTF ID */ if ( prog -> obj -> gen_loader ) { bpf_gen__record_attach_target ( prog -> obj -> gen_loader , attach_name , attach_type ) ; * btf_obj_fd = 0 ; * btf_type_id = 1 ; } else { err = find_kernel_btf_id ( prog -> obj , attach_name , attach_type , btf_obj_fd , btf_type_id ) ; } if ( err ) { pr_warn ( \"prog '%s': failed to find kernel BTF type ID of '%s': %d\\n\" , prog -> name , attach_name , err ) ; return err ; } return 0 ; }",
    "resources/libbpf/src/libbpf.c@libbpf_attach_type_by_name": "int libbpf_attach_type_by_name ( const char * name , enum bpf_attach_type * attach_type ) { char * type_names ; const struct bpf_sec_def * sec_def ; if ( ! name ) return libbpf_err ( - EINVAL ) ; sec_def = find_sec_def ( name ) ; if ( ! sec_def ) { pr_debug ( \"failed to guess attach type based on ELF section name '%s'\\n\" , name ) ; type_names = libbpf_get_type_names ( true ) ; if ( type_names != NULL ) { pr_debug ( \"attachable section(type) names are:%s\\n\" , type_names ) ; free ( type_names ) ; } return libbpf_err ( - EINVAL ) ; } if ( sec_def -> prog_prepare_load_fn != libbpf_prepare_prog_load ) return libbpf_err ( - EINVAL ) ; if ( ! ( sec_def -> cookie & SEC_ATTACHABLE ) ) return libbpf_err ( - EINVAL ) ; * attach_type = sec_def -> expected_attach_type ; return 0 ; }",
    "resources/libbpf/src/libbpf.c@bpf_map__fd": "int bpf_map__fd ( const struct bpf_map * map ) { if ( ! map ) return libbpf_err ( - EINVAL ) ; if ( ! map_is_created ( map ) ) return - 1 ; return map -> fd ; }",
    "resources/libbpf/src/libbpf.c@map_uses_real_name": "static bool map_uses_real_name ( const struct bpf_map * map ) { /* Since libbpf started to support custom .data.* and .rodata.* maps,\n\t * their user-visible name differs from kernel-visible name. Users see\n\t * such map's corresponding ELF section name as a map name.\n\t * This check distinguishes .data/.rodata from .data.* and .rodata.*\n\t * maps to know which name has to be returned to the user.\n\t */ if ( map -> libbpf_type == LIBBPF_MAP_DATA && strcmp ( map -> real_name , DATA_SEC ) != 0 ) return true ; if ( map -> libbpf_type == LIBBPF_MAP_RODATA && strcmp ( map -> real_name , RODATA_SEC ) != 0 ) return true ; return false ; }",
    "resources/libbpf/src/libbpf.c@bpf_map__name": "const char * bpf_map__name ( const struct bpf_map * map ) { if ( ! map ) return NULL ; if ( map_uses_real_name ( map ) ) return map -> real_name ; return map -> name ; }",
    "resources/libbpf/src/libbpf.c@bpf_map__type": "enum bpf_map_type bpf_map__type ( const struct bpf_map * map ) { return map -> def . type ; }",
    "resources/libbpf/src/libbpf.c@bpf_map__set_type": "int bpf_map__set_type ( struct bpf_map * map , enum bpf_map_type type ) { if ( map_is_created ( map ) ) return libbpf_err ( - EBUSY ) ; map -> def . type = type ; return 0 ; }",
    "resources/libbpf/src/libbpf.c@bpf_map__map_flags": "__u32 bpf_map__map_flags ( const struct bpf_map * map ) { return map -> def . map_flags ; }",
    "resources/libbpf/src/libbpf.c@bpf_map__set_map_flags": "int bpf_map__set_map_flags ( struct bpf_map * map , __u32 flags ) { if ( map_is_created ( map ) ) return libbpf_err ( - EBUSY ) ; map -> def . map_flags = flags ; return 0 ; }",
    "resources/libbpf/src/libbpf.c@bpf_map__map_extra": "__u64 bpf_map__map_extra ( const struct bpf_map * map ) { return map -> map_extra ; }",
    "resources/libbpf/src/libbpf.c@bpf_map__set_map_extra": "int bpf_map__set_map_extra ( struct bpf_map * map , __u64 map_extra ) { if ( map_is_created ( map ) ) return libbpf_err ( - EBUSY ) ; map -> map_extra = map_extra ; return 0 ; }",
    "resources/libbpf/src/libbpf.c@bpf_map__numa_node": "__u32 bpf_map__numa_node ( const struct bpf_map * map ) { return map -> numa_node ; }",
    "resources/libbpf/src/libbpf.c@bpf_map__set_numa_node": "int bpf_map__set_numa_node ( struct bpf_map * map , __u32 numa_node ) { if ( map_is_created ( map ) ) return libbpf_err ( - EBUSY ) ; map -> numa_node = numa_node ; return 0 ; }",
    "resources/libbpf/src/libbpf.c@bpf_map__key_size": "__u32 bpf_map__key_size ( const struct bpf_map * map ) { return map -> def . key_size ; }",
    "resources/libbpf/src/libbpf.c@bpf_map__set_key_size": "int bpf_map__set_key_size ( struct bpf_map * map , __u32 size ) { if ( map_is_created ( map ) ) return libbpf_err ( - EBUSY ) ; map -> def . key_size = size ; return 0 ; }",
    "resources/libbpf/src/libbpf.c@bpf_map__value_size": "__u32 bpf_map__value_size ( const struct bpf_map * map ) { return map -> def . value_size ; }",
    "resources/libbpf/src/libbpf.c@map_btf_datasec_resize": "static int map_btf_datasec_resize ( struct bpf_map * map , __u32 size ) { struct btf * btf ; struct btf_type * datasec_type , * var_type ; struct btf_var_secinfo * var ; const struct btf_type * array_type ; const struct btf_array * array ; int vlen , element_sz , new_array_id ; __u32 nr_elements ; /* check btf existence */ btf = bpf_object__btf ( map -> obj ) ; if ( ! btf ) return - ENOENT ; /* verify map is datasec */ datasec_type = btf_type_by_id ( btf , bpf_map__btf_value_type_id ( map ) ) ; if ( ! btf_is_datasec ( datasec_type ) ) { pr_warn ( \"map '%s': cannot be resized, map value type is not a datasec\\n\" , bpf_map__name ( map ) ) ; return - EINVAL ; } /* verify datasec has at least one var */ vlen = btf_vlen ( datasec_type ) ; if ( vlen == 0 ) { pr_warn ( \"map '%s': cannot be resized, map value datasec is empty\\n\" , bpf_map__name ( map ) ) ; return - EINVAL ; } /* verify last var in the datasec is an array */ var = & btf_var_secinfos ( datasec_type ) [ vlen - 1 ] ; var_type = btf_type_by_id ( btf , var -> type ) ; array_type = skip_mods_and_typedefs ( btf , var_type -> type , NULL ) ; if ( ! btf_is_array ( array_type ) ) { pr_warn ( \"map '%s': cannot be resized, last var must be an array\\n\" , bpf_map__name ( map ) ) ; return - EINVAL ; } /* verify request size aligns with array */ array = btf_array ( array_type ) ; element_sz = btf__resolve_size ( btf , array -> type ) ; if ( element_sz <= 0 || ( size - var -> offset ) % element_sz != 0 ) { pr_warn ( \"map '%s': cannot be resized, element size (%d) doesn't align with new total size (%u)\\n\" , bpf_map__name ( map ) , element_sz , size ) ; return - EINVAL ; } /* create a new array based on the existing array, but with new length */ nr_elements = ( size - var -> offset ) / element_sz ; new_array_id = btf__add_array ( btf , array -> index_type , array -> type , nr_elements ) ; if ( new_array_id < 0 ) return new_array_id ; /* adding a new btf type invalidates existing pointers to btf objects,\n\t * so refresh pointers before proceeding\n\t */ datasec_type = btf_type_by_id ( btf , map -> btf_value_type_id ) ; var = & btf_var_secinfos ( datasec_type ) [ vlen - 1 ] ; var_type = btf_type_by_id ( btf , var -> type ) ; /* finally update btf info */ datasec_type -> size = size ; var -> size = size - var -> offset ; var_type -> type = new_array_id ; return 0 ; }",
    "resources/libbpf/src/libbpf.c@bpf_map__set_value_size": "int bpf_map__set_value_size ( struct bpf_map * map , __u32 size ) { if ( map -> obj -> loaded || map -> reused ) return libbpf_err ( - EBUSY ) ; if ( map -> mmaped ) { size_t mmap_old_sz , mmap_new_sz ; int err ; if ( map -> def . type != BPF_MAP_TYPE_ARRAY ) return - EOPNOTSUPP ; mmap_old_sz = bpf_map_mmap_sz ( map ) ; mmap_new_sz = array_map_mmap_sz ( size , map -> def . max_entries ) ; err = bpf_map_mmap_resize ( map , mmap_old_sz , mmap_new_sz ) ; if ( err ) { pr_warn ( \"map '%s': failed to resize memory-mapped region: %d\\n\" , bpf_map__name ( map ) , err ) ; return err ; } err = map_btf_datasec_resize ( map , size ) ; if ( err && err != - ENOENT ) { pr_warn ( \"map '%s': failed to adjust resized BTF, clearing BTF key/value info: %d\\n\" , bpf_map__name ( map ) , err ) ; map -> btf_value_type_id = 0 ; map -> btf_key_type_id = 0 ; } } map -> def . value_size = size ; return 0 ; }",
    "resources/libbpf/src/libbpf.c@bpf_map__btf_key_type_id": "__u32 bpf_map__btf_key_type_id ( const struct bpf_map * map ) { return map ? map -> btf_key_type_id : 0 ; }",
    "resources/libbpf/src/libbpf.c@bpf_map__btf_value_type_id": "__u32 bpf_map__btf_value_type_id ( const struct bpf_map * map ) { return map ? map -> btf_value_type_id : 0 ; }",
    "resources/libbpf/src/libbpf.c@bpf_map__set_initial_value": "int bpf_map__set_initial_value ( struct bpf_map * map , const void * data , size_t size ) { size_t actual_sz ; if ( map -> obj -> loaded || map -> reused ) return libbpf_err ( - EBUSY ) ; if ( ! map -> mmaped || map -> libbpf_type == LIBBPF_MAP_KCONFIG ) return libbpf_err ( - EINVAL ) ; if ( map -> def . type == BPF_MAP_TYPE_ARENA ) actual_sz = map -> obj -> arena_data_sz ; else actual_sz = map -> def . value_size ; if ( size != actual_sz ) return libbpf_err ( - EINVAL ) ; memcpy ( map -> mmaped , data , size ) ; return 0 ; }",
    "resources/libbpf/src/libbpf.c@bpf_map__initial_value": "void * bpf_map__initial_value ( const struct bpf_map * map , size_t * psize ) { if ( bpf_map__is_struct_ops ( map ) ) { if ( psize ) * psize = map -> def . value_size ; return map -> st_ops -> data ; } if ( ! map -> mmaped ) return NULL ; if ( map -> def . type == BPF_MAP_TYPE_ARENA ) * psize = map -> obj -> arena_data_sz ; else * psize = map -> def . value_size ; return map -> mmaped ; }",
    "resources/libbpf/src/libbpf.c@bpf_map__is_internal": "",
    "resources/libbpf/src/libbpf.c@bpf_map__ifindex": "__u32 bpf_map__ifindex ( const struct bpf_map * map ) { return map -> map_ifindex ; }",
    "resources/libbpf/src/libbpf.c@bpf_map__set_ifindex": "int bpf_map__set_ifindex ( struct bpf_map * map , __u32 ifindex ) { if ( map_is_created ( map ) ) return libbpf_err ( - EBUSY ) ; map -> map_ifindex = ifindex ; return 0 ; }",
    "resources/libbpf/src/libbpf.c@bpf_map__set_inner_map_fd": "int bpf_map__set_inner_map_fd ( struct bpf_map * map , int fd ) { if ( ! bpf_map_type__is_map_in_map ( map -> def . type ) ) { pr_warn ( \"error: unsupported map type\\n\" ) ; return libbpf_err ( - EINVAL ) ; } if ( map -> inner_map_fd != - 1 ) { pr_warn ( \"error: inner_map_fd already specified\\n\" ) ; return libbpf_err ( - EINVAL ) ; } if ( map -> inner_map ) { bpf_map__destroy ( map -> inner_map ) ; zfree ( & map -> inner_map ) ; } map -> inner_map_fd = fd ; return 0 ; }",
    "resources/libbpf/src/libbpf.c@__bpf_map__iter": "static struct bpf_map * __bpf_map__iter ( const struct bpf_map * m , const struct bpf_object * obj , int i ) { ssize_t idx ; struct bpf_map * s , * e ; if ( ! obj || ! obj -> maps ) return errno = EINVAL , NULL ; s = obj -> maps ; e = obj -> maps + obj -> nr_maps ; if ( ( m < s ) || ( m >= e ) ) { pr_warn ( \"error in %s: map handler doesn't belong to object\\n\" , __func__ ) ; return errno = EINVAL , NULL ; } idx = ( m - obj -> maps ) + i ; if ( idx >= obj -> nr_maps || idx < 0 ) return NULL ; return & obj -> maps [ idx ] ; }",
    "resources/libbpf/src/libbpf.c@bpf_object__next_map": "struct bpf_map * bpf_object__next_map ( const struct bpf_object * obj , const struct bpf_map * prev ) { if ( prev == NULL ) return obj -> maps ; return __bpf_map__iter ( prev , obj , 1 ) ; }",
    "resources/libbpf/src/libbpf.c@bpf_object__prev_map": "struct bpf_map * bpf_object__prev_map ( const struct bpf_object * obj , const struct bpf_map * next ) { if ( next == NULL ) { if ( ! obj -> nr_maps ) return NULL ; return obj -> maps + obj -> nr_maps - 1 ; } return __bpf_map__iter ( next , obj , - 1 ) ; }",
    "resources/libbpf/src/libbpf.c@bpf_object__find_map_by_name": "struct bpf_map * bpf_object__find_map_by_name ( const struct bpf_object * obj , const char * name ) { struct bpf_map * pos ; bpf_object__for_each_map ( pos , obj ) { /* if it's a special internal map name (which always starts\n\t\t * with dot) then check if that special name matches the\n\t\t * real map name (ELF section name)\n\t\t */ if ( name [ 0 ] == '.' ) { if ( pos -> real_name && strcmp ( pos -> real_name , name ) == 0 ) return pos ; continue ; } /* otherwise map name has to be an exact match */ if ( map_uses_real_name ( pos ) ) { if ( strcmp ( pos -> real_name , name ) == 0 ) return pos ; continue ; } if ( strcmp ( pos -> name , name ) == 0 ) return pos ; } return errno = ENOENT , NULL ; }",
    "resources/libbpf/src/libbpf.c@bpf_object__find_map_fd_by_name": "int bpf_object__find_map_fd_by_name ( const struct bpf_object * obj , const char * name ) { return bpf_map__fd ( bpf_object__find_map_by_name ( obj , name ) ) ; }",
    "resources/libbpf/src/libbpf.c@validate_map_op": "static int validate_map_op ( const struct bpf_map * map , size_t key_sz , size_t value_sz , bool check_value_sz ) { if ( ! map_is_created ( map ) ) /* map is not yet created */ return - ENOENT ; if ( map -> def . key_size != key_sz ) { pr_warn ( \"map '%s': unexpected key size %zu provided, expected %u\\n\" , map -> name , key_sz , map -> def . key_size ) ; return - EINVAL ; } if ( map -> fd < 0 ) { pr_warn ( \"map '%s': can't use BPF map without FD (was it created?)\\n\" , map -> name ) ; return - EINVAL ; } if ( ! check_value_sz ) return 0 ; switch ( map -> def . type ) { case BPF_MAP_TYPE_PERCPU_ARRAY : case BPF_MAP_TYPE_PERCPU_HASH : case BPF_MAP_TYPE_LRU_PERCPU_HASH : case BPF_MAP_TYPE_PERCPU_CGROUP_STORAGE : { int num_cpu = libbpf_num_possible_cpus ( ) ; size_t elem_sz = roundup ( map -> def . value_size , 8 ) ; if ( value_sz != num_cpu * elem_sz ) { pr_warn ( \"map '%s': unexpected value size %zu provided for per-CPU map, expected %d * %zu = %zd\\n\" , map -> name , value_sz , num_cpu , elem_sz , num_cpu * elem_sz ) ; return - EINVAL ; } break ; } default : if ( map -> def . value_size != value_sz ) { pr_warn ( \"map '%s': unexpected value size %zu provided, expected %u\\n\" , map -> name , value_sz , map -> def . value_size ) ; return - EINVAL ; } break ; } return 0 ; }",
    "resources/libbpf/src/libbpf.c@bpf_map__lookup_elem": "int bpf_map__lookup_elem ( const struct bpf_map * map , const void * key , size_t key_sz , void * value , size_t value_sz , __u64 flags ) { int err ; err = validate_map_op ( map , key_sz , value_sz , true ) ; if ( err ) return libbpf_err ( err ) ; return bpf_map_lookup_elem_flags ( map -> fd , key , value , flags ) ; }",
    "resources/libbpf/src/libbpf.c@bpf_map__update_elem": "int bpf_map__update_elem ( const struct bpf_map * map , const void * key , size_t key_sz , const void * value , size_t value_sz , __u64 flags ) { int err ; err = validate_map_op ( map , key_sz , value_sz , true ) ; if ( err ) return libbpf_err ( err ) ; return bpf_map_update_elem ( map -> fd , key , value , flags ) ; }",
    "resources/libbpf/src/libbpf.c@bpf_map__delete_elem": "int bpf_map__delete_elem ( const struct bpf_map * map , const void * key , size_t key_sz , __u64 flags ) { int err ; err = validate_map_op ( map , key_sz , 0 , false /* check_value_sz */ ) ; if ( err ) return libbpf_err ( err ) ; return bpf_map_delete_elem_flags ( map -> fd , key , flags ) ; }",
    "resources/libbpf/src/libbpf.c@bpf_map__lookup_and_delete_elem": "int bpf_map__lookup_and_delete_elem ( const struct bpf_map * map , const void * key , size_t key_sz , void * value , size_t value_sz , __u64 flags ) { int err ; err = validate_map_op ( map , key_sz , value_sz , true ) ; if ( err ) return libbpf_err ( err ) ; return bpf_map_lookup_and_delete_elem_flags ( map -> fd , key , value , flags ) ; }",
    "resources/libbpf/src/libbpf.c@bpf_map__get_next_key": "int bpf_map__get_next_key ( const struct bpf_map * map , const void * cur_key , void * next_key , size_t key_sz ) { int err ; err = validate_map_op ( map , key_sz , 0 , false /* check_value_sz */ ) ; if ( err ) return libbpf_err ( err ) ; return bpf_map_get_next_key ( map -> fd , cur_key , next_key ) ; }",
    "resources/libbpf/src/libbpf.c@libbpf_get_error": "long libbpf_get_error ( const void * ptr ) { if ( ! IS_ERR_OR_NULL ( ptr ) ) return 0 ; if ( IS_ERR ( ptr ) ) errno = - PTR_ERR ( ptr ) ; /* If ptr == NULL, then errno should be already set by the failing\n\t * API, because libbpf never returns NULL on success and it now always\n\t * sets errno on error. So no extra errno handling for ptr == NULL\n\t * case.\n\t */ return - errno ; }",
    "resources/libbpf/src/libbpf.c@bpf_link__update_program": "int bpf_link__update_program ( struct bpf_link * link , struct bpf_program * prog ) { int ret ; int prog_fd = bpf_program__fd ( prog ) ; if ( prog_fd < 0 ) { pr_warn ( \"prog '%s': can't use BPF program without FD (was it loaded?)\\n\" , prog -> name ) ; return libbpf_err ( - EINVAL ) ; } ret = bpf_link_update ( bpf_link__fd ( link ) , prog_fd , NULL ) ; return libbpf_err_errno ( ret ) ; }",
    "resources/libbpf/src/libbpf.c@bpf_link__disconnect": "void bpf_link__disconnect ( struct bpf_link * link ) { link -> disconnected = true ; }",
    "resources/libbpf/src/libbpf.c@bpf_link__destroy": "int bpf_link__destroy ( struct bpf_link * link ) { int err = 0 ; if ( IS_ERR_OR_NULL ( link ) ) return 0 ; if ( ! link -> disconnected && link -> detach ) err = link -> detach ( link ) ; if ( link -> pin_path ) free ( link -> pin_path ) ; if ( link -> dealloc ) link -> dealloc ( link ) ; else free ( link ) ; return libbpf_err ( err ) ; }",
    "resources/libbpf/src/libbpf.c@bpf_link__fd": "int bpf_link__fd ( const struct bpf_link * link ) { return link -> fd ; }",
    "resources/libbpf/src/libbpf.c@bpf_link__pin_path": "const char * bpf_link__pin_path ( const struct bpf_link * link ) { return link -> pin_path ; }",
    "resources/libbpf/src/libbpf.c@bpf_link__detach_fd": "static int bpf_link__detach_fd ( struct bpf_link * link ) { return libbpf_err_errno ( close ( link -> fd ) ) ; }",
    "resources/libbpf/src/libbpf.c@bpf_link__open": "struct bpf_link * bpf_link__open ( const char * path ) { struct bpf_link * link ; int fd ; fd = bpf_obj_get ( path ) ; if ( fd < 0 ) { fd = - errno ; pr_warn ( \"failed to open link at %s: %d\\n\" , path , fd ) ; return libbpf_err_ptr ( fd ) ; } link = calloc ( 1 , sizeof ( * link ) ) ; if ( ! link ) { close ( fd ) ; return libbpf_err_ptr ( - ENOMEM ) ; } link -> detach = & bpf_link__detach_fd ; link -> fd = fd ; link -> pin_path = strdup ( path ) ; if ( ! link -> pin_path ) { bpf_link__destroy ( link ) ; return libbpf_err_ptr ( - ENOMEM ) ; } return link ; }",
    "resources/libbpf/src/libbpf.c@bpf_link__detach": "int bpf_link__detach ( struct bpf_link * link ) { return bpf_link_detach ( link -> fd ) ? - errno : 0 ; }",
    "resources/libbpf/src/libbpf.c@bpf_link__pin": "int bpf_link__pin ( struct bpf_link * link , const char * path ) { int err ; if ( link -> pin_path ) return libbpf_err ( - EBUSY ) ; err = make_parent_dir ( path ) ; if ( err ) return libbpf_err ( err ) ; err = check_path ( path ) ; if ( err ) return libbpf_err ( err ) ; link -> pin_path = strdup ( path ) ; if ( ! link -> pin_path ) return libbpf_err ( - ENOMEM ) ; if ( bpf_obj_pin ( link -> fd , link -> pin_path ) ) { err = - errno ; zfree ( & link -> pin_path ) ; return libbpf_err ( err ) ; } pr_debug ( \"link fd=%d: pinned at %s\\n\" , link -> fd , link -> pin_path ) ; return 0 ; }",
    "resources/libbpf/src/libbpf.c@bpf_link__unpin": "int bpf_link__unpin ( struct bpf_link * link ) { int err ; if ( ! link -> pin_path ) return libbpf_err ( - EINVAL ) ; err = unlink ( link -> pin_path ) ; if ( err != 0 ) return - errno ; pr_debug ( \"link fd=%d: unpinned from %s\\n\" , link -> fd , link -> pin_path ) ; zfree ( & link -> pin_path ) ; return 0 ; }",
    "resources/libbpf/src/libbpf.c@bpf_link_perf_detach": "static int bpf_link_perf_detach ( struct bpf_link * link ) { struct bpf_link_perf * perf_link = container_of ( link , struct bpf_link_perf , link ) ; int err = 0 ; if ( ioctl ( perf_link -> perf_event_fd , PERF_EVENT_IOC_DISABLE , 0 ) < 0 ) err = - errno ; if ( perf_link -> perf_event_fd != link -> fd ) close ( perf_link -> perf_event_fd ) ; close ( link -> fd ) ; /* legacy uprobe/kprobe needs to be removed after perf event fd closure */ if ( perf_link -> legacy_probe_name ) { if ( perf_link -> legacy_is_kprobe ) { err = remove_kprobe_event_legacy ( perf_link -> legacy_probe_name , perf_link -> legacy_is_retprobe ) ; } else { err = remove_uprobe_event_legacy ( perf_link -> legacy_probe_name , perf_link -> legacy_is_retprobe ) ; } } return err ; }",
    "resources/libbpf/src/libbpf.c@bpf_link_perf_dealloc": "static void bpf_link_perf_dealloc ( struct bpf_link * link ) { struct bpf_link_perf * perf_link = container_of ( link , struct bpf_link_perf , link ) ; free ( perf_link -> legacy_probe_name ) ; free ( perf_link ) ; }",
    "resources/libbpf/src/libbpf.c@bpf_program__attach_perf_event_opts": "struct bpf_link * bpf_program__attach_perf_event_opts ( const struct bpf_program * prog , int pfd , const struct bpf_perf_event_opts * opts ) { char errmsg [ STRERR_BUFSIZE ] ; struct bpf_link_perf * link ; int prog_fd , link_fd = - 1 , err ; bool force_ioctl_attach ; if ( ! OPTS_VALID ( opts , bpf_perf_event_opts ) ) return libbpf_err_ptr ( - EINVAL ) ; if ( pfd < 0 ) { pr_warn ( \"prog '%s': invalid perf event FD %d\\n\" , prog -> name , pfd ) ; return libbpf_err_ptr ( - EINVAL ) ; } prog_fd = bpf_program__fd ( prog ) ; if ( prog_fd < 0 ) { pr_warn ( \"prog '%s': can't attach BPF program without FD (was it loaded?)\\n\" , prog -> name ) ; return libbpf_err_ptr ( - EINVAL ) ; } link = calloc ( 1 , sizeof ( * link ) ) ; if ( ! link ) return libbpf_err_ptr ( - ENOMEM ) ; link -> link . detach = & bpf_link_perf_detach ; link -> link . dealloc = & bpf_link_perf_dealloc ; link -> perf_event_fd = pfd ; force_ioctl_attach = OPTS_GET ( opts , force_ioctl_attach , false ) ; if ( kernel_supports ( prog -> obj , FEAT_PERF_LINK ) && ! force_ioctl_attach ) { DECLARE_LIBBPF_OPTS ( bpf_link_create_opts , link_opts , . perf_event . bpf_cookie = OPTS_GET ( opts , bpf_cookie , 0 ) ) ; link_fd = bpf_link_create ( prog_fd , pfd , BPF_PERF_EVENT , & link_opts ) ; if ( link_fd < 0 ) { err = - errno ; pr_warn ( \"prog '%s': failed to create BPF link for perf_event FD %d: %d (%s)\\n\" , prog -> name , pfd , err , libbpf_strerror_r ( err , errmsg , sizeof ( errmsg ) ) ) ; goto err_out ; } link -> link . fd = link_fd ; } else { if ( OPTS_GET ( opts , bpf_cookie , 0 ) ) { pr_warn ( \"prog '%s': user context value is not supported\\n\" , prog -> name ) ; err = - EOPNOTSUPP ; goto err_out ; } if ( ioctl ( pfd , PERF_EVENT_IOC_SET_BPF , prog_fd ) < 0 ) { err = - errno ; pr_warn ( \"prog '%s': failed to attach to perf_event FD %d: %s\\n\" , prog -> name , pfd , libbpf_strerror_r ( err , errmsg , sizeof ( errmsg ) ) ) ; if ( err == - EPROTO ) pr_warn ( \"prog '%s': try add PERF_SAMPLE_CALLCHAIN to or remove exclude_callchain_[kernel|user] from pfd %d\\n\" , prog -> name , pfd ) ; goto err_out ; } link -> link . fd = pfd ; } if ( ioctl ( pfd , PERF_EVENT_IOC_ENABLE , 0 ) < 0 ) { err = - errno ; pr_warn ( \"prog '%s': failed to enable perf_event FD %d: %s\\n\" , prog -> name , pfd , libbpf_strerror_r ( err , errmsg , sizeof ( errmsg ) ) ) ; goto err_out ; } return & link -> link ; err_out : if ( link_fd >= 0 ) close ( link_fd ) ; free ( link ) ; return libbpf_err_ptr ( err ) ; }",
    "resources/libbpf/src/libbpf.c@bpf_program__attach_perf_event": "struct bpf_link * bpf_program__attach_perf_event ( const struct bpf_program * prog , int pfd ) { return bpf_program__attach_perf_event_opts ( prog , pfd , NULL ) ; }",
    "resources/libbpf/src/libbpf.c@parse_uint_from_file": "static int parse_uint_from_file ( const char * file , const char * fmt ) { char buf [ STRERR_BUFSIZE ] ; int err , ret ; FILE * f ; f = fopen ( file , \"re\" ) ; if ( ! f ) { err = - errno ; pr_debug ( \"failed to open '%s': %s\\n\" , file , libbpf_strerror_r ( err , buf , sizeof ( buf ) ) ) ; return err ; } err = fscanf ( f , fmt , & ret ) ; if ( err != 1 ) { err = err == EOF ? - EIO : - errno ; pr_debug ( \"failed to parse '%s': %s\\n\" , file , libbpf_strerror_r ( err , buf , sizeof ( buf ) ) ) ; fclose ( f ) ; return err ; } fclose ( f ) ; return ret ; }",
    "resources/libbpf/src/libbpf.c@determine_kprobe_perf_type": "static int determine_kprobe_perf_type ( void ) { const char * file = \"/sys/bus/event_source/devices/kprobe/type\" ; return parse_uint_from_file ( file , \"%d\\n\" ) ; }",
    "resources/libbpf/src/libbpf.c@determine_uprobe_perf_type": "static int determine_uprobe_perf_type ( void ) { const char * file = \"/sys/bus/event_source/devices/uprobe/type\" ; return parse_uint_from_file ( file , \"%d\\n\" ) ; }",
    "resources/libbpf/src/libbpf.c@determine_kprobe_retprobe_bit": "static int determine_kprobe_retprobe_bit ( void ) { const char * file = \"/sys/bus/event_source/devices/kprobe/format/retprobe\" ; return parse_uint_from_file ( file , \"config:%d\\n\" ) ; }",
    "resources/libbpf/src/libbpf.c@determine_uprobe_retprobe_bit": "static int determine_uprobe_retprobe_bit ( void ) { const char * file = \"/sys/bus/event_source/devices/uprobe/format/retprobe\" ; return parse_uint_from_file ( file , \"config:%d\\n\" ) ; }",
    "resources/libbpf/src/libbpf.c@perf_event_open_probe": "static int perf_event_open_probe ( bool uprobe , bool retprobe , const char * name , uint64_t offset , int pid , size_t ref_ctr_off ) { const size_t attr_sz = sizeof ( struct perf_event_attr ) ; struct perf_event_attr attr ; char errmsg [ STRERR_BUFSIZE ] ; int type , pfd ; if ( ( __u64 ) ref_ctr_off >= ( 1ULL << PERF_UPROBE_REF_CTR_OFFSET_BITS ) ) return - EINVAL ; memset ( & attr , 0 , attr_sz ) ; type = uprobe ? determine_uprobe_perf_type ( ) : determine_kprobe_perf_type ( ) ; if ( type < 0 ) { pr_warn ( \"failed to determine %s perf type: %s\\n\" , uprobe ? \"uprobe\" : \"kprobe\" , libbpf_strerror_r ( type , errmsg , sizeof ( errmsg ) ) ) ; return type ; } if ( retprobe ) { int bit = uprobe ? determine_uprobe_retprobe_bit ( ) : determine_kprobe_retprobe_bit ( ) ; if ( bit < 0 ) { pr_warn ( \"failed to determine %s retprobe bit: %s\\n\" , uprobe ? \"uprobe\" : \"kprobe\" , libbpf_strerror_r ( bit , errmsg , sizeof ( errmsg ) ) ) ; return bit ; } attr . config |= 1 << bit ; } attr . size = attr_sz ; attr . type = type ; attr . config |= ( __u64 ) ref_ctr_off << PERF_UPROBE_REF_CTR_OFFSET_SHIFT ; attr . config1 = ptr_to_u64 ( name ) ; /* kprobe_func or uprobe_path */ attr . config2 = offset ; /* kprobe_addr or probe_offset */ /* pid filter is meaningful only for uprobes */ pfd = syscall ( __NR_perf_event_open , & attr , pid < 0 ? - 1 : pid /* pid */ , pid == - 1 ? 0 : - 1 /* cpu */ , - 1 /* group_fd */ , PERF_FLAG_FD_CLOEXEC ) ; return pfd >= 0 ? pfd : - errno ; }",
    "resources/libbpf/src/libbpf.c@append_to_file": "static int append_to_file ( const char * file , const char * fmt , ... ) { int fd , n , err = 0 ; va_list ap ; char buf [ 1024 ] ; va_start ( ap , fmt ) ; n = vsnprintf ( buf , sizeof ( buf ) , fmt , ap ) ; va_end ( ap ) ; if ( n < 0 || n >= sizeof ( buf ) ) return - EINVAL ; fd = open ( file , O_WRONLY | O_APPEND | O_CLOEXEC , 0 ) ; if ( fd < 0 ) return - errno ; if ( write ( fd , buf , n ) < 0 ) err = - errno ; close ( fd ) ; return err ; }",
    "resources/libbpf/src/libbpf.c@use_debugfs": "static bool use_debugfs ( void ) { static int has_debugfs = - 1 ; if ( has_debugfs < 0 ) has_debugfs = faccessat ( AT_FDCWD , DEBUGFS , F_OK , AT_EACCESS ) == 0 ; return has_debugfs == 1 ; }",
    "resources/libbpf/src/libbpf.c@tracefs_path": "static const char * tracefs_path ( void ) { return use_debugfs ( ) ? DEBUGFS : TRACEFS ; }",
    "resources/libbpf/src/libbpf.c@tracefs_kprobe_events": "static const char * tracefs_kprobe_events ( void ) { return use_debugfs ( ) ? DEBUGFS \"/kprobe_events\" : TRACEFS \"/kprobe_events\" ; }",
    "resources/libbpf/src/libbpf.c@tracefs_uprobe_events": "static const char * tracefs_uprobe_events ( void ) { return use_debugfs ( ) ? DEBUGFS \"/uprobe_events\" : TRACEFS \"/uprobe_events\" ; }",
    "resources/libbpf/src/libbpf.c@tracefs_available_filter_functions": "static const char * tracefs_available_filter_functions ( void ) { return use_debugfs ( ) ? DEBUGFS \"/available_filter_functions\" : TRACEFS \"/available_filter_functions\" ; }",
    "resources/libbpf/src/libbpf.c@tracefs_available_filter_functions_addrs": "static const char * tracefs_available_filter_functions_addrs ( void ) { return use_debugfs ( ) ? DEBUGFS \"/available_filter_functions_addrs\" : TRACEFS \"/available_filter_functions_addrs\" ; }",
    "resources/libbpf/src/libbpf.c@gen_kprobe_legacy_event_name": "static void gen_kprobe_legacy_event_name ( char * buf , size_t buf_sz , const char * kfunc_name , size_t offset ) { static int index = 0 ; int i ; snprintf ( buf , buf_sz , \"libbpf_%u_%s_0x%zx_%d\" , getpid ( ) , kfunc_name , offset , __sync_fetch_and_add ( & index , 1 ) ) ; /* sanitize binary_path in the probe name */ for ( i = 0 ; buf [ i ] ; i ++ ) { if ( ! isalnum ( buf [ i ] ) ) buf [ i ] = '_' ; } }",
    "resources/libbpf/src/libbpf.c@add_kprobe_event_legacy": "static int add_kprobe_event_legacy ( const char * probe_name , bool retprobe , const char * kfunc_name , size_t offset ) { return append_to_file ( tracefs_kprobe_events ( ) , \"%c:%s/%s %s+0x%zx\" , retprobe ? 'r' : 'p' , retprobe ? \"kretprobes\" : \"kprobes\" , probe_name , kfunc_name , offset ) ; }",
    "resources/libbpf/src/libbpf.c@remove_kprobe_event_legacy": "static int remove_kprobe_event_legacy ( const char * probe_name , bool retprobe ) { return append_to_file ( tracefs_kprobe_events ( ) , \"-:%s/%s\" , retprobe ? \"kretprobes\" : \"kprobes\" , probe_name ) ; }",
    "resources/libbpf/src/libbpf.c@determine_kprobe_perf_type_legacy": "static int determine_kprobe_perf_type_legacy ( const char * probe_name , bool retprobe ) { char file [ 256 ] ; snprintf ( file , sizeof ( file ) , \"%s/events/%s/%s/id\" , tracefs_path ( ) , retprobe ? \"kretprobes\" : \"kprobes\" , probe_name ) ; return parse_uint_from_file ( file , \"%d\\n\" ) ; }",
    "resources/libbpf/src/libbpf.c@perf_event_kprobe_open_legacy": "static int perf_event_kprobe_open_legacy ( const char * probe_name , bool retprobe , const char * kfunc_name , size_t offset , int pid ) { const size_t attr_sz = sizeof ( struct perf_event_attr ) ; struct perf_event_attr attr ; char errmsg [ STRERR_BUFSIZE ] ; int type , pfd , err ; err = add_kprobe_event_legacy ( probe_name , retprobe , kfunc_name , offset ) ; if ( err < 0 ) { pr_warn ( \"failed to add legacy kprobe event for '%s+0x%zx': %s\\n\" , kfunc_name , offset , libbpf_strerror_r ( err , errmsg , sizeof ( errmsg ) ) ) ; return err ; } type = determine_kprobe_perf_type_legacy ( probe_name , retprobe ) ; if ( type < 0 ) { err = type ; pr_warn ( \"failed to determine legacy kprobe event id for '%s+0x%zx': %s\\n\" , kfunc_name , offset , libbpf_strerror_r ( err , errmsg , sizeof ( errmsg ) ) ) ; goto err_clean_legacy ; } memset ( & attr , 0 , attr_sz ) ; attr . size = attr_sz ; attr . config = type ; attr . type = PERF_TYPE_TRACEPOINT ; pfd = syscall ( __NR_perf_event_open , & attr , pid < 0 ? - 1 : pid , /* pid */ pid == - 1 ? 0 : - 1 , /* cpu */ - 1 /* group_fd */ , PERF_FLAG_FD_CLOEXEC ) ; if ( pfd < 0 ) { err = - errno ; pr_warn ( \"legacy kprobe perf_event_open() failed: %s\\n\" , libbpf_strerror_r ( err , errmsg , sizeof ( errmsg ) ) ) ; goto err_clean_legacy ; } return pfd ; err_clean_legacy : /* Clear the newly added legacy kprobe_event */ remove_kprobe_event_legacy ( probe_name , retprobe ) ; return err ; }",
    "resources/libbpf/src/libbpf.c@arch_specific_syscall_pfx": "static const char * arch_specific_syscall_pfx ( void ) { # if defined ( __x86_64__ ) return \"x64\" ; # elif defined ( __i386__ ) return \"ia32\" ; # elif defined ( __s390x__ ) return \"s390x\" ; # elif defined ( __s390__ ) return \"s390\" ; # elif defined ( __arm__ ) return \"arm\" ; # elif defined ( __aarch64__ ) return \"arm64\" ; # elif defined ( __mips__ ) return \"mips\" ; # elif defined ( __riscv ) return \"riscv\" ; # elif defined ( __powerpc__ ) return \"powerpc\" ; # elif defined ( __powerpc64__ ) return \"powerpc64\" ; # else return NULL ; # endif }",
    "resources/libbpf/src/libbpf.c@probe_kern_syscall_wrapper": "int probe_kern_syscall_wrapper ( int token_fd ) { char syscall_name [ 64 ] ; const char * ksys_pfx ; ksys_pfx = arch_specific_syscall_pfx ( ) ; if ( ! ksys_pfx ) return 0 ; snprintf ( syscall_name , sizeof ( syscall_name ) , \"__%s_sys_bpf\" , ksys_pfx ) ; if ( determine_kprobe_perf_type ( ) >= 0 ) { int pfd ; pfd = perf_event_open_probe ( false , false , syscall_name , 0 , getpid ( ) , 0 ) ; if ( pfd >= 0 ) close ( pfd ) ; return pfd >= 0 ? 1 : 0 ; } else { /* legacy mode */ char probe_name [ 128 ] ; gen_kprobe_legacy_event_name ( probe_name , sizeof ( probe_name ) , syscall_name , 0 ) ; if ( add_kprobe_event_legacy ( probe_name , false , syscall_name , 0 ) < 0 ) return 0 ; ( void ) remove_kprobe_event_legacy ( probe_name , false ) ; return 1 ; } }",
    "resources/libbpf/src/libbpf.c@bpf_program__attach_kprobe_opts": "struct bpf_link * bpf_program__attach_kprobe_opts ( const struct bpf_program * prog , const char * func_name , const struct bpf_kprobe_opts * opts ) { DECLARE_LIBBPF_OPTS ( bpf_perf_event_opts , pe_opts ) ; enum probe_attach_mode attach_mode ; char errmsg [ STRERR_BUFSIZE ] ; char * legacy_probe = NULL ; struct bpf_link * link ; size_t offset ; bool retprobe , legacy ; int pfd , err ; if ( ! OPTS_VALID ( opts , bpf_kprobe_opts ) ) return libbpf_err_ptr ( - EINVAL ) ; attach_mode = OPTS_GET ( opts , attach_mode , PROBE_ATTACH_MODE_DEFAULT ) ; retprobe = OPTS_GET ( opts , retprobe , false ) ; offset = OPTS_GET ( opts , offset , 0 ) ; pe_opts . bpf_cookie = OPTS_GET ( opts , bpf_cookie , 0 ) ; legacy = determine_kprobe_perf_type ( ) < 0 ; switch ( attach_mode ) { case PROBE_ATTACH_MODE_LEGACY : legacy = true ; pe_opts . force_ioctl_attach = true ; break ; case PROBE_ATTACH_MODE_PERF : if ( legacy ) return libbpf_err_ptr ( - ENOTSUP ) ; pe_opts . force_ioctl_attach = true ; break ; case PROBE_ATTACH_MODE_LINK : if ( legacy || ! kernel_supports ( prog -> obj , FEAT_PERF_LINK ) ) return libbpf_err_ptr ( - ENOTSUP ) ; break ; case PROBE_ATTACH_MODE_DEFAULT : break ; default : return libbpf_err_ptr ( - EINVAL ) ; } if ( ! legacy ) { pfd = perf_event_open_probe ( false /* uprobe */ , retprobe , func_name , offset , - 1 /* pid */ , 0 /* ref_ctr_off */ ) ; } else { char probe_name [ 256 ] ; gen_kprobe_legacy_event_name ( probe_name , sizeof ( probe_name ) , func_name , offset ) ; legacy_probe = strdup ( probe_name ) ; if ( ! legacy_probe ) return libbpf_err_ptr ( - ENOMEM ) ; pfd = perf_event_kprobe_open_legacy ( legacy_probe , retprobe , func_name , offset , - 1 /* pid */ ) ; } if ( pfd < 0 ) { err = - errno ; pr_warn ( \"prog '%s': failed to create %s '%s+0x%zx' perf event: %s\\n\" , prog -> name , retprobe ? \"kretprobe\" : \"kprobe\" , func_name , offset , libbpf_strerror_r ( err , errmsg , sizeof ( errmsg ) ) ) ; goto err_out ; } link = bpf_program__attach_perf_event_opts ( prog , pfd , & pe_opts ) ; err = libbpf_get_error ( link ) ; if ( err ) { close ( pfd ) ; pr_warn ( \"prog '%s': failed to attach to %s '%s+0x%zx': %s\\n\" , prog -> name , retprobe ? \"kretprobe\" : \"kprobe\" , func_name , offset , libbpf_strerror_r ( err , errmsg , sizeof ( errmsg ) ) ) ; goto err_clean_legacy ; } if ( legacy ) { struct bpf_link_perf * perf_link = container_of ( link , struct bpf_link_perf , link ) ; perf_link -> legacy_probe_name = legacy_probe ; perf_link -> legacy_is_kprobe = true ; perf_link -> legacy_is_retprobe = retprobe ; } return link ; err_clean_legacy : if ( legacy ) remove_kprobe_event_legacy ( legacy_probe , retprobe ) ; err_out : free ( legacy_probe ) ; return libbpf_err_ptr ( err ) ; }",
    "resources/libbpf/src/libbpf.c@bpf_program__attach_kprobe": "struct bpf_link * bpf_program__attach_kprobe ( const struct bpf_program * prog , bool retprobe , const char * func_name ) { DECLARE_LIBBPF_OPTS ( bpf_kprobe_opts , opts , . retprobe = retprobe , ) ; return bpf_program__attach_kprobe_opts ( prog , func_name , & opts ) ; }",
    "resources/libbpf/src/libbpf.c@bpf_program__attach_ksyscall": "struct bpf_link * bpf_program__attach_ksyscall ( const struct bpf_program * prog , const char * syscall_name , const struct bpf_ksyscall_opts * opts ) { LIBBPF_OPTS ( bpf_kprobe_opts , kprobe_opts ) ; char func_name [ 128 ] ; if ( ! OPTS_VALID ( opts , bpf_ksyscall_opts ) ) return libbpf_err_ptr ( - EINVAL ) ; if ( kernel_supports ( prog -> obj , FEAT_SYSCALL_WRAPPER ) ) { /* arch_specific_syscall_pfx() should never return NULL here\n\t\t * because it is guarded by kernel_supports(). However, since\n\t\t * compiler does not know that we have an explicit conditional\n\t\t * as well.\n\t\t */ snprintf ( func_name , sizeof ( func_name ) , \"__%s_sys_%s\" , arch_specific_syscall_pfx ( ) ? : \"\" , syscall_name ) ; } else { snprintf ( func_name , sizeof ( func_name ) , \"__se_sys_%s\" , syscall_name ) ; } kprobe_opts . retprobe = OPTS_GET ( opts , retprobe , false ) ; kprobe_opts . bpf_cookie = OPTS_GET ( opts , bpf_cookie , 0 ) ; return bpf_program__attach_kprobe_opts ( prog , func_name , & kprobe_opts ) ; }",
    "resources/libbpf/src/libbpf.c@glob_match": "",
    "resources/libbpf/src/libbpf.c@avail_func_cmp": "static int avail_func_cmp ( const void * a , const void * b ) { return strcmp ( * ( const char * * ) a , * ( const char * * ) b ) ; }",
    "resources/libbpf/src/libbpf.c@avail_kallsyms_cb": "static int avail_kallsyms_cb ( unsigned long long sym_addr , char sym_type , const char * sym_name , void * ctx ) { struct avail_kallsyms_data * data = ctx ; struct kprobe_multi_resolve * res = data -> res ; int err ; if ( ! bsearch ( & sym_name , data -> syms , data -> cnt , sizeof ( * data -> syms ) , avail_func_cmp ) ) return 0 ; err = libbpf_ensure_mem ( ( void * * ) & res -> addrs , & res -> cap , sizeof ( * res -> addrs ) , res -> cnt + 1 ) ; if ( err ) return err ; res -> addrs [ res -> cnt ++ ] = ( unsigned long ) sym_addr ; return 0 ; }",
    "resources/libbpf/src/libbpf.c@libbpf_available_kallsyms_parse": "static int libbpf_available_kallsyms_parse ( struct kprobe_multi_resolve * res ) { const char * available_functions_file = tracefs_available_filter_functions ( ) ; struct avail_kallsyms_data data ; char sym_name [ 500 ] ; FILE * f ; int err = 0 , ret , i ; char * * syms = NULL ; size_t cap = 0 , cnt = 0 ; f = fopen ( available_functions_file , \"re\" ) ; if ( ! f ) { err = - errno ; pr_warn ( \"failed to open %s: %d\\n\" , available_functions_file , err ) ; return err ; } while ( true ) { char * name ; ret = fscanf ( f , \"%499s%*[^\\n]\\n\" , sym_name ) ; if ( ret == EOF && feof ( f ) ) break ; if ( ret != 1 ) { pr_warn ( \"failed to parse available_filter_functions entry: %d\\n\" , ret ) ; err = - EINVAL ; goto cleanup ; } if ( ! glob_match ( sym_name , res -> pattern ) ) continue ; err = libbpf_ensure_mem ( ( void * * ) & syms , & cap , sizeof ( * syms ) , cnt + 1 ) ; if ( err ) goto cleanup ; name = strdup ( sym_name ) ; if ( ! name ) { err = - errno ; goto cleanup ; } syms [ cnt ++ ] = name ; } /* no entries found, bail out */ if ( cnt == 0 ) { err = - ENOENT ; goto cleanup ; } /* sort available functions */ qsort ( syms , cnt , sizeof ( * syms ) , avail_func_cmp ) ; data . syms = syms ; data . res = res ; data . cnt = cnt ; libbpf_kallsyms_parse ( avail_kallsyms_cb , & data ) ; if ( res -> cnt == 0 ) err = - ENOENT ; cleanup : for ( i = 0 ; i < cnt ; i ++ ) free ( ( char * ) syms [ i ] ) ; free ( syms ) ; fclose ( f ) ; return err ; }",
    "resources/libbpf/src/libbpf.c@has_available_filter_functions_addrs": "static bool has_available_filter_functions_addrs ( void ) { return access ( tracefs_available_filter_functions_addrs ( ) , R_OK ) != - 1 ; }",
    "resources/libbpf/src/libbpf.c@libbpf_available_kprobes_parse": "static int libbpf_available_kprobes_parse ( struct kprobe_multi_resolve * res ) { const char * available_path = tracefs_available_filter_functions_addrs ( ) ; char sym_name [ 500 ] ; FILE * f ; int ret , err = 0 ; unsigned long long sym_addr ; f = fopen ( available_path , \"re\" ) ; if ( ! f ) { err = - errno ; pr_warn ( \"failed to open %s: %d\\n\" , available_path , err ) ; return err ; } while ( true ) { ret = fscanf ( f , \"%llx %499s%*[^\\n]\\n\" , & sym_addr , sym_name ) ; if ( ret == EOF && feof ( f ) ) break ; if ( ret != 2 ) { pr_warn ( \"failed to parse available_filter_functions_addrs entry: %d\\n\" , ret ) ; err = - EINVAL ; goto cleanup ; } if ( ! glob_match ( sym_name , res -> pattern ) ) continue ; err = libbpf_ensure_mem ( ( void * * ) & res -> addrs , & res -> cap , sizeof ( * res -> addrs ) , res -> cnt + 1 ) ; if ( err ) goto cleanup ; res -> addrs [ res -> cnt ++ ] = ( unsigned long ) sym_addr ; } if ( res -> cnt == 0 ) err = - ENOENT ; cleanup : fclose ( f ) ; return err ; }",
    "resources/libbpf/src/libbpf.c@bpf_program__attach_kprobe_multi_opts": "struct bpf_link * bpf_program__attach_kprobe_multi_opts ( const struct bpf_program * prog , const char * pattern , const struct bpf_kprobe_multi_opts * opts ) { LIBBPF_OPTS ( bpf_link_create_opts , lopts ) ; struct kprobe_multi_resolve res = { . pattern = pattern , } ; struct bpf_link * link = NULL ; char errmsg [ STRERR_BUFSIZE ] ; const unsigned long * addrs ; int err , link_fd , prog_fd ; const __u64 * cookies ; const char * * syms ; bool retprobe ; size_t cnt ; if ( ! OPTS_VALID ( opts , bpf_kprobe_multi_opts ) ) return libbpf_err_ptr ( - EINVAL ) ; prog_fd = bpf_program__fd ( prog ) ; if ( prog_fd < 0 ) { pr_warn ( \"prog '%s': can't attach BPF program without FD (was it loaded?)\\n\" , prog -> name ) ; return libbpf_err_ptr ( - EINVAL ) ; } syms = OPTS_GET ( opts , syms , false ) ; addrs = OPTS_GET ( opts , addrs , false ) ; cnt = OPTS_GET ( opts , cnt , false ) ; cookies = OPTS_GET ( opts , cookies , false ) ; if ( ! pattern && ! addrs && ! syms ) return libbpf_err_ptr ( - EINVAL ) ; if ( pattern && ( addrs || syms || cookies || cnt ) ) return libbpf_err_ptr ( - EINVAL ) ; if ( ! pattern && ! cnt ) return libbpf_err_ptr ( - EINVAL ) ; if ( addrs && syms ) return libbpf_err_ptr ( - EINVAL ) ; if ( pattern ) { if ( has_available_filter_functions_addrs ( ) ) err = libbpf_available_kprobes_parse ( & res ) ; else err = libbpf_available_kallsyms_parse ( & res ) ; if ( err ) goto error ; addrs = res . addrs ; cnt = res . cnt ; } retprobe = OPTS_GET ( opts , retprobe , false ) ; lopts . kprobe_multi . syms = syms ; lopts . kprobe_multi . addrs = addrs ; lopts . kprobe_multi . cookies = cookies ; lopts . kprobe_multi . cnt = cnt ; lopts . kprobe_multi . flags = retprobe ? BPF_F_KPROBE_MULTI_RETURN : 0 ; link = calloc ( 1 , sizeof ( * link ) ) ; if ( ! link ) { err = - ENOMEM ; goto error ; } link -> detach = & bpf_link__detach_fd ; link_fd = bpf_link_create ( prog_fd , 0 , BPF_TRACE_KPROBE_MULTI , & lopts ) ; if ( link_fd < 0 ) { err = - errno ; pr_warn ( \"prog '%s': failed to attach: %s\\n\" , prog -> name , libbpf_strerror_r ( err , errmsg , sizeof ( errmsg ) ) ) ; goto error ; } link -> fd = link_fd ; free ( res . addrs ) ; return link ; error : free ( link ) ; free ( res . addrs ) ; return libbpf_err_ptr ( err ) ; }",
    "resources/libbpf/src/libbpf.c@attach_kprobe": "static int attach_kprobe ( const struct bpf_program * prog , long cookie , struct bpf_link * * link ) { DECLARE_LIBBPF_OPTS ( bpf_kprobe_opts , opts ) ; unsigned long offset = 0 ; const char * func_name ; char * func ; int n ; * link = NULL ; /* no auto-attach for SEC(\"kprobe\") and SEC(\"kretprobe\") */ if ( strcmp ( prog -> sec_name , \"kprobe\" ) == 0 || strcmp ( prog -> sec_name , \"kretprobe\" ) == 0 ) return 0 ; opts . retprobe = str_has_pfx ( prog -> sec_name , \"kretprobe/\" ) ; if ( opts . retprobe ) func_name = prog -> sec_name + sizeof ( \"kretprobe/\" ) - 1 ; else func_name = prog -> sec_name + sizeof ( \"kprobe/\" ) - 1 ; n = sscanf ( func_name , \"%m[a-zA-Z0-9_.]+%li\" , & func , & offset ) ; if ( n < 1 ) { pr_warn ( \"kprobe name is invalid: %s\\n\" , func_name ) ; return - EINVAL ; } if ( opts . retprobe && offset != 0 ) { free ( func ) ; pr_warn ( \"kretprobes do not support offset specification\\n\" ) ; return - EINVAL ; } opts . offset = offset ; * link = bpf_program__attach_kprobe_opts ( prog , func , & opts ) ; free ( func ) ; return libbpf_get_error ( * link ) ; }",
    "resources/libbpf/src/libbpf.c@attach_ksyscall": "static int attach_ksyscall ( const struct bpf_program * prog , long cookie , struct bpf_link * * link ) { LIBBPF_OPTS ( bpf_ksyscall_opts , opts ) ; const char * syscall_name ; * link = NULL ; /* no auto-attach for SEC(\"ksyscall\") and SEC(\"kretsyscall\") */ if ( strcmp ( prog -> sec_name , \"ksyscall\" ) == 0 || strcmp ( prog -> sec_name , \"kretsyscall\" ) == 0 ) return 0 ; opts . retprobe = str_has_pfx ( prog -> sec_name , \"kretsyscall/\" ) ; if ( opts . retprobe ) syscall_name = prog -> sec_name + sizeof ( \"kretsyscall/\" ) - 1 ; else syscall_name = prog -> sec_name + sizeof ( \"ksyscall/\" ) - 1 ; * link = bpf_program__attach_ksyscall ( prog , syscall_name , & opts ) ; return * link ? 0 : - errno ; }",
    "resources/libbpf/src/libbpf.c@attach_kprobe_multi": "static int attach_kprobe_multi ( const struct bpf_program * prog , long cookie , struct bpf_link * * link ) { LIBBPF_OPTS ( bpf_kprobe_multi_opts , opts ) ; const char * spec ; char * pattern ; int n ; * link = NULL ; /* no auto-attach for SEC(\"kprobe.multi\") and SEC(\"kretprobe.multi\") */ if ( strcmp ( prog -> sec_name , \"kprobe.multi\" ) == 0 || strcmp ( prog -> sec_name , \"kretprobe.multi\" ) == 0 ) return 0 ; opts . retprobe = str_has_pfx ( prog -> sec_name , \"kretprobe.multi/\" ) ; if ( opts . retprobe ) spec = prog -> sec_name + sizeof ( \"kretprobe.multi/\" ) - 1 ; else spec = prog -> sec_name + sizeof ( \"kprobe.multi/\" ) - 1 ; n = sscanf ( spec , \"%m[a-zA-Z0-9_.*?]\" , & pattern ) ; if ( n < 1 ) { pr_warn ( \"kprobe multi pattern is invalid: %s\\n\" , pattern ) ; return - EINVAL ; } * link = bpf_program__attach_kprobe_multi_opts ( prog , pattern , & opts ) ; free ( pattern ) ; return libbpf_get_error ( * link ) ; }",
    "resources/libbpf/src/libbpf.c@attach_uprobe_multi": "static int attach_uprobe_multi ( const struct bpf_program * prog , long cookie , struct bpf_link * * link ) { char * probe_type = NULL , * binary_path = NULL , * func_name = NULL ; LIBBPF_OPTS ( bpf_uprobe_multi_opts , opts ) ; int n , ret = - EINVAL ; * link = NULL ; n = sscanf ( prog -> sec_name , \"%m[^/]/%m[^:]:%m[^\\n]\" , & probe_type , & binary_path , & func_name ) ; switch ( n ) { case 1 : /* handle SEC(\"u[ret]probe\") - format is valid, but auto-attach is impossible. */ ret = 0 ; break ; case 3 : opts . retprobe = strcmp ( probe_type , \"uretprobe.multi\" ) == 0 ; * link = bpf_program__attach_uprobe_multi ( prog , - 1 , binary_path , func_name , & opts ) ; ret = libbpf_get_error ( * link ) ; break ; default : pr_warn ( \"prog '%s': invalid format of section definition '%s'\\n\" , prog -> name , prog -> sec_name ) ; break ; } free ( probe_type ) ; free ( binary_path ) ; free ( func_name ) ; return ret ; }",
    "resources/libbpf/src/libbpf.c@gen_uprobe_legacy_event_name": "static void gen_uprobe_legacy_event_name ( char * buf , size_t buf_sz , const char * binary_path , uint64_t offset ) { int i ; snprintf ( buf , buf_sz , \"libbpf_%u_%s_0x%zx\" , getpid ( ) , binary_path , ( size_t ) offset ) ; /* sanitize binary_path in the probe name */ for ( i = 0 ; buf [ i ] ; i ++ ) { if ( ! isalnum ( buf [ i ] ) ) buf [ i ] = '_' ; } }",
    "resources/libbpf/src/libbpf.c@add_uprobe_event_legacy": "static inline int add_uprobe_event_legacy ( const char * probe_name , bool retprobe , const char * binary_path , size_t offset ) { return append_to_file ( tracefs_uprobe_events ( ) , \"%c:%s/%s %s:0x%zx\" , retprobe ? 'r' : 'p' , retprobe ? \"uretprobes\" : \"uprobes\" , probe_name , binary_path , offset ) ; }",
    "resources/libbpf/src/libbpf.c@remove_uprobe_event_legacy": "static inline int remove_uprobe_event_legacy ( const char * probe_name , bool retprobe ) { return append_to_file ( tracefs_uprobe_events ( ) , \"-:%s/%s\" , retprobe ? \"uretprobes\" : \"uprobes\" , probe_name ) ; }",
    "resources/libbpf/src/libbpf.c@determine_uprobe_perf_type_legacy": "static int determine_uprobe_perf_type_legacy ( const char * probe_name , bool retprobe ) { char file [ 512 ] ; snprintf ( file , sizeof ( file ) , \"%s/events/%s/%s/id\" , tracefs_path ( ) , retprobe ? \"uretprobes\" : \"uprobes\" , probe_name ) ; return parse_uint_from_file ( file , \"%d\\n\" ) ; }",
    "resources/libbpf/src/libbpf.c@perf_event_uprobe_open_legacy": "static int perf_event_uprobe_open_legacy ( const char * probe_name , bool retprobe , const char * binary_path , size_t offset , int pid ) { const size_t attr_sz = sizeof ( struct perf_event_attr ) ; struct perf_event_attr attr ; int type , pfd , err ; err = add_uprobe_event_legacy ( probe_name , retprobe , binary_path , offset ) ; if ( err < 0 ) { pr_warn ( \"failed to add legacy uprobe event for %s:0x%zx: %d\\n\" , binary_path , ( size_t ) offset , err ) ; return err ; } type = determine_uprobe_perf_type_legacy ( probe_name , retprobe ) ; if ( type < 0 ) { err = type ; pr_warn ( \"failed to determine legacy uprobe event id for %s:0x%zx: %d\\n\" , binary_path , offset , err ) ; goto err_clean_legacy ; } memset ( & attr , 0 , attr_sz ) ; attr . size = attr_sz ; attr . config = type ; attr . type = PERF_TYPE_TRACEPOINT ; pfd = syscall ( __NR_perf_event_open , & attr , pid < 0 ? - 1 : pid , /* pid */ pid == - 1 ? 0 : - 1 , /* cpu */ - 1 /* group_fd */ , PERF_FLAG_FD_CLOEXEC ) ; if ( pfd < 0 ) { err = - errno ; pr_warn ( \"legacy uprobe perf_event_open() failed: %d\\n\" , err ) ; goto err_clean_legacy ; } return pfd ; err_clean_legacy : /* Clear the newly added legacy uprobe_event */ remove_uprobe_event_legacy ( probe_name , retprobe ) ; return err ; }",
    "resources/libbpf/src/libbpf.c@elf_find_func_offset_from_archive": "static long elf_find_func_offset_from_archive ( const char * archive_path , const char * file_name , const char * func_name ) { struct zip_archive * archive ; struct zip_entry entry ; long ret ; Elf * elf ; archive = zip_archive_open ( archive_path ) ; if ( IS_ERR ( archive ) ) { ret = PTR_ERR ( archive ) ; pr_warn ( \"zip: failed to open %s: %ld\\n\" , archive_path , ret ) ; return ret ; } ret = zip_archive_find_entry ( archive , file_name , & entry ) ; if ( ret ) { pr_warn ( \"zip: could not find archive member %s in %s: %ld\\n\" , file_name , archive_path , ret ) ; goto out ; } pr_debug ( \"zip: found entry for %s in %s at 0x%lx\\n\" , file_name , archive_path , ( unsigned long ) entry . data_offset ) ; if ( entry . compression ) { pr_warn ( \"zip: entry %s of %s is compressed and cannot be handled\\n\" , file_name , archive_path ) ; ret = - LIBBPF_ERRNO__FORMAT ; goto out ; } elf = elf_memory ( ( void * ) entry . data , entry . data_length ) ; if ( ! elf ) { pr_warn ( \"elf: could not read elf file %s from %s: %s\\n\" , file_name , archive_path , elf_errmsg ( - 1 ) ) ; ret = - LIBBPF_ERRNO__LIBELF ; goto out ; } ret = elf_find_func_offset ( elf , file_name , func_name ) ; if ( ret > 0 ) { pr_debug ( \"elf: symbol address match for %s of %s in %s: 0x%x + 0x%lx = 0x%lx\\n\" , func_name , file_name , archive_path , entry . data_offset , ret , ret + entry . data_offset ) ; ret += entry . data_offset ; } elf_end ( elf ) ; out : zip_archive_close ( archive ) ; return ret ; }",
    "resources/libbpf/src/libbpf.c@arch_specific_lib_paths": "static const char * arch_specific_lib_paths ( void ) { /*\n\t * Based on https://packages.debian.org/sid/libc6.\n\t *\n\t * Assume that the traced program is built for the same architecture\n\t * as libbpf, which should cover the vast majority of cases.\n\t */ # if defined ( __x86_64__ ) return \"/lib/x86_64-linux-gnu\" ; # elif defined ( __i386__ ) return \"/lib/i386-linux-gnu\" ; # elif defined ( __s390x__ ) return \"/lib/s390x-linux-gnu\" ; # elif defined ( __s390__ ) return \"/lib/s390-linux-gnu\" ; # elif defined ( __arm__ ) && defined ( __SOFTFP__ ) return \"/lib/arm-linux-gnueabi\" ; # elif defined ( __arm__ ) && ! defined ( __SOFTFP__ ) return \"/lib/arm-linux-gnueabihf\" ; # elif defined ( __aarch64__ ) return \"/lib/aarch64-linux-gnu\" ; # elif defined ( __mips__ ) && defined ( __MIPSEL__ ) && _MIPS_SZLONG == 64 return \"/lib/mips64el-linux-gnuabi64\" ; # elif defined ( __mips__ ) && defined ( __MIPSEL__ ) && _MIPS_SZLONG == 32 return \"/lib/mipsel-linux-gnu\" ; # elif defined ( __powerpc64__ ) && __BYTE_ORDER__ == __ORDER_LITTLE_ENDIAN__ return \"/lib/powerpc64le-linux-gnu\" ; # elif defined ( __sparc__ ) && defined ( __arch64__ ) return \"/lib/sparc64-linux-gnu\" ; # elif defined ( __riscv ) && __riscv_xlen == 64 return \"/lib/riscv64-linux-gnu\" ; # else return NULL ; # endif }",
    "resources/libbpf/src/libbpf.c@resolve_full_path": "static int resolve_full_path ( const char * file , char * result , size_t result_sz ) { const char * search_paths [ 3 ] = { } ; int i , perm ; if ( str_has_sfx ( file , \".so\" ) || strstr ( file , \".so.\" ) ) { search_paths [ 0 ] = getenv ( \"LD_LIBRARY_PATH\" ) ; search_paths [ 1 ] = \"/usr/lib64:/usr/lib\" ; search_paths [ 2 ] = arch_specific_lib_paths ( ) ; perm = R_OK ; } else { search_paths [ 0 ] = getenv ( \"PATH\" ) ; search_paths [ 1 ] = \"/usr/bin:/usr/sbin\" ; perm = R_OK | X_OK ; } for ( i = 0 ; i < ARRAY_SIZE ( search_paths ) ; i ++ ) { const char * s ; if ( ! search_paths [ i ] ) continue ; for ( s = search_paths [ i ] ; s != NULL ; s = strchr ( s , ':' ) ) { char * next_path ; int seg_len ; if ( s [ 0 ] == ':' ) s ++ ; next_path = strchr ( s , ':' ) ; seg_len = next_path ? next_path - s : strlen ( s ) ; if ( ! seg_len ) continue ; snprintf ( result , result_sz , \"%.*s/%s\" , seg_len , s , file ) ; /* ensure it has required permissions */ if ( faccessat ( AT_FDCWD , result , perm , AT_EACCESS ) < 0 ) continue ; pr_debug ( \"resolved '%s' to '%s'\\n\" , file , result ) ; return 0 ; } } return - ENOENT ; }",
    "resources/libbpf/src/libbpf.c@bpf_program__attach_uprobe_multi": "struct bpf_link * bpf_program__attach_uprobe_multi ( const struct bpf_program * prog , pid_t pid , const char * path , const char * func_pattern , const struct bpf_uprobe_multi_opts * opts ) { const unsigned long * ref_ctr_offsets = NULL , * offsets = NULL ; LIBBPF_OPTS ( bpf_link_create_opts , lopts ) ; unsigned long * resolved_offsets = NULL ; int err = 0 , link_fd , prog_fd ; struct bpf_link * link = NULL ; char errmsg [ STRERR_BUFSIZE ] ; char full_path [ PATH_MAX ] ; const __u64 * cookies ; const char * * syms ; size_t cnt ; if ( ! OPTS_VALID ( opts , bpf_uprobe_multi_opts ) ) return libbpf_err_ptr ( - EINVAL ) ; prog_fd = bpf_program__fd ( prog ) ; if ( prog_fd < 0 ) { pr_warn ( \"prog '%s': can't attach BPF program without FD (was it loaded?)\\n\" , prog -> name ) ; return libbpf_err_ptr ( - EINVAL ) ; } syms = OPTS_GET ( opts , syms , NULL ) ; offsets = OPTS_GET ( opts , offsets , NULL ) ; ref_ctr_offsets = OPTS_GET ( opts , ref_ctr_offsets , NULL ) ; cookies = OPTS_GET ( opts , cookies , NULL ) ; cnt = OPTS_GET ( opts , cnt , 0 ) ; /*\n\t * User can specify 2 mutually exclusive set of inputs:\n\t *\n\t * 1) use only path/func_pattern/pid arguments\n\t *\n\t * 2) use path/pid with allowed combinations of:\n\t *    syms/offsets/ref_ctr_offsets/cookies/cnt\n\t *\n\t *    - syms and offsets are mutually exclusive\n\t *    - ref_ctr_offsets and cookies are optional\n\t *\n\t * Any other usage results in error.\n\t */ if ( ! path ) return libbpf_err_ptr ( - EINVAL ) ; if ( ! func_pattern && cnt == 0 ) return libbpf_err_ptr ( - EINVAL ) ; if ( func_pattern ) { if ( syms || offsets || ref_ctr_offsets || cookies || cnt ) return libbpf_err_ptr ( - EINVAL ) ; } else { if ( ! ! syms == ! ! offsets ) return libbpf_err_ptr ( - EINVAL ) ; } if ( func_pattern ) { if ( ! strchr ( path , '/' ) ) { err = resolve_full_path ( path , full_path , sizeof ( full_path ) ) ; if ( err ) { pr_warn ( \"prog '%s': failed to resolve full path for '%s': %d\\n\" , prog -> name , path , err ) ; return libbpf_err_ptr ( err ) ; } path = full_path ; } err = elf_resolve_pattern_offsets ( path , func_pattern , & resolved_offsets , & cnt ) ; if ( err < 0 ) return libbpf_err_ptr ( err ) ; offsets = resolved_offsets ; } else if ( syms ) { err = elf_resolve_syms_offsets ( path , cnt , syms , & resolved_offsets , STT_FUNC ) ; if ( err < 0 ) return libbpf_err_ptr ( err ) ; offsets = resolved_offsets ; } lopts . uprobe_multi . path = path ; lopts . uprobe_multi . offsets = offsets ; lopts . uprobe_multi . ref_ctr_offsets = ref_ctr_offsets ; lopts . uprobe_multi . cookies = cookies ; lopts . uprobe_multi . cnt = cnt ; lopts . uprobe_multi . flags = OPTS_GET ( opts , retprobe , false ) ? BPF_F_UPROBE_MULTI_RETURN : 0 ; if ( pid == 0 ) pid = getpid ( ) ; if ( pid > 0 ) lopts . uprobe_multi . pid = pid ; link = calloc ( 1 , sizeof ( * link ) ) ; if ( ! link ) { err = - ENOMEM ; goto error ; } link -> detach = & bpf_link__detach_fd ; link_fd = bpf_link_create ( prog_fd , 0 , BPF_TRACE_UPROBE_MULTI , & lopts ) ; if ( link_fd < 0 ) { err = - errno ; pr_warn ( \"prog '%s': failed to attach multi-uprobe: %s\\n\" , prog -> name , libbpf_strerror_r ( err , errmsg , sizeof ( errmsg ) ) ) ; goto error ; } link -> fd = link_fd ; free ( resolved_offsets ) ; return link ; error : free ( resolved_offsets ) ; free ( link ) ; return libbpf_err_ptr ( err ) ; }",
    "resources/libbpf/src/libbpf.c@bpf_program__attach_uprobe_opts": "",
    "resources/libbpf/src/libbpf.c@attach_uprobe": "static int attach_uprobe ( const struct bpf_program * prog , long cookie , struct bpf_link * * link ) { DECLARE_LIBBPF_OPTS ( bpf_uprobe_opts , opts ) ; char * probe_type = NULL , * binary_path = NULL , * func_name = NULL , * func_off ; int n , c , ret = - EINVAL ; long offset = 0 ; * link = NULL ; n = sscanf ( prog -> sec_name , \"%m[^/]/%m[^:]:%m[^\\n]\" , & probe_type , & binary_path , & func_name ) ; switch ( n ) { case 1 : /* handle SEC(\"u[ret]probe\") - format is valid, but auto-attach is impossible. */ ret = 0 ; break ; case 2 : pr_warn ( \"prog '%s': section '%s' missing ':function[+offset]' specification\\n\" , prog -> name , prog -> sec_name ) ; break ; case 3 : /* check if user specifies `+offset`, if yes, this should be\n\t\t * the last part of the string, make sure sscanf read to EOL\n\t\t */ func_off = strrchr ( func_name , '+' ) ; if ( func_off ) { n = sscanf ( func_off , \"+%li%n\" , & offset , & c ) ; if ( n == 1 && * ( func_off + c ) == '\\0' ) func_off [ 0 ] = '\\0' ; else offset = 0 ; } opts . retprobe = strcmp ( probe_type , \"uretprobe\" ) == 0 || strcmp ( probe_type , \"uretprobe.s\" ) == 0 ; if ( opts . retprobe && offset != 0 ) { pr_warn ( \"prog '%s': uretprobes do not support offset specification\\n\" , prog -> name ) ; break ; } opts . func_name = func_name ; * link = bpf_program__attach_uprobe_opts ( prog , - 1 , binary_path , offset , & opts ) ; ret = libbpf_get_error ( * link ) ; break ; default : pr_warn ( \"prog '%s': invalid format of section definition '%s'\\n\" , prog -> name , prog -> sec_name ) ; break ; } free ( probe_type ) ; free ( binary_path ) ; free ( func_name ) ; return ret ; }",
    "resources/libbpf/src/libbpf.c@bpf_program__attach_uprobe": "struct bpf_link * bpf_program__attach_uprobe ( const struct bpf_program * prog , bool retprobe , pid_t pid , const char * binary_path , size_t func_offset ) { DECLARE_LIBBPF_OPTS ( bpf_uprobe_opts , opts , . retprobe = retprobe ) ; return bpf_program__attach_uprobe_opts ( prog , pid , binary_path , func_offset , & opts ) ; }",
    "resources/libbpf/src/libbpf.c@bpf_program__attach_usdt": "struct bpf_link * bpf_program__attach_usdt ( const struct bpf_program * prog , pid_t pid , const char * binary_path , const char * usdt_provider , const char * usdt_name , const struct bpf_usdt_opts * opts ) { char resolved_path [ 512 ] ; struct bpf_object * obj = prog -> obj ; struct bpf_link * link ; __u64 usdt_cookie ; int err ; if ( ! OPTS_VALID ( opts , bpf_uprobe_opts ) ) return libbpf_err_ptr ( - EINVAL ) ; if ( bpf_program__fd ( prog ) < 0 ) { pr_warn ( \"prog '%s': can't attach BPF program without FD (was it loaded?)\\n\" , prog -> name ) ; return libbpf_err_ptr ( - EINVAL ) ; } if ( ! binary_path ) return libbpf_err_ptr ( - EINVAL ) ; if ( ! strchr ( binary_path , '/' ) ) { err = resolve_full_path ( binary_path , resolved_path , sizeof ( resolved_path ) ) ; if ( err ) { pr_warn ( \"prog '%s': failed to resolve full path for '%s': %d\\n\" , prog -> name , binary_path , err ) ; return libbpf_err_ptr ( err ) ; } binary_path = resolved_path ; } /* USDT manager is instantiated lazily on first USDT attach. It will\n\t * be destroyed together with BPF object in bpf_object__close().\n\t */ if ( IS_ERR ( obj -> usdt_man ) ) return libbpf_ptr ( obj -> usdt_man ) ; if ( ! obj -> usdt_man ) { obj -> usdt_man = usdt_manager_new ( obj ) ; if ( IS_ERR ( obj -> usdt_man ) ) return libbpf_ptr ( obj -> usdt_man ) ; } usdt_cookie = OPTS_GET ( opts , usdt_cookie , 0 ) ; link = usdt_manager_attach_usdt ( obj -> usdt_man , prog , pid , binary_path , usdt_provider , usdt_name , usdt_cookie ) ; err = libbpf_get_error ( link ) ; if ( err ) return libbpf_err_ptr ( err ) ; return link ; }",
    "resources/libbpf/src/libbpf.c@attach_usdt": "static int attach_usdt ( const struct bpf_program * prog , long cookie , struct bpf_link * * link ) { char * path = NULL , * provider = NULL , * name = NULL ; const char * sec_name ; int n , err ; sec_name = bpf_program__section_name ( prog ) ; if ( strcmp ( sec_name , \"usdt\" ) == 0 ) { /* no auto-attach for just SEC(\"usdt\") */ * link = NULL ; return 0 ; } n = sscanf ( sec_name , \"usdt/%m[^:]:%m[^:]:%m[^:]\" , & path , & provider , & name ) ; if ( n != 3 ) { pr_warn ( \"invalid section '%s', expected SEC(\\\"usdt/<path>:<provider>:<name>\\\")\\n\" , sec_name ) ; err = - EINVAL ; } else { * link = bpf_program__attach_usdt ( prog , - 1 /* any process */ , path , provider , name , NULL ) ; err = libbpf_get_error ( * link ) ; } free ( path ) ; free ( provider ) ; free ( name ) ; return err ; }",
    "resources/libbpf/src/libbpf.c@determine_tracepoint_id": "static int determine_tracepoint_id ( const char * tp_category , const char * tp_name ) { char file [ PATH_MAX ] ; int ret ; ret = snprintf ( file , sizeof ( file ) , \"%s/events/%s/%s/id\" , tracefs_path ( ) , tp_category , tp_name ) ; if ( ret < 0 ) return - errno ; if ( ret >= sizeof ( file ) ) { pr_debug ( \"tracepoint %s/%s path is too long\\n\" , tp_category , tp_name ) ; return - E2BIG ; } return parse_uint_from_file ( file , \"%d\\n\" ) ; }",
    "resources/libbpf/src/libbpf.c@perf_event_open_tracepoint": "static int perf_event_open_tracepoint ( const char * tp_category , const char * tp_name ) { const size_t attr_sz = sizeof ( struct perf_event_attr ) ; struct perf_event_attr attr ; char errmsg [ STRERR_BUFSIZE ] ; int tp_id , pfd , err ; tp_id = determine_tracepoint_id ( tp_category , tp_name ) ; if ( tp_id < 0 ) { pr_warn ( \"failed to determine tracepoint '%s/%s' perf event ID: %s\\n\" , tp_category , tp_name , libbpf_strerror_r ( tp_id , errmsg , sizeof ( errmsg ) ) ) ; return tp_id ; } memset ( & attr , 0 , attr_sz ) ; attr . type = PERF_TYPE_TRACEPOINT ; attr . size = attr_sz ; attr . config = tp_id ; pfd = syscall ( __NR_perf_event_open , & attr , - 1 /* pid */ , 0 /* cpu */ , - 1 /* group_fd */ , PERF_FLAG_FD_CLOEXEC ) ; if ( pfd < 0 ) { err = - errno ; pr_warn ( \"tracepoint '%s/%s' perf_event_open() failed: %s\\n\" , tp_category , tp_name , libbpf_strerror_r ( err , errmsg , sizeof ( errmsg ) ) ) ; return err ; } return pfd ; }",
    "resources/libbpf/src/libbpf.c@bpf_program__attach_tracepoint_opts": "struct bpf_link * bpf_program__attach_tracepoint_opts ( const struct bpf_program * prog , const char * tp_category , const char * tp_name , const struct bpf_tracepoint_opts * opts ) { DECLARE_LIBBPF_OPTS ( bpf_perf_event_opts , pe_opts ) ; char errmsg [ STRERR_BUFSIZE ] ; struct bpf_link * link ; int pfd , err ; if ( ! OPTS_VALID ( opts , bpf_tracepoint_opts ) ) return libbpf_err_ptr ( - EINVAL ) ; pe_opts . bpf_cookie = OPTS_GET ( opts , bpf_cookie , 0 ) ; pfd = perf_event_open_tracepoint ( tp_category , tp_name ) ; if ( pfd < 0 ) { pr_warn ( \"prog '%s': failed to create tracepoint '%s/%s' perf event: %s\\n\" , prog -> name , tp_category , tp_name , libbpf_strerror_r ( pfd , errmsg , sizeof ( errmsg ) ) ) ; return libbpf_err_ptr ( pfd ) ; } link = bpf_program__attach_perf_event_opts ( prog , pfd , & pe_opts ) ; err = libbpf_get_error ( link ) ; if ( err ) { close ( pfd ) ; pr_warn ( \"prog '%s': failed to attach to tracepoint '%s/%s': %s\\n\" , prog -> name , tp_category , tp_name , libbpf_strerror_r ( err , errmsg , sizeof ( errmsg ) ) ) ; return libbpf_err_ptr ( err ) ; } return link ; }",
    "resources/libbpf/src/libbpf.c@bpf_program__attach_tracepoint": "struct bpf_link * bpf_program__attach_tracepoint ( const struct bpf_program * prog , const char * tp_category , const char * tp_name ) { return bpf_program__attach_tracepoint_opts ( prog , tp_category , tp_name , NULL ) ; }",
    "resources/libbpf/src/libbpf.c@attach_tp": "static int attach_tp ( const struct bpf_program * prog , long cookie , struct bpf_link * * link ) { char * sec_name , * tp_cat , * tp_name ; * link = NULL ; /* no auto-attach for SEC(\"tp\") or SEC(\"tracepoint\") */ if ( strcmp ( prog -> sec_name , \"tp\" ) == 0 || strcmp ( prog -> sec_name , \"tracepoint\" ) == 0 ) return 0 ; sec_name = strdup ( prog -> sec_name ) ; if ( ! sec_name ) return - ENOMEM ; /* extract \"tp/<category>/<name>\" or \"tracepoint/<category>/<name>\" */ if ( str_has_pfx ( prog -> sec_name , \"tp/\" ) ) tp_cat = sec_name + sizeof ( \"tp/\" ) - 1 ; else tp_cat = sec_name + sizeof ( \"tracepoint/\" ) - 1 ; tp_name = strchr ( tp_cat , '/' ) ; if ( ! tp_name ) { free ( sec_name ) ; return - EINVAL ; } * tp_name = '\\0' ; tp_name ++ ; * link = bpf_program__attach_tracepoint ( prog , tp_cat , tp_name ) ; free ( sec_name ) ; return libbpf_get_error ( * link ) ; }",
    "resources/libbpf/src/libbpf.c@bpf_program__attach_raw_tracepoint_opts": "struct bpf_link * bpf_program__attach_raw_tracepoint_opts ( const struct bpf_program * prog , const char * tp_name , struct bpf_raw_tracepoint_opts * opts ) { LIBBPF_OPTS ( bpf_raw_tp_opts , raw_opts ) ; char errmsg [ STRERR_BUFSIZE ] ; struct bpf_link * link ; int prog_fd , pfd ; if ( ! OPTS_VALID ( opts , bpf_raw_tracepoint_opts ) ) return libbpf_err_ptr ( - EINVAL ) ; prog_fd = bpf_program__fd ( prog ) ; if ( prog_fd < 0 ) { pr_warn ( \"prog '%s': can't attach before loaded\\n\" , prog -> name ) ; return libbpf_err_ptr ( - EINVAL ) ; } link = calloc ( 1 , sizeof ( * link ) ) ; if ( ! link ) return libbpf_err_ptr ( - ENOMEM ) ; link -> detach = & bpf_link__detach_fd ; raw_opts . tp_name = tp_name ; raw_opts . cookie = OPTS_GET ( opts , cookie , 0 ) ; pfd = bpf_raw_tracepoint_open_opts ( prog_fd , & raw_opts ) ; if ( pfd < 0 ) { pfd = - errno ; free ( link ) ; pr_warn ( \"prog '%s': failed to attach to raw tracepoint '%s': %s\\n\" , prog -> name , tp_name , libbpf_strerror_r ( pfd , errmsg , sizeof ( errmsg ) ) ) ; return libbpf_err_ptr ( pfd ) ; } link -> fd = pfd ; return link ; }",
    "resources/libbpf/src/libbpf.c@bpf_program__attach_raw_tracepoint": "struct bpf_link * bpf_program__attach_raw_tracepoint ( const struct bpf_program * prog , const char * tp_name ) { return bpf_program__attach_raw_tracepoint_opts ( prog , tp_name , NULL ) ; }",
    "resources/libbpf/src/libbpf.c@attach_raw_tp": "static int attach_raw_tp ( const struct bpf_program * prog , long cookie , struct bpf_link * * link ) { static const char * const prefixes [ ] = { \"raw_tp\" , \"raw_tracepoint\" , \"raw_tp.w\" , \"raw_tracepoint.w\" , } ; size_t i ; const char * tp_name = NULL ; * link = NULL ; for ( i = 0 ; i < ARRAY_SIZE ( prefixes ) ; i ++ ) { size_t pfx_len ; if ( ! str_has_pfx ( prog -> sec_name , prefixes [ i ] ) ) continue ; pfx_len = strlen ( prefixes [ i ] ) ; /* no auto-attach case of, e.g., SEC(\"raw_tp\") */ if ( prog -> sec_name [ pfx_len ] == '\\0' ) return 0 ; if ( prog -> sec_name [ pfx_len ] != '/' ) continue ; tp_name = prog -> sec_name + pfx_len + 1 ; break ; } if ( ! tp_name ) { pr_warn ( \"prog '%s': invalid section name '%s'\\n\" , prog -> name , prog -> sec_name ) ; return - EINVAL ; } * link = bpf_program__attach_raw_tracepoint ( prog , tp_name ) ; return libbpf_get_error ( * link ) ; }",
    "resources/libbpf/src/libbpf.c@bpf_program__attach_btf_id": "static struct bpf_link * bpf_program__attach_btf_id ( const struct bpf_program * prog , const struct bpf_trace_opts * opts ) { LIBBPF_OPTS ( bpf_link_create_opts , link_opts ) ; char errmsg [ STRERR_BUFSIZE ] ; struct bpf_link * link ; int prog_fd , pfd ; if ( ! OPTS_VALID ( opts , bpf_trace_opts ) ) return libbpf_err_ptr ( - EINVAL ) ; prog_fd = bpf_program__fd ( prog ) ; if ( prog_fd < 0 ) { pr_warn ( \"prog '%s': can't attach before loaded\\n\" , prog -> name ) ; return libbpf_err_ptr ( - EINVAL ) ; } link = calloc ( 1 , sizeof ( * link ) ) ; if ( ! link ) return libbpf_err_ptr ( - ENOMEM ) ; link -> detach = & bpf_link__detach_fd ; /* libbpf is smart enough to redirect to BPF_RAW_TRACEPOINT_OPEN on old kernels */ link_opts . tracing . cookie = OPTS_GET ( opts , cookie , 0 ) ; pfd = bpf_link_create ( prog_fd , 0 , bpf_program__expected_attach_type ( prog ) , & link_opts ) ; if ( pfd < 0 ) { pfd = - errno ; free ( link ) ; pr_warn ( \"prog '%s': failed to attach: %s\\n\" , prog -> name , libbpf_strerror_r ( pfd , errmsg , sizeof ( errmsg ) ) ) ; return libbpf_err_ptr ( pfd ) ; } link -> fd = pfd ; return link ; }",
    "resources/libbpf/src/libbpf.c@bpf_program__attach_trace": "struct bpf_link * bpf_program__attach_trace ( const struct bpf_program * prog ) { return bpf_program__attach_btf_id ( prog , NULL ) ; }",
    "resources/libbpf/src/libbpf.c@bpf_program__attach_trace_opts": "struct bpf_link * bpf_program__attach_trace_opts ( const struct bpf_program * prog , const struct bpf_trace_opts * opts ) { return bpf_program__attach_btf_id ( prog , opts ) ; }",
    "resources/libbpf/src/libbpf.c@bpf_program__attach_lsm": "struct bpf_link * bpf_program__attach_lsm ( const struct bpf_program * prog ) { return bpf_program__attach_btf_id ( prog , NULL ) ; }",
    "resources/libbpf/src/libbpf.c@attach_trace": "static int attach_trace ( const struct bpf_program * prog , long cookie , struct bpf_link * * link ) { * link = bpf_program__attach_trace ( prog ) ; return libbpf_get_error ( * link ) ; }",
    "resources/libbpf/src/libbpf.c@attach_lsm": "static int attach_lsm ( const struct bpf_program * prog , long cookie , struct bpf_link * * link ) { * link = bpf_program__attach_lsm ( prog ) ; return libbpf_get_error ( * link ) ; }",
    "resources/libbpf/src/libbpf.c@bpf_program_attach_fd": "static struct bpf_link * bpf_program_attach_fd ( const struct bpf_program * prog , int target_fd , const char * target_name , const struct bpf_link_create_opts * opts ) { enum bpf_attach_type attach_type ; char errmsg [ STRERR_BUFSIZE ] ; struct bpf_link * link ; int prog_fd , link_fd ; prog_fd = bpf_program__fd ( prog ) ; if ( prog_fd < 0 ) { pr_warn ( \"prog '%s': can't attach before loaded\\n\" , prog -> name ) ; return libbpf_err_ptr ( - EINVAL ) ; } link = calloc ( 1 , sizeof ( * link ) ) ; if ( ! link ) return libbpf_err_ptr ( - ENOMEM ) ; link -> detach = & bpf_link__detach_fd ; attach_type = bpf_program__expected_attach_type ( prog ) ; link_fd = bpf_link_create ( prog_fd , target_fd , attach_type , opts ) ; if ( link_fd < 0 ) { link_fd = - errno ; free ( link ) ; pr_warn ( \"prog '%s': failed to attach to %s: %s\\n\" , prog -> name , target_name , libbpf_strerror_r ( link_fd , errmsg , sizeof ( errmsg ) ) ) ; return libbpf_err_ptr ( link_fd ) ; } link -> fd = link_fd ; return link ; }",
    "resources/libbpf/src/libbpf.c@bpf_program__attach_cgroup": "struct bpf_link * bpf_program__attach_cgroup ( const struct bpf_program * prog , int cgroup_fd ) { return bpf_program_attach_fd ( prog , cgroup_fd , \"cgroup\" , NULL ) ; }",
    "resources/libbpf/src/libbpf.c@bpf_program__attach_netns": "struct bpf_link * bpf_program__attach_netns ( const struct bpf_program * prog , int netns_fd ) { return bpf_program_attach_fd ( prog , netns_fd , \"netns\" , NULL ) ; }",
    "resources/libbpf/src/libbpf.c@bpf_program__attach_xdp": "struct bpf_link * bpf_program__attach_xdp ( const struct bpf_program * prog , int ifindex ) { /* target_fd/target_ifindex use the same field in LINK_CREATE */ return bpf_program_attach_fd ( prog , ifindex , \"xdp\" , NULL ) ; }",
    "resources/libbpf/src/libbpf.c@bpf_program__attach_tcx": "struct bpf_link * bpf_program__attach_tcx ( const struct bpf_program * prog , int ifindex , const struct bpf_tcx_opts * opts ) { LIBBPF_OPTS ( bpf_link_create_opts , link_create_opts ) ; __u32 relative_id ; int relative_fd ; if ( ! OPTS_VALID ( opts , bpf_tcx_opts ) ) return libbpf_err_ptr ( - EINVAL ) ; relative_id = OPTS_GET ( opts , relative_id , 0 ) ; relative_fd = OPTS_GET ( opts , relative_fd , 0 ) ; /* validate we don't have unexpected combinations of non-zero fields */ if ( ! ifindex ) { pr_warn ( \"prog '%s': target netdevice ifindex cannot be zero\\n\" , prog -> name ) ; return libbpf_err_ptr ( - EINVAL ) ; } if ( relative_fd && relative_id ) { pr_warn ( \"prog '%s': relative_fd and relative_id cannot be set at the same time\\n\" , prog -> name ) ; return libbpf_err_ptr ( - EINVAL ) ; } link_create_opts . tcx . expected_revision = OPTS_GET ( opts , expected_revision , 0 ) ; link_create_opts . tcx . relative_fd = relative_fd ; link_create_opts . tcx . relative_id = relative_id ; link_create_opts . flags = OPTS_GET ( opts , flags , 0 ) ; /* target_fd/target_ifindex use the same field in LINK_CREATE */ return bpf_program_attach_fd ( prog , ifindex , \"tcx\" , & link_create_opts ) ; }",
    "resources/libbpf/src/libbpf.c@bpf_program__attach_netkit": "struct bpf_link * bpf_program__attach_netkit ( const struct bpf_program * prog , int ifindex , const struct bpf_netkit_opts * opts ) { LIBBPF_OPTS ( bpf_link_create_opts , link_create_opts ) ; __u32 relative_id ; int relative_fd ; if ( ! OPTS_VALID ( opts , bpf_netkit_opts ) ) return libbpf_err_ptr ( - EINVAL ) ; relative_id = OPTS_GET ( opts , relative_id , 0 ) ; relative_fd = OPTS_GET ( opts , relative_fd , 0 ) ; /* validate we don't have unexpected combinations of non-zero fields */ if ( ! ifindex ) { pr_warn ( \"prog '%s': target netdevice ifindex cannot be zero\\n\" , prog -> name ) ; return libbpf_err_ptr ( - EINVAL ) ; } if ( relative_fd && relative_id ) { pr_warn ( \"prog '%s': relative_fd and relative_id cannot be set at the same time\\n\" , prog -> name ) ; return libbpf_err_ptr ( - EINVAL ) ; } link_create_opts . netkit . expected_revision = OPTS_GET ( opts , expected_revision , 0 ) ; link_create_opts . netkit . relative_fd = relative_fd ; link_create_opts . netkit . relative_id = relative_id ; link_create_opts . flags = OPTS_GET ( opts , flags , 0 ) ; return bpf_program_attach_fd ( prog , ifindex , \"netkit\" , & link_create_opts ) ; }",
    "resources/libbpf/src/libbpf.c@bpf_program__attach_freplace": "struct bpf_link * bpf_program__attach_freplace ( const struct bpf_program * prog , int target_fd , const char * attach_func_name ) { int btf_id ; if ( ! ! target_fd != ! ! attach_func_name ) { pr_warn ( \"prog '%s': supply none or both of target_fd and attach_func_name\\n\" , prog -> name ) ; return libbpf_err_ptr ( - EINVAL ) ; } if ( prog -> type != BPF_PROG_TYPE_EXT ) { pr_warn ( \"prog '%s': only BPF_PROG_TYPE_EXT can attach as freplace\" , prog -> name ) ; return libbpf_err_ptr ( - EINVAL ) ; } if ( target_fd ) { LIBBPF_OPTS ( bpf_link_create_opts , target_opts ) ; btf_id = libbpf_find_prog_btf_id ( attach_func_name , target_fd ) ; if ( btf_id < 0 ) return libbpf_err_ptr ( btf_id ) ; target_opts . target_btf_id = btf_id ; return bpf_program_attach_fd ( prog , target_fd , \"freplace\" , & target_opts ) ; } else { /* no target, so use raw_tracepoint_open for compatibility\n\t\t * with old kernels\n\t\t */ return bpf_program__attach_trace ( prog ) ; } }",
    "resources/libbpf/src/libbpf.c@bpf_program__attach_iter": "struct bpf_link * bpf_program__attach_iter ( const struct bpf_program * prog , const struct bpf_iter_attach_opts * opts ) { DECLARE_LIBBPF_OPTS ( bpf_link_create_opts , link_create_opts ) ; char errmsg [ STRERR_BUFSIZE ] ; struct bpf_link * link ; int prog_fd , link_fd ; __u32 target_fd = 0 ; if ( ! OPTS_VALID ( opts , bpf_iter_attach_opts ) ) return libbpf_err_ptr ( - EINVAL ) ; link_create_opts . iter_info = OPTS_GET ( opts , link_info , ( void * ) 0 ) ; link_create_opts . iter_info_len = OPTS_GET ( opts , link_info_len , 0 ) ; prog_fd = bpf_program__fd ( prog ) ; if ( prog_fd < 0 ) { pr_warn ( \"prog '%s': can't attach before loaded\\n\" , prog -> name ) ; return libbpf_err_ptr ( - EINVAL ) ; } link = calloc ( 1 , sizeof ( * link ) ) ; if ( ! link ) return libbpf_err_ptr ( - ENOMEM ) ; link -> detach = & bpf_link__detach_fd ; link_fd = bpf_link_create ( prog_fd , target_fd , BPF_TRACE_ITER , & link_create_opts ) ; if ( link_fd < 0 ) { link_fd = - errno ; free ( link ) ; pr_warn ( \"prog '%s': failed to attach to iterator: %s\\n\" , prog -> name , libbpf_strerror_r ( link_fd , errmsg , sizeof ( errmsg ) ) ) ; return libbpf_err_ptr ( link_fd ) ; } link -> fd = link_fd ; return link ; }",
    "resources/libbpf/src/libbpf.c@attach_iter": "static int attach_iter ( const struct bpf_program * prog , long cookie , struct bpf_link * * link ) { * link = bpf_program__attach_iter ( prog , NULL ) ; return libbpf_get_error ( * link ) ; }",
    "resources/libbpf/src/libbpf.c@bpf_program__attach_netfilter": "struct bpf_link * bpf_program__attach_netfilter ( const struct bpf_program * prog , const struct bpf_netfilter_opts * opts ) { LIBBPF_OPTS ( bpf_link_create_opts , lopts ) ; struct bpf_link * link ; int prog_fd , link_fd ; if ( ! OPTS_VALID ( opts , bpf_netfilter_opts ) ) return libbpf_err_ptr ( - EINVAL ) ; prog_fd = bpf_program__fd ( prog ) ; if ( prog_fd < 0 ) { pr_warn ( \"prog '%s': can't attach before loaded\\n\" , prog -> name ) ; return libbpf_err_ptr ( - EINVAL ) ; } link = calloc ( 1 , sizeof ( * link ) ) ; if ( ! link ) return libbpf_err_ptr ( - ENOMEM ) ; link -> detach = & bpf_link__detach_fd ; lopts . netfilter . pf = OPTS_GET ( opts , pf , 0 ) ; lopts . netfilter . hooknum = OPTS_GET ( opts , hooknum , 0 ) ; lopts . netfilter . priority = OPTS_GET ( opts , priority , 0 ) ; lopts . netfilter . flags = OPTS_GET ( opts , flags , 0 ) ; link_fd = bpf_link_create ( prog_fd , 0 , BPF_NETFILTER , & lopts ) ; if ( link_fd < 0 ) { char errmsg [ STRERR_BUFSIZE ] ; link_fd = - errno ; free ( link ) ; pr_warn ( \"prog '%s': failed to attach to netfilter: %s\\n\" , prog -> name , libbpf_strerror_r ( link_fd , errmsg , sizeof ( errmsg ) ) ) ; return libbpf_err_ptr ( link_fd ) ; } link -> fd = link_fd ; return link ; }",
    "resources/libbpf/src/libbpf.c@bpf_program__attach": "struct bpf_link * bpf_program__attach ( const struct bpf_program * prog ) { struct bpf_link * link = NULL ; int err ; if ( ! prog -> sec_def || ! prog -> sec_def -> prog_attach_fn ) return libbpf_err_ptr ( - EOPNOTSUPP ) ; if ( bpf_program__fd ( prog ) < 0 ) { pr_warn ( \"prog '%s': can't attach BPF program without FD (was it loaded?)\\n\" , prog -> name ) ; return libbpf_err_ptr ( - EINVAL ) ; } err = prog -> sec_def -> prog_attach_fn ( prog , prog -> sec_def -> cookie , & link ) ; if ( err ) return libbpf_err_ptr ( err ) ; /* When calling bpf_program__attach() explicitly, auto-attach support\n\t * is expected to work, so NULL returned link is considered an error.\n\t * This is different for skeleton's attach, see comment in\n\t * bpf_object__attach_skeleton().\n\t */ if ( ! link ) return libbpf_err_ptr ( - EOPNOTSUPP ) ; return link ; }",
    "resources/libbpf/src/libbpf.c@bpf_link__detach_struct_ops": "static int bpf_link__detach_struct_ops ( struct bpf_link * link ) { struct bpf_link_struct_ops * st_link ; __u32 zero = 0 ; st_link = container_of ( link , struct bpf_link_struct_ops , link ) ; if ( st_link -> map_fd < 0 ) /* w/o a real link */ return bpf_map_delete_elem ( link -> fd , & zero ) ; return close ( link -> fd ) ; }",
    "resources/libbpf/src/libbpf.c@bpf_map__attach_struct_ops": "struct bpf_link * bpf_map__attach_struct_ops ( const struct bpf_map * map ) { struct bpf_link_struct_ops * link ; __u32 zero = 0 ; int err , fd ; if ( ! bpf_map__is_struct_ops ( map ) ) return libbpf_err_ptr ( - EINVAL ) ; if ( map -> fd < 0 ) { pr_warn ( \"map '%s': can't attach BPF map without FD (was it created?)\\n\" , map -> name ) ; return libbpf_err_ptr ( - EINVAL ) ; } link = calloc ( 1 , sizeof ( * link ) ) ; if ( ! link ) return libbpf_err_ptr ( - EINVAL ) ; /* kern_vdata should be prepared during the loading phase. */ err = bpf_map_update_elem ( map -> fd , & zero , map -> st_ops -> kern_vdata , 0 ) ; /* It can be EBUSY if the map has been used to create or\n\t * update a link before.  We don't allow updating the value of\n\t * a struct_ops once it is set.  That ensures that the value\n\t * never changed.  So, it is safe to skip EBUSY.\n\t */ if ( err && ( ! ( map -> def . map_flags & BPF_F_LINK ) || err != - EBUSY ) ) { free ( link ) ; return libbpf_err_ptr ( err ) ; } link -> link . detach = bpf_link__detach_struct_ops ; if ( ! ( map -> def . map_flags & BPF_F_LINK ) ) { /* w/o a real link */ link -> link . fd = map -> fd ; link -> map_fd = - 1 ; return & link -> link ; } fd = bpf_link_create ( map -> fd , 0 , BPF_STRUCT_OPS , NULL ) ; if ( fd < 0 ) { free ( link ) ; return libbpf_err_ptr ( fd ) ; } link -> link . fd = fd ; link -> map_fd = map -> fd ; return & link -> link ; }",
    "resources/libbpf/src/libbpf.c@bpf_link__update_map": "int bpf_link__update_map ( struct bpf_link * link , const struct bpf_map * map ) { struct bpf_link_struct_ops * st_ops_link ; __u32 zero = 0 ; int err ; if ( ! bpf_map__is_struct_ops ( map ) ) return - EINVAL ; if ( map -> fd < 0 ) { pr_warn ( \"map '%s': can't use BPF map without FD (was it created?)\\n\" , map -> name ) ; return - EINVAL ; } st_ops_link = container_of ( link , struct bpf_link_struct_ops , link ) ; /* Ensure the type of a link is correct */ if ( st_ops_link -> map_fd < 0 ) return - EINVAL ; err = bpf_map_update_elem ( map -> fd , & zero , map -> st_ops -> kern_vdata , 0 ) ; /* It can be EBUSY if the map has been used to create or\n\t * update a link before.  We don't allow updating the value of\n\t * a struct_ops once it is set.  That ensures that the value\n\t * never changed.  So, it is safe to skip EBUSY.\n\t */ if ( err && err != - EBUSY ) return err ; err = bpf_link_update ( link -> fd , map -> fd , NULL ) ; if ( err < 0 ) return err ; st_ops_link -> map_fd = map -> fd ; return 0 ; }",
    "resources/libbpf/src/libbpf.c@perf_event_read_simple": "static enum bpf_perf_event_ret perf_event_read_simple ( void * mmap_mem , size_t mmap_size , size_t page_size , void * * copy_mem , size_t * copy_size , bpf_perf_event_print_t fn , void * private_data ) { struct perf_event_mmap_page * header = mmap_mem ; __u64 data_head = ring_buffer_read_head ( header ) ; __u64 data_tail = header -> data_tail ; void * base = ( ( __u8 * ) header ) + page_size ; int ret = LIBBPF_PERF_EVENT_CONT ; struct perf_event_header * ehdr ; size_t ehdr_size ; while ( data_head != data_tail ) { ehdr = base + ( data_tail & ( mmap_size - 1 ) ) ; ehdr_size = ehdr -> size ; if ( ( ( void * ) ehdr ) + ehdr_size > base + mmap_size ) { void * copy_start = ehdr ; size_t len_first = base + mmap_size - copy_start ; size_t len_secnd = ehdr_size - len_first ; if ( * copy_size < ehdr_size ) { free ( * copy_mem ) ; * copy_mem = malloc ( ehdr_size ) ; if ( ! * copy_mem ) { * copy_size = 0 ; ret = LIBBPF_PERF_EVENT_ERROR ; break ; } * copy_size = ehdr_size ; } memcpy ( * copy_mem , copy_start , len_first ) ; memcpy ( * copy_mem + len_first , base , len_secnd ) ; ehdr = * copy_mem ; } ret = fn ( ehdr , private_data ) ; data_tail += ehdr_size ; if ( ret != LIBBPF_PERF_EVENT_CONT ) break ; } ring_buffer_write_tail ( header , data_tail ) ; return libbpf_err ( ret ) ; }",
    "resources/libbpf/src/libbpf.c@perf_buffer__free_cpu_buf": "static void perf_buffer__free_cpu_buf ( struct perf_buffer * pb , struct perf_cpu_buf * cpu_buf ) { if ( ! cpu_buf ) return ; if ( cpu_buf -> base && munmap ( cpu_buf -> base , pb -> mmap_size + pb -> page_size ) ) pr_warn ( \"failed to munmap cpu_buf #%d\\n\" , cpu_buf -> cpu ) ; if ( cpu_buf -> fd >= 0 ) { ioctl ( cpu_buf -> fd , PERF_EVENT_IOC_DISABLE , 0 ) ; close ( cpu_buf -> fd ) ; } free ( cpu_buf -> buf ) ; free ( cpu_buf ) ; }",
    "resources/libbpf/src/libbpf.c@perf_buffer__free": "void perf_buffer__free ( struct perf_buffer * pb ) { int i ; if ( IS_ERR_OR_NULL ( pb ) ) return ; if ( pb -> cpu_bufs ) { for ( i = 0 ; i < pb -> cpu_cnt ; i ++ ) { struct perf_cpu_buf * cpu_buf = pb -> cpu_bufs [ i ] ; if ( ! cpu_buf ) continue ; bpf_map_delete_elem ( pb -> map_fd , & cpu_buf -> map_key ) ; perf_buffer__free_cpu_buf ( pb , cpu_buf ) ; } free ( pb -> cpu_bufs ) ; } if ( pb -> epoll_fd >= 0 ) close ( pb -> epoll_fd ) ; free ( pb -> events ) ; free ( pb ) ; }",
    "resources/libbpf/src/libbpf.c@perf_buffer__open_cpu_buf": "static struct perf_cpu_buf * perf_buffer__open_cpu_buf ( struct perf_buffer * pb , struct perf_event_attr * attr , int cpu , int map_key ) { struct perf_cpu_buf * cpu_buf ; char msg [ STRERR_BUFSIZE ] ; int err ; cpu_buf = calloc ( 1 , sizeof ( * cpu_buf ) ) ; if ( ! cpu_buf ) return ERR_PTR ( - ENOMEM ) ; cpu_buf -> pb = pb ; cpu_buf -> cpu = cpu ; cpu_buf -> map_key = map_key ; cpu_buf -> fd = syscall ( __NR_perf_event_open , attr , - 1 /* pid */ , cpu , - 1 , PERF_FLAG_FD_CLOEXEC ) ; if ( cpu_buf -> fd < 0 ) { err = - errno ; pr_warn ( \"failed to open perf buffer event on cpu #%d: %s\\n\" , cpu , libbpf_strerror_r ( err , msg , sizeof ( msg ) ) ) ; goto error ; } cpu_buf -> base = mmap ( NULL , pb -> mmap_size + pb -> page_size , PROT_READ | PROT_WRITE , MAP_SHARED , cpu_buf -> fd , 0 ) ; if ( cpu_buf -> base == MAP_FAILED ) { cpu_buf -> base = NULL ; err = - errno ; pr_warn ( \"failed to mmap perf buffer on cpu #%d: %s\\n\" , cpu , libbpf_strerror_r ( err , msg , sizeof ( msg ) ) ) ; goto error ; } if ( ioctl ( cpu_buf -> fd , PERF_EVENT_IOC_ENABLE , 0 ) < 0 ) { err = - errno ; pr_warn ( \"failed to enable perf buffer event on cpu #%d: %s\\n\" , cpu , libbpf_strerror_r ( err , msg , sizeof ( msg ) ) ) ; goto error ; } return cpu_buf ; error : perf_buffer__free_cpu_buf ( pb , cpu_buf ) ; return ( struct perf_cpu_buf * ) ERR_PTR ( err ) ; }",
    "resources/libbpf/src/libbpf.c@perf_buffer__new": "struct perf_buffer * perf_buffer__new ( int map_fd , size_t page_cnt , perf_buffer_sample_fn sample_cb , perf_buffer_lost_fn lost_cb , void * ctx , const struct perf_buffer_opts * opts ) { const size_t attr_sz = sizeof ( struct perf_event_attr ) ; struct perf_buffer_params p = { } ; struct perf_event_attr attr ; __u32 sample_period ; if ( ! OPTS_VALID ( opts , perf_buffer_opts ) ) return libbpf_err_ptr ( - EINVAL ) ; sample_period = OPTS_GET ( opts , sample_period , 1 ) ; if ( ! sample_period ) sample_period = 1 ; memset ( & attr , 0 , attr_sz ) ; attr . size = attr_sz ; attr . config = PERF_COUNT_SW_BPF_OUTPUT ; attr . type = PERF_TYPE_SOFTWARE ; attr . sample_type = PERF_SAMPLE_RAW ; attr . sample_period = sample_period ; attr . wakeup_events = sample_period ; p . attr = & attr ; p . sample_cb = sample_cb ; p . lost_cb = lost_cb ; p . ctx = ctx ; return libbpf_ptr ( __perf_buffer__new ( map_fd , page_cnt , & p ) ) ; }",
    "resources/libbpf/src/libbpf.c@perf_buffer__new_raw": "struct perf_buffer * perf_buffer__new_raw ( int map_fd , size_t page_cnt , struct perf_event_attr * attr , perf_buffer_event_fn event_cb , void * ctx , const struct perf_buffer_raw_opts * opts ) { struct perf_buffer_params p = { } ; if ( ! attr ) return libbpf_err_ptr ( - EINVAL ) ; if ( ! OPTS_VALID ( opts , perf_buffer_raw_opts ) ) return libbpf_err_ptr ( - EINVAL ) ; p . attr = attr ; p . event_cb = event_cb ; p . ctx = ctx ; p . cpu_cnt = OPTS_GET ( opts , cpu_cnt , 0 ) ; p . cpus = OPTS_GET ( opts , cpus , NULL ) ; p . map_keys = OPTS_GET ( opts , map_keys , NULL ) ; return libbpf_ptr ( __perf_buffer__new ( map_fd , page_cnt , & p ) ) ; }",
    "resources/libbpf/src/libbpf.c@__perf_buffer__new": "static struct perf_buffer * __perf_buffer__new ( int map_fd , size_t page_cnt , struct perf_buffer_params * p ) { const char * online_cpus_file = \"/sys/devices/system/cpu/online\" ; struct bpf_map_info map ; char msg [ STRERR_BUFSIZE ] ; struct perf_buffer * pb ; bool * online = NULL ; __u32 map_info_len ; int err , i , j , n ; if ( page_cnt == 0 || ( page_cnt & ( page_cnt - 1 ) ) ) { pr_warn ( \"page count should be power of two, but is %zu\\n\" , page_cnt ) ; return ERR_PTR ( - EINVAL ) ; } /* best-effort sanity checks */ memset ( & map , 0 , sizeof ( map ) ) ; map_info_len = sizeof ( map ) ; err = bpf_map_get_info_by_fd ( map_fd , & map , & map_info_len ) ; if ( err ) { err = - errno ; /* if BPF_OBJ_GET_INFO_BY_FD is supported, will return\n\t\t * -EBADFD, -EFAULT, or -E2BIG on real error\n\t\t */ if ( err != - EINVAL ) { pr_warn ( \"failed to get map info for map FD %d: %s\\n\" , map_fd , libbpf_strerror_r ( err , msg , sizeof ( msg ) ) ) ; return ERR_PTR ( err ) ; } pr_debug ( \"failed to get map info for FD %d; API not supported? Ignoring...\\n\" , map_fd ) ; } else { if ( map . type != BPF_MAP_TYPE_PERF_EVENT_ARRAY ) { pr_warn ( \"map '%s' should be BPF_MAP_TYPE_PERF_EVENT_ARRAY\\n\" , map . name ) ; return ERR_PTR ( - EINVAL ) ; } } pb = calloc ( 1 , sizeof ( * pb ) ) ; if ( ! pb ) return ERR_PTR ( - ENOMEM ) ; pb -> event_cb = p -> event_cb ; pb -> sample_cb = p -> sample_cb ; pb -> lost_cb = p -> lost_cb ; pb -> ctx = p -> ctx ; pb -> page_size = getpagesize ( ) ; pb -> mmap_size = pb -> page_size * page_cnt ; pb -> map_fd = map_fd ; pb -> epoll_fd = epoll_create1 ( EPOLL_CLOEXEC ) ; if ( pb -> epoll_fd < 0 ) { err = - errno ; pr_warn ( \"failed to create epoll instance: %s\\n\" , libbpf_strerror_r ( err , msg , sizeof ( msg ) ) ) ; goto error ; } if ( p -> cpu_cnt > 0 ) { pb -> cpu_cnt = p -> cpu_cnt ; } else { pb -> cpu_cnt = libbpf_num_possible_cpus ( ) ; if ( pb -> cpu_cnt < 0 ) { err = pb -> cpu_cnt ; goto error ; } if ( map . max_entries && map . max_entries < pb -> cpu_cnt ) pb -> cpu_cnt = map . max_entries ; } pb -> events = calloc ( pb -> cpu_cnt , sizeof ( * pb -> events ) ) ; if ( ! pb -> events ) { err = - ENOMEM ; pr_warn ( \"failed to allocate events: out of memory\\n\" ) ; goto error ; } pb -> cpu_bufs = calloc ( pb -> cpu_cnt , sizeof ( * pb -> cpu_bufs ) ) ; if ( ! pb -> cpu_bufs ) { err = - ENOMEM ; pr_warn ( \"failed to allocate buffers: out of memory\\n\" ) ; goto error ; } err = parse_cpu_mask_file ( online_cpus_file , & online , & n ) ; if ( err ) { pr_warn ( \"failed to get online CPU mask: %d\\n\" , err ) ; goto error ; } for ( i = 0 , j = 0 ; i < pb -> cpu_cnt ; i ++ ) { struct perf_cpu_buf * cpu_buf ; int cpu , map_key ; cpu = p -> cpu_cnt > 0 ? p -> cpus [ i ] : i ; map_key = p -> cpu_cnt > 0 ? p -> map_keys [ i ] : i ; /* in case user didn't explicitly requested particular CPUs to\n\t\t * be attached to, skip offline/not present CPUs\n\t\t */ if ( p -> cpu_cnt <= 0 && ( cpu >= n || ! online [ cpu ] ) ) continue ; cpu_buf = perf_buffer__open_cpu_buf ( pb , p -> attr , cpu , map_key ) ; if ( IS_ERR ( cpu_buf ) ) { err = PTR_ERR ( cpu_buf ) ; goto error ; } pb -> cpu_bufs [ j ] = cpu_buf ; err = bpf_map_update_elem ( pb -> map_fd , & map_key , & cpu_buf -> fd , 0 ) ; if ( err ) { err = - errno ; pr_warn ( \"failed to set cpu #%d, key %d -> perf FD %d: %s\\n\" , cpu , map_key , cpu_buf -> fd , libbpf_strerror_r ( err , msg , sizeof ( msg ) ) ) ; goto error ; } pb -> events [ j ] . events = EPOLLIN ; pb -> events [ j ] . data . ptr = cpu_buf ; if ( epoll_ctl ( pb -> epoll_fd , EPOLL_CTL_ADD , cpu_buf -> fd , & pb -> events [ j ] ) < 0 ) { err = - errno ; pr_warn ( \"failed to epoll_ctl cpu #%d perf FD %d: %s\\n\" , cpu , cpu_buf -> fd , libbpf_strerror_r ( err , msg , sizeof ( msg ) ) ) ; goto error ; } j ++ ; } pb -> cpu_cnt = j ; free ( online ) ; return pb ; error : free ( online ) ; if ( pb ) perf_buffer__free ( pb ) ; return ERR_PTR ( err ) ; }",
    "resources/libbpf/src/libbpf.c@perf_buffer__process_record": "static enum bpf_perf_event_ret perf_buffer__process_record ( struct perf_event_header * e , void * ctx ) { struct perf_cpu_buf * cpu_buf = ctx ; struct perf_buffer * pb = cpu_buf -> pb ; void * data = e ; /* user wants full control over parsing perf event */ if ( pb -> event_cb ) return pb -> event_cb ( pb -> ctx , cpu_buf -> cpu , e ) ; switch ( e -> type ) { case PERF_RECORD_SAMPLE : { struct perf_sample_raw * s = data ; if ( pb -> sample_cb ) pb -> sample_cb ( pb -> ctx , cpu_buf -> cpu , s -> data , s -> size ) ; break ; } case PERF_RECORD_LOST : { struct perf_sample_lost * s = data ; if ( pb -> lost_cb ) pb -> lost_cb ( pb -> ctx , cpu_buf -> cpu , s -> lost ) ; break ; } default : pr_warn ( \"unknown perf sample type %d\\n\" , e -> type ) ; return LIBBPF_PERF_EVENT_ERROR ; } return LIBBPF_PERF_EVENT_CONT ; }",
    "resources/libbpf/src/libbpf.c@perf_buffer__process_records": "static int perf_buffer__process_records ( struct perf_buffer * pb , struct perf_cpu_buf * cpu_buf ) { enum bpf_perf_event_ret ret ; ret = perf_event_read_simple ( cpu_buf -> base , pb -> mmap_size , pb -> page_size , & cpu_buf -> buf , & cpu_buf -> buf_size , perf_buffer__process_record , cpu_buf ) ; if ( ret != LIBBPF_PERF_EVENT_CONT ) return ret ; return 0 ; }",
    "resources/libbpf/src/libbpf.c@perf_buffer__epoll_fd": "int perf_buffer__epoll_fd ( const struct perf_buffer * pb ) { return pb -> epoll_fd ; }",
    "resources/libbpf/src/libbpf.c@perf_buffer__poll": "int perf_buffer__poll ( struct perf_buffer * pb , int timeout_ms ) { int i , cnt , err ; cnt = epoll_wait ( pb -> epoll_fd , pb -> events , pb -> cpu_cnt , timeout_ms ) ; if ( cnt < 0 ) return - errno ; for ( i = 0 ; i < cnt ; i ++ ) { struct perf_cpu_buf * cpu_buf = pb -> events [ i ] . data . ptr ; err = perf_buffer__process_records ( pb , cpu_buf ) ; if ( err ) { pr_warn ( \"error while processing records: %d\\n\" , err ) ; return libbpf_err ( err ) ; } } return cnt ; }",
    "resources/libbpf/src/libbpf.c@perf_buffer__buffer_cnt": "size_t perf_buffer__buffer_cnt ( const struct perf_buffer * pb ) { return pb -> cpu_cnt ; }",
    "resources/libbpf/src/libbpf.c@perf_buffer__buffer_fd": "int perf_buffer__buffer_fd ( const struct perf_buffer * pb , size_t buf_idx ) { struct perf_cpu_buf * cpu_buf ; if ( buf_idx >= pb -> cpu_cnt ) return libbpf_err ( - EINVAL ) ; cpu_buf = pb -> cpu_bufs [ buf_idx ] ; if ( ! cpu_buf ) return libbpf_err ( - ENOENT ) ; return cpu_buf -> fd ; }",
    "resources/libbpf/src/libbpf.c@perf_buffer__buffer": "int perf_buffer__buffer ( struct perf_buffer * pb , int buf_idx , void * * buf , size_t * buf_size ) { struct perf_cpu_buf * cpu_buf ; if ( buf_idx >= pb -> cpu_cnt ) return libbpf_err ( - EINVAL ) ; cpu_buf = pb -> cpu_bufs [ buf_idx ] ; if ( ! cpu_buf ) return libbpf_err ( - ENOENT ) ; * buf = cpu_buf -> base ; * buf_size = pb -> mmap_size ; return 0 ; }",
    "resources/libbpf/src/libbpf.c@perf_buffer__consume_buffer": "int perf_buffer__consume_buffer ( struct perf_buffer * pb , size_t buf_idx ) { struct perf_cpu_buf * cpu_buf ; if ( buf_idx >= pb -> cpu_cnt ) return libbpf_err ( - EINVAL ) ; cpu_buf = pb -> cpu_bufs [ buf_idx ] ; if ( ! cpu_buf ) return libbpf_err ( - ENOENT ) ; return perf_buffer__process_records ( pb , cpu_buf ) ; }",
    "resources/libbpf/src/libbpf.c@perf_buffer__consume": "int perf_buffer__consume ( struct perf_buffer * pb ) { int i , err ; for ( i = 0 ; i < pb -> cpu_cnt ; i ++ ) { struct perf_cpu_buf * cpu_buf = pb -> cpu_bufs [ i ] ; if ( ! cpu_buf ) continue ; err = perf_buffer__process_records ( pb , cpu_buf ) ; if ( err ) { pr_warn ( \"perf_buffer: failed to process records in buffer #%d: %d\\n\" , i , err ) ; return libbpf_err ( err ) ; } } return 0 ; }",
    "resources/libbpf/src/libbpf.c@bpf_program__set_attach_target": "int bpf_program__set_attach_target ( struct bpf_program * prog , int attach_prog_fd , const char * attach_func_name ) { int btf_obj_fd = 0 , btf_id = 0 , err ; if ( ! prog || attach_prog_fd < 0 ) return libbpf_err ( - EINVAL ) ; if ( prog -> obj -> loaded ) return libbpf_err ( - EINVAL ) ; if ( attach_prog_fd && ! attach_func_name ) { /* remember attach_prog_fd and let bpf_program__load() find\n\t\t * BTF ID during the program load\n\t\t */ prog -> attach_prog_fd = attach_prog_fd ; return 0 ; } if ( attach_prog_fd ) { btf_id = libbpf_find_prog_btf_id ( attach_func_name , attach_prog_fd ) ; if ( btf_id < 0 ) return libbpf_err ( btf_id ) ; } else { if ( ! attach_func_name ) return libbpf_err ( - EINVAL ) ; /* load btf_vmlinux, if not yet */ err = bpf_object__load_vmlinux_btf ( prog -> obj , true ) ; if ( err ) return libbpf_err ( err ) ; err = find_kernel_btf_id ( prog -> obj , attach_func_name , prog -> expected_attach_type , & btf_obj_fd , & btf_id ) ; if ( err ) return libbpf_err ( err ) ; } prog -> attach_btf_id = btf_id ; prog -> attach_btf_obj_fd = btf_obj_fd ; prog -> attach_prog_fd = attach_prog_fd ; return 0 ; }",
    "resources/libbpf/src/libbpf.c@parse_cpu_mask_str": "int parse_cpu_mask_str ( const char * s , bool * * mask , int * mask_sz ) { int err = 0 , n , len , start , end = - 1 ; bool * tmp ; * mask = NULL ; * mask_sz = 0 ; /* Each sub string separated by ',' has format \\d+-\\d+ or \\d+ */ while ( * s ) { if ( * s == ',' || * s == '\\n' ) { s ++ ; continue ; } n = sscanf ( s , \"%d%n-%d%n\" , & start , & len , & end , & len ) ; if ( n <= 0 || n > 2 ) { pr_warn ( \"Failed to get CPU range %s: %d\\n\" , s , n ) ; err = - EINVAL ; goto cleanup ; } else if ( n == 1 ) { end = start ; } if ( start < 0 || start > end ) { pr_warn ( \"Invalid CPU range [%d,%d] in %s\\n\" , start , end , s ) ; err = - EINVAL ; goto cleanup ; } tmp = realloc ( * mask , end + 1 ) ; if ( ! tmp ) { err = - ENOMEM ; goto cleanup ; } * mask = tmp ; memset ( tmp + * mask_sz , 0 , start - * mask_sz ) ; memset ( tmp + start , 1 , end - start + 1 ) ; * mask_sz = end + 1 ; s += len ; } if ( ! * mask_sz ) { pr_warn ( \"Empty CPU range\\n\" ) ; return - EINVAL ; } return 0 ; cleanup : free ( * mask ) ; * mask = NULL ; return err ; }",
    "resources/libbpf/src/libbpf.c@parse_cpu_mask_file": "int parse_cpu_mask_file ( const char * fcpu , bool * * mask , int * mask_sz ) { int fd , err = 0 , len ; char buf [ 128 ] ; fd = open ( fcpu , O_RDONLY | O_CLOEXEC ) ; if ( fd < 0 ) { err = - errno ; pr_warn ( \"Failed to open cpu mask file %s: %d\\n\" , fcpu , err ) ; return err ; } len = read ( fd , buf , sizeof ( buf ) ) ; close ( fd ) ; if ( len <= 0 ) { err = len ? - errno : - EINVAL ; pr_warn ( \"Failed to read cpu mask from %s: %d\\n\" , fcpu , err ) ; return err ; } if ( len >= sizeof ( buf ) ) { pr_warn ( \"CPU mask is too big in file %s\\n\" , fcpu ) ; return - E2BIG ; } buf [ len ] = '\\0' ; return parse_cpu_mask_str ( buf , mask , mask_sz ) ; }",
    "resources/libbpf/src/libbpf.c@libbpf_num_possible_cpus": "int libbpf_num_possible_cpus ( void ) { static const char * fcpu = \"/sys/devices/system/cpu/possible\" ; static int cpus ; int err , n , i , tmp_cpus ; bool * mask ; tmp_cpus = READ_ONCE ( cpus ) ; if ( tmp_cpus > 0 ) return tmp_cpus ; err = parse_cpu_mask_file ( fcpu , & mask , & n ) ; if ( err ) return libbpf_err ( err ) ; tmp_cpus = 0 ; for ( i = 0 ; i < n ; i ++ ) { if ( mask [ i ] ) tmp_cpus ++ ; } free ( mask ) ; WRITE_ONCE ( cpus , tmp_cpus ) ; return tmp_cpus ; }",
    "resources/libbpf/src/libbpf.c@populate_skeleton_maps": "static int populate_skeleton_maps ( const struct bpf_object * obj , struct bpf_map_skeleton * maps , size_t map_cnt ) { int i ; for ( i = 0 ; i < map_cnt ; i ++ ) { struct bpf_map * * map = maps [ i ] . map ; const char * name = maps [ i ] . name ; void * * mmaped = maps [ i ] . mmaped ; * map = bpf_object__find_map_by_name ( obj , name ) ; if ( ! * map ) { pr_warn ( \"failed to find skeleton map '%s'\\n\" , name ) ; return - ESRCH ; } /* externs shouldn't be pre-setup from user code */ if ( mmaped && ( * map ) -> libbpf_type != LIBBPF_MAP_KCONFIG ) * mmaped = ( * map ) -> mmaped ; } return 0 ; }",
    "resources/libbpf/src/libbpf.c@populate_skeleton_progs": "static int populate_skeleton_progs ( const struct bpf_object * obj , struct bpf_prog_skeleton * progs , size_t prog_cnt ) { int i ; for ( i = 0 ; i < prog_cnt ; i ++ ) { struct bpf_program * * prog = progs [ i ] . prog ; const char * name = progs [ i ] . name ; * prog = bpf_object__find_program_by_name ( obj , name ) ; if ( ! * prog ) { pr_warn ( \"failed to find skeleton program '%s'\\n\" , name ) ; return - ESRCH ; } } return 0 ; }",
    "resources/libbpf/src/libbpf.c@bpf_object__open_skeleton": "int bpf_object__open_skeleton ( struct bpf_object_skeleton * s , const struct bpf_object_open_opts * opts ) { DECLARE_LIBBPF_OPTS ( bpf_object_open_opts , skel_opts , . object_name = s -> name , ) ; struct bpf_object * obj ; int err ; /* Attempt to preserve opts->object_name, unless overriden by user\n\t * explicitly. Overwriting object name for skeletons is discouraged,\n\t * as it breaks global data maps, because they contain object name\n\t * prefix as their own map name prefix. When skeleton is generated,\n\t * bpftool is making an assumption that this name will stay the same.\n\t */ if ( opts ) { memcpy ( & skel_opts , opts , sizeof ( * opts ) ) ; if ( ! opts -> object_name ) skel_opts . object_name = s -> name ; } obj = bpf_object__open_mem ( s -> data , s -> data_sz , & skel_opts ) ; err = libbpf_get_error ( obj ) ; if ( err ) { pr_warn ( \"failed to initialize skeleton BPF object '%s': %d\\n\" , s -> name , err ) ; return libbpf_err ( err ) ; } * s -> obj = obj ; err = populate_skeleton_maps ( obj , s -> maps , s -> map_cnt ) ; if ( err ) { pr_warn ( \"failed to populate skeleton maps for '%s': %d\\n\" , s -> name , err ) ; return libbpf_err ( err ) ; } err = populate_skeleton_progs ( obj , s -> progs , s -> prog_cnt ) ; if ( err ) { pr_warn ( \"failed to populate skeleton progs for '%s': %d\\n\" , s -> name , err ) ; return libbpf_err ( err ) ; } return 0 ; }",
    "resources/libbpf/src/libbpf.c@bpf_object__open_subskeleton": "int bpf_object__open_subskeleton ( struct bpf_object_subskeleton * s ) { int err , len , var_idx , i ; const char * var_name ; const struct bpf_map * map ; struct btf * btf ; __u32 map_type_id ; const struct btf_type * map_type , * var_type ; const struct bpf_var_skeleton * var_skel ; struct btf_var_secinfo * var ; if ( ! s -> obj ) return libbpf_err ( - EINVAL ) ; btf = bpf_object__btf ( s -> obj ) ; if ( ! btf ) { pr_warn ( \"subskeletons require BTF at runtime (object %s)\\n\" , bpf_object__name ( s -> obj ) ) ; return libbpf_err ( - errno ) ; } err = populate_skeleton_maps ( s -> obj , s -> maps , s -> map_cnt ) ; if ( err ) { pr_warn ( \"failed to populate subskeleton maps: %d\\n\" , err ) ; return libbpf_err ( err ) ; } err = populate_skeleton_progs ( s -> obj , s -> progs , s -> prog_cnt ) ; if ( err ) { pr_warn ( \"failed to populate subskeleton maps: %d\\n\" , err ) ; return libbpf_err ( err ) ; } for ( var_idx = 0 ; var_idx < s -> var_cnt ; var_idx ++ ) { var_skel = & s -> vars [ var_idx ] ; map = * var_skel -> map ; map_type_id = bpf_map__btf_value_type_id ( map ) ; map_type = btf__type_by_id ( btf , map_type_id ) ; if ( ! btf_is_datasec ( map_type ) ) { pr_warn ( \"type for map '%1$s' is not a datasec: %2$s\" , bpf_map__name ( map ) , __btf_kind_str ( btf_kind ( map_type ) ) ) ; return libbpf_err ( - EINVAL ) ; } len = btf_vlen ( map_type ) ; var = btf_var_secinfos ( map_type ) ; for ( i = 0 ; i < len ; i ++ , var ++ ) { var_type = btf__type_by_id ( btf , var -> type ) ; var_name = btf__name_by_offset ( btf , var_type -> name_off ) ; if ( strcmp ( var_name , var_skel -> name ) == 0 ) { * var_skel -> addr = map -> mmaped + var -> offset ; break ; } } } return 0 ; }",
    "resources/libbpf/src/libbpf.c@bpf_object__destroy_subskeleton": "void bpf_object__destroy_subskeleton ( struct bpf_object_subskeleton * s ) { if ( ! s ) return ; free ( s -> maps ) ; free ( s -> progs ) ; free ( s -> vars ) ; free ( s ) ; }",
    "resources/libbpf/src/libbpf.c@bpf_object__load_skeleton": "int bpf_object__load_skeleton ( struct bpf_object_skeleton * s ) { int i , err ; err = bpf_object__load ( * s -> obj ) ; if ( err ) { pr_warn ( \"failed to load BPF skeleton '%s': %d\\n\" , s -> name , err ) ; return libbpf_err ( err ) ; } for ( i = 0 ; i < s -> map_cnt ; i ++ ) { struct bpf_map * map = * s -> maps [ i ] . map ; size_t mmap_sz = bpf_map_mmap_sz ( map ) ; int prot , map_fd = map -> fd ; void * * mmaped = s -> maps [ i ] . mmaped ; if ( ! mmaped ) continue ; if ( ! ( map -> def . map_flags & BPF_F_MMAPABLE ) ) { * mmaped = NULL ; continue ; } if ( map -> def . type == BPF_MAP_TYPE_ARENA ) { * mmaped = map -> mmaped ; continue ; } if ( map -> def . map_flags & BPF_F_RDONLY_PROG ) prot = PROT_READ ; else prot = PROT_READ | PROT_WRITE ; /* Remap anonymous mmap()-ed \"map initialization image\" as\n\t\t * a BPF map-backed mmap()-ed memory, but preserving the same\n\t\t * memory address. This will cause kernel to change process'\n\t\t * page table to point to a different piece of kernel memory,\n\t\t * but from userspace point of view memory address (and its\n\t\t * contents, being identical at this point) will stay the\n\t\t * same. This mapping will be released by bpf_object__close()\n\t\t * as per normal clean up procedure, so we don't need to worry\n\t\t * about it from skeleton's clean up perspective.\n\t\t */ * mmaped = mmap ( map -> mmaped , mmap_sz , prot , MAP_SHARED | MAP_FIXED , map_fd , 0 ) ; if ( * mmaped == MAP_FAILED ) { err = - errno ; * mmaped = NULL ; pr_warn ( \"failed to re-mmap() map '%s': %d\\n\" , bpf_map__name ( map ) , err ) ; return libbpf_err ( err ) ; } } return 0 ; }",
    "resources/libbpf/src/libbpf.c@bpf_object__attach_skeleton": "int bpf_object__attach_skeleton ( struct bpf_object_skeleton * s ) { int i , err ; for ( i = 0 ; i < s -> prog_cnt ; i ++ ) { struct bpf_program * prog = * s -> progs [ i ] . prog ; struct bpf_link * * link = s -> progs [ i ] . link ; if ( ! prog -> autoload || ! prog -> autoattach ) continue ; /* auto-attaching not supported for this program */ if ( ! prog -> sec_def || ! prog -> sec_def -> prog_attach_fn ) continue ; /* if user already set the link manually, don't attempt auto-attach */ if ( * link ) continue ; err = prog -> sec_def -> prog_attach_fn ( prog , prog -> sec_def -> cookie , link ) ; if ( err ) { pr_warn ( \"prog '%s': failed to auto-attach: %d\\n\" , bpf_program__name ( prog ) , err ) ; return libbpf_err ( err ) ; } /* It's possible that for some SEC() definitions auto-attach\n\t\t * is supported in some cases (e.g., if definition completely\n\t\t * specifies target information), but is not in other cases.\n\t\t * SEC(\"uprobe\") is one such case. If user specified target\n\t\t * binary and function name, such BPF program can be\n\t\t * auto-attached. But if not, it shouldn't trigger skeleton's\n\t\t * attach to fail. It should just be skipped.\n\t\t * attach_fn signals such case with returning 0 (no error) and\n\t\t * setting link to NULL.\n\t\t */ } return 0 ; }",
    "resources/libbpf/src/libbpf.c@bpf_object__detach_skeleton": "void bpf_object__detach_skeleton ( struct bpf_object_skeleton * s ) { int i ; for ( i = 0 ; i < s -> prog_cnt ; i ++ ) { struct bpf_link * * link = s -> progs [ i ] . link ; bpf_link__destroy ( * link ) ; * link = NULL ; } }",
    "resources/libbpf/src/libbpf.c@bpf_object__destroy_skeleton": "void bpf_object__destroy_skeleton ( struct bpf_object_skeleton * s ) { if ( ! s ) return ; if ( s -> progs ) bpf_object__detach_skeleton ( s ) ; if ( s -> obj ) bpf_object__close ( * s -> obj ) ; free ( s -> maps ) ; free ( s -> progs ) ; free ( s ) ; }",
    "resources/libbpf/src/nlattr.c@libbpf_nla_data": "static inline void * libbpf_nla_data ( const struct nlattr * nla ) { return ( void * ) nla + NLA_HDRLEN ; }",
    "resources/libbpf/src/nlattr.c@libbpf_nla_getattr_u8": "static inline uint8_t libbpf_nla_getattr_u8 ( const struct nlattr * nla ) { return * ( uint8_t * ) libbpf_nla_data ( nla ) ; }",
    "resources/libbpf/src/nlattr.c@libbpf_nla_getattr_u16": "static inline uint16_t libbpf_nla_getattr_u16 ( const struct nlattr * nla ) { return * ( uint16_t * ) libbpf_nla_data ( nla ) ; }",
    "resources/libbpf/src/nlattr.c@libbpf_nla_getattr_u32": "static inline uint32_t libbpf_nla_getattr_u32 ( const struct nlattr * nla ) { return * ( uint32_t * ) libbpf_nla_data ( nla ) ; }",
    "resources/libbpf/src/nlattr.c@libbpf_nla_getattr_u64": "static inline uint64_t libbpf_nla_getattr_u64 ( const struct nlattr * nla ) { return * ( uint64_t * ) libbpf_nla_data ( nla ) ; }",
    "resources/libbpf/src/nlattr.c@libbpf_nla_getattr_str": "static inline const char * libbpf_nla_getattr_str ( const struct nlattr * nla ) { return ( const char * ) libbpf_nla_data ( nla ) ; }",
    "resources/libbpf/src/nlattr.c@libbpf_nla_len": "static inline int libbpf_nla_len ( const struct nlattr * nla ) { return nla -> nla_len - NLA_HDRLEN ; }",
    "resources/libbpf/src/nlattr.c@nla_data": "static inline struct nlattr * nla_data ( struct nlattr * nla ) { return ( struct nlattr * ) ( ( void * ) nla + NLA_HDRLEN ) ; }",
    "resources/libbpf/src/nlattr.c@req_tail": "static inline struct nlattr * req_tail ( struct libbpf_nla_req * req ) { return ( struct nlattr * ) ( ( void * ) req + NLMSG_ALIGN ( req -> nh . nlmsg_len ) ) ; }",
    "resources/libbpf/src/nlattr.c@nlattr_add": "static inline int nlattr_add ( struct libbpf_nla_req * req , int type , const void * data , int len ) { struct nlattr * nla ; if ( NLMSG_ALIGN ( req -> nh . nlmsg_len ) + NLA_ALIGN ( NLA_HDRLEN + len ) > sizeof ( * req ) ) return - EMSGSIZE ; if ( ! ! data != ! ! len ) return - EINVAL ; nla = req_tail ( req ) ; nla -> nla_type = type ; nla -> nla_len = NLA_HDRLEN + len ; if ( data ) memcpy ( nla_data ( nla ) , data , len ) ; req -> nh . nlmsg_len = NLMSG_ALIGN ( req -> nh . nlmsg_len ) + NLA_ALIGN ( nla -> nla_len ) ; return 0 ; }",
    "resources/libbpf/src/nlattr.c@nlattr_begin_nested": "static inline struct nlattr * nlattr_begin_nested ( struct libbpf_nla_req * req , int type ) { struct nlattr * tail ; tail = req_tail ( req ) ; if ( nlattr_add ( req , type | NLA_F_NESTED , NULL , 0 ) ) return NULL ; return tail ; }",
    "resources/libbpf/src/nlattr.c@nlattr_end_nested": "static inline void nlattr_end_nested ( struct libbpf_nla_req * req , struct nlattr * tail ) { tail -> nla_len = ( void * ) req_tail ( req ) - ( void * ) tail ; }",
    "resources/libbpf/src/nlattr.c@btf_kind": "static inline __u16 btf_kind ( const struct btf_type * t ) { return BTF_INFO_KIND ( t -> info ) ; }",
    "resources/libbpf/src/nlattr.c@btf_vlen": "static inline __u16 btf_vlen ( const struct btf_type * t ) { return BTF_INFO_VLEN ( t -> info ) ; }",
    "resources/libbpf/src/nlattr.c@btf_kflag": "static inline bool btf_kflag ( const struct btf_type * t ) { return BTF_INFO_KFLAG ( t -> info ) ; }",
    "resources/libbpf/src/nlattr.c@btf_is_void": "static inline bool btf_is_void ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_UNKN ; }",
    "resources/libbpf/src/nlattr.c@btf_is_int": "static inline bool btf_is_int ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_INT ; }",
    "resources/libbpf/src/nlattr.c@btf_is_ptr": "static inline bool btf_is_ptr ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_PTR ; }",
    "resources/libbpf/src/nlattr.c@btf_is_array": "static inline bool btf_is_array ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_ARRAY ; }",
    "resources/libbpf/src/nlattr.c@btf_is_struct": "static inline bool btf_is_struct ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_STRUCT ; }",
    "resources/libbpf/src/nlattr.c@btf_is_union": "static inline bool btf_is_union ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_UNION ; }",
    "resources/libbpf/src/nlattr.c@btf_is_composite": "static inline bool btf_is_composite ( const struct btf_type * t ) { __u16 kind = btf_kind ( t ) ; return kind == BTF_KIND_STRUCT || kind == BTF_KIND_UNION ; }",
    "resources/libbpf/src/nlattr.c@btf_is_enum": "static inline bool btf_is_enum ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_ENUM ; }",
    "resources/libbpf/src/nlattr.c@btf_is_enum64": "static inline bool btf_is_enum64 ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_ENUM64 ; }",
    "resources/libbpf/src/nlattr.c@btf_is_fwd": "static inline bool btf_is_fwd ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_FWD ; }",
    "resources/libbpf/src/nlattr.c@btf_is_typedef": "static inline bool btf_is_typedef ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_TYPEDEF ; }",
    "resources/libbpf/src/nlattr.c@btf_is_volatile": "static inline bool btf_is_volatile ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_VOLATILE ; }",
    "resources/libbpf/src/nlattr.c@btf_is_const": "static inline bool btf_is_const ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_CONST ; }",
    "resources/libbpf/src/nlattr.c@btf_is_restrict": "static inline bool btf_is_restrict ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_RESTRICT ; }",
    "resources/libbpf/src/nlattr.c@btf_is_mod": "static inline bool btf_is_mod ( const struct btf_type * t ) { __u16 kind = btf_kind ( t ) ; return kind == BTF_KIND_VOLATILE || kind == BTF_KIND_CONST || kind == BTF_KIND_RESTRICT || kind == BTF_KIND_TYPE_TAG ; }",
    "resources/libbpf/src/nlattr.c@btf_is_func": "static inline bool btf_is_func ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_FUNC ; }",
    "resources/libbpf/src/nlattr.c@btf_is_func_proto": "static inline bool btf_is_func_proto ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_FUNC_PROTO ; }",
    "resources/libbpf/src/nlattr.c@btf_is_var": "static inline bool btf_is_var ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_VAR ; }",
    "resources/libbpf/src/nlattr.c@btf_is_datasec": "static inline bool btf_is_datasec ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_DATASEC ; }",
    "resources/libbpf/src/nlattr.c@btf_is_float": "static inline bool btf_is_float ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_FLOAT ; }",
    "resources/libbpf/src/nlattr.c@btf_is_decl_tag": "static inline bool btf_is_decl_tag ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_DECL_TAG ; }",
    "resources/libbpf/src/nlattr.c@btf_is_type_tag": "static inline bool btf_is_type_tag ( const struct btf_type * t ) { return btf_kind ( t ) == BTF_KIND_TYPE_TAG ; }",
    "resources/libbpf/src/nlattr.c@btf_is_any_enum": "static inline bool btf_is_any_enum ( const struct btf_type * t ) { return btf_is_enum ( t ) || btf_is_enum64 ( t ) ; }",
    "resources/libbpf/src/nlattr.c@btf_kind_core_compat": "static inline bool btf_kind_core_compat ( const struct btf_type * t1 , const struct btf_type * t2 ) { return btf_kind ( t1 ) == btf_kind ( t2 ) || ( btf_is_any_enum ( t1 ) && btf_is_any_enum ( t2 ) ) ; }",
    "resources/libbpf/src/nlattr.c@btf_int_encoding": "static inline __u8 btf_int_encoding ( const struct btf_type * t ) { return BTF_INT_ENCODING ( * ( __u32 * ) ( t + 1 ) ) ; }",
    "resources/libbpf/src/nlattr.c@btf_int_offset": "static inline __u8 btf_int_offset ( const struct btf_type * t ) { return BTF_INT_OFFSET ( * ( __u32 * ) ( t + 1 ) ) ; }",
    "resources/libbpf/src/nlattr.c@btf_int_bits": "static inline __u8 btf_int_bits ( const struct btf_type * t ) { return BTF_INT_BITS ( * ( __u32 * ) ( t + 1 ) ) ; }",
    "resources/libbpf/src/nlattr.c@btf_array": "static inline struct btf_array * btf_array ( const struct btf_type * t ) { return ( struct btf_array * ) ( t + 1 ) ; }",
    "resources/libbpf/src/nlattr.c@btf_enum": "static inline struct btf_enum * btf_enum ( const struct btf_type * t ) { return ( struct btf_enum * ) ( t + 1 ) ; }",
    "resources/libbpf/src/nlattr.c@btf_enum64": "static inline struct btf_enum64 * btf_enum64 ( const struct btf_type * t ) { return ( struct btf_enum64 * ) ( t + 1 ) ; }",
    "resources/libbpf/src/nlattr.c@btf_enum64_value": "static inline __u64 btf_enum64_value ( const struct btf_enum64 * e ) { /* struct btf_enum64 is introduced in Linux 6.0, which is very\n\t * bleeding-edge. Here we are avoiding relying on struct btf_enum64\n\t * definition coming from kernel UAPI headers to support wider range\n\t * of system-wide kernel headers.\n\t *\n\t * Given this header can be also included from C++ applications, that\n\t * further restricts C tricks we can use (like using compatible\n\t * anonymous struct). So just treat struct btf_enum64 as\n\t * a three-element array of u32 and access second (lo32) and third\n\t * (hi32) elements directly.\n\t *\n\t * For reference, here is a struct btf_enum64 definition:\n\t *\n\t * const struct btf_enum64 {\n\t *\t__u32\tname_off;\n\t *\t__u32\tval_lo32;\n\t *\t__u32\tval_hi32;\n\t * };\n\t */ const __u32 * e64 = ( const __u32 * ) e ; return ( ( __u64 ) e64 [ 2 ] << 32 ) | e64 [ 1 ] ; }",
    "resources/libbpf/src/nlattr.c@btf_members": "static inline struct btf_member * btf_members ( const struct btf_type * t ) { return ( struct btf_member * ) ( t + 1 ) ; }",
    "resources/libbpf/src/nlattr.c@btf_member_bit_offset": "static inline __u32 btf_member_bit_offset ( const struct btf_type * t , __u32 member_idx ) { const struct btf_member * m = btf_members ( t ) + member_idx ; bool kflag = btf_kflag ( t ) ; return kflag ? BTF_MEMBER_BIT_OFFSET ( m -> offset ) : m -> offset ; }",
    "resources/libbpf/src/nlattr.c@btf_member_bitfield_size": "static inline __u32 btf_member_bitfield_size ( const struct btf_type * t , __u32 member_idx ) { const struct btf_member * m = btf_members ( t ) + member_idx ; bool kflag = btf_kflag ( t ) ; return kflag ? BTF_MEMBER_BITFIELD_SIZE ( m -> offset ) : 0 ; }",
    "resources/libbpf/src/nlattr.c@btf_params": "static inline struct btf_param * btf_params ( const struct btf_type * t ) { return ( struct btf_param * ) ( t + 1 ) ; }",
    "resources/libbpf/src/nlattr.c@btf_var": "static inline struct btf_var * btf_var ( const struct btf_type * t ) { return ( struct btf_var * ) ( t + 1 ) ; }",
    "resources/libbpf/src/nlattr.c@btf_var_secinfos": "static inline struct btf_var_secinfo * btf_var_secinfos ( const struct btf_type * t ) { return ( struct btf_var_secinfo * ) ( t + 1 ) ; }",
    "resources/libbpf/src/nlattr.c@btf_decl_tag": "static inline struct btf_decl_tag * btf_decl_tag ( const struct btf_type * t ) { return ( struct btf_decl_tag * ) ( t + 1 ) ; }",
    "resources/libbpf/src/nlattr.c@str_has_sfx": "static inline bool str_has_sfx ( const char * str , const char * sfx ) { size_t str_len = strlen ( str ) ; size_t sfx_len = strlen ( sfx ) ; if ( sfx_len > str_len ) return false ; return strcmp ( str + str_len - sfx_len , sfx ) == 0 ; }",
    "resources/libbpf/src/nlattr.c@libbpf_reallocarray": "static inline void * libbpf_reallocarray ( void * ptr , size_t nmemb , size_t size ) { size_t total ; # if __has_builtin ( __builtin_mul_overflow ) if ( unlikely ( __builtin_mul_overflow ( nmemb , size , & total ) ) ) return NULL ; # else if ( size == 0 || nmemb > ULONG_MAX / size ) return NULL ; total = nmemb * size ; # endif return realloc ( ptr , total ) ; }",
    "resources/libbpf/src/nlattr.c@libbpf_strlcpy": "static inline void libbpf_strlcpy ( char * dst , const char * src , size_t sz ) { size_t i ; if ( sz == 0 ) return ; sz -- ; for ( i = 0 ; i < sz && src [ i ] ; i ++ ) dst [ i ] = src [ i ] ; dst [ i ] = '\\0' ; }",
    "resources/libbpf/src/nlattr.c@btf_func_linkage": "static inline enum btf_func_linkage btf_func_linkage ( const struct btf_type * t ) { return ( enum btf_func_linkage ) ( int ) btf_vlen ( t ) ; }",
    "resources/libbpf/src/nlattr.c@btf_type_info": "static inline __u32 btf_type_info ( int kind , int vlen , int kflag ) { return ( kflag << 31 ) | ( kind << 24 ) | vlen ; }",
    "resources/libbpf/src/nlattr.c@libbpf_is_mem_zeroed": "static inline bool libbpf_is_mem_zeroed ( const char * p , ssize_t len ) { while ( len > 0 ) { if ( * p ) return false ; p ++ ; len -- ; } return true ; }",
    "resources/libbpf/src/nlattr.c@libbpf_validate_opts": "static inline bool libbpf_validate_opts ( const char * opts , size_t opts_sz , size_t user_sz , const char * type_name ) { if ( user_sz < sizeof ( size_t ) ) { pr_warn ( \"%s size (%zu) is too small\\n\" , type_name , user_sz ) ; return false ; } if ( ! libbpf_is_mem_zeroed ( opts + opts_sz , ( ssize_t ) user_sz - opts_sz ) ) { pr_warn ( \"%s has non-zero extra bytes\\n\" , type_name ) ; return false ; } return true ; }",
    "resources/libbpf/src/nlattr.c@libbpf_err": "static inline int libbpf_err ( int ret ) { if ( ret < 0 ) errno = - ret ; return ret ; }",
    "resources/libbpf/src/nlattr.c@libbpf_err_errno": "static inline int libbpf_err_errno ( int ret ) { /* errno is already assumed to be set on error */ return ret < 0 ? - errno : ret ; }",
    "resources/libbpf/src/nlattr.c@libbpf_err_ptr": "static inline void * libbpf_err_ptr ( int err ) { /* set errno on error, this doesn't break anything */ errno = - err ; return NULL ; }",
    "resources/libbpf/src/nlattr.c@libbpf_ptr": "static inline void * libbpf_ptr ( void * ret ) { /* set errno on error, this doesn't break anything */ if ( IS_ERR ( ret ) ) errno = - PTR_ERR ( ret ) ; return IS_ERR ( ret ) ? NULL : ret ; }",
    "resources/libbpf/src/nlattr.c@str_is_empty": "static inline bool str_is_empty ( const char * s ) { return ! s || ! s [ 0 ] ; }",
    "resources/libbpf/src/nlattr.c@is_ldimm64_insn": "static inline bool is_ldimm64_insn ( struct bpf_insn * insn ) { return insn -> code == ( BPF_LD | BPF_IMM | BPF_DW ) ; }",
    "resources/libbpf/src/nlattr.c@dup_good_fd": "static inline int dup_good_fd ( int fd ) { if ( fd < 0 ) return fd ; return fcntl ( fd , F_DUPFD_CLOEXEC , 3 ) ; }",
    "resources/libbpf/src/nlattr.c@ensure_good_fd": "static inline int ensure_good_fd ( int fd ) { int old_fd = fd , saved_errno ; if ( fd < 0 ) return fd ; if ( fd < 3 ) { fd = dup_good_fd ( fd ) ; saved_errno = errno ; close ( old_fd ) ; errno = saved_errno ; if ( fd < 0 ) { pr_warn ( \"failed to dup FD %d to FD > 2: %d\\n\" , old_fd , - saved_errno ) ; errno = saved_errno ; } } return fd ; }",
    "resources/libbpf/src/nlattr.c@sys_dup2": "static inline int sys_dup2 ( int oldfd , int newfd ) { # ifdef __NR_dup2 return syscall ( __NR_dup2 , oldfd , newfd ) ; # else return syscall ( __NR_dup3 , oldfd , newfd , 0 ) ; # endif }",
    "resources/libbpf/src/nlattr.c@reuse_fd": "static inline int reuse_fd ( int fixed_fd , int tmp_fd ) { int err ; err = sys_dup2 ( tmp_fd , fixed_fd ) ; err = err < 0 ? - errno : 0 ; close ( tmp_fd ) ; /* clean up temporary FD */ return err ; }",
    "resources/libbpf/src/nlattr.c@is_pow_of_2": "static inline bool is_pow_of_2 ( size_t x ) { return x && ( x & ( x - 1 ) ) == 0 ; }",
    "resources/libbpf/src/nlattr.c@nla_next": "static struct nlattr * nla_next ( const struct nlattr * nla , int * remaining ) { int totlen = NLA_ALIGN ( nla -> nla_len ) ; * remaining -= totlen ; return ( struct nlattr * ) ( ( void * ) nla + totlen ) ; }",
    "resources/libbpf/src/nlattr.c@nla_ok": "static int nla_ok ( const struct nlattr * nla , int remaining ) { return remaining >= ( int ) sizeof ( * nla ) && nla -> nla_len >= sizeof ( * nla ) && nla -> nla_len <= remaining ; }",
    "resources/libbpf/src/nlattr.c@nla_type": "static int nla_type ( const struct nlattr * nla ) { return nla -> nla_type & NLA_TYPE_MASK ; }",
    "resources/libbpf/src/nlattr.c@validate_nla": "static int validate_nla ( struct nlattr * nla , int maxtype , struct libbpf_nla_policy * policy ) { struct libbpf_nla_policy * pt ; unsigned int minlen = 0 ; int type = nla_type ( nla ) ; if ( type < 0 || type > maxtype ) return 0 ; pt = & policy [ type ] ; if ( pt -> type > LIBBPF_NLA_TYPE_MAX ) return 0 ; if ( pt -> minlen ) minlen = pt -> minlen ; else if ( pt -> type != LIBBPF_NLA_UNSPEC ) minlen = nla_attr_minlen [ pt -> type ] ; if ( libbpf_nla_len ( nla ) < minlen ) return - 1 ; if ( pt -> maxlen && libbpf_nla_len ( nla ) > pt -> maxlen ) return - 1 ; if ( pt -> type == LIBBPF_NLA_STRING ) { char * data = libbpf_nla_data ( nla ) ; if ( data [ libbpf_nla_len ( nla ) - 1 ] != '\\0' ) return - 1 ; } return 0 ; }",
    "resources/libbpf/src/nlattr.c@nlmsg_len": "static inline int nlmsg_len ( const struct nlmsghdr * nlh ) { return nlh -> nlmsg_len - NLMSG_HDRLEN ; }",
    "resources/libbpf/src/nlattr.c@libbpf_nla_parse": "int libbpf_nla_parse ( struct nlattr * tb [ ] , int maxtype , struct nlattr * head , int len , struct libbpf_nla_policy * policy ) { struct nlattr * nla ; int rem , err ; memset ( tb , 0 , sizeof ( struct nlattr * ) * ( maxtype + 1 ) ) ; libbpf_nla_for_each_attr ( nla , head , len , rem ) { int type = nla_type ( nla ) ; if ( type > maxtype ) continue ; if ( policy ) { err = validate_nla ( nla , maxtype , policy ) ; if ( err < 0 ) goto errout ; } if ( tb [ type ] ) pr_warn ( \"Attribute of type %#x found multiple times in message, \" \"previous attribute is being ignored.\\n\" , type ) ; tb [ type ] = nla ; } err = 0 ; errout : return err ; }",
    "resources/libbpf/src/nlattr.c@libbpf_nla_parse_nested": "int libbpf_nla_parse_nested ( struct nlattr * tb [ ] , int maxtype , struct nlattr * nla , struct libbpf_nla_policy * policy ) { return libbpf_nla_parse ( tb , maxtype , libbpf_nla_data ( nla ) , libbpf_nla_len ( nla ) , policy ) ; }",
    "resources/libbpf/src/nlattr.c@libbpf_nla_dump_errormsg": "int libbpf_nla_dump_errormsg ( struct nlmsghdr * nlh ) { struct libbpf_nla_policy extack_policy [ NLMSGERR_ATTR_MAX + 1 ] = { [ NLMSGERR_ATTR_MSG ] = { . type = LIBBPF_NLA_STRING } , [ NLMSGERR_ATTR_OFFS ] = { . type = LIBBPF_NLA_U32 } , } ; struct nlattr * tb [ NLMSGERR_ATTR_MAX + 1 ] , * attr ; struct nlmsgerr * err ; char * errmsg = NULL ; int hlen , alen ; /* no TLVs, nothing to do here */ if ( ! ( nlh -> nlmsg_flags & NLM_F_ACK_TLVS ) ) return 0 ; err = ( struct nlmsgerr * ) NLMSG_DATA ( nlh ) ; hlen = sizeof ( * err ) ; /* if NLM_F_CAPPED is set then the inner err msg was capped */ if ( ! ( nlh -> nlmsg_flags & NLM_F_CAPPED ) ) hlen += nlmsg_len ( & err -> msg ) ; attr = ( struct nlattr * ) ( ( void * ) err + hlen ) ; alen = ( void * ) nlh + nlh -> nlmsg_len - ( void * ) attr ; if ( libbpf_nla_parse ( tb , NLMSGERR_ATTR_MAX , attr , alen , extack_policy ) != 0 ) { pr_warn ( \"Failed to parse extended error attributes\\n\" ) ; return 0 ; } if ( tb [ NLMSGERR_ATTR_MSG ] ) errmsg = ( char * ) libbpf_nla_data ( tb [ NLMSGERR_ATTR_MSG ] ) ; pr_warn ( \"Kernel error message: %s\\n\" , errmsg ) ; return 0 ; }"
}